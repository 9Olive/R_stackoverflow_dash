QuestionId,Title,Topic_k2,Topic_k3,Topic_k4,Body_Que,Body_Ans
7311996,Symbolic derivatives and simplification in R,2,2,4,"<p>In R, I have the following expression for which I would like to take sucessive derivatives with respect to <code>s</code> (<code>theta</code> and <code>nu</code> are nothing but unspecified parameters):</p>

<pre><code>expr &lt;- expression(exp((nu / (theta * (1 - nu))) *
  (1 - (1 + theta * s / nu)^(1 - nu))))
</code></pre>

<p>To do this, I recursively use the <a href=""http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/base/html/deriv.html"">D()</a> function that computes derivatives of simple expressions, symbolically. </p>

<p>But that function does not perform any simplification, i.e., it does not reduce the result into a simpler form.</p>

<p>If you try to take the 10th derivative, say, you will see that the result is very awesome and it requires a lot of computing time. At the limit, it is practically impossible, at least on my computer, to compute the 15th derivative.</p>

<p>Hence, I believe it is worth to try to simplify the <code>n-1</code>th derivative before computing the <code>n</code>th derivative. </p>

<p>I think it is possible to simplify expressions in R thanks to the <a href=""http://cran.r-project.org/web/packages/Ryacas/Ryacas.pdf"">Ryacas</a> package. </p>

<p>However, my tests are not conclusive...</p>

<p>Does anyone of you has some experience with such a problem?
Does anyone could give me some advice?</p>

<p>Thank you in advance!</p>
","<p>For what it's worth, this seems pretty in easy in <a href=""http://sagemath.org"" rel=""nofollow"">Sage</a> .  I haven't ever done more than putter with it, but I could do this by doing the derivative computation in Sage and then cutting &amp; pasting the resulting expression into R (<strong>ugly</strong> but seems to get this particular job done). (Scroll down to the bottom of the code block for a few lines of R code ...) Sage also has an <a href=""http://wiki.sagemath.org/R"" rel=""nofollow"">R interface</a>, although I haven't tried it out.</p>

<p>In Sage (see <a href=""http://www.sagenb.org/home/pub/3121"" rel=""nofollow"">http://www.sagenb.org/home/pub/3121</a>) [one could make the expression a little more compact/easier to cut and paste, although even less readable, by using one-letter variable names (i.e. <code>n</code> and <code>t</code> instead of <code>nu</code> and <code>theta</code>)]:</p>

<pre><code>nu = var('nu'); theta=var('theta')
s= var('s'); myexpr = exp((nu / (theta * (1 - nu))) *
  (1 - (1 + theta * s / nu)^(1 - nu)))
simplify(derivative(myexpr,s,15))
##
</code></pre>

<p>In R, cutting and pasting from Sage:</p>

<pre><code>Rderivexpr &lt;- expression(
-(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*
(nu + 7)*(nu +
8)*(nu + 9)*(nu + 10)*(nu + 11)*(nu + 12)*(nu + 13)*theta^14*(s*theta/nu
+ 1)^(-nu - 14)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^13 - 6435*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu + 4)^2*(nu
+ 5)^2*(nu + 6)*theta^13*(s*theta/nu + 1)^(-2*nu - 13)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^11 - 5005*(nu + 1)^2*(nu +
2)^2*(nu + 3)^2*(nu + 4)^2*(nu + 5)*(nu + 6)*(nu +
7)*theta^13*(s*theta/nu + 1)^(-2*nu - 13)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^11 - 3003*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*theta^13*(s*theta/nu +
1)^(-2*nu - 13)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^11 - 1365*(nu + 1)^2*(nu + 2)^2*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*theta^13*(s*theta/nu + 1)^(-2*nu
- 13)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^11 -
455*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu
+ 8)*(nu + 9)*(nu + 10)*theta^13*(s*theta/nu + 1)^(-2*nu -
13)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^11 -
15*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu +
8)*(nu + 9)*(nu + 10)*(nu + 11)*(nu + 12)*theta^13*(s*theta/nu +
1)^(-2*nu - 13)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^12 - 105*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu
+ 6)*(nu + 7)*(nu + 8)*(nu + 9)*(nu + 10)*(nu + 11)*theta^13*(s*theta/nu
+ 1)^(-2*nu - 13)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^11 - 126126*(nu + 1)^3*(nu + 2)^3*(nu +
3)^3*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^9 - 630630*(nu + 1)^3*(nu + 2)^3*(nu +
3)^2*(nu + 4)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 - 225225*(nu + 1)^3*(nu +
2)^3*(nu + 3)*(nu + 4)*(nu + 5)*theta^12*(s*theta/nu + 1)^(-3*nu -
12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 -
210210*(nu + 1)^3*(nu + 2)^2*(nu + 3)^2*(nu + 4)^2*theta^12*(s*theta/nu
+ 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^9 - 360360*(nu + 1)^3*(nu + 2)^2*(nu + 3)^2*(nu + 4)*(nu +
5)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^9 - 225225*(nu + 1)^3*(nu + 2)^2*(nu +
3)*(nu + 4)*(nu + 5)*(nu + 6)*theta^12*(s*theta/nu + 1)^(-3*nu -
12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 -
50050*(nu + 1)^3*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^9 - 25740*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)^2*(nu + 5)^2*theta^12*(s*theta/nu + 1)^(-3*nu -
12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^10 -
45045*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu + 4)^2*(nu + 5)*(nu +
6)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^10 - 180180*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)^2*(nu + 5)*theta^12*(s*theta/nu + 1)^(-3*nu -
12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 -
30030*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^10 - 135135*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)*(nu + 5)*(nu + 6)*theta^12*(s*theta/nu + 1)^(-3*nu -
12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 -
15015*(nu + 1)^2*(nu + 2)^2*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*(nu + 8)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^10 - 75075*(nu + 1)^2*(nu +
2)^2*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*theta^12*(s*theta/nu +
1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^9 - 5460*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*theta^12*(s*theta/nu + 1)^(-3*nu
- 12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^10 -
30030*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*(nu + 8)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 - 105*(nu + 1)*(nu + 2)*(nu
+ 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*(nu + 10)*(nu
+ 11)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^11 - 1365*(nu + 1)*(nu + 2)*(nu + 3)*(nu
+ 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*(nu +
10)*theta^12*(s*theta/nu + 1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^10 - 4095*(nu + 1)*(nu + 2)*(nu + 3)*(nu
+ 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*theta^12*(s*theta/nu +
1)^(-3*nu - 12)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^9 - 2627625*(nu + 1)^4*(nu + 2)^3*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^7 - 6306300*(nu + 1)^4*(nu + 2)^2*(nu +
3)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^7 - 1401400*(nu + 1)^4*(nu + 2)*(nu +
3)*(nu + 4)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 - 1891890*(nu + 1)^3*(nu +
2)^3*(nu + 3)^2*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 - 1576575*(nu + 1)^3*(nu +
2)^3*(nu + 3)*(nu + 4)*theta^11*(s*theta/nu + 1)^(-4*nu -
11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 -
4729725*(nu + 1)^3*(nu + 2)^3*(nu + 3)*theta^11*(s*theta/nu + 1)^(-4*nu
- 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
2522520*(nu + 1)^3*(nu + 2)^2*(nu + 3)^2*(nu + 4)*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^8 - 3783780*(nu + 1)^3*(nu + 2)^2*(nu +
3)^2*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^7 - 1801800*(nu + 1)^3*(nu + 2)^2*(nu +
3)*(nu + 4)*(nu + 5)*theta^11*(s*theta/nu + 1)^(-4*nu -
11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 -
6306300*(nu + 1)^3*(nu + 2)^2*(nu + 3)*(nu + 4)*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^7 - 450450*(nu + 1)^3*(nu + 2)*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 - 1801800*(nu + 1)^3*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*theta^11*(s*theta/nu + 1)^(-4*nu -
11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
180180*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu + 4)^2*(nu +
5)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^9 - 630630*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)^2*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 - 135135*(nu + 1)^2*(nu +
2)^2*(nu + 3)^2*(nu + 4)*(nu + 5)*(nu + 6)*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^9 - 1081080*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu + 4)*(nu
+ 5)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^8 - 1891890*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 - 75075*(nu + 1)^2*(nu +
2)^2*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^9 - 675675*(nu + 1)^2*(nu + 2)^2*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 - 1351350*(nu + 1)^2*(nu +
2)^2*(nu + 3)*(nu + 4)*(nu + 5)*theta^11*(s*theta/nu + 1)^(-4*nu -
11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
30030*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*(nu + 8)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 - 300300*(nu + 1)^2*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^8 - 675675*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 - 455*(nu + 1)*(nu + 2)*(nu
+ 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*(nu +
10)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^10 - 8190*(nu + 1)*(nu + 2)*(nu + 3)*(nu
+ 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*(nu + 9)*theta^11*(s*theta/nu +
1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^9 - 45045*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu
+ 6)*(nu + 7)*(nu + 8)*theta^11*(s*theta/nu + 1)^(-4*nu -
11)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 -
75075*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*theta^11*(s*theta/nu + 1)^(-4*nu - 11)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^7 - 1401400*(nu + 1)^5*theta^10*(s*theta/nu
+ 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^5 - 15765750*(nu + 1)^4*(nu + 2)^2*theta^10*(s*theta/nu +
1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^6 - 8408400*(nu + 1)^4*(nu + 2)*(nu +
3)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^6 - 21021000*(nu + 1)^4*(nu +
2)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^5 - 4729725*(nu + 1)^3*(nu + 2)^3*(nu +
3)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^7 - 7882875*(nu + 1)^3*(nu +
2)^3*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^6 - 3783780*(nu + 1)^3*(nu + 2)^2*(nu +
3)^2*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^7 - 6306300*(nu + 1)^3*(nu + 2)^2*(nu +
3)*(nu + 4)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 - 37837800*(nu + 1)^3*(nu +
2)^2*(nu + 3)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 - 23648625*(nu + 1)^3*(nu +
2)^2*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^5 - 1801800*(nu + 1)^3*(nu + 2)*(nu +
3)*(nu + 4)*(nu + 5)*theta^10*(s*theta/nu + 1)^(-5*nu -
10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
12612600*(nu + 1)^3*(nu + 2)*(nu + 3)*(nu + 4)*theta^10*(s*theta/nu +
1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^6 - 18918900*(nu + 1)^3*(nu + 2)*(nu +
3)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^5 - 210210*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*(nu + 4)^2*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 - 360360*(nu + 1)^2*(nu +
2)^2*(nu + 3)^2*(nu + 4)*(nu + 5)*theta^10*(s*theta/nu + 1)^(-5*nu -
10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 -
3783780*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu + 4)*theta^10*(s*theta/nu +
1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^7 - 5675670*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^6 - 225225*(nu + 1)^2*(nu + 2)^2*(nu +
3)*(nu + 4)*(nu + 5)*(nu + 6)*theta^10*(s*theta/nu + 1)^(-5*nu -
10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 -
2702700*(nu + 1)^2*(nu + 2)^2*(nu + 3)*(nu + 4)*(nu +
5)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^7 - 9459450*(nu + 1)^2*(nu + 2)^2*(nu +
3)*(nu + 4)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 - 9459450*(nu + 1)^2*(nu +
2)^2*(nu + 3)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 - 100100*(nu + 1)^2*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*theta^10*(s*theta/nu +
1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^8 - 1351350*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 - 5405400*(nu + 1)^2*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*theta^10*(s*theta/nu + 1)^(-5*nu -
10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 -
6306300*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*theta^10*(s*theta/nu +
1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^5 - 1365*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu
+ 6)*(nu + 7)*(nu + 8)*(nu + 9)*theta^10*(s*theta/nu + 1)^(-5*nu -
10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^9 -
30030*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu
+ 8)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^8 - 225225*(nu + 1)*(nu + 2)*(nu +
3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*theta^10*(s*theta/nu + 1)^(-5*nu
- 10)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
675675*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu +
6)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^6 - 675675*(nu + 1)*(nu + 2)*(nu + 3)*(nu +
4)*(nu + 5)*theta^10*(s*theta/nu + 1)^(-5*nu - 10)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 - 21021000*(nu + 1)^4*(nu +
2)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^5 - 21021000*(nu + 1)^4*theta^9*(s*theta/nu +
1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^4 - 2627625*(nu + 1)^3*(nu + 2)^3*theta^9*(s*theta/nu +
1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^6 - 12612600*(nu + 1)^3*(nu + 2)^2*(nu +
3)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^6 - 47297250*(nu + 1)^3*(nu +
2)^2*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^5 - 4204200*(nu + 1)^3*(nu + 2)*(nu +
3)*(nu + 4)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 - 37837800*(nu + 1)^3*(nu +
2)*(nu + 3)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 - 94594500*(nu + 1)^3*(nu +
2)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^4 - 21021000*(nu + 1)^3*theta^9*(s*theta/nu +

1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^3 - 630630*(nu + 1)^2*(nu + 2)^2*(nu + 3)^2*(nu +
4)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^7 - 3783780*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^6 - 450450*(nu + 1)^2*(nu + 2)^2*(nu +
3)*(nu + 4)*(nu + 5)*theta^9*(s*theta/nu + 1)^(-6*nu -
9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
6306300*(nu + 1)^2*(nu + 2)^2*(nu + 3)*(nu + 4)*theta^9*(s*theta/nu +
1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^6 - 28378350*(nu + 1)^2*(nu + 2)^2*(nu +
3)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^5 - 23648625*(nu + 1)^2*(nu +
2)^2*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^4 - 225225*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu
+ 4)*(nu + 5)*(nu + 6)*theta^9*(s*theta/nu + 1)^(-6*nu -
9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^7 -
3603600*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*(nu +
5)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^6 - 18918900*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu
+ 4)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^5 - 37837800*(nu + 1)^2*(nu + 2)*(nu +
3)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^4 - 23648625*(nu + 1)^2*(nu +
2)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^3 - 3003*(nu + 1)*(nu + 2)*(nu + 3)*(nu +
4)*(nu + 5)*(nu + 6)*(nu + 7)*(nu + 8)*theta^9*(s*theta/nu + 1)^(-6*nu -
9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^8 -
75075*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu +
7)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^7 - 675675*(nu + 1)*(nu + 2)*(nu + 3)*(nu +
4)*(nu + 5)*(nu + 6)*theta^9*(s*theta/nu + 1)^(-6*nu -
9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 -
2702700*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*theta^9*(s*theta/nu
+ 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^5 - 4729725*(nu + 1)*(nu + 2)*(nu + 3)*(nu +
4)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^4 - 2837835*(nu + 1)*(nu + 2)*(nu +
3)*theta^9*(s*theta/nu + 1)^(-6*nu - 9)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^3 - 7007000*(nu + 1)^4*theta^8*(s*theta/nu +
1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^4 - 7882875*(nu + 1)^3*(nu + 2)^2*theta^8*(s*theta/nu +
1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^5 - 6306300*(nu + 1)^3*(nu + 2)*(nu +
3)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^5 - 63063000*(nu + 1)^3*(nu +
2)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^4 - 63063000*(nu + 1)^3*theta^8*(s*theta/nu +
1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^3 - 378378*(nu + 1)^2*(nu + 2)^2*(nu +
3)^2*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^6 - 630630*(nu + 1)^2*(nu + 2)^2*(nu +
3)*(nu + 4)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 - 9459450*(nu + 1)^2*(nu +
2)^2*(nu + 3)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 - 23648625*(nu + 1)^2*(nu +
2)^2*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^4 - 360360*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu
+ 4)*(nu + 5)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 - 6306300*(nu + 1)^2*(nu +
2)*(nu + 3)*(nu + 4)*theta^8*(s*theta/nu + 1)^(-7*nu -
8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 -
37837800*(nu + 1)^2*(nu + 2)*(nu + 3)*theta^8*(s*theta/nu + 1)^(-7*nu -
8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^4 -
94594500*(nu + 1)^2*(nu + 2)*theta^8*(s*theta/nu + 1)^(-7*nu -
8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^3 -
47297250*(nu + 1)^2*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu
+ 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^2 - 5005*(nu + 1)*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu + 6)*(nu + 7)*theta^8*(s*theta/nu +
1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^7 - 135135*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu +
5)*(nu + 6)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^6 - 1351350*(nu + 1)*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*theta^8*(s*theta/nu + 1)^(-7*nu -
8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 -
6306300*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*theta^8*(s*theta/nu +
1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^4 - 14189175*(nu + 1)*(nu + 2)*(nu +
3)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^3 - 14189175*(nu + 1)*(nu +
2)*theta^8*(s*theta/nu + 1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^2 - 4729725*(nu + 1)*theta^8*(s*theta/nu +
1)^(-7*nu - 8)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu - 6306300*(nu + 1)^3*(nu + 2)*theta^7*(s*theta/nu +
1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^4 - 21021000*(nu + 1)^3*theta^7*(s*theta/nu + 1)^(-8*nu -
7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^3 -
630630*(nu + 1)^2*(nu + 2)^2*(nu + 3)*theta^7*(s*theta/nu + 1)^(-8*nu -
7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 -
4729725*(nu + 1)^2*(nu + 2)^2*theta^7*(s*theta/nu + 1)^(-8*nu -
7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^4 -
420420*(nu + 1)^2*(nu + 2)*(nu + 3)*(nu + 4)*theta^7*(s*theta/nu +
1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^5 - 7567560*(nu + 1)^2*(nu + 2)*(nu +
3)*theta^7*(s*theta/nu + 1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^4 - 47297250*(nu + 1)^2*(nu +
2)*theta^7*(s*theta/nu + 1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta))/nu^3 - 63063000*(nu + 1)^2*theta^7*(s*theta/nu +
1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^2 - 6435*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*(nu + 5)*(nu
+ 6)*theta^7*(s*theta/nu + 1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^6 - 180180*(nu + 1)*(nu + 2)*(nu + 3)*(nu +
4)*(nu + 5)*theta^7*(s*theta/nu + 1)^(-8*nu - 7)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 - 1891890*(nu + 1)*(nu +
2)*(nu + 3)*(nu + 4)*theta^7*(s*theta/nu + 1)^(-8*nu -
7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^4 -
9459450*(nu + 1)*(nu + 2)*(nu + 3)*theta^7*(s*theta/nu + 1)^(-8*nu -
7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^3 -
23648625*(nu + 1)*(nu + 2)*theta^7*(s*theta/nu + 1)^(-8*nu -
7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^2 -
28378350*(nu + 1)*theta^7*(s*theta/nu + 1)^(-8*nu - 7)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu - 2027025*theta^7*(s*theta/nu
+ 1)^(-8*nu - 7)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta)) - 1401400*(nu + 1)^3*theta^6*(s*theta/nu + 1)^(-9*nu -
6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^3 -
225225*(nu + 1)^2*(nu + 2)^2*theta^6*(s*theta/nu + 1)^(-9*nu -
6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^4 -
360360*(nu + 1)^2*(nu + 2)*(nu + 3)*theta^6*(s*theta/nu + 1)^(-9*nu -
6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^4 -
6306300*(nu + 1)^2*(nu + 2)*theta^6*(s*theta/nu + 1)^(-9*nu -
6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^3 -
18918900*(nu + 1)^2*theta^6*(s*theta/nu + 1)^(-9*nu - 6)*e^(((s*theta/nu
+ 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^2 - 6435*(nu + 1)*(nu +
2)*(nu + 3)*(nu + 4)*(nu + 5)*theta^6*(s*theta/nu + 1)^(-9*nu -
6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^5 -
180180*(nu + 1)*(nu + 2)*(nu + 3)*(nu + 4)*theta^6*(s*theta/nu +
1)^(-9*nu - 6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^4 - 1891890*(nu + 1)*(nu + 2)*(nu + 3)*theta^6*(s*theta/nu
+ 1)^(-9*nu - 6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^3 - 9459450*(nu + 1)*(nu + 2)*theta^6*(s*theta/nu +
1)^(-9*nu - 6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^2 - 23648625*(nu + 1)*theta^6*(s*theta/nu + 1)^(-9*nu -
6)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu -
4729725*theta^6*(s*theta/nu + 1)^(-9*nu - 6)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta)) - 225225*(nu + 1)^2*(nu +
2)*theta^5*(s*theta/nu + 1)^(-10*nu - 5)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^3 - 1801800*(nu + 1)^2*theta^5*(s*theta/nu
+ 1)^(-10*nu - 5)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu^2 - 5005*(nu + 1)*(nu + 2)*(nu + 3)*(nu +
4)*theta^5*(s*theta/nu + 1)^(-10*nu - 5)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^4 - 135135*(nu + 1)*(nu + 2)*(nu +
3)*theta^5*(s*theta/nu + 1)^(-10*nu - 5)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^3 - 1351350*(nu + 1)*(nu +
2)*theta^5*(s*theta/nu + 1)^(-10*nu - 5)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^2 - 6306300*(nu + 1)*theta^5*(s*theta/nu +
1)^(-10*nu - 5)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu - 2837835*theta^5*(s*theta/nu + 1)^(-10*nu -
5)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta)) - 50050*(nu
+ 1)^2*theta^4*(s*theta/nu + 1)^(-11*nu - 4)*e^(((s*theta/nu + 1)^(-nu +
1) - 1)*nu/((nu - 1)*theta))/nu^2 - 3003*(nu + 1)*(nu + 2)*(nu +
3)*theta^4*(s*theta/nu + 1)^(-11*nu - 4)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^3 - 75075*(nu + 1)*(nu +
2)*theta^4*(s*theta/nu + 1)^(-11*nu - 4)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu^2 - 675675*(nu + 1)*theta^4*(s*theta/nu +
1)^(-11*nu - 4)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/nu - 675675*theta^4*(s*theta/nu + 1)^(-11*nu -
4)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta)) - 1365*(nu +
1)*(nu + 2)*theta^3*(s*theta/nu + 1)^(-12*nu - 3)*e^(((s*theta/nu +
1)^(-nu + 1) - 1)*nu/((nu - 1)*theta))/nu^2 - 30030*(nu +
1)*theta^3*(s*theta/nu + 1)^(-12*nu - 3)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu - 75075*theta^3*(s*theta/nu + 1)^(-12*nu -
3)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta)) - 455*(nu +
1)*theta^2*(s*theta/nu + 1)^(-13*nu - 2)*e^(((s*theta/nu + 1)^(-nu + 1)
- 1)*nu/((nu - 1)*theta))/nu - 4095*theta^2*(s*theta/nu + 1)^(-13*nu -
2)*e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu - 1)*theta)) -
105*theta*(s*theta/nu + 1)^(-14*nu - 1)*e^(((s*theta/nu + 1)^(-nu + 1) -
1)*nu/((nu - 1)*theta)) - e^(((s*theta/nu + 1)^(-nu + 1) - 1)*nu/((nu -
1)*theta))/(s*theta/nu + 1)^(15*nu))
dfun &lt;- function(x,nu=1,theta=1) {
  eval(Rderivexpr,list(s=x,e=exp(1),nu=nu,theta=theta))
}
curve(dfun(x,nu=0.5,theta=0.5),from=5,to=6)
</code></pre>
"
33453835,Split and rearrange a data frame in list using R,1,1,1,"<p>I am using R and with a data frame that has data for different years and various countries(numeric) 
I need to split it and rearrange them in various lists (9col and 22Rows) based on the country and the years(the xcolumns) . I have represented it in .xls as it was easier for the explanation. Or at least I hope so!!
this is a extract of the dataset. there is the rowRef and colRef that are to indicate the position where the data should go in the list.  </p>

<p>here is an example data frame<br>
 
 <code>ColRef &lt;- c(rep(1,6), rep(2,4),rep(5,4),rep(7,7),rep(9,3))
  RowRef &lt;- c(1:3,8,19,22,1:3,8,1:3,8,1:3,15,8,19,22,1:3)
  country &lt;- c(rep(2,19),rep(4,5))
  X2007 &lt;- c(986,421,33,2,NA,NA,653,355,90,1,NA,NA,NA,NA,16,6,0,0,3,NA,NA,1650,777,123)
  X2008 &lt;- c(1092,376,44,1,NA,NA,3581,289,95,1,NA,NA,NA,NA,17,4,0,NA,5,NA,NA,4667,666,139)
  X2009 &lt;- c(1307,346,71,2,NA,NA,3182,301,121,3,NA,NA,NA,NA,26,12,0,0,9,NA,NA,4499,647,192)
X2010 &lt;- c(1536,348,56,1,NA,NA,4441,718,224,7,NA,NA,NA,NA,24,10,0,NA,0,NA,NA,5991,1069,281)
  DataMix &lt;- data.frame(ColRef,RowRef, country, X2007, X2008,X2009,X2010)</code>     </p>

<p><a href=""https://i.stack.imgur.com/Uy8q5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uy8q5.jpg"" alt=""enter image description here""></a></p>

<p>And below is the representation of the wanted result  -> a list with various lists each of (9colx22rows)for each country and for each year. The number that had RefCol 1 and RefRow 1 become the first element, etc... the column named x2007 ... etc are the different years.  </p>

<p>If anyone could tell me how to do this in R, I would be most grateful, as I am struggling around not reaching any result ! I have been searching for a solution but I could not find it. Maybe I have the wrong approach and keywords. Thank you</p>

<p><a href=""https://i.stack.imgur.com/uiZbF.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uiZbF.jpg"" alt=""country 2 year 2007""></a> </p>

<p><a href=""https://i.stack.imgur.com/wF1c7.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wF1c7.jpg"" alt=""country 2 year 2008""></a> </p>

<p><a href=""https://i.stack.imgur.com/quOnS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/quOnS.jpg"" alt=""country 4 year 2007""></a></p>
","<p>I've updated my answer based on the dataset you provided.
Note that I'm still naming it <code>dt</code> and the <code>Country</code> column has a capital ""C"".</p>

<pre><code>library(dplyr)
library(tidyr)

ColRef &lt;- c(rep(1,6), rep(2,4),rep(5,4),rep(7,7),rep(9,3))
RowRef &lt;- c(1:3,8,19,22,1:3,8,1:3,8,1:3,15,8,19,22,1:3)
Country &lt;- c(rep(2,19),rep(4,5))
X2007 &lt;- c(986,421,33,2,NA,NA,653,355,90,1,NA,NA,NA,NA,16,6,0,0,3,NA,NA,1650,777,123)
X2008 &lt;- c(1092,376,44,1,NA,NA,3581,289,95,1,NA,NA,NA,NA,17,4,0,NA,5,NA,NA,4667,666,139)
X2009 &lt;- c(1307,346,71,2,NA,NA,3182,301,121,3,NA,NA,NA,NA,26,12,0,0,9,NA,NA,4499,647,192)
X2010 &lt;- c(1536,348,56,1,NA,NA,4441,718,224,7,NA,NA,NA,NA,24,10,0,NA,0,NA,NA,5991,1069,281)
dt &lt;- data.frame(ColRef,RowRef, Country, X2007, X2008,X2009,X2010)

dt

#    ColRef RowRef Country X2007 X2008 X2009 X2010
# 1       1      1       2   986  1092  1307  1536
# 2       1      2       2   421   376   346   348
# 3       1      3       2    33    44    71    56
# 4       1      8       2     2     1     2     1
# 5       1     19       2    NA    NA    NA    NA
# 6       1     22       2    NA    NA    NA    NA
# 7       2      1       2   653  3581  3182  4441
# 8       2      2       2   355   289   301   718
# 9       2      3       2    90    95   121   224
# 10      2      8       2     1     1     3     7
# 11      5      1       2    NA    NA    NA    NA
# 12      5      2       2    NA    NA    NA    NA
# 13      5      3       2    NA    NA    NA    NA
# 14      5      8       2    NA    NA    NA    NA
# 15      7      1       2    16    17    26    24
# 16      7      2       2     6     4    12    10
# 17      7      3       2     0     0     0     0
# 18      7     15       2     0    NA     0    NA
# 19      7      8       2     3     5     9     0
# 20      7     19       4    NA    NA    NA    NA
# 21      7     22       4    NA    NA    NA    NA
# 22      9      1       4  1650  4667  4499  5991
# 23      9      2       4   777   666   647  1069
# 24      9      3       4   123   139   192   281


# create vector of unique countries and X columns
Country_vector = unique(dt$Country)
X_vector = dt %&gt;% select(matches(""X"")) %&gt;% names()

# create a list to store results
List_res = list()

# starting point of list element counter
n = 1

for (i in Country_vector) {  # i is the countries
  for (j in X_vector) {  # j is the X columns

    # keep appropriate rows and columns
    dt_test =
      dt %&gt;%
      filter(Country == i) %&gt;%
      select(ColRef, RowRef, matches(j)) 

    # create all possible combinations of ColRef and RowRef
    dt_test2 =
      expand.grid(ColRef = seq(min(dt$ColRef), max(dt$ColRef)),
                  RowRef = seq(min(dt$RowRef), max(dt$RowRef))) %&gt;%
      left_join(dt_test, by=c(""ColRef"",""RowRef""))   # join your actual values

    # rename your X column so you can handle it no matter what your input is
    names(dt_test2)[3] = ""X""

    # replace NA values with zeros
    dt_test2[,""X""] = ifelse(is.na(dt_test2[,""X""]), 0, dt_test2[,""X""])

    # reshape dataset
    dt_test2 %&gt;%
      spread(ColRef, X) -&gt; List_res[[n]]

    # set the name of that element based on specific country and X column (at this iteration)
    names(List_res)[n] = paste(i,j)

    # get ready for next element and next iteration
    n = n+1
  } 
}


# check results
List_res

# $`2 X2007`
# RowRef   1   2 3 4 5 6  7 8 9
# 1       1 986 653 0 0 0 0 16 0 0
# 2       2 421 355 0 0 0 0  6 0 0
# 3       3  33  90 0 0 0 0  0 0 0
# 4       4   0   0 0 0 0 0  0 0 0
# 5       5   0   0 0 0 0 0  0 0 0
# 6       6   0   0 0 0 0 0  0 0 0
# 7       7   0   0 0 0 0 0  0 0 0
# 8       8   2   1 0 0 0 0  3 0 0
# 9       9   0   0 0 0 0 0  0 0 0
# 10     10   0   0 0 0 0 0  0 0 0
# 11     11   0   0 0 0 0 0  0 0 0
# 12     12   0   0 0 0 0 0  0 0 0
# 13     13   0   0 0 0 0 0  0 0 0
# 14     14   0   0 0 0 0 0  0 0 0
# 15     15   0   0 0 0 0 0  0 0 0
# 16     16   0   0 0 0 0 0  0 0 0
# 17     17   0   0 0 0 0 0  0 0 0
# 18     18   0   0 0 0 0 0  0 0 0
# 19     19   0   0 0 0 0 0  0 0 0
# 20     20   0   0 0 0 0 0  0 0 0
# 21     21   0   0 0 0 0 0  0 0 0
# 22     22   0   0 0 0 0 0  0 0 0
# 
# $`2 X2008`
# RowRef    1    2 3 4 5 6  7 8 9
# 1       1 1092 3581 0 0 0 0 17 0 0
# 2       2  376  289 0 0 0 0  4 0 0
# 3       3   44   95 0 0 0 0  0 0 0
# 4       4    0    0 0 0 0 0  0 0 0
# 5       5    0    0 0 0 0 0  0 0 0
# 6       6    0    0 0 0 0 0  0 0 0
# 7       7    0    0 0 0 0 0  0 0 0
# 8       8    1    1 0 0 0 0  5 0 0
# 9       9    0    0 0 0 0 0  0 0 0
# 10     10    0    0 0 0 0 0  0 0 0
# 11     11    0    0 0 0 0 0  0 0 0
# 12     12    0    0 0 0 0 0  0 0 0
# 13     13    0    0 0 0 0 0  0 0 0
# 14     14    0    0 0 0 0 0  0 0 0
# 15     15    0    0 0 0 0 0  0 0 0
# 16     16    0    0 0 0 0 0  0 0 0
# 17     17    0    0 0 0 0 0  0 0 0
# 18     18    0    0 0 0 0 0  0 0 0
# 19     19    0    0 0 0 0 0  0 0 0
# 20     20    0    0 0 0 0 0  0 0 0
# 21     21    0    0 0 0 0 0  0 0 0
# 22     22    0    0 0 0 0 0  0 0 0
# 
# $`2 X2009`
# RowRef    1    2 3 4 5 6  7 8 9
# 1       1 1307 3182 0 0 0 0 26 0 0
# 2       2  346  301 0 0 0 0 12 0 0
# 3       3   71  121 0 0 0 0  0 0 0
# 4       4    0    0 0 0 0 0  0 0 0
# 5       5    0    0 0 0 0 0  0 0 0
# 6       6    0    0 0 0 0 0  0 0 0
# 7       7    0    0 0 0 0 0  0 0 0
# 8       8    2    3 0 0 0 0  9 0 0
# 9       9    0    0 0 0 0 0  0 0 0
# 10     10    0    0 0 0 0 0  0 0 0
# 11     11    0    0 0 0 0 0  0 0 0
# 12     12    0    0 0 0 0 0  0 0 0
# 13     13    0    0 0 0 0 0  0 0 0
# 14     14    0    0 0 0 0 0  0 0 0
# 15     15    0    0 0 0 0 0  0 0 0
# 16     16    0    0 0 0 0 0  0 0 0
# 17     17    0    0 0 0 0 0  0 0 0
# 18     18    0    0 0 0 0 0  0 0 0
# 19     19    0    0 0 0 0 0  0 0 0
# 20     20    0    0 0 0 0 0  0 0 0
# 21     21    0    0 0 0 0 0  0 0 0
# 22     22    0    0 0 0 0 0  0 0 0
# 
# $`2 X2010`
# RowRef    1    2 3 4 5 6  7 8 9
# 1       1 1536 4441 0 0 0 0 24 0 0
# 2       2  348  718 0 0 0 0 10 0 0
# 3       3   56  224 0 0 0 0  0 0 0
# 4       4    0    0 0 0 0 0  0 0 0
# 5       5    0    0 0 0 0 0  0 0 0
# 6       6    0    0 0 0 0 0  0 0 0
# 7       7    0    0 0 0 0 0  0 0 0
# 8       8    1    7 0 0 0 0  0 0 0
# 9       9    0    0 0 0 0 0  0 0 0
# 10     10    0    0 0 0 0 0  0 0 0
# 11     11    0    0 0 0 0 0  0 0 0
# 12     12    0    0 0 0 0 0  0 0 0
# 13     13    0    0 0 0 0 0  0 0 0
# 14     14    0    0 0 0 0 0  0 0 0
# 15     15    0    0 0 0 0 0  0 0 0
# 16     16    0    0 0 0 0 0  0 0 0
# 17     17    0    0 0 0 0 0  0 0 0
# 18     18    0    0 0 0 0 0  0 0 0
# 19     19    0    0 0 0 0 0  0 0 0
# 20     20    0    0 0 0 0 0  0 0 0
# 21     21    0    0 0 0 0 0  0 0 0
# 22     22    0    0 0 0 0 0  0 0 0
# 
# $`4 X2007`
# RowRef 1 2 3 4 5 6 7 8    9
# 1       1 0 0 0 0 0 0 0 0 1650
# 2       2 0 0 0 0 0 0 0 0  777
# 3       3 0 0 0 0 0 0 0 0  123
# 4       4 0 0 0 0 0 0 0 0    0
# 5       5 0 0 0 0 0 0 0 0    0
# 6       6 0 0 0 0 0 0 0 0    0
# 7       7 0 0 0 0 0 0 0 0    0
# 8       8 0 0 0 0 0 0 0 0    0
# 9       9 0 0 0 0 0 0 0 0    0
# 10     10 0 0 0 0 0 0 0 0    0
# 11     11 0 0 0 0 0 0 0 0    0
# 12     12 0 0 0 0 0 0 0 0    0
# 13     13 0 0 0 0 0 0 0 0    0
# 14     14 0 0 0 0 0 0 0 0    0
# 15     15 0 0 0 0 0 0 0 0    0
# 16     16 0 0 0 0 0 0 0 0    0
# 17     17 0 0 0 0 0 0 0 0    0
# 18     18 0 0 0 0 0 0 0 0    0
# 19     19 0 0 0 0 0 0 0 0    0
# 20     20 0 0 0 0 0 0 0 0    0
# 21     21 0 0 0 0 0 0 0 0    0
# 22     22 0 0 0 0 0 0 0 0    0
# 
# $`4 X2008`
# RowRef 1 2 3 4 5 6 7 8    9
# 1       1 0 0 0 0 0 0 0 0 4667
# 2       2 0 0 0 0 0 0 0 0  666
# 3       3 0 0 0 0 0 0 0 0  139
# 4       4 0 0 0 0 0 0 0 0    0
# 5       5 0 0 0 0 0 0 0 0    0
# 6       6 0 0 0 0 0 0 0 0    0
# 7       7 0 0 0 0 0 0 0 0    0
# 8       8 0 0 0 0 0 0 0 0    0
# 9       9 0 0 0 0 0 0 0 0    0
# 10     10 0 0 0 0 0 0 0 0    0
# 11     11 0 0 0 0 0 0 0 0    0
# 12     12 0 0 0 0 0 0 0 0    0
# 13     13 0 0 0 0 0 0 0 0    0
# 14     14 0 0 0 0 0 0 0 0    0
# 15     15 0 0 0 0 0 0 0 0    0
# 16     16 0 0 0 0 0 0 0 0    0
# 17     17 0 0 0 0 0 0 0 0    0
# 18     18 0 0 0 0 0 0 0 0    0
# 19     19 0 0 0 0 0 0 0 0    0
# 20     20 0 0 0 0 0 0 0 0    0
# 21     21 0 0 0 0 0 0 0 0    0
# 22     22 0 0 0 0 0 0 0 0    0
# 
# $`4 X2009`
# RowRef 1 2 3 4 5 6 7 8    9
# 1       1 0 0 0 0 0 0 0 0 4499
# 2       2 0 0 0 0 0 0 0 0  647
# 3       3 0 0 0 0 0 0 0 0  192
# 4       4 0 0 0 0 0 0 0 0    0
# 5       5 0 0 0 0 0 0 0 0    0
# 6       6 0 0 0 0 0 0 0 0    0
# 7       7 0 0 0 0 0 0 0 0    0
# 8       8 0 0 0 0 0 0 0 0    0
# 9       9 0 0 0 0 0 0 0 0    0
# 10     10 0 0 0 0 0 0 0 0    0
# 11     11 0 0 0 0 0 0 0 0    0
# 12     12 0 0 0 0 0 0 0 0    0
# 13     13 0 0 0 0 0 0 0 0    0
# 14     14 0 0 0 0 0 0 0 0    0
# 15     15 0 0 0 0 0 0 0 0    0
# 16     16 0 0 0 0 0 0 0 0    0
# 17     17 0 0 0 0 0 0 0 0    0
# 18     18 0 0 0 0 0 0 0 0    0
# 19     19 0 0 0 0 0 0 0 0    0
# 20     20 0 0 0 0 0 0 0 0    0
# 21     21 0 0 0 0 0 0 0 0    0
# 22     22 0 0 0 0 0 0 0 0    0
# 
# $`4 X2010`
# RowRef 1 2 3 4 5 6 7 8    9
# 1       1 0 0 0 0 0 0 0 0 5991
# 2       2 0 0 0 0 0 0 0 0 1069
# 3       3 0 0 0 0 0 0 0 0  281
# 4       4 0 0 0 0 0 0 0 0    0
# 5       5 0 0 0 0 0 0 0 0    0
# 6       6 0 0 0 0 0 0 0 0    0
# 7       7 0 0 0 0 0 0 0 0    0
# 8       8 0 0 0 0 0 0 0 0    0
# 9       9 0 0 0 0 0 0 0 0    0
# 10     10 0 0 0 0 0 0 0 0    0
# 11     11 0 0 0 0 0 0 0 0    0
# 12     12 0 0 0 0 0 0 0 0    0
# 13     13 0 0 0 0 0 0 0 0    0
# 14     14 0 0 0 0 0 0 0 0    0
# 15     15 0 0 0 0 0 0 0 0    0
# 16     16 0 0 0 0 0 0 0 0    0
# 17     17 0 0 0 0 0 0 0 0    0
# 18     18 0 0 0 0 0 0 0 0    0
# 19     19 0 0 0 0 0 0 0 0    0
# 20     20 0 0 0 0 0 0 0 0    0
# 21     21 0 0 0 0 0 0 0 0    0
# 22     22 0 0 0 0 0 0 0 0    0
</code></pre>

<p>I'm adding another approach which gives you the same result, not in separate lists, but in a big data frame with 2 key columns <code>Country</code> and <code>X</code> and also doesn't need to use a for loop (not a fan of loops personally).</p>

<pre><code>library(dplyr)
library(tidyr)

ColRef &lt;- c(rep(1,6), rep(2,4),rep(5,4),rep(7,7),rep(9,3))
RowRef &lt;- c(1:3,8,19,22,1:3,8,1:3,8,1:3,15,8,19,22,1:3)
Country &lt;- c(rep(2,19),rep(4,5))
X2007 &lt;- c(986,421,33,2,NA,NA,653,355,90,1,NA,NA,NA,NA,16,6,0,0,3,NA,NA,1650,777,123)
X2008 &lt;- c(1092,376,44,1,NA,NA,3581,289,95,1,NA,NA,NA,NA,17,4,0,NA,5,NA,NA,4667,666,139)
X2009 &lt;- c(1307,346,71,2,NA,NA,3182,301,121,3,NA,NA,NA,NA,26,12,0,0,9,NA,NA,4499,647,192)
X2010 &lt;- c(1536,348,56,1,NA,NA,4441,718,224,7,NA,NA,NA,NA,24,10,0,NA,0,NA,NA,5991,1069,281)
dt &lt;- data.frame(ColRef,RowRef, Country, X2007, X2008,X2009,X2010)

# reshape dataset
dt2 = dt %&gt;% gather(X,value,X2007:X2010)

# function
ff = function(i,j){
  expand.grid(ColRef = seq(min(dt$ColRef), max(dt$ColRef)),
              RowRef = seq(min(dt$RowRef), max(dt$RowRef))) %&gt;%
    left_join(dt2[dt2$Country==i &amp; dt2$X==j,], by=c(""ColRef"",""RowRef"")) %&gt;%
    select(-Country, -X) %&gt;%
    spread(ColRef, value)}

# create all combinations between country and X values and apply the function
expand.grid(Country = unique(dt2$Country),
            X = unique(dt2$X)) %&gt;%
  group_by(Country, X) %&gt;%
  do(ff(.$Country, .$X)) %&gt;%
  ungroup() %&gt;%       ## don't really need it if you don't mind keeping the grouping
  print(n = nrow(.))  ## needed only to view/print all rows

# Source: local data frame [176 x 12]
# 
#     Country      X RowRef     1     2     3     4     5     6     7     8     9
#       (dbl) (fctr)  (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl)
# 1         2  X2007      1   986   653    NA    NA    NA    NA    16    NA    NA
# 2         2  X2007      2   421   355    NA    NA    NA    NA     6    NA    NA
# 3         2  X2007      3    33    90    NA    NA    NA    NA     0    NA    NA
# 4         2  X2007      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 5         2  X2007      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 6         2  X2007      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 7         2  X2007      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 8         2  X2007      8     2     1    NA    NA    NA    NA     3    NA    NA
# 9         2  X2007      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 10        2  X2007     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 11        2  X2007     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 12        2  X2007     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 13        2  X2007     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 14        2  X2007     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 15        2  X2007     15    NA    NA    NA    NA    NA    NA     0    NA    NA
# 16        2  X2007     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 17        2  X2007     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 18        2  X2007     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 19        2  X2007     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 20        2  X2007     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 21        2  X2007     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 22        2  X2007     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 23        2  X2008      1  1092  3581    NA    NA    NA    NA    17    NA    NA
# 24        2  X2008      2   376   289    NA    NA    NA    NA     4    NA    NA
# 25        2  X2008      3    44    95    NA    NA    NA    NA     0    NA    NA
# 26        2  X2008      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 27        2  X2008      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 28        2  X2008      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 29        2  X2008      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 30        2  X2008      8     1     1    NA    NA    NA    NA     5    NA    NA
# 31        2  X2008      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 32        2  X2008     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 33        2  X2008     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 34        2  X2008     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 35        2  X2008     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 36        2  X2008     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 37        2  X2008     15    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 38        2  X2008     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 39        2  X2008     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 40        2  X2008     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 41        2  X2008     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 42        2  X2008     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 43        2  X2008     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 44        2  X2008     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 45        2  X2009      1  1307  3182    NA    NA    NA    NA    26    NA    NA
# 46        2  X2009      2   346   301    NA    NA    NA    NA    12    NA    NA
# 47        2  X2009      3    71   121    NA    NA    NA    NA     0    NA    NA
# 48        2  X2009      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 49        2  X2009      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 50        2  X2009      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 51        2  X2009      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 52        2  X2009      8     2     3    NA    NA    NA    NA     9    NA    NA
# 53        2  X2009      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 54        2  X2009     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 55        2  X2009     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 56        2  X2009     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 57        2  X2009     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 58        2  X2009     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 59        2  X2009     15    NA    NA    NA    NA    NA    NA     0    NA    NA
# 60        2  X2009     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 61        2  X2009     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 62        2  X2009     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 63        2  X2009     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 64        2  X2009     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 65        2  X2009     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 66        2  X2009     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 67        2  X2010      1  1536  4441    NA    NA    NA    NA    24    NA    NA
# 68        2  X2010      2   348   718    NA    NA    NA    NA    10    NA    NA
# 69        2  X2010      3    56   224    NA    NA    NA    NA     0    NA    NA
# 70        2  X2010      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 71        2  X2010      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 72        2  X2010      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 73        2  X2010      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 74        2  X2010      8     1     7    NA    NA    NA    NA     0    NA    NA
# 75        2  X2010      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 76        2  X2010     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 77        2  X2010     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 78        2  X2010     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 79        2  X2010     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 80        2  X2010     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 81        2  X2010     15    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 82        2  X2010     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 83        2  X2010     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 84        2  X2010     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 85        2  X2010     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 86        2  X2010     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 87        2  X2010     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 88        2  X2010     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 89        4  X2007      1    NA    NA    NA    NA    NA    NA    NA    NA  1650
# 90        4  X2007      2    NA    NA    NA    NA    NA    NA    NA    NA   777
# 91        4  X2007      3    NA    NA    NA    NA    NA    NA    NA    NA   123
# 92        4  X2007      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 93        4  X2007      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 94        4  X2007      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 95        4  X2007      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 96        4  X2007      8    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 97        4  X2007      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 98        4  X2007     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 99        4  X2007     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 100       4  X2007     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 101       4  X2007     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 102       4  X2007     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 103       4  X2007     15    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 104       4  X2007     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 105       4  X2007     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 106       4  X2007     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 107       4  X2007     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 108       4  X2007     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 109       4  X2007     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 110       4  X2007     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 111       4  X2008      1    NA    NA    NA    NA    NA    NA    NA    NA  4667
# 112       4  X2008      2    NA    NA    NA    NA    NA    NA    NA    NA   666
# 113       4  X2008      3    NA    NA    NA    NA    NA    NA    NA    NA   139
# 114       4  X2008      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 115       4  X2008      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 116       4  X2008      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 117       4  X2008      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 118       4  X2008      8    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 119       4  X2008      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 120       4  X2008     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 121       4  X2008     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 122       4  X2008     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 123       4  X2008     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 124       4  X2008     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 125       4  X2008     15    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 126       4  X2008     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 127       4  X2008     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 128       4  X2008     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 129       4  X2008     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 130       4  X2008     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 131       4  X2008     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 132       4  X2008     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 133       4  X2009      1    NA    NA    NA    NA    NA    NA    NA    NA  4499
# 134       4  X2009      2    NA    NA    NA    NA    NA    NA    NA    NA   647
# 135       4  X2009      3    NA    NA    NA    NA    NA    NA    NA    NA   192
# 136       4  X2009      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 137       4  X2009      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 138       4  X2009      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 139       4  X2009      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 140       4  X2009      8    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 141       4  X2009      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 142       4  X2009     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 143       4  X2009     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 144       4  X2009     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 145       4  X2009     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 146       4  X2009     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 147       4  X2009     15    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 148       4  X2009     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 149       4  X2009     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 150       4  X2009     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 151       4  X2009     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 152       4  X2009     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 153       4  X2009     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 154       4  X2009     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 155       4  X2010      1    NA    NA    NA    NA    NA    NA    NA    NA  5991
# 156       4  X2010      2    NA    NA    NA    NA    NA    NA    NA    NA  1069
# 157       4  X2010      3    NA    NA    NA    NA    NA    NA    NA    NA   281
# 158       4  X2010      4    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 159       4  X2010      5    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 160       4  X2010      6    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 161       4  X2010      7    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 162       4  X2010      8    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 163       4  X2010      9    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 164       4  X2010     10    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 165       4  X2010     11    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 166       4  X2010     12    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 167       4  X2010     13    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 168       4  X2010     14    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 169       4  X2010     15    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 170       4  X2010     16    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 171       4  X2010     17    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 172       4  X2010     18    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 173       4  X2010     19    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 174       4  X2010     20    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 175       4  X2010     21    NA    NA    NA    NA    NA    NA    NA    NA    NA
# 176       4  X2010     22    NA    NA    NA    NA    NA    NA    NA    NA    NA
</code></pre>

<p>So, you can see that if you pick <code>Country = 2</code> and <code>X = X2007</code> you have a subset which is the same as your first list (previously), etc.</p>
"
35809428,Multi POST query (session mode),2,2,4,"<p>I am trying to interrogate this <a href=""https://compare.switchon.vic.gov.au/welcome"" rel=""nofollow"">site</a> to get the list of offers. 
The problem is that we need to fill 2 forms (2 POST queries) before receiving the final result. </p>

<p>This what I have done so far:</p>

<p>First I am sending the first POST after setting the cookies:</p>

<pre><code>library(httr)
set_cookies(.cookies = c(a = ""1"", b = ""2""))
first_url &lt;- ""https://compare.switchon.vic.gov.au/submit""
body &lt;- list(energy_category=""electricity"",
             location=""home"",
             ""location-home""=""shift"",
             ""retailer-company""="""",
             postcode=""3000"",
             distributor=7,
             zone=1,
             energy_concession=0,
             ""file-provider""="""",
             solar=0,
             solar_feedin_tariff="""",
             disclaimer_chkbox=""disclaimer_selected"")
qr&lt;- POST(first_url,
          encode=""form"",
          body=body)
</code></pre>

<p>Then trying to retrieve the offers using the second post query:</p>

<pre><code>gov_url &lt;- ""https://compare.switchon.vic.gov.au/energy_questionnaire/submit""
qr1&lt;- POST(gov_url,
          encode=""form"",
          body=list(`person-count`=1,
                    `room-count`=1,
                    `refrigerator-count`=1,
                    `gas-type`=4,
                    `pool-heating`=0,
                    spaceheating=""none"",
                    spacecooling=""none"",
                    `cloth-dryer`=0,
                    waterheating=""other""),
          set_cookies(a = 1, b = 2))
)
library(XML)
dc &lt;- htmlParse(qr1)
</code></pre>

<p>But unfortunately I get a message indicating the end of session. Many thanks for any help to resolve this. </p>

<h3>update add cookies:</h3>

<p>I added the cookies and the intermediate GET, but I still don't have any of the results.</p>

<pre><code>library(httr)
first_url &lt;- ""https://compare.switchon.vic.gov.au/submit""
body &lt;- list(energy_category=""electricity"",
             location=""home"",
             ""location-home""=""shift"",
             ""retailer-company""="""",
             postcode=3000,
             distributor=7,
             zone=1,
             energy_concession=0,
             ""file-provider""="""",
             solar=0,
             solar_feedin_tariff="""",
             disclaimer_chkbox=""disclaimer_selected"")
qr&lt;- POST(first_url,
          encode=""form"",
          body=body,
          config=set_cookies(a = 1, b = 2))

xx &lt;- GET(""https://compare.switchon.vic.gov.au/energy_questionnaire"",config=set_cookies(a = 1, b = 2))

gov_url &lt;- ""https://compare.switchon.vic.gov.au/energy_questionnaire/submit""
qr1&lt;- POST(gov_url,
           encode=""form"",
           body=list(
             `person-count`=1,
             `room-count`=1,
             `refrigerator-count`=1,
             `gas-type`=4,
             `pool-heating`=0,
             spaceheating=""none"",
             spacecooling=""none"",
             `cloth-dryer`=0,
             waterheating=""other""),
           config=set_cookies(a = 1, b = 2))

library(XML)
dc &lt;- htmlParse(qr1)
</code></pre>
","<p>using a python <a href=""http://docs.python-requests.org/en/master/user/advanced/#session-objects"" rel=""noreferrer"">requests.Session</a> object with the following data gets to the results page:</p>

<pre><code>form1 = {""energy_category"": ""electricity"",
         ""location"": ""home"",
         ""location-home"": ""shift"",
         ""distributor"": ""7"",
         ""postcode"": ""3000"",
         ""energy_concession"": ""0"",
         ""solar"": ""0"",
         ""disclaimer_chkbox"": ""disclaimer_selected"",
         }


form2 = {""person-count"":""1"",
""room-count"":""4"",
""refrigerator-count"":""0"",
""gas-type"":""3"",
""pool-heating"":""0"",
""spaceheating[]"":""none"",
""spacecooling[]"":""none"",
""cloth-dryer"":""0"",
""waterheating[]"":""other""}

sub_url = ""https://compare.switchon.vic.gov.au/submit""

with requests.Session() as s:
    s.post(sub_url, data=form1)
    r = (s.get(""https://compare.switchon.vic.gov.au/energy_questionnaire""))
    s.post(""https://compare.switchon.vic.gov.au/energy_questionnaire/submit"",
           data=form2)
    r = s.get(""https://compare.switchon.vic.gov.au/offers"")
    print(r.content)
</code></pre>

<p>You should see the matching <code>h1</code> in the returned html that you see on the page:</p>

<pre><code>          &lt;h1&gt;Your electricity offers&lt;/h1&gt;
</code></pre>

<p>Or using scrapy form requests:</p>

<pre><code>import scrapy

class Spider(scrapy.Spider):
    name = 'comp'
    start_urls = ['https://compare.switchon.vic.gov.au/energy_questionnaire/submit']

    form1 = {""energy_category"": ""electricity"",
             ""location"": ""home"",
             ""location-home"": ""shift"",
             ""distributor"": ""7"",
             ""postcode"": ""3000"",
             ""energy_concession"": ""0"",
             ""solar"": ""0"",
             ""disclaimer_chkbox"": ""disclaimer_selected"",
             }

    sub_url = ""https://compare.switchon.vic.gov.au/submit""
    form2 = {""person-count"":""1"",
    ""room-count"":""4"",
    ""refrigerator-count"":""0"",
    ""gas-type"":""3"",
    ""pool-heating"":""0"",
    ""spaceheating[]"":""none"",
    ""spacecooling[]"":""none"",
    ""cloth-dryer"":""0"",
    ""waterheating[]"":""other""}

    def start_requests(self):
        return [scrapy.FormRequest(
            self.sub_url,
            formdata=form1,
            callback=self.parse
        )]

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata=form2,
            callback=self.after
        )

    def after(self, response):
        print(""&lt;h1&gt;Your electricity offers&lt;/h1&gt;"" in response.body)
</code></pre>

<p>Which we can verify has the <code>""&lt;h1&gt;Your electricity offers&lt;/h1&gt;""</code>:</p>

<pre><code>2016-03-07 12:27:31 [scrapy] DEBUG: Crawled (200) &lt;GET https://compare.switchon.vic.gov.au/offers#list/electricity&gt; (referer: https://compare.switchon.vic.gov.au/energy_questionnaire)
True
2016-03-07 12:27:31 [scrapy] INFO: Closing spider (finished)
</code></pre>

<p>The next problem is the actual data is dynamically rendered which you can verify if you look at the source of the results page, you can actually get all the provider in json format:</p>

<pre><code>with requests.Session() as s:
    s.post(sub_url, data=form1)
    r = (s.get(""https://compare.switchon.vic.gov.au/energy_questionnaire""))
    s.post(""https://compare.switchon.vic.gov.au/energy_questionnaire/submit"",
           data=form2)
    r = s.get(""https://compare.switchon.vic.gov.au/service/offers"")
    print(r.json())
</code></pre>

<p>A snippet of which is:</p>

<pre><code>{u'pageMetaData': {u'showDual': False, u'isGas': False, u'showTouToggle': True, u'isElectricityInitial': True, u'showLoopback': False, u'isElectricity': True}, u'offersList': [{u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.peopleenergy.com.au', u'offerId': u'PEO33707SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1410, u'offerType': u'Standing offer', u'offerName': u'Residential 5-Day Time of Use', u'conditionalPrice': 1410, u'fullDiscountedPrice': 1390, u'greenPower': 0, u'retailerName': u'People Energy', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Time of use', u'retailerPhone': u'1300 788 970', u'isPartDual': False, u'retailerId': u'7322', u'isTouOffer': True, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'1645', u'exitFeeCount': 0, u'timeDefinition': u'Local time', u'retailerImageUrl': u'img/retailers/big/peopleenergy.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.peopleenergy.com.au', u'offerId': u'PEO33773SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1500, u'offerType': u'Standing offer', u'offerName': u'Residential Peak Anytime', u'conditionalPrice': 1500, u'fullDiscountedPrice': 1480, u'greenPower': 0, u'retailerName': u'People Energy', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Single rate', u'retailerPhone': u'1300 788 970', u'isPartDual': False, u'retailerId': u'7322', u'isTouOffer': False, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'1649', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/peopleenergy.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.energythatcould.com.au', u'offerId': u'PAC33683SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1400, u'offerType': u'Standing offer', u'offerName': u'Vic Home Flex', u'conditionalPrice': 1400, u'fullDiscountedPrice': 1400, u'greenPower': 0, u'retailerName': u'Pacific Hydro Retail Pty Ltd', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Flexible Pricing', u'retailerPhone': u'1800 010 648', u'isPartDual': False, u'retailerId': u'15902', u'isTouOffer': False, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'1666', u'exitFeeCount': 0, u'timeDefinition': u'Local time', u'retailerImageUrl': u'img/retailers/big/pachydro.jpg'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.energythatcould.com.au', u'offerId': u'PAC33679SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1340, u'offerType': u'Standing offer', u'offerName': u'Vic Home Flex', u'conditionalPrice': 1340, u'fullDiscountedPrice': 1340, u'greenPower': 0, u'retailerName': u'Pacific Hydro Retail Pty Ltd', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Single rate', u'retailerPhone': u'1800 010 648', u'isPartDual': False, u'retailerId': u'15902', u'isTouOffer': False, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'1680', u'exitFeeCount': 0, u'timeDefinition': u'Local time', u'retailerImageUrl': u'img/retailers/big/pachydro.jpg'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 10, u'retailerUrl': u'www.commander.com', u'offerId': u'M2E30367MR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': True, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1370, u'offerType': u'Market offer', u'offerName': u'Citipower Commander Residential Market Offer (CE3CPR-MAT1 + PF1/TF1/GF1)', u'conditionalPrice': 1370, u'fullDiscountedPrice': 1160, u'greenPower': 0, u'retailerName': u'Commander Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': True, u'greenpowerChargeType': None, u'tariffType': u'Single rate', u'retailerPhone': u'13 39 14', u'isPartDual': False, u'retailerId': u'13667', u'isTouOffer': False, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'2384', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/commanderpowergas.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 10, u'retailerUrl': u'www.commander.com', u'offerId': u'M2E30359MR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': True, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1330, u'offerType': u'Market offer', u'offerName': u'Citipower Commander Residential Market Offer (Flexible Pricing (Peak, Shoulder and Off Peak) (CE3CPR-MCFP1 + PF1/TF1/GF1)', u'conditionalPrice': 1330, u'fullDiscountedPrice': 1140, u'greenPower': 0, u'retailerName': u'Commander Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': True, u'greenpowerChargeType': None, u'tariffType': u'Time of use', u'retailerPhone': u'13 39 14', u'isPartDual': False, u'retailerId': u'13667', u'isTouOffer': True, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'2386', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/commanderpowergas.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 10, u'retailerUrl': u'www.commander.com', u'offerId': u'M2E33241MR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': True, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1300, u'offerType': u'Market offer', u'offerName': u'Citipower Commander Residential Market Offer (Peak / Off Peak) (CE3CPR-MPK1OP1)', u'conditionalPrice': 1300, u'fullDiscountedPrice': 1100, u'greenPower': 0, u'retailerName': u'Commander Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': True, u'greenpowerChargeType': None, u'tariffType': u'Time of use', u'retailerPhone': u'13 39 14', u'isPartDual': False, u'retailerId': u'13667', u'isTouOffer': True, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'2389', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/commanderpowergas.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.commander.com', u'offerId': u'M2E30379SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1370, u'offerType': u'Standing offer', u'offerName': u'Citipower Commander Residential Standing Offer (CE3CPR-SAT1 + PF1/TF1/GF1)', u'conditionalPrice': 1370, u'fullDiscountedPrice': 1370, u'greenPower': 0, u'retailerName': u'Commander Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Single rate', u'retailerPhone': u'13 39 14', u'isPartDual': False, u'retailerId': u'13667', u'isTouOffer': False, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'2391', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/commanderpowergas.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.commander.com', u'offerId': u'M2E30369SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1330, u'offerType': u'Standing offer', u'offerName': u'Citipower Commander Residential Standing Offer (Flexible Pricing (Peak, Shoulder and Off Peak) (CE3CPR-SCFP1 + PF1/TF1/GF1)', u'conditionalPrice': 1330, u'fullDiscountedPrice': 1330, u'greenPower': 0, u'retailerName': u'Commander Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Time of use', u'retailerPhone': u'13 39 14', u'isPartDual': False, u'retailerId': u'13667', u'isTouOffer': True, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'2393', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/commanderpowergas.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.commander.com', u'offerId': u'M2E30375SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1300, u'offerType': u'Standing offer', u'offerName': u'Citipower Commander Residential Standing Offer (Peak / Off Peak) (CE3CPR-SPK1OP1)', u'conditionalPrice': 1300, u'fullDiscountedPrice': 1300, u'greenPower': 0, u'retailerName': u'Commander Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType': u'Time of use', u'retailerPhone': u'13 39 14', u'isPartDual': False, u'retailerId': u'13667', u'isTouOffer': True, u'solarType': None, u'estimatedSolarCredit': 0, u'offerKey': u'2395', u'exitFeeCount': 0, u'timeDefinition': u'AEST only', u'retailerImageUrl': u'img/retailers/big/commanderpowergas.png'}], u'isClosed': False, u'isChecked': False, u'offerFuelType': 0}, {u'offerDetails': [{u'coolingOffPeriod': 0, u'retailerUrl': u'www.dodo.com/powerandgas', u'offerId': u'DOD32903SR', u'contractLengthCount': 1, u'exitFee': [0], u'hasIncentive': False, u'tariffDetails': {}, u'greenpowerAmount': 0, u'isDirectDebitOnly': False, u'basePrice': 1320, u'offerType': u'Standing offer', u'offerName': u'Citipower Res No Term Standing Offer (Common Form Flex Plan) (E3CPR-SCFP1)', u'conditionalPrice': 1320, u'fullDiscountedPrice': 1320, u'greenPower': 0, u'retailerName': u'Dodo Power &amp; Gas', u'intrinsicGreenpowerPercentage': u'0.0000', u'contractLength': [u'None'], u'hasPayOnTimeDiscount': False, u'greenpowerChargeType': None, u'tariffType':   
</code></pre>

<p>Then if you look at the requests later, for example when you click the compare selected button on the results page, there is a request like:</p>

<pre><code>https://compare.switchon.vic.gov.au/service/offer/tariff/9090/9092
</code></pre>

<p>So you may be able to mimic what happens by filtering using the tariff or some variation.</p>

<p>You can actually get all the data as json, if you enter the same values as below into the forms:</p>

<pre><code>form1 = {""energy_category"": ""electricity"",
         ""location"": ""home"",
         ""location-home"": ""shift"",
         ""distributor"": ""7"",
         ""postcode"": ""3000"",
         ""energy_concession"": ""0"",
         ""solar"": ""0"",
         ""disclaimer_chkbox"": ""disclaimer_selected""
         }

form2 = {""person-count"":""1"",
        ""room-count"":""1"",
        ""refrigerator-count"":""1"",
        ""gas-type"":""4"",
        ""pool-heating"":""0"",
        ""spaceheating[]"":""none"",
        ""spacecooling[]"":""none"",
        ""cloth-dryer"":""0"",
        ""cloth-dryer-freq-weekday"":"""",
        ""waterheating[]"":""other""}


import json
with requests.Session() as s:
    s.post(sub_url, data=form1)
    r = (s.get(""https://compare.switchon.vic.gov.au/energy_questionnaire""))
    s.post(""https://compare.switchon.vic.gov.au/energy_questionnaire/submit"",
           data=form2)
    js = s.get(""https://compare.switchon.vic.gov.au/service/offers"").json()[""offersList""]
    by_discount = sorted(js, key=lambda d: d[""offerDetails""][0][""fullDiscountedPrice""])
</code></pre>

<p>If we just pull the first two values from the list ordered by the total discount price:</p>

<pre><code>from pprint import pprint as pp
pp(by_discount[:2])
</code></pre>

<p>You will get:</p>

<pre><code>[{u'isChecked': False,
  u'isClosed': False,
  u'offerDetails': [{u'basePrice': 980,
                     u'conditionalPrice': 980,
                     u'contractLength': [u'None'],
                     u'contractLengthCount': 1,
                     u'coolingOffPeriod': 10,
                     u'estimatedSolarCredit': 0,
                     u'exitFee': [0],
                     u'exitFeeCount': 1,
                     u'fullDiscountedPrice': 660,
                     u'greenPower': 0,
                     u'greenpowerAmount': 0,
                     u'greenpowerChargeType': None,
                     u'hasIncentive': False,
                     u'hasPayOnTimeDiscount': True,
                     u'intrinsicGreenpowerPercentage': u'0.0000',
                     u'isDirectDebitOnly': False,
                     u'isPartDual': False,
                     u'isTouOffer': False,
                     u'offerId': u'GLO40961MR',
                     u'offerKey': u'7636',
                     u'offerName': u'GLO SWITCH',
                     u'offerType': u'Market offer',
                     u'retailerId': u'31206',
                     u'retailerImageUrl': u'img/retailers/big/globird.jpg',
                     u'retailerName': u'GloBird Energy',
                     u'retailerPhone': u'(03) 8560 4199',
                     u'retailerUrl': u'http://www.globirdenergy.com.au/switchon/',
                     u'solarType': None,
                     u'tariffDetails': {},
                     u'tariffType': u'Single rate',
                     u'timeDefinition': u'Local time'}],
  u'offerFuelType': 0},
 {u'isChecked': False,
  u'isClosed': False,
  u'offerDetails': [{u'basePrice': 1080,
                     u'conditionalPrice': 1080,
                     u'contractLength': [u'None'],
                     u'contractLengthCount': 1,
                     u'coolingOffPeriod': 10,
                     u'estimatedSolarCredit': 0,
                     u'exitFee': [0],
                     u'exitFeeCount': 1,
                     u'fullDiscountedPrice': 720,
                     u'greenPower': 0,
                     u'greenpowerAmount': 0,
                     u'greenpowerChargeType': None,
                     u'hasIncentive': False,
                     u'hasPayOnTimeDiscount': True,
                     u'intrinsicGreenpowerPercentage': u'0.0000',
                     u'isDirectDebitOnly': False,
                     u'isPartDual': False,
                     u'isTouOffer': True,
                     u'offerId': u'GLO41009MR',
                     u'offerKey': u'7642',
                     u'offerName': u'GLO SWITCH',
                     u'offerType': u'Market offer',
                     u'retailerId': u'31206',
                     u'retailerImageUrl': u'img/retailers/big/globird.jpg',
                     u'retailerName': u'GloBird Energy',
                     u'retailerPhone': u'(03) 8560 4199',
                     u'retailerUrl': u'http://www.globirdenergy.com.au/switchon/',
                     u'solarType': None,
                     u'tariffDetails': {},
                     u'tariffType': u'Time of use',
                     u'timeDefinition': u'Local time'}],
  u'offerFuelType': 0}]
</code></pre>

<p>Which should match what you see on the page when you click the <code>""DISCOUNTED PRICE""</code> filter button.</p>

<p>For the normal view it seems to be ordered by <code>conditionalPrice</code> or <code>basePrice</code>, again pulling just the two first values should match what you see on the webpage:</p>

<pre><code> base = sorted(js, key=lambda d: d[""offerDetails""][0][""conditionalPrice""])

from pprint import pprint as pp
pp(base[:2])

[{u'isChecked': False,
  u'isClosed': False,
  u'offerDetails': [{u'basePrice': 740,
                     u'conditionalPrice': 740,
                     u'contractLength': [u'None'],
                     u'contractLengthCount': 1,
                     u'coolingOffPeriod': 0,
                     u'estimatedSolarCredit': 0,
                     u'exitFee': [0],
                     u'exitFeeCount': 0,
                     u'fullDiscountedPrice': 740,
                     u'greenPower': 0,
                     u'greenpowerAmount': 0,
                     u'greenpowerChargeType': None,
                     u'hasIncentive': False,
                     u'hasPayOnTimeDiscount': False,
                     u'intrinsicGreenpowerPercentage': u'0.0000',
                     u'isDirectDebitOnly': False,
                     u'isPartDual': False,
                     u'isTouOffer': False,
                     u'offerId': u'NEX42694SR',
                     u'offerKey': u'9092',
                     u'offerName': u'Citpower Single Rate Residential',
                     u'offerType': u'Standing offer',
                     u'retailerId': u'35726',
                     u'retailerImageUrl': u'img/retailers/big/nextbusinessenergy.jpg',
                     u'retailerName': u'Next Business Energy Pty Ltd',
                     u'retailerPhone': u'1300 466 398',
                     u'retailerUrl': u'http://www.nextbusinessenergy.com.au/',
                     u'solarType': None,
                     u'tariffDetails': {},
                     u'tariffType': u'Single rate',
                     u'timeDefinition': u'Local time'}],
  u'offerFuelType': 0},
 {u'isChecked': False,
  u'isClosed': False,
  u'offerDetails': [{u'basePrice': 780,
                     u'conditionalPrice': 780,
                     u'contractLength': [u'None'],
                     u'contractLengthCount': 1,
                     u'coolingOffPeriod': 0,
                     u'estimatedSolarCredit': 0,
                     u'exitFee': [0],
                     u'exitFeeCount': 0,
                     u'fullDiscountedPrice': 780,
                     u'greenPower': 0,
                     u'greenpowerAmount': 0,
                     u'greenpowerChargeType': None,
                     u'hasIncentive': False,
                     u'hasPayOnTimeDiscount': False,
                     u'intrinsicGreenpowerPercentage': u'0.0000',
                     u'isDirectDebitOnly': False,
                     u'isPartDual': False,
                     u'isTouOffer': False,
                     u'offerId': u'NEX42699SR',
                     u'offerKey': u'9090',
                     u'offerName': u'Citpower Residential Flexible Pricing',
                     u'offerType': u'Standing offer',
                     u'retailerId': u'35726',
                     u'retailerImageUrl': u'img/retailers/big/nextbusinessenergy.jpg',
                     u'retailerName': u'Next Business Energy Pty Ltd',
                     u'retailerPhone': u'1300 466 398',
                     u'retailerUrl': u'http://www.nextbusinessenergy.com.au/',
                     u'solarType': None,
                     u'tariffDetails': {},
                     u'tariffType': u'Flexible Pricing',
                     u'timeDefinition': u'Local time'}],
  u'offerFuelType': 0}]
</code></pre>

<p>You can see all the json returned in firebug console if you click the <code>https://compare.switchon.vic.gov.au/service/offers</code> get entry then hit r<em>esponse</em>:</p>

<p><a href=""https://i.stack.imgur.com/emUkl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/emUkl.png"" alt=""enter image description here""></a></p>

<p>You should be able to pull each field that you want from that.</p>

<p>The output actually has a few extras results which you don't see on the page unless you toggle the tou button below:  </p>

<p><a href=""https://i.stack.imgur.com/9G5vx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9G5vx.png"" alt=""enter image description here""></a></p>

<p>You can filter those from the results so you exactly match the default output or give an option to include with a helper function:</p>

<pre><code>def order_by(l, k, is_tou=False):
    if not is_tou:
        filt = filter(lambda x: not x[""offerDetails""][0][""isTouOffer""], l)
        return sorted(filt, key=lambda d: d[""offerDetails""][0][k])
    return sorted(l, key=lambda d: d[""offerDetails""][0][k])

import json
with requests.Session() as s:
    s.post(sub_url, data=form1)
    r = (s.get(""https://compare.switchon.vic.gov.au/energy_questionnaire""))
    s.post(""https://compare.switchon.vic.gov.au/energy_questionnaire/submit"",
           data=form2)
    js = s.get(""https://compare.switchon.vic.gov.au/service/offers"").json()[""offersList""]
    by_price = by_discount(js, ""conditionalPrice"", False)

print(by_price[:3)
</code></pre>

<p>If you check the output you will see origin energy third with a price of 840 in  the results with the switch on or 860 for AGL when it is off, you can apply the same to the discount output:</p>

<p><a href=""https://i.stack.imgur.com/fMc4X.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fMc4X.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/26CdD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/26CdD.png"" alt=""enter image description here""></a></p>

<p>The regular output also seems to be ordered by <em>conditionalPrice</em> if you check the source the two js functions that get called for ordering are:</p>

<pre><code> ng-click=""changeSortingField('conditionalPrice')""
 ng-click=""changeSortingField('fullDiscountedPrice')""
</code></pre>

<p>So that should now definitely completely match the site output.</p>
"
43537539,How can i perform normal R-functions for hadoop remote on SQL Server?,2,2,2,"<p>how can I perform normal R-Code on a SQL Server without using the Microsoft rx-functions? I think the ComputeContext ""RxInSqlServer"" isn't the right one? But I couldn't find good Information about the other ComputeContext-options.</p>

<p>Is this possible with this Statement?</p>

<pre><code>rxSetComputeContext(ComputeContext)
</code></pre>

<p>Or can I only use it to perform rx-functions? An other Option could be to set the Server Connection in RStudio or VisualStudio?</p>

<p>My Problem is: I want analyse data from hadoop via ODBC-Connection on the SQL Server, so I would like to use the performance of the remote SQL Server and not the data in SQL Server. And then I want analyse the hadoop-data with sparklyr.</p>

<p>Summary: I want to use the performance from the remote server and not the SQL Server data. So RStudio should run not local, it should perform and use the memory of the remote server.</p>

<p>Thanks!</p>
","<p>The concept of a compute context in Microsoft R Server is, “Where will the computation be performed?”</p>

<p>When setting compute context, you are telling Microsoft R Server that computation will occur on either the local machine (with either “local” or “localpar” compute contexts), or, the script will be executed on a remote machine which has Microsoft R Server installed on it. Remote compute contexts are defined by creating a compute context object, and then setting the context to that object. </p>

<p>For SQL Server, you would create an RxInSqlServer() object, and then call rxSetComputeContext() on that object. For Hadoop, the object would be created via the  RxHadoopMR() call.</p>

<p>In code, it would look something like:</p>

<pre><code>CC &lt;- RxHadoopMR( &lt; context defined here &gt; )
rxSetComputeContext(CC)
</code></pre>

<p>To see usage on defining a context, please see documentation (Enter ""?RxHadoopMR"" in the R Client, no quotes).</p>

<p>Any call to an ""rx"" function after this will be performed on the Hadoop cluster, with no data being transferred to the client; other than the results.</p>

<p>RxInSqlServer() would follow the same pattern.</p>

<p>Note: To perform any remote computation, Microsoft R Server must be installed on that machine. </p>

<p>If you wish to run a standard R function on a remote compute context, you must wrap that function in a call to rxExec(). rxExec() is desinged as an interface to parallelize any Open Source R function and allow for its execution on a remote context. Please see documentation (enter ""?rxExec"" in the R Client, no quotes) for usage. </p>

<p>For information on efficient parallelization, please see this blog: <a href=""https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2016/11/14/performance-optimization-when-using-rxexec-to-parallelize-algorithms/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2016/11/14/performance-optimization-when-using-rxexec-to-parallelize-algorithms/</a></p>

<p>You called out ""without using the Microsoft rx-functions"" and I am interpreting this as, ""I would like to use Open Source R Algorithms on data in-SQL Server"", with Microsoft R Server, you must use rxExec() as the interface to run Open Source R. If you want to use no rx functions at all, you will need to query the data to your local machine, and then use Open Source R. To interface with a remote context using Microsoft R Server, the bare minimum is using rxExec().</p>

<p>This is how you will be able to achieve the first part of your ask, ""how can I perform normal R-Code on a SQL Server without using the Microsoft rx-functions? I think the ComputeContext ""RxInSqlServer"" isn't the right one?""</p>

<hr>

<p>For your second ask, ""My Problem is: I want analyse data from hadoop via ODBC-Connection on the SQL Server, so I would like to use the performance of the remote SQL Server and not the data in SQL Server. And then I want analyse the hadoop-data with sparklyr.""</p>

<p>First, I'd like to comment that with the release of Microsoft R Server 9.1, you can use sparklyr in-line with an MRS Spark connection, for some examples, please see this blog: <a href=""https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2017/04/19/new-features-in-9-1-microsoft-r-server-with-sparklyr-interoperability/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2017/04/19/new-features-in-9-1-microsoft-r-server-with-sparklyr-interoperability/</a></p>

<p>Secondly, what you are trying to do is very involved. I can think of two ways that this is possible.</p>

<p>One is, if you have SQL Server PolyBase, you can configure SQL Server to make a virtual table referencing data in Hadoop, similar to Hive. After you have referenced your Hadoop data in SQl Server, you would use an RxInSqlServer() compute context on these tables. This would analyse the data in SQL Server, and return the results to the client. </p>

<p>Here is a detailed blog explaining an end-to-end setup on Cloudera and SQL Server: <a href=""https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2016/10/17/integrating-polybase-with-cloudera-using-active-directory-authentication/"" rel=""nofollow noreferrer"">https://blogs.msdn.microsoft.com/microsoftrservertigerteam/2016/10/17/integrating-polybase-with-cloudera-using-active-directory-authentication/</a></p>

<p>The Second, which I would <strong>NOT</strong> recommend, is <strong>untested</strong>, <strong>hacky</strong>, and has the following prereqs:</p>

<p>1) Your Hadoop cluster must have OpenSSH installed and configured
2) Your SQL Server Machine must have the ability to SSH into your Hadoop Cluster
3) You must be able to place an SSH Key on your SQL Server machine in a directory which the R Services process has the ability to access</p>

<p>And I need to add another disclaimer here, there is <strong>No Guarantee</strong> of this working, and, likely, it will not work. The software was not designed to operate in this fashion.</p>

<p>You would then do the following:</p>

<ul>
<li>On your client machine, you would define a custom function which contains the analysis that you wish to perform, this can be Open Source R Function, rx functions, or a mix. </li>
<li>In this custom function, before calling any other R or rx functions, you would define a RxHadoopMR compute context object which points to your cluster, referencing the SSH key in the directory on the SQL Server machine as if you were executing from that machine. (in the same way that you would define the RxHadoopMR object if you were to do a remote Hadoop operation from your client machine). </li>
<li>Within this custom function, immediately after RxHadoopMR() is defined, you would call rxSetComputeContext() on your defined RxHadoopMR() object</li>
<li>Still in this custom function, write the actual script which will operate on the data in Hadoop. </li>
<li>After this function is defined, you would define an RxInSqlServer() compute context object on the client machine. </li>
<li>You would set your compute context to RxInSqlServer()</li>
<li>Then you would call rxExec() with your custom function as an input.</li>
</ul>

<p>What this will do is execute your custom function on the SQL Server machine, which would hopefully cause it to define its compute context as your Hadoop cluster, and pull the data over SSH for analysis on the SQL Server machine; returning the results to client.</p>

<p>With that said, this is not how Microsoft R Server was designed to be used, and if you wish to optimize performance, please use Option One and configure PolyBase.</p>
"
16238043,Parse XML based on attributes and text values of related nodes,2,2,4,"<p>I have used the XML package to parse both HTML and XML before, and have a rudimentary grasp of xPath.  However I've been asked to consider XML data where the important bits are determined by a combination of text and attributes of the elements themselves, as well as those in related nodes.  I've never done that.  For example</p>

<p>[updated example, slightly more expansive]</p>

<pre><code>&lt;Catalogue&gt;
&lt;Bookstore id=""ID910705541""&gt;
  &lt;location&gt;foo bar&lt;/location&gt;
  &lt;books&gt;
    &lt;book category=""A"" id=""1""&gt;
        &lt;title&gt;Alpha&lt;/title&gt;
        &lt;author ref=""1""&gt;Matthew&lt;/author&gt;
        &lt;author&gt;Mark&lt;/author&gt;
        &lt;author&gt;Luke&lt;/author&gt;
        &lt;author ref=""2""&gt;John&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""B"" id=""10""&gt;
        &lt;title&gt;Beta&lt;/title&gt;
        &lt;author ref=""1""&gt;Huey&lt;/author&gt;
        &lt;author&gt;Duey&lt;/author&gt;
        &lt;author&gt;Louie&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""D"" id=""100""&gt;
        &lt;title&gt;Gamma&lt;/title&gt;
        &lt;author ref=""1""&gt;Tweedle Dee&lt;/author&gt;
        &lt;author ref=""2""&gt;Tweedle Dum&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
  &lt;/books&gt;
  &lt;/Bookstore&gt; 
&lt;Bookstore id=""ID910700051""&gt;
  &lt;location&gt;foo&lt;/location&gt;
  &lt;books&gt;
    &lt;book category=""A"" id=""1""&gt;
        &lt;title&gt;Happy&lt;/title&gt;
        &lt;author&gt;Dopey&lt;/author&gt;
        &lt;author&gt;Bashful&lt;/author&gt;
        &lt;author&gt;Doc&lt;/author&gt;
        &lt;author ref=""1""&gt;Grumpy&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""B"" id=""10""&gt;
        &lt;title&gt;Ni&lt;/title&gt;
        &lt;author ref=""1""&gt;John&lt;/author&gt;
        &lt;author ref=""2""&gt;Paul&lt;/author&gt;
        &lt;author ref=""3""&gt;George&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""D"" id=""100""&gt;
        &lt;title&gt;San&lt;/title&gt;
        &lt;author ref=""1""&gt;Ringo&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
  &lt;/books&gt;
 &lt;/Bookstore&gt; 
&lt;Bookstore id=""ID910715717""&gt;
    &lt;location&gt;bar&lt;/location&gt;
  &lt;books&gt;
    &lt;book category=""A"" id=""1""&gt;
        &lt;title&gt;Un&lt;/title&gt;
        &lt;author ref=""1""&gt;Winkin&lt;/author&gt;
        &lt;author&gt;Blinkin&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""B"" id=""10""&gt;
        &lt;title&gt;Deux&lt;/title&gt;
        &lt;author&gt;Nod&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""D"" id=""100""&gt;
        &lt;title&gt;Trois&lt;/title&gt;
        &lt;author&gt;Manny&lt;/author&gt;
        &lt;author&gt;Moe&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
  &lt;/books&gt;
 &lt;/Bookstore&gt; 
&lt;/Catalogue&gt;
</code></pre>

<p>I would like to extract all author names where:
1) the location element has a text value that contains ""NY""
2) the author element does NOT contain a ""ref"" attribute; that is where ref is not present in the author tag</p>

<p>I will ultimately need to concatenate the extracted authors together within a given bookstore, so that my resulting data frame is one row per store.  I'd like to preserve the bookstore id as an additional field in my data frame so that I can uniqely reference each store.
Since only the first bokstore is in NY, results from this simple example would look something like:</p>

<pre><code>1 Jane Smith John Doe Karl Pearson William Gosset
</code></pre>

<p>If another bookstore contained ""NY"" in its location, it would comprise the second row, and so forth.</p>

<p>Am I asking too much of R to parser under these convoluted conditions?</p>
","<pre><code>require(XML)

xdata &lt;- xmlParse(apptext)
xpathSApply(xdata,'//*/location[text()[contains(.,""NY"")]]/following-sibling::books/.//author[not(@ref)]')
#[[1]]
#&lt;author&gt;Jane Smith&lt;/author&gt; 

#[[2]]
#&lt;author&gt;John Doe&lt;/author&gt; 

#[[3]]
#&lt;author&gt;Karl Pearson&lt;/author&gt; 

#[[4]]
#&lt;author&gt;William Gosset&lt;/author&gt; 
</code></pre>

<p>Breakdown:</p>

<p>Get all locations containing 'NY'</p>

<pre><code>//*/location[text()[contains(.,""NY"")]]
</code></pre>

<p>Get the books sibling of these nodes</p>

<pre><code>/following-sibling::books
</code></pre>

<p>from these notes get all authors without a ref attribute</p>

<pre><code>/.//author[not(@ref)]
</code></pre>

<p>Use xmlValue if you want the text:</p>

<pre><code>&gt; xpathSApply(xdata,'//*/location[text()[contains(.,""NY"")]]/following-sibling::books/.//author[not(@ref)]',xmlValue)
[1] ""Jane Smith""     ""John Doe""       ""Karl Pearson""   ""William Gosset""
</code></pre>

<p>UPDATE:</p>

<pre><code>child.nodes &lt;- xpathSApply(xdata,'//*/location[text()[contains(.,""NY"")]]/following-sibling::books/.//author[not(@ref)]')

ans.func&lt;-function(x){
    xpathSApply(x,'.//ancestor::bookstore[@id]/@id')
}

sapply(child.nodes,ans.func)
# id  id  id  id 
#""1"" ""1"" ""1"" ""1"" 
</code></pre>

<p>UPDATE 2:</p>

<p>With your changed data</p>

<pre><code>xdata &lt;- '&lt;Catalogue&gt;
&lt;Bookstore id=""ID910705541""&gt;
  &lt;location&gt;foo bar&lt;/location&gt;
  &lt;books&gt;
    &lt;book category=""A"" id=""1""&gt;
        &lt;title&gt;Alpha&lt;/title&gt;
        &lt;author ref=""1""&gt;Matthew&lt;/author&gt;
        &lt;author&gt;Mark&lt;/author&gt;
        &lt;author&gt;Luke&lt;/author&gt;
        &lt;author ref=""2""&gt;John&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""B"" id=""10""&gt;
        &lt;title&gt;Beta&lt;/title&gt;
        &lt;author ref=""1""&gt;Huey&lt;/author&gt;
        &lt;author&gt;Duey&lt;/author&gt;
        &lt;author&gt;Louie&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""D"" id=""100""&gt;
        &lt;title&gt;Gamma&lt;/title&gt;
        &lt;author ref=""1""&gt;Tweedle Dee&lt;/author&gt;
        &lt;author ref=""2""&gt;Tweedle Dum&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
  &lt;/books&gt;
  &lt;/Bookstore&gt; 
&lt;Bookstore id=""ID910700051""&gt;
  &lt;location&gt;foo&lt;/location&gt;
  &lt;books&gt;
    &lt;book category=""A"" id=""1""&gt;
        &lt;title&gt;Happy&lt;/title&gt;
        &lt;author&gt;Dopey&lt;/author&gt;
        &lt;author&gt;Bashful&lt;/author&gt;
        &lt;author&gt;Doc&lt;/author&gt;
        &lt;author ref=""1""&gt;Grumpy&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""B"" id=""10""&gt;
        &lt;title&gt;Ni&lt;/title&gt;
        &lt;author ref=""1""&gt;John&lt;/author&gt;
        &lt;author ref=""2""&gt;Paul&lt;/author&gt;
        &lt;author ref=""3""&gt;George&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""D"" id=""100""&gt;
        &lt;title&gt;San&lt;/title&gt;
        &lt;author ref=""1""&gt;Ringo&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
  &lt;/books&gt;
 &lt;/Bookstore&gt; 
&lt;Bookstore id=""ID910715717""&gt;
    &lt;location&gt;bar&lt;/location&gt;
  &lt;books&gt;
    &lt;book category=""A"" id=""1""&gt;
        &lt;title&gt;Un&lt;/title&gt;
        &lt;author ref=""1""&gt;Winkin&lt;/author&gt;
        &lt;author&gt;Blinkin&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""B"" id=""10""&gt;
        &lt;title&gt;Deux&lt;/title&gt;
        &lt;author&gt;Nod&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book category=""D"" id=""100""&gt;
        &lt;title&gt;Trois&lt;/title&gt;
        &lt;author&gt;Manny&lt;/author&gt;
        &lt;author&gt;Moe&lt;/author&gt;
        &lt;year&gt;2005&lt;/year&gt;
        &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
  &lt;/books&gt;
 &lt;/Bookstore&gt; 
&lt;/Catalogue&gt;'
</code></pre>

<p>Note previously you had <code>bookstore</code> now <code>Bookstore</code>. <code>NY</code> is gone so I have used <code>foo</code></p>

<pre><code>require(XML)
xdata &lt;- xmlParse(xdata)
child.nodes &lt;- getNodeSet(xdata,'//*/location[text()[contains(.,""foo"")]]/following-sibling::books/.//author[not(@ref)]')

ans.func&lt;-function(x){
  xpathSApply(x,'.//ancestor::Bookstore[@id]/@id')
}

sapply(child.nodes,ans.func)
#           id            id            id            id            id 
#""ID910705541"" ""ID910705541"" ""ID910705541"" ""ID910705541"" ""ID910700051"" 
#           id            id 
#""ID910700051"" ""ID910700051""

xpathSApply(xdata,'//*/location[text()[contains(.,""foo"")]]/following-sibling::books/.//author[not(@ref)]',xmlValue)
# [1] ""Mark""    ""Luke""    ""Duey""    ""Louie""   ""Dopey""   ""Bashful"" ""Doc""    
</code></pre>
"
41971150,Add vline to geom_density and shade confidence interval of mean R,1,1,1,"<p>After reading through different posts, I found out how to add a vline of mean to density plots as shown <a href=""http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/"" rel=""nofollow noreferrer"">here</a>. 
Using the data provided in the above link:</p>

<p>1) How can one add 95% confidence intervals around the mean using geom_ribbon? 
CIs can be computed as </p>

<pre><code>#computation of the standard error of the mean
sem&lt;-sd(x)/sqrt(length(x))
#95% confidence intervals of the mean
c(mean(x)-2*sem,mean(x)+2*sem)
</code></pre>

<p>2) How can one limit the vline to the region under the curve? You will see in the picture below that vline plots outside the curve.</p>

<p>Sample data very close to my real problem can be found at <a href=""https://www.dropbox.com/s/bvvfdpgekbjyjh0/test.csv?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/bvvfdpgekbjyjh0/test.csv?dl=0</a></p>

<p><a href=""https://i.stack.imgur.com/1nlY3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1nlY3.png"" alt=""sample plot""></a></p>

<p><strong>UPDATE</strong></p>

<p>Using real data in the link above, I have tried the following using @beetroot's answer.</p>

<pre><code># Find the mean of each group
dat=me
library(dplyr)
library(plyr)
cdat &lt;- ddply(data,.(direction,cond), summarise, rating.mean=mean(rating,na.rm=T))# summarize by season and variable
cdat

#ggplot
p=ggplot(data,aes(x = rating)) + 
  geom_density(aes(colour = cond),size=1.3,adjust=4)+
  facet_grid(.~direction, scales=""free"")+
  xlab(NULL) + ylab(""Density"")
p=p+coord_cartesian(xlim = c(0, 130))+scale_color_manual(name="""",values=c(""blue"",""#00BA38"",""#F8766D""))+
  scale_fill_manual(values=c(""blue"", ""#00BA38"", ""#F8766D""))+
  theme(legend.title = element_text(colour=""black"", size=15, face=""plain""))+
  theme(legend.text = element_text(colour=""black"", size = 15, face = ""plain""))+
  theme(title = red.bold.italic.text, axis.title = red.bold.italic.text)+
  theme(strip.text.x = element_text(size=20, color=""black"",face=""plain""))+ # facet labels
  ggtitle(""SAMPLE A"") +theme(plot.title = element_text(size = 20, face = ""bold""))+
    theme(axis.text = blue.bold.italic.16.text)+ theme(legend.position = ""none"")+
  geom_vline(data=cdat, aes(xintercept=rating.mean, color=cond),linetype=""dotted"",size=1)
p
</code></pre>

<p><a href=""https://i.stack.imgur.com/3Qj6u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Qj6u.png"" alt=""sample plot from data""></a></p>

<pre><code>## implementing @beetroot's code to restrict lines under the curve and shade CIs around the mean
# I will use ddply for mean and CIs
cdat &lt;- ddply(data,.(direction,cond), summarise, rating.mean=mean(rating,na.rm=T),
              sem = sd(rating,na.rm=T)/sqrt(length(rating)),
              ci.low = mean(rating,na.rm=T) - 2*sem,
              ci.upp = mean(rating,na.rm=T) + 2*sem)# summarize by direction and variable


#In order to limit the lines to the outline of the curves you first need to find out which y values
#of the curves correspond to the means, e.g. by accessing the density values with ggplot_build and 
#using approx:

   cdat.dens &lt;- ggplot_build(ggplot(data, aes(x=rating, colour=cond)) +
                              facet_grid(.~direction, scales=""free"")+
                              geom_density(aes(colour = cond),size=1.3,adjust=4))$data[[1]] %&gt;%
  mutate(cond = ifelse(group==1, ""A"",
                       ifelse(group==2, ""B"",""C""))) %&gt;%
  left_join(cdat) %&gt;%
  select(y, x, cond, rating.mean, sem, ci.low, ci.upp) %&gt;%
  group_by(cond) %&gt;%
  mutate(dens.mean = approx(x, y, xout = rating.mean)[[2]],
         dens.cilow = approx(x, y, xout = ci.low)[[2]],
         dens.ciupp = approx(x, y, xout = ci.upp)[[2]]) %&gt;%
  select(-y, -x) %&gt;%
  slice(1)

 cdat.dens

#---
 #You can then combine everything with various geom_segments:

   ggplot(data, aes(x=rating, colour=cond)) +
   geom_density(data = data, aes(x = rating, colour = cond),size=1.3,adjust=4) +facet_grid(.~direction, scales=""free"")+
   geom_segment(data = cdat.dens, aes(x = rating.mean, xend = rating.mean, y = 0, yend = dens.mean, colour = cond),
                linetype = ""dashed"", size = 1) +
   geom_segment(data = cdat.dens, aes(x = ci.low, xend = ci.low, y = 0, yend = dens.cilow, colour = cond),
                linetype = ""dotted"", size = 1) +
   geom_segment(data = cdat.dens, aes(x = ci.upp, xend = ci.upp, y = 0, yend = dens.ciupp, colour = cond),
                linetype = ""dotted"", size = 1)
</code></pre>

<p>Gives this:</p>

<p><a href=""https://i.stack.imgur.com/dscCo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dscCo.png"" alt=""enter image description here""></a></p>

<p>You will notice the mean and CIs are not aligned as in the original plot. What am I not doing right @beetroot?</p>
","<p>Using the data from the link, you can calculate the mean, se and ci like so (I suggest using <code>dplyr</code>, the successor of <code>plyr</code>):</p>

<pre><code>set.seed(1234)
dat &lt;- data.frame(cond = factor(rep(c(""A"",""B""), each=200)), 
                  rating = c(rnorm(200),rnorm(200, mean=.8)))

library(ggplot2)
library(dplyr)
cdat &lt;- dat %&gt;%
  group_by(cond) %&gt;%
  summarise(rating.mean = mean(rating),
            sem = sd(rating)/sqrt(length(rating)),
            ci.low = mean(rating) - 2*sem,
            ci.upp = mean(rating) + 2*sem)
</code></pre>

<p>In order to limit the lines to the outline of the curves you first need to find out which y values of the curves correspond to the means, e.g. by accessing the density values with <code>ggplot_build</code> and using <code>approx</code>:</p>

<pre><code>cdat.dens &lt;- ggplot_build(ggplot(dat, aes(x=rating, colour=cond)) + geom_density())$data[[1]] %&gt;%
  mutate(cond = ifelse(group == 1, ""A"", ""B"")) %&gt;%
  left_join(cdat) %&gt;%
  select(y, x, cond, rating.mean, sem, ci.low, ci.upp) %&gt;%
  group_by(cond) %&gt;%
  mutate(dens.mean = approx(x, y, xout = rating.mean)[[2]],
         dens.cilow = approx(x, y, xout = ci.low)[[2]],
         dens.ciupp = approx(x, y, xout = ci.upp)[[2]]) %&gt;%
  select(-y, -x) %&gt;%
  slice(1)

&gt; cdat.dens
Source: local data frame [2 x 8]
Groups: cond [2]

   cond rating.mean        sem     ci.low     ci.upp dens.mean dens.cilow dens.ciupp
  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
1     A -0.05775928 0.07217200 -0.2021033 0.08658471 0.3865929   0.403623  0.3643583
2     B  0.87324927 0.07120697  0.7308353 1.01566320 0.3979347   0.381683  0.4096153
</code></pre>

<p>You can then combine everything with various <code>geom_segment</code>s:</p>

<pre><code>ggplot() +
  geom_density(data = dat, aes(x = rating, colour = cond)) +
  geom_segment(data = cdat.dens, aes(x = rating.mean, xend = rating.mean, y = 0, yend = dens.mean, colour = cond),
             linetype = ""dashed"", size = 1) +
  geom_segment(data = cdat.dens, aes(x = ci.low, xend = ci.low, y = 0, yend = dens.cilow, colour = cond),
             linetype = ""dotted"", size = 1) +
  geom_segment(data = cdat.dens, aes(x = ci.upp, xend = ci.upp, y = 0, yend = dens.ciupp, colour = cond),
               linetype = ""dotted"", size = 1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/IJJ6N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IJJ6N.png"" alt=""enter image description here""></a></p>

<p>As Axeman pointed out you can create a polygon based on the ribbon area as explained in <a href=""https://stackoverflow.com/questions/12429333/how-to-shade-a-region-under-a-curve-using-ggplot2"">this answer</a>.</p>

<p>So for your data you can subset and add the additional rows like so:</p>

<pre><code>ribbon &lt;- ggplot_build(ggplot(dat, aes(x=rating, colour=cond)) + geom_density())$data[[1]] %&gt;%
  mutate(cond = ifelse(group == 1, ""A"", ""B"")) %&gt;%
  left_join(cdat.dens) %&gt;%
  group_by(cond) %&gt;%
  filter(x &gt;= ci.low &amp; x &lt;= ci.upp) %&gt;%
  select(cond, x, y)

ribbon &lt;- rbind(data.frame(cond = c(""A"", ""B""), x = c(-0.2021033, 0.7308353), y = c(0, 0)), 
                as.data.frame(ribbon), 
                data.frame(cond = c(""A"", ""B""), x = c(0.08658471, 1.01566320), y = c(0, 0)))
</code></pre>

<p>And add <code>geom_polygon</code> to the plot:</p>

<pre><code>ggplot() +
  geom_polygon(data = ribbon, aes(x = x, y = y, fill = cond), alpha = .5) +
  geom_density(data = dat, aes(x = rating, colour = cond)) +
  geom_segment(data = cdat.dens, aes(x = rating.mean, xend = rating.mean, y = 0, yend = dens.mean, colour = cond),
             linetype = ""dashed"", size = 1) +
  geom_segment(data = cdat.dens, aes(x = ci.low, xend = ci.low, y = 0, yend = dens.cilow, colour = cond),
             linetype = ""dotted"", size = 1) +
  geom_segment(data = cdat.dens, aes(x = ci.upp, xend = ci.upp, y = 0, yend = dens.ciupp, colour = cond),
               linetype = ""dotted"", size = 1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/KVpk9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KVpk9.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here's the adapted code for your real data. It's just a bit tricky to incorporate two groups instead of one:</p>

<pre><code>cdat &lt;- dat %&gt;%
  group_by(direction, cond) %&gt;%
  summarise(rating.mean = mean(rating, na.rm = TRUE),
            sem = sd(rating, na.rm = TRUE)/sqrt(length(rating)),
            ci.low = mean(rating, na.rm = TRUE) - 2*sem,
            ci.upp = mean(rating, na.rm = TRUE) + 2*sem)

cdat.dens &lt;- ggplot_build(ggplot(dat, aes(x=rating, colour=interaction(direction, cond))) + geom_density())$data[[1]] %&gt;%
  mutate(cond = ifelse((group == 1 | group == 2 | group == 3 | group == 4), ""A"",
                        ifelse((group == 5 | group == 6 | group == 7 | group == 8), ""B"", ""C"")),
         direction = ifelse((group == 1 | group == 5 | group == 9), ""EAST"",
                            ifelse((group == 2 | group == 6 | group == 10), ""NORTH"",
                                   ifelse((group == 3 | group == 7 | group == 11), ""SOUTH"", ""WEST"")))) %&gt;%
  left_join(cdat) %&gt;%
  select(y, x, cond, direction, rating.mean, sem, ci.low, ci.upp) %&gt;%
  group_by(cond, direction) %&gt;%
  mutate(dens.mean = approx(x, y, xout = rating.mean)[[2]],
         dens.cilow = approx(x, y, xout = ci.low)[[2]],
         dens.ciupp = approx(x, y, xout = ci.upp)[[2]]) %&gt;%
  select(-y, -x) %&gt;%
  slice(1)

ggplot() +
  geom_density(data = dat, aes(x = rating, colour = cond)) +
  geom_segment(data = cdat.dens, aes(x = rating.mean, xend = rating.mean, y = 0, yend = dens.mean, colour = cond),
               linetype = ""dashed"", size = 1) +
  geom_segment(data = cdat.dens, aes(x = ci.low, xend = ci.low, y = 0, yend = dens.cilow, colour = cond),
               linetype = ""dotted"", size = 1) +
  geom_segment(data = cdat.dens, aes(x = ci.upp, xend = ci.upp, y = 0, yend = dens.ciupp, colour = cond),
               linetype = ""dotted"", size = 1) +
  facet_wrap(~direction)
</code></pre>

<p><a href=""https://i.stack.imgur.com/jPZOG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jPZOG.png"" alt=""enter image description here""></a></p>
"
41068012,Simplify this grid such that each row and column has 1 value,2,3,3,"<p>Example code here:</p>

<pre><code>&gt; temp2
  a b c d e f g h
i 1 1 0 0 0 1 0 1
j 0 1 0 0 0 1 0 1
k 0 1 1 0 0 1 1 1
l 0 0 0 0 1 0 0 1
m 0 0 1 1 0 0 1 1
n 0 0 1 1 0 0 1 1
o 0 0 0 1 0 0 1 1
p 0 0 0 0 1 0 0 1

&gt; dput(temp2)
structure(list(a = c(1, 0, 0, 0, 0, 0, 0, 0), b = c(1, 1, 1, 
0, 0, 0, 0, 0), c = c(0, 0, 1, 0, 1, 1, 0, 0), d = c(0, 0, 0, 
0, 1, 1, 1, 0), e = c(0, 0, 0, 1, 0, 0, 0, 1), f = c(1, 1, 1, 
0, 0, 0, 0, 0), g = c(0, 0, 1, 0, 1, 1, 1, 0), h = c(1, 1, 1, 
1, 1, 1, 1, 1)), .Names = c(""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", 
""h""), class = ""data.frame"", row.names = c(""i"", ""j"", ""k"", ""l"", 
""m"", ""n"", ""o"", ""p""))
</code></pre>

<p>I have this 8x8 grid of 1s and 0s. I need to solve for some grid where each row and each column has exactly one 1 and the rest 0s, but the 1 has to be in a place where the original grid has a 1. It's almost like a sudoku question but not exactly. Any thoughts on how to get started?</p>

<p>I would need some function that can do this for a general grid, not simply this specific one. We can assume that there's always a solution grid, given some starting grid.</p>

<p>Thanks!</p>

<p>Edit: a valid solution </p>

<pre><code>&gt; temp3
  a b c d e f g h
i 1 0 0 0 0 0 0 0
j 0 1 0 0 0 0 0 0
k 0 0 0 0 0 1 0 0
l 0 0 0 0 1 0 0 0
m 0 0 0 1 0 0 0 0
n 0 0 1 0 0 0 0 0
o 0 0 0 0 0 0 1 0
p 0 0 0 0 0 0 0 1
</code></pre>

<p>EDIT2: given that there's only 8! unique solutions for any grid, i may attempt a brute force / matching approach. </p>
","<p>This can be solved as a transportation problem or as an integer programming problem.  We also show a one-line solution using only base R which generates random matrices for which each row and each columns column sums to 1 filtering out and returning the ones satisfying the additional constraints that each element of the solution matrix be less than or equal to the corresponding element of <code>temp2</code>.</p>

<p><strong>1) transportation problem</strong> Using <code>lp.transport</code> in lpSolve we can solve it in one statement:</p>

<pre><code>library(lpSolve)

res &lt;- lp.transport(as.matrix(temp2), ""max"", 
  rep(""="", 8), rep(1, 8), rep(""="", 8), rep(1, 8), integers = 0:1)

res
## Success: the objective function is 8

soln &lt;- array(res$solution, dim(temp2))

# verify

all(colSums(soln)==1) &amp;&amp; all(rowSums(soln)==1) &amp;&amp; all(temp2&gt;=soln) &amp;&amp; all(soln %in% 0:1)
## [1] TRUE
</code></pre>

<p><strong>2) integer programming</strong></p>

<p>If X is the solution we have specified the row and column constraints but have not specified the X &lt;= temp2 constraints since they will be satisfied automatically as no solution putting a 1 where a temp2 0 is can have the maximum objective of 8.</p>

<pre><code>library(lpSolve)

n &lt;- nrow(temp2)
obj &lt;- unlist(temp2)
const_row &lt;- t(sapply(1:n, function(i) c(row(temp2)) == i)) # each row sums to 1
const_col &lt;- t(sapply(1:n, function(i) c(col(temp2)) == i)) # each col sums to 1
const.mat &lt;- rbind(const_row, const_col)
res &lt;- lp(""max"", obj, const.mat, ""="", 1, all.bin = TRUE)
res
## Success: the objective function is 8

soln &lt;- array(res$solution, dim(temp2))

# verify

all(colSums(soln)==1) &amp;&amp; all(rowSums(soln)==1) &amp;&amp; all(temp2&gt;=soln) &amp;&amp; all(soln %in% 0:1)
## [1] TRUE
</code></pre>

<p>(Note that by the same argument we could have relaxed the problem to a linear programming problem provided we add 0 &lt;= soln[i, j] &lt;= 1 constraints since by the same argument that allowed us to omit the soln[i, j] &lt;= temp2[i, j] constraints the maximization will force the soln elements to be 0 or 1 anyways.)</p>

<p><strong>2a)</strong> This approach is longer but does spell out the X &lt;= temp2 constraints explicitly:</p>

<pre><code>n &lt;- nrow(temp2)
obj &lt;- numeric(n*n)
const1 &lt;- diag(n*n) # soln[i,j] &lt;= temp2[i,j]
const2 &lt;- t(sapply(1:n, function(i) c(row(temp2)) == i)) # each row sums to 1
const3 &lt;- t(sapply(1:n, function(i) c(col(temp2)) == i)) # each col sums to 1
const.mat &lt;- rbind(const1, const2, const3)
const.dir &lt;- rep(c(""&lt;="", ""=""), c(n*n, 2*n))
const.rhs &lt;- c(unlist(temp2), rep(1, 2*n))

res &lt;- lp(""max"", obj, const.mat, const.dir, const.rhs, all.bin = TRUE)
res
## Success: the objective function is 0

soln &lt;- array(res$solution, dim(temp2))

# verify

all(colSums(soln)==1) &amp;&amp; all(rowSums(soln)==1) &amp;&amp; all(temp2&gt;=soln) &amp;&amp; all(soln %in% 0:1)
## [1] TRUE
</code></pre>

<p><strong>2b)</strong> Note that if X is the solution matrix then in X &lt;= temp2 only the positions of X corresponding to zeros in temp2 actually constrain so we could eliminate any constraint corresponding to a 1 in temp2 in the (2a) solution.  With this change all constraints become equality constraints.</p>

<pre><code>n &lt;- nrow(temp2)
obj &lt;- numeric(n*n)
const1 &lt;- diag(n*n)[unlist(temp2) == 0, ]
const2 &lt;- t(sapply(1:n, function(i) c(row(temp2)) == i)) # each row sums to 1
const3 &lt;- t(sapply(1:n, function(i) c(col(temp2)) == i)) # each col sums to 1
const.mat &lt;- rbind(const1, const2, const3)
const.dir &lt;- ""=""
const.rhs &lt;- c(numeric(nrow(const1)), rep(1, 2*n))

res &lt;- lp(""max"", obj, const.mat, const.dir, const.rhs, all.bin = TRUE)
res
## Success: the objective function is 0

soln &lt;- array(res$solution, dim(temp2))

# verify

all(colSums(soln)==1) &amp;&amp; all(rowSums(soln)==1) &amp;&amp; all(temp2&gt;=soln) &amp;&amp; all(soln %in% 0:1)
## [1] TRUE
</code></pre>

<p>In fact, we could go further and remove the variables that correspond to zero elements of <code>temp2</code>.</p>

<p><strong>3) r2dtable</strong>  Here we use <code>rd2table to generate 10,000 8x8 tables whose rows and columns sum to 1 and then filter them to pick out only those satisfying the  X &lt; temp2 constrainsts.  With</code>temp2` from the question and the random seed shown has found 3 solutions.  If with different inputs it finds no solutions then try generating a higher number of random proposals.  This approach does not use any packages.</p>

<pre><code>set.seed(123) # for reproducibility
Filter(function(x) all(x &lt;= temp2), r2dtable(10000, rep(1, 8), rep(1, 8)))
</code></pre>

<p>giving:</p>

<pre><code>[[1]]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    0    0    0    0    0    0    0
[2,]    0    0    0    0    0    1    0    0
[3,]    0    1    0    0    0    0    0    0
[4,]    0    0    0    0    0    0    0    1
[5,]    0    0    0    0    0    0    1    0
[6,]    0    0    1    0    0    0    0    0
[7,]    0    0    0    1    0    0    0    0
[8,]    0    0    0    0    1    0    0    0

[[2]]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    0    0    0    0    0    0    0
[2,]    0    0    0    0    0    1    0    0
[3,]    0    1    0    0    0    0    0    0
[4,]    0    0    0    0    1    0    0    0
[5,]    0    0    0    1    0    0    0    0
[6,]    0    0    1    0    0    0    0    0
[7,]    0    0    0    0    0    0    1    0
[8,]    0    0    0    0    0    0    0    1

[[3]]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    0    0    0    0    0    0    0
[2,]    0    1    0    0    0    0    0    0
[3,]    0    0    0    0    0    1    0    0
[4,]    0    0    0    0    1    0    0    0
[5,]    0    0    1    0    0    0    0    0
[6,]    0    0    0    0    0    0    1    0
[7,]    0    0    0    1    0    0    0    0
[8,]    0    0    0    0    0    0    0    1
</code></pre>
"
27439021,Import data into R - argument is empty,2,2,2,"<p>I am trying to use a R package called GOSemSim, it requires to import a lot of data into variables with a specific format like this:</p>

<pre><code>data1 = c(""one"", ""two"", ""three"")
data2 = c(""A"", ""B"", ""C"")
</code></pre>

<p>When the list of data that I try to import into a variable is longer than 293 then I get the following error message:</p>

<pre><code>argument 293 is empty
</code></pre>

<p>THere are no error with the """" or comma, I computed it with linux, it does not matter what data it is.</p>

<p>This is really weird basically, I tried on two computers with no luck. I tried to import it as a CSV file but the R package won't allow it.</p>

<p>Anyone knows why you cannot import more than 293 data?</p>

<p>Update:
Here is the code and my data at the same time, it is a one liner in R which has never been a problem for me!</p>

<pre><code>OQ = c(""GO:0000003"", ""GO:0000070"", ""GO:0000077"", ""GO:0000079"", ""GO:0000082"", ""GO:0000086"", ""GO:0000122"", ""GO:0000212"", ""GO:0000226"", ""GO:0000278"", ""GO:0000279"", ""GO:0000280"", ""GO:0000724"", ""GO:0000725"", ""GO:0000819"", ""GO:0000910"", ""GO:0001932"", ""GO:0002118"", ""GO:0002121"", ""GO:0002165"", ""GO:0003002"", ""GO:0003006"", ""GO:0006022"", ""GO:0006030"", ""GO:0006040"", ""GO:0006139"", ""GO:0006259"", ""GO:0006260"", ""GO:0006261"", ""GO:0006267"", ""GO:0006270"", ""GO:0006275"", ""GO:0006277"", ""GO:0006281"", ""GO:0006302"", ""GO:0006304"", ""GO:0006305"", ""GO:0006306"", ""GO:0006310"", ""GO:0006323"", ""GO:0006325"", ""GO:0006342"", ""GO:0006351"", ""GO:0006355"", ""GO:0006357"", ""GO:0006366"", ""GO:0006464"", ""GO:0006468"", ""GO:0006479"", ""GO:0006725"", ""GO:0006807"", ""GO:0006928"", ""GO:0006950"", ""GO:0006974"", ""GO:0006996"", ""GO:0007010"", ""GO:0007017"", ""GO:0007018"", ""GO:0007049"", ""GO:0007051"", ""GO:0007059"", ""GO:0007062"", ""GO:0007067"", ""GO:0007076"", ""GO:0007088"", ""GO:0007093"", ""GO:0007095"", ""GO:0007098"", ""GO:0007126"", ""GO:0007127"", ""GO:0007131"", ""GO:0007140"", ""GO:0007141"", ""GO:0007143"", ""GO:0007154"", ""GO:0007155"", ""GO:0007156"", ""GO:0007259"", ""GO:0007266"", ""GO:0007275"", ""GO:0007276"", ""GO:0007281"", ""GO:0007282"", ""GO:0007292"", ""GO:0007304"", ""GO:0007307"", ""GO:0007346"", ""GO:0007350"", ""GO:0007365"", ""GO:0007367"", ""GO:0007379"", ""GO:0007389"", ""GO:0007399"", ""GO:0007400"", ""GO:0007417"", ""GO:0007420"", ""GO:0007423"", ""GO:0007444"", ""GO:0007472"", ""GO:0007476"", ""GO:0007552"", ""GO:0007560"", ""GO:0008104"", ""GO:0008213"", ""GO:0008283"", ""GO:0008284"", ""GO:0008315"", ""GO:0008356"", ""GO:0009059"", ""GO:0009611"", ""GO:0009653"", ""GO:0009790"", ""GO:0009791"", ""GO:0009880"", ""GO:0009886"", ""GO:0009887"", ""GO:0009888"", ""GO:0009889"", ""GO:0009890"", ""GO:0009892"", ""GO:0009893"", ""GO:0009896"", ""GO:0009968"", ""GO:0009987"", ""GO:0010032"", ""GO:0010033"", ""GO:0010092"", ""GO:0010389"", ""GO:0010468"", ""GO:0010498"", ""GO:0010556"", ""GO:0010558"", ""GO:0010564"", ""GO:0010604"", ""GO:0010605"", ""GO:0010608"", ""GO:0010629"", ""GO:0010648"", ""GO:0010948"", ""GO:0014016"", ""GO:0014017"", ""GO:0014070"", ""GO:0016043"", ""GO:0016055"", ""GO:0016070"", ""GO:0016310"", ""GO:0016319"", ""GO:0016321"", ""GO:0016441"", ""GO:0016458"", ""GO:0016568"", ""GO:0016569"", ""GO:0016570"", ""GO:0016571"", ""GO:0016572"", ""GO:0017145"", ""GO:0018130"", ""GO:0019219"", ""GO:0019222"", ""GO:0019438"", ""GO:0019827"", ""GO:0019953"", ""GO:0022402"", ""GO:0022403"", ""GO:0022404"", ""GO:0022412"", ""GO:0022414"", ""GO:0022610"", ""GO:0023052"", ""GO:0023057"", ""GO:0030111"", ""GO:0030154"", ""GO:0030178"", ""GO:0030182"", ""GO:0030261"", ""GO:0030422"", ""GO:0030703"", ""GO:0030727"", ""GO:0031023"", ""GO:0031047"", ""GO:0031050"", ""GO:0031056"", ""GO:0031060"", ""GO:0031123"", ""GO:0031145"", ""GO:0031175"", ""GO:0031323"", ""GO:0031324"", ""GO:0031325"", ""GO:0031326"", ""GO:0031327"", ""GO:0031331"", ""GO:0031398"", ""GO:0031399"", ""GO:0031401"", ""GO:0031570"", ""GO:0031572"", ""GO:0031935"", ""GO:0032268"", ""GO:0032270"", ""GO:0032501"", ""GO:0032502"", ""GO:0032504"", ""GO:0032507"", ""GO:0032774"", ""GO:0032776"", ""GO:0032886"", ""GO:0033043"", ""GO:0033044"", ""GO:0033260"", ""GO:0033301"", ""GO:0033554"", ""GO:0034622"", ""GO:0034641"", ""GO:0034645"", ""GO:0034654"", ""GO:0034754"", ""GO:0034968"", ""GO:0035023"", ""GO:0035107"", ""GO:0035114"", ""GO:0035120"", ""GO:0035186"", ""GO:0035194"", ""GO:0035195"", ""GO:0035220"", ""GO:0035282"", ""GO:0035295"", ""GO:0035825"", ""GO:0036211"", ""GO:0036388"", ""GO:0040029"", ""GO:0042060"", ""GO:0042221"", ""GO:0042445"", ""GO:0043009"", ""GO:0043066"", ""GO:0043069"", ""GO:0043161"", ""GO:0043170"", ""GO:0043331"", ""GO:0043412"", ""GO:0043414"", ""GO:0043549"", ""GO:0043631"", ""GO:0043933"", ""GO:0044237"", ""GO:0044249"", ""GO:0044260"", ""GO:0044271"", ""GO:0044419"", ""GO:0044700"", ""GO:0044702"", ""GO:0044703"", ""GO:0044707"", ""GO:0044728"", ""GO:0044763"", ""GO:0044767"", ""GO:0044770"", ""GO:0044771"", ""GO:0044772"", ""GO:0044773"", ""GO:0044774"", ""GO:0044786"", ""GO:0044818"", ""GO:0044839"", ""GO:0044843"", ""GO:0044848"", ""GO:0045132"", ""GO:0045165"", ""GO:0045168"", ""GO:0045185"", ""GO:0045448"", ""GO:0045455"", ""GO:0045787"", ""GO:0045814"", ""GO:0045859"", ""GO:0045892"", ""GO:0045931"", ""GO:0045934"", ""GO:0046331"", ""GO:0046425"", ""GO:0046483"", ""GO:0046580"", ""GO:0046605"", ""GO:0046777"", ""GO:0048070"", ""GO:0048134"", ""GO:0048135"", ""GO:0048285"", ""GO:0048311"", ""GO:0048468"", ""GO:0048477"", ""GO:0048513"", ""GO:0048518"", ""GO:0048519"", ""GO:0048522"", ""GO:0048523"", ""GO:0048563"", ""GO:0048569"", ""GO:0048583"", ""GO:0048585"", ""GO:0048609"", ""GO:0048646"", ""GO:0048666"", ""GO:0048699"", ""GO:0048704"", ""GO:0048705"", ""GO:0048706"", ""GO:0048707"", ""GO:0048731"", ""GO:0048736"", ""GO:0048737"", ""GO:0048754"", ""GO:0048856"", ""GO:0048863"", ""GO:0048865"", ""GO:0048867"", ""GO:0048869"", ""GO:0050789"", ""GO:0050793"", ""GO:0050794"", ""GO:0050896"", ""GO:0051052"", ""GO:0051058"", ""GO:0051128"", ""GO:0051171"", ""GO:0051172"", ""GO:0051225"", ""GO:0051235"", ""GO:0051246"", ""GO:0051247"", ""GO:0051252"", ""GO:0051253"", ""GO:0051276"", ""GO:0051297"", ""GO:0051299"", ""GO:0051301"", ""GO:0051302"", ""GO:0051321"", ""GO:0051325"", ""GO:0051329"", ""GO:0051338"", ""GO:0051351"", ""GO:0051443"", ""GO:0051445"", ""GO:0051641"", ""GO:0051646"", ""GO:0051651"", ""GO:0051704"", ""GO:0051716"", ""GO:0051726"", ""GO:0051783"", ""GO:0051785"", ""GO:0060255"", ""GO:0060429"", ""GO:0060548"", ""GO:0060688"", ""GO:0060966"", ""GO:0060968"", ""GO:0060993"", ""GO:0061138"", ""GO:0065003"", ""GO:0065004"", ""GO:0065007"", ""GO:0070192"", ""GO:0070507"", ""GO:0070887"", ""GO:0070918"", ""GO:0071103"", ""GO:0071359"", ""GO:0071822"", ""GO:0071824"", ""GO:0071840"", ""GO:0071897"", ""GO:0071900"", ""GO:0072028"", ""GO:0072078"", ""GO:0072079"", ""GO:0072088"", ""GO:0080090"", ""GO:0090068"", ""GO:0090304"", ""GO:0090306"", ""GO:0098609"", ""GO:1901071"", ""GO:1901360"", ""GO:1901362"", ""GO:1901576"", ""GO:1901987"", ""GO:1901988"", ""GO:1901990"", ""GO:1901991"", ""GO:1902275"", ""GO:1902299"", ""GO:1902589"", ""GO:1902679"", ""GO:1902749"", ""GO:1903046"", ""GO:1903047"", ""GO:1903308"", ""GO:1903322"", ""GO:2000026"", ""GO:2000112"", ""GO:2000113"", ""GO:2001141"")
</code></pre>
","<p>The error message in itself is informative. If one tries to make it reproducible, it's best to work with small subsets. It usually helps to have a dead stare at your data before trying to reproduce the behavior. For example,</p>

<pre><code>OQ = c(""GO:0000003"", ""GO:2001141"", )
</code></pre>

<p>Notice that there are two elements of this character vector. Or are they?</p>

<pre><code>Error in c(""GO:0000003"", ""GO:2001141"", ) : argument 3 is empty
</code></pre>

<p>Number 3 is the key. R is expecting three elements. Notice the comma after the second element. Once you remove it, you'll be able to create the <code>QQ</code> variable. Scan your real example. I'm sure there's a <code>, ,</code> somewhere.</p>

<p><em>EDIT</em></p>

<p>I tried copy/pasting your code into a script in Rstudio and it produced the error you describe. If you scroll right, you'll notice that syntax coloring is not working at around position 5000. I have folded the code so that it fits on screen and it runs fine.</p>

<p>This is how I folded the vector and it worked.</p>

<pre><code>OQ = c(""GO:0000003"", ""GO:0000070"", ""GO:0000077"", ""GO:0000079"", ""GO:0000082"", ""GO:0000086"", ""GO:0000122"",
       ""GO:0000212"", ""GO:0000226"", ""GO:0000278"", ""GO:0000279"", ""GO:0000280"", ""GO:0000724"", ""GO:0000725"", 
       ""GO:0000819"", ""GO:0000910"", ""GO:0001932"", ""GO:0002118"", ""GO:0002121"", ""GO:0002165"", ""GO:0003002"", 
       ""GO:0003006"", ""GO:0006022"", ""GO:0006030"", ""GO:0006040"", ""GO:0006139"", ""GO:0006259"", ""GO:0006260"", 
       ""GO:0006261"", ""GO:0006267"", ""GO:0006270"", ""GO:0006275"", ""GO:0006277"", ""GO:0006281"", ""GO:0006302"", 
       ""GO:0006304"", ""GO:0006305"", ""GO:0006306"", ""GO:0006310"", ""GO:0006323"", ""GO:0006325"", ""GO:0006342"", 
       ""GO:0006351"", ""GO:0006355"", ""GO:0006357"", ""GO:0006366"", ""GO:0006464"", ""GO:0006468"", ""GO:0006479"", 
       ""GO:0006725"", ""GO:0006807"", ""GO:0006928"", ""GO:0006950"", ""GO:0006974"", ""GO:0006996"", ""GO:0007010"", 
       ""GO:0007017"", ""GO:0007018"", ""GO:0007049"", ""GO:0007051"", ""GO:0007059"", ""GO:0007062"", ""GO:0007067"", 
       ""GO:0007076"", ""GO:0007088"", ""GO:0007093"", ""GO:0007095"", ""GO:0007098"", ""GO:0007126"", ""GO:0007127"", 
       ""GO:0007131"", ""GO:0007140"", ""GO:0007141"", ""GO:0007143"", ""GO:0007154"", ""GO:0007155"", ""GO:0007156"", 
       ""GO:0007259"", ""GO:0007266"", ""GO:0007275"", ""GO:0007276"", ""GO:0007281"", ""GO:0007282"", ""GO:0007292"", 
       ""GO:0007304"", ""GO:0007307"", ""GO:0007346"", ""GO:0007350"", ""GO:0007365"", ""GO:0007367"", ""GO:0007379"", 
       ""GO:0007389"", ""GO:0007399"", ""GO:0007400"", ""GO:0007417"", ""GO:0007420"", ""GO:0007423"", ""GO:0007444"", 
       ""GO:0007472"", ""GO:0007476"", ""GO:0007552"", ""GO:0007560"", ""GO:0008104"", ""GO:0008213"", ""GO:0008283"", 
       ""GO:0008284"", ""GO:0008315"", ""GO:0008356"", ""GO:0009059"", ""GO:0009611"", ""GO:0009653"", ""GO:0009790"", 
       ""GO:0009791"", ""GO:0009880"", ""GO:0009886"", ""GO:0009887"", ""GO:0009888"", ""GO:0009889"", ""GO:0009890"", 
       ""GO:0009892"", ""GO:0009893"", ""GO:0009896"", ""GO:0009968"", ""GO:0009987"", ""GO:0010032"", ""GO:0010033"", 
       ""GO:0010092"", ""GO:0010389"", ""GO:0010468"", ""GO:0010498"", ""GO:0010556"", ""GO:0010558"", ""GO:0010564"", 
       ""GO:0010604"", ""GO:0010605"", ""GO:0010608"", ""GO:0010629"", ""GO:0010648"", ""GO:0010948"", ""GO:0014016"", 
       ""GO:0014017"", ""GO:0014070"", ""GO:0016043"", ""GO:0016055"", ""GO:0016070"", ""GO:0016310"", ""GO:0016319"", 
       ""GO:0016321"", ""GO:0016441"", ""GO:0016458"", ""GO:0016568"", ""GO:0016569"", ""GO:0016570"", ""GO:0016571"", 
       ""GO:0016572"", ""GO:0017145"", ""GO:0018130"", ""GO:0019219"", ""GO:0019222"", ""GO:0019438"", ""GO:0019827"", 
       ""GO:0019953"", ""GO:0022402"", ""GO:0022403"", ""GO:0022404"", ""GO:0022412"", ""GO:0022414"", ""GO:0022610"", 
       ""GO:0023052"", ""GO:0023057"", ""GO:0030111"", ""GO:0030154"", ""GO:0030178"", ""GO:0030182"", ""GO:0030261"", 
       ""GO:0030422"", ""GO:0030703"", ""GO:0030727"", ""GO:0031023"", ""GO:0031047"", ""GO:0031050"", ""GO:0031056"", 
       ""GO:0031060"", ""GO:0031123"", ""GO:0031145"", ""GO:0031175"", ""GO:0031323"", ""GO:0031324"", ""GO:0031325"", 
       ""GO:0031326"", ""GO:0031327"", ""GO:0031331"", ""GO:0031398"", ""GO:0031399"", ""GO:0031401"", ""GO:0031570"", 
       ""GO:0031572"", ""GO:0031935"", ""GO:0032268"", ""GO:0032270"", ""GO:0032501"", ""GO:0032502"", ""GO:0032504"", 
       ""GO:0032507"", ""GO:0032774"", ""GO:0032776"", ""GO:0032886"", ""GO:0033043"", ""GO:0033044"", ""GO:0033260"", 
       ""GO:0033301"", ""GO:0033554"", ""GO:0034622"", ""GO:0034641"", ""GO:0034645"", ""GO:0034654"", ""GO:0034754"", 
       ""GO:0034968"", ""GO:0035023"", ""GO:0035107"", ""GO:0035114"", ""GO:0035120"", ""GO:0035186"", ""GO:0035194"", 
       ""GO:0035195"", ""GO:0035220"", ""GO:0035282"", ""GO:0035295"", ""GO:0035825"", ""GO:0036211"", ""GO:0036388"", 
       ""GO:0040029"", ""GO:0042060"", ""GO:0042221"", ""GO:0042445"", ""GO:0043009"", ""GO:0043066"", ""GO:0043069"", 
       ""GO:0043161"", ""GO:0043170"", ""GO:0043331"", ""GO:0043412"", ""GO:0043414"", ""GO:0043549"", ""GO:0043631"", 
       ""GO:0043933"", ""GO:0044237"", ""GO:0044249"", ""GO:0044260"", ""GO:0044271"", ""GO:0044419"", ""GO:0044700"", 
       ""GO:0044702"", ""GO:0044703"", ""GO:0044707"", ""GO:0044728"", ""GO:0044763"", ""GO:0044767"", ""GO:0044770"", 
       ""GO:0044771"", ""GO:0044772"", ""GO:0044773"", ""GO:0044774"", ""GO:0044786"", ""GO:0044818"", ""GO:0044839"", 
       ""GO:0044843"", ""GO:0044848"", ""GO:0045132"", ""GO:0045165"", ""GO:0045168"", ""GO:0045185"", ""GO:0045448"", 
       ""GO:0045455"", ""GO:0045787"", ""GO:0045814"", ""GO:0045859"", ""GO:0045892"", ""GO:0045931"", ""GO:0045934"", 
       ""GO:0046331"", ""GO:0046425"", ""GO:0046483"", ""GO:0046580"", ""GO:0046605"", ""GO:0046777"", ""GO:0048070"", 
       ""GO:0048134"", ""GO:0048135"", ""GO:0048285"", ""GO:0048311"", ""GO:0048468"", ""GO:0048477"", ""GO:0048513"", 
       ""GO:0048518"", ""GO:0048519"", ""GO:0048522"", ""GO:0048523"", ""GO:0048563"", ""GO:0048569"", ""GO:0048583"", 
       ""GO:0048585"", ""GO:0048609"", ""GO:0048646"", ""GO:0048666"", ""GO:0048699"", ""GO:0048704"", ""GO:0048705"", 
       ""GO:0048706"", ""GO:0048707"", ""GO:0048731"", ""GO:0048736"", ""GO:0048737"", ""GO:0048754"", ""GO:0048856"", 
       ""GO:0048863"", ""GO:0048865"", ""GO:0048867"", ""GO:0048869"", ""GO:0050789"", ""GO:0050793"", ""GO:0050794"", 
       ""GO:0050896"", ""GO:0051052"", ""GO:0051058"", ""GO:0051128"", ""GO:0051171"", ""GO:0051172"", ""GO:0051225"", 
       ""GO:0051235"", ""GO:0051246"", ""GO:0051247"", ""GO:0051252"", ""GO:0051253"", ""GO:0051276"", ""GO:0051297"", 
       ""GO:0051299"", ""GO:0051301"", ""GO:0051302"", ""GO:0051321"", ""GO:0051325"", ""GO:0051329"", ""GO:0051338"", 
       ""GO:0051351"", ""GO:0051443"", ""GO:0051445"", ""GO:0051641"", ""GO:0051646"", ""GO:0051651"", ""GO:0051704"", 
       ""GO:0051716"", ""GO:0051726"", ""GO:0051783"", ""GO:0051785"", ""GO:0060255"", ""GO:0060429"", ""GO:0060548"", 
       ""GO:0060688"", ""GO:0060966"", ""GO:0060968"", ""GO:0060993"", ""GO:0061138"", ""GO:0065003"", ""GO:0065004"", 
       ""GO:0065007"", ""GO:0070192"", ""GO:0070507"", ""GO:0070887"", ""GO:0070918"", ""GO:0071103"", ""GO:0071359"", 
       ""GO:0071822"", ""GO:0071824"", ""GO:0071840"", ""GO:0071897"", ""GO:0071900"", ""GO:0072028"", ""GO:0072078"", 
       ""GO:0072079"", ""GO:0072088"", ""GO:0080090"", ""GO:0090068"", ""GO:0090304"", ""GO:0090306"", ""GO:0098609"", 
       ""GO:1901071"", ""GO:1901360"", ""GO:1901362"", ""GO:1901576"", ""GO:1901987"", ""GO:1901988"", ""GO:1901990"", 
       ""GO:1901991"", ""GO:1902275"", ""GO:1902299"", ""GO:1902589"", ""GO:1902679"", ""GO:1902749"", ""GO:1903046"", 
       ""GO:1903047"", ""GO:1903308"", ""GO:1903322"", ""GO:2000026"", ""GO:2000112"", ""GO:2000113"", ""GO:2001141"")
</code></pre>
"
36280673,Scraping headlines and dates from Yahoo Finance using R,2,2,4,"<p>I'm trying to scrape news with R from Yahoo Finance webpage to build a table with two columns: date and news headlines.
Following the instructions from <a href=""https://stackoverflow.com/questions/19030229/yahoo-finance-headlines-webpage-scraping-with-r"">here</a> I correctly create a column with news headlines; next step is to get the date and add it as a column to the table.</p>

<p>I guess I need just to modify this command:</p>

<pre><code>out_dt &lt;- xpathSApply(d, ""//ul[contains(@class,'newsheadlines')]/following::ul/li/a"", xmlValue)
</code></pre>

<p>in order to get the date instead of the headlines from, as an example, this code:</p>

<pre><code>&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8""&gt;&lt;title&gt;BMPS.MI Headlines | BANCA MPS Stock - Yahoo! Finance&lt;/title&gt;&lt;script type=""text/javascript"" src=""http://l.yimg.com/a/i/us/fi/03rd/yg_csstare_nobgcolor.js""&gt;&lt;/script&gt;&lt;link rel=""stylesheet"" href=""http://l.yimg.com/zz/combo?kx/yucs/uh3/uh/1138/css/uh_non_mail-min.css&amp;amp;kx/yucs/uh3s/atomic/84/css/atomic-min.css&amp;amp;kx/yucs/uh_common/meta/3/css/meta-min.css&amp;amp;kx/yucs/uh3/top-bar/366/css/no_icons-min.css&amp;amp;kx/yucs/uh3/search/css/588/blue_border-min.css&amp;amp;kx/yucs/uh3/get-the-app/151/css/get_the_app-min.css&amp;amp;bm/lib/fi/common/p/d/static/css/2.0.356981/2.0.0/mini/yfi_yoda_legacy_lego_concat.css&amp;amp;bm/lib/fi/common/p/d/static/css/2.0.356981/2.0.0/mini/yfi_symbol_suggest.css&amp;amp;bm/lib/fi/common/p/d/static/css/2.0.356981/2.0.0/mini/yui_helper.css&amp;amp;bm/lib/fi/common/p/d/static/css/2.0.356981/2.0.0/mini/yfi_theme_teal.css&amp;amp;bm/lib/fi/common/p/d/static/css/2.0.356981/2.0.0/mini/yfi_follow_quote.css&amp;amp;bm/lib/fi/common/p/d/static/css/2.0.356981/2.0.0/mini/yfi_follow_stencil.css"" type=""text/css""&gt;&lt;script language=""javascript""&gt;
ll_js = new Array();
&lt;/script&gt;&lt;script type=""text/javascript"" src=""http://l1.yimg.com/bm/combo?fi/common/p/d/static/js/2.0.356981/2.0.0/mini/yui-min-3.9.1.js&amp;amp;fi/common/p/d/static/js/2.0.356981/yui_2.8.0/build/yuiloader-dom-event/2.0.0/mini/yuiloader-dom-event.js&amp;amp;fi/common/p/d/static/js/2.0.356981/yui_2.8.0/build/container/2.0.0/mini/container.js&amp;amp;fi/common/p/d/static/js/2.0.356981/yui_2.8.0/build/datasource/2.0.0/mini/datasource.js&amp;amp;fi/common/p/d/static/js/2.0.356981/yui_2.8.0/build/autocomplete/2.0.0/mini/autocomplete.js""&gt;&lt;/script&gt;&lt;script language=""javascript""&gt;
YUI.YUICfg = {""base"":""http:\/\/l.yimg.com\/"",""comboBase"":""http:\/\/l.yimg.com\/zz\/combo?"",""combine"":true,""allowRollup"":true,""maxURLLength"":""2000""}
YUI.YUICfg.root = 'yui:'+YUI.version+'/build/';
YUI.applyConfig(YUI.YUICfg); 
&lt;/script&gt;&lt;script language=""javascript""&gt;
ll_js.push({
    'success_callback' : function() {
            YUI().use('stencil', 'follow-quote', 'node', function (Y) {
                var conf = {'xhrBase': '/', 'lang': 'en-US', 'region': 'US', 'loginUrl': 'https://login.yahoo.com/config/login_verify2?&amp;.done=http://finance.yahoo.com/q?s=BMPS.MI&amp;.intl=us'};

                Y.Media.FollowQuote.init(conf, function () {
                    var exchNode = null,
                        followSecClass = """",
                        followHtml = """",
                        followNode = null;

                    followSecClass = Y.Media.FollowQuote.getFollowSectionClass();
                    followHtml = Y.Media.FollowQuote.getFollowBtnHTML({ ticker: 'BMPS.MI', addl_classes: ""follow-quote-always-visible"", showFollowText: true });
                    followNode = Y.Node.create(followHtml);
                    exchNode = Y.one("".wl_sign"");
                    if (!Y.Lang.isNull(exchNode)) { 
                        exchNode.append(followNode);

                    }

                });
            });
    }
});
</code></pre>

<p>Any suggestion?</p>
","<p>You can use <code>rvest</code> as follows:</p>

<pre><code>require(rvest)
doc &lt;- read_html(""http://finance.yahoo.com/q/h?s=AAPL+Headlines"")
scope &lt;- doc %&gt;% html_nodes(""#yfncsumtab li"")
res &lt;- lapply(scope, function(li){
  data.frame(stringsAsFactors = FALSE,
    date = li %&gt;% html_node(""cite span"") %&gt;% html_text,
    headline = li %&gt;% html_node(""a"") %&gt;% html_text
    )
})
do.call(rbind, res)
</code></pre>

<p>This gives you:</p>

<pre><code>                date                                                                                  headline
1   (Tue 3:49AM EDT)                                   US hacks iPhone, ends legal battle but questions linger
2   (Tue 1:27AM EDT)                           Amazon Echo turns into a sleeper hit, offsetting Fire's failure
3   (Tue 1:00AM EDT)                                       Why Everyone Loses in Apple’s Fight Against the FBI
4  (Tue 12:36AM EDT) [$$] US drops Apple case, Japan's negative rate bounty and the criminals paid not to kill
5  (Tue 12:25AM EDT)                              U.S. succeeds in cracking Apple's iPhone, drops legal action
6  (Tue 12:00AM EDT)  [$$] Brussels Attacks: Belgium Turns to U.S. for Help in Scouring Seized Laptops, Phones
7      (Mon, Mar 28)                [$$] FBI Opens San Bernardino Shooter’s iPhone; U.S. Drops Demand on Apple
8      (Mon, Mar 28)                                              Wolverton: Encyption debate isn't going away
9      (Mon, Mar 28)                                            [$$] US drops Apple case after cracking iPhone
10     (Mon, Mar 28)         Words of warning — not celebration — in Silicon Valley after FBI ends Apple fight
11     (Mon, Mar 28)                               [$$] FBI Opens Shooter's iPhone; U.S. Drops Demand on Apple
12     (Mon, Mar 28)                                           FBI hacks into terrorist’s iPhone without Apple
13     (Mon, Mar 28)                                  Justice Department cracks iPhone; withdraws legal action
14     (Mon, Mar 28)                                Apple responds: 'This case should have never been brought'
15     (Mon, Mar 28)                           IPhone Security Is the Casualty in Apple's Victory Over the FBI
16     (Mon, Mar 28)                           Cracked Apple iPhone By F.B.I. Puts Spotlight On Apple Security
17     (Mon, Mar 28)                                    DOJ Drops Apple Case: Bloomberg West (Full Show 03/28)
18     (Mon, Mar 28)                                          Apple, Inc.'s New iPhone SE: Off to a Big Start?
19     (Mon, Mar 28)                                               AP Explains: Apple vs. FBI _ What Happened?
20     (Mon, Mar 28)                                                  PRESS DIGEST- Financial Times - March 29
</code></pre>

<p>I do leave the date-parsing to you.</p>

<p>Another alternative would be taking the date from the h3-heading as follows</p>

<pre><code>require(rvest)
doc &lt;- read_html(""http://finance.yahoo.com/q/h?s=AAPL+Headlines"")
scope &lt;- doc %&gt;% html_nodes(""#yfncsumtab"")
dates &lt;- scope %&gt;% html_nodes(""h3 span"") %&gt;% html_text()
headlines &lt;- scope %&gt;% html_nodes(""h3 + ul"") %&gt;% lapply(. %&gt;% html_nodes(""li a"") %&gt;% html_text)

# combine both
do.call(rbind,Map(cbind, dates, headlines))
</code></pre>

<p>Which results in the following matrix</p>

<pre><code>      [,1]                      [,2]                                                                                       
 [1,] ""Tuesday, March 29, 2016"" ""March 29 Premarket Briefing: 10 Things You Should Know""                                   
 [2,] ""Tuesday, March 29, 2016"" ""You might soon be able to pay for goods in-store using Facebook Messenger""                
 [3,] ""Tuesday, March 29, 2016"" ""FBI unlocks iPhone""                                                                       
 [4,] ""Tuesday, March 29, 2016"" ""US hacks iPhone, ends legal battle but questions linger""                                  
 [5,] ""Tuesday, March 29, 2016"" ""Amazon Echo turns into a sleeper hit, offsetting Fire's failure""                          
 [6,] ""Tuesday, March 29, 2016"" ""Why Everyone Loses in Apple’s Fight Against the FBI""                                      
 [7,] ""Tuesday, March 29, 2016"" ""[$$] US drops Apple case, Japan's negative rate bounty and the criminals paid not to kill""
 [8,] ""Tuesday, March 29, 2016"" ""U.S. succeeds in cracking Apple's iPhone, drops legal action""                             
 [9,] ""Tuesday, March 29, 2016"" ""[$$] Brussels Attacks: Belgium Turns to U.S. for Help in Scouring Seized Laptops, Phones"" 
[10,] ""Monday, March 28, 2016""  ""[$$] FBI Opens San Bernardino Shooter’s iPhone; U.S. Drops Demand on Apple""               
[11,] ""Monday, March 28, 2016""  ""Wolverton: Encyption debate isn't going away""                                             
[12,] ""Monday, March 28, 2016""  ""[$$] US drops Apple case after cracking iPhone""                                           
[13,] ""Monday, March 28, 2016""  ""Words of warning — not celebration — in Silicon Valley after FBI ends Apple fight""        
[14,] ""Monday, March 28, 2016""  ""[$$] FBI Opens Shooter's iPhone; U.S. Drops Demand on Apple""                              
[15,] ""Monday, March 28, 2016""  ""FBI hacks into terrorist’s iPhone without Apple""                                          
[16,] ""Monday, March 28, 2016""  ""Justice Department cracks iPhone; withdraws legal action""                                 
[17,] ""Monday, March 28, 2016""  ""Apple responds: 'This case should have never been brought'""                               
[18,] ""Monday, March 28, 2016""  ""IPhone Security Is the Casualty in Apple's Victory Over the FBI""                          
[19,] ""Monday, March 28, 2016""  ""Cracked Apple iPhone By F.B.I. Puts Spotlight On Apple Security""                          
[20,] ""Monday, March 28, 2016""  ""DOJ Drops Apple Case: Bloomberg West (Full Show 03/28)""  
</code></pre>

<p>Also in the second case i leave the date-parsing to you</p>
"
42584748,Linear regression with only previous values in moving window,1,1,1,"<p>I have a huge dataset and would like to perform a rolling linear regression over a window of 60. However, I want that only the 60 previous values are considered for the linear regression.</p>

<p>My Dataframe DF consists of following Columns:</p>

<pre><code>Date          Company   Y     X1   X2
01.01.2015    Mill     0.13   -1    -3
01.02.2015    Mill     0.16   1    5 
01.03.2015    Mill     0.83   3    4
01.04.2015    Mill     -0.83  23   4
01.01.1988    Hall    0.23    1    3
01.02.1988    Hall    0.24    23   2
01.03.1988    Hall    0.78    19   -9
01.04.1988    Hall    0.73    4    12
01.05.1988    Hall    0.72    5    12
01.11.2008    Jopo    0.12    0.9  32
01.12.2008    Jopo    0.13    10   32
01.01.2009    Jopo    0.32    0.2  10
01.02.2009    Jopo    0.32    2    -1
</code></pre>

<p>I have several thousand companies and data for several months for each company. The regression has to be done for every month of a company, with the rolling window of 60 previous months of this specific company. </p>

<p>In the given example, assuming only a rolling window of 3, I want for company Mill a regression on 01.04.2015 with the data from 01.01-01.03-2015. For company Hall I want regressions on 01.04 and 01.05.1988, and for Jopo I want a regression on 01.02.2009.</p>

<p>Ideally, the results will be pasted together with Company and Date in a new data frame, as I have to keep working with this data and have to analyse it more.</p>

<p>Following code should do the trick for the rolling regression, however it does not use the previous 60 dates, but 59 and includes the current date too:</p>

<p>library(zoo)</p>

<pre><code>rolled &lt;- function(df) {                                    
    rollapply(df, width = 60,
        FUN = function(z) coef(lm(Y ~ X1+X2, data = as.data.frame(z))),
        by.column = FALSE, align = ""right""
)
}    
</code></pre>

<p>Following code does the regression dependent on the company name, as I want to make regressions for each individual company, independend from the other companies. </p>

<pre><code>Test &lt;- do.call(""rbind"", by(DF[c(""Y"", ""X1"", ""X2"")], DF[c( ""Name"")], rolled))
</code></pre>

<p>How do I incorporate, that only the 60 previous values are used for the regression? And maybe someone knows how to show also ""Company"" and ""Date"" in the results? Thanks for your help!</p>
","<p>Assume <code>DF</code> is as given reproducibly in the Note at the end.  Use <code>by</code> to split <code>DF</code> into company rows and apply the anonymous function using <code>rollapplyr</code>. Note that <code>rollapplyr</code> can take for the <code>width</code> a list argument with the offsets of the positions to use.  For example, <code>list(-seq(3))</code> means use the 3 prior rows (as suggested in the question) but not the current row (which would have position 0).</p>

<pre><code>library(zoo)

# w &lt;- 60    
w &lt;- 3
Coef &lt;- function(x) coef(lm(as.data.frame(x)))
do.call(""rbind"", by(DF, DF$Company, function(x) 
    cbind(x, rollapplyr(x[3:5], list(-seq(w)), Coef, fill = NA, by.column = FALSE))))
</code></pre>

<p>giving:</p>

<pre><code>              Date Company     Y   X1 X2 (Intercept)         X1         X2
Hall.5  01.01.1988    Hall  0.23  1.0  3          NA         NA         NA
Hall.6  01.02.1988    Hall  0.24 23.0  2          NA         NA         NA
Hall.7  01.03.1988    Hall  0.78 19.0 -9          NA         NA         NA
Hall.8  01.04.1988    Hall  0.73  4.0 12     0.37711 -0.0017480 -0.0484553
Hall.9  01.05.1988    Hall  0.72  5.0 12     1.30333 -0.0433333 -0.0333333
Jopo.10 01.11.2008    Jopo  0.12  0.9 32          NA         NA         NA
Jopo.11 01.12.2008    Jopo  0.13 10.0 32          NA         NA         NA
Jopo.12 01.01.2009    Jopo  0.32  0.2 10          NA         NA         NA
Jopo.13 01.02.2009    Jopo  0.32  2.0 -1     0.41104  0.0010989 -0.0091259
Mill.1  01.01.2015    Mill  0.13 -1.0 -3          NA         NA         NA
Mill.2  01.02.2015    Mill  0.16  1.0  5          NA         NA         NA
Mill.3  01.03.2015    Mill  0.83  3.0  4          NA         NA         NA
Mill.4  01.04.2015    Mill -0.83 23.0  4     0.21611  0.2994444 -0.0711111
</code></pre>

<p>You could also try this:</p>

<pre><code>library(broom)
fun &lt;- function(x) unlist(tidy(lm(as.data.frame(x)))[, -1]) 
do.call(""rbind"", by(DF, DF$Company, function(x) 
 cbind(x, rollapplyr(x[3:5], list(-(seq(w))), fun, fill = NA, by.column = FALSE))))
</code></pre>

<p>which gives:</p>

<pre><code>              Date Company     Y   X1 X2 estimate1    estimate2    estimate3
Hall.5  01.01.1988    Hall  0.23  1.0  3        NA           NA           NA
Hall.6  01.02.1988    Hall  0.24 23.0  2        NA           NA           NA
Hall.7  01.03.1988    Hall  0.78 19.0 -9        NA           NA           NA
Hall.8  01.04.1988    Hall  0.73  4.0 12 0.3771138 -0.001747967 -0.048455285
Hall.9  01.05.1988    Hall  0.72  5.0 12 1.3033333 -0.043333333 -0.033333333
Jopo.10 01.11.2008    Jopo  0.12  0.9 32        NA           NA           NA
Jopo.11 01.12.2008    Jopo  0.13 10.0 32        NA           NA           NA
Jopo.12 01.01.2009    Jopo  0.32  0.2 10        NA           NA           NA
Jopo.13 01.02.2009    Jopo  0.32  2.0 -1 0.4110390  0.001098901 -0.009125874
Mill.1  01.01.2015    Mill  0.13 -1.0 -3        NA           NA           NA
Mill.2  01.02.2015    Mill  0.16  1.0  5        NA           NA           NA
Mill.3  01.03.2015    Mill  0.83  3.0  4        NA           NA           NA
Mill.4  01.04.2015    Mill -0.83 23.0  4 0.2161111  0.299444444 -0.071111111
        std.error1 std.error2 std.error3 statistic1 statistic2 statistic3
Hall.5          NA         NA         NA         NA         NA         NA
Hall.6          NA         NA         NA         NA         NA         NA
Hall.7          NA         NA         NA         NA         NA         NA
Hall.8         NaN        NaN        NaN        NaN        NaN        NaN
Hall.9         NaN        NaN        NaN        NaN        NaN        NaN
Jopo.10         NA         NA         NA         NA         NA         NA
Jopo.11         NA         NA         NA         NA         NA         NA
Jopo.12         NA         NA         NA         NA         NA         NA
Jopo.13        NaN        NaN        NaN        NaN        NaN        NaN
Mill.1          NA         NA         NA         NA         NA         NA
Mill.2          NA         NA         NA         NA         NA         NA
Mill.3          NA         NA         NA         NA         NA         NA
Mill.4         NaN        NaN        NaN        NaN        NaN        NaN
        p.value1 p.value2 p.value3
Hall.5        NA       NA       NA
Hall.6        NA       NA       NA
Hall.7        NA       NA       NA
Hall.8       NaN      NaN      NaN
Hall.9       NaN      NaN      NaN
Jopo.10       NA       NA       NA
Jopo.11       NA       NA       NA
Jopo.12       NA       NA       NA
Jopo.13      NaN      NaN      NaN
Mill.1        NA       NA       NA
Mill.2        NA       NA       NA
Mill.3        NA       NA       NA
Mill.4       NaN      NaN      NaN
&gt; 
</code></pre>

<p><strong>Alternative</strong></p>

<p>Another possibility is to use a width of <code>w+1</code> and then remove the last component.</p>

<pre><code># w &lt;- 60    
w &lt;- 3 
Coef1 &lt;- function(x) coef(lm(as.data.frame(head(x, -1))))
do.call(""rbind"", by(DF, DF$Company, function(x) 
    cbind(x, rollapplyr(x[3:5], w+1, Coef1, fill = NA, by.column = FALSE))))
</code></pre>

<p><strong>Fewer than w+1 rows in a company</strong> </p>

<p>If there are companies with fewer than w+1 rows then try this.  It uses the <code>partial=TRUE</code> argument of <code>rollapplyr</code> to compute <code>lm</code> with fewer rows and modifies <code>Coef</code> accordingly so that it will continue to work:</p>

<pre><code># w &lt;- 60    
w &lt;- 3
Coef &lt;- function(x) coef(lm(as.data.frame(matrix(x, c(nrow(x), 1)))))
do.call(""rbind"", by(DF, DF$Company, function(x) cbind(x, 
  rollapplyr(x[3:5], list(-seq(w)), Coef, partial = TRUE, by.column = FALSE))))
</code></pre>

<p><strong>Note:</strong> Input <code>DF</code> is:</p>

<pre><code>Lines &lt;- ""Date          Company   Y     X1   X2
01.01.2015    Mill     0.13   -1    -3
01.02.2015    Mill     0.16   1    5 
01.03.2015    Mill     0.83   3    4
01.04.2015    Mill     -0.83  23   4
01.01.1988    Hall    0.23    1    3
01.02.1988    Hall    0.24    23   2
01.03.1988    Hall    0.78    19   -9
01.04.1988    Hall    0.73    4    12
01.05.1988    Hall    0.72    5    12
01.11.2008    Jopo    0.12    0.9  32
01.12.2008    Jopo    0.13    10   32
01.01.2009    Jopo    0.32    0.2  10
01.02.2009    Jopo    0.32    2    -1""
DF &lt;- read.table(text = Lines, header = TRUE, as.is = TRUE)
</code></pre>
"
44336345,Running R from Mac OSX terminal,2,2,2,"<p>I've searched the web, and I'm still unclear on how to run R from the Mac terminal.  I have Rstudio and the standalone R app installed.  I thought I could just type ""R"" from the command line as I do with ""python"", but that doesn't work.  Is it necessary to edit the PATH in my bash profile?  If so, how do I give the correct location of R?  </p>

<p>Thanks for any help</p>

<h2>Edits after receiving comments</h2>

<p>So, I'm running Sierra, and when I type ""r"" or ""R"" at the terminal, I get ""-bash: R: command not found.""  If I type, ""which R"" in the terminal I do not get any output.  </p>

<p>Here is the output from ""echo $PATH"": /usr/local/heroku/bin:/opt/local/bin:/opt/local/sbin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/Users/samuelcolon/anaconda/bin:/Library/Frameworks/Python.framework/Versions/3.5/bin:/Users/samuelcolon/.rvm/gems/ruby-2.1.0/bin:/Users/samuelcolon/.rvm/gems/ruby-2.1.0@global/bin:/Users/samuelcolon/.rvm/rubies/ruby-2.1.0/bin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/samuelcolon/.rvm/bin:/Users/samuelcolon/.rvm/bin</p>

<p>As for the installation, I believe I downloaded it directly from cran.r-project.org a while ago.  I can locate the GUI in my applications and open it--
I have version 3.13.  Is it possible, I only have R.app installed but not R?  Perhaps that's the reason I'm getting the 'command not found' when typing ""R"" into the terminal?</p>

<p>Generally, I've been working in RStudio, but I'd still like to access R from the terminal and also to find where things are located.  I'm fine with removing and re-installing R if it's easiest to start from square one.  I hope the extra detail helps, and I appreciate the responses.  </p>
","<p><strong>An answer for those not that familiar with Terminal and Bash.</strong> </p>

<p>I have done a fresh update install of R from the R.org cran site as part of seeking an answer to your question. </p>

<p>I found this latest install version 3.4.0 installs R for access in Terminal, and also installs R.app as part of the package. </p>

<p>To my understanding, reading support docs, if you have an older version of R it will update that. However it will not update an installation of R installed by the anaconda package.</p>

<hr>

<p><strong><em>Where are the R files stored?</em></strong></p>

<p>I can only assume that with a fresh install of the latest R, R will work for you in Terminal. </p>

<p>To learn where the R files are that are being accessed - in Terminal after starting R, and in R.app, type:</p>

<p><code>&gt;R.home()</code></p>

<hr>

<p><strong><em>In my case as example:</em></strong> </p>

<p>In R.app - the R version 3.4.0 is accessed in the top directory (not my user folder): </p>

<pre><code>R.home()
[1] ""/Library/Frameworks/R.framework/Resources""
</code></pre>

<p>In Terminal - the R version 3.3.2 is accessed in the Anaconda package, again in the top level directory.</p>

<pre><code>R.home()
[1] ""/anaconda/lib/R""
</code></pre>

<p>So I have two different versions of R, and Terminal accesses a different version to R.app.  </p>

<hr>

<p><strong><em>How can I ensure I access the same version in Terminal as I do in the R.app?</em></strong>  </p>

<p>For someone familiar with bash, and how the whole bash command system works I am sure there is a well constructed command. All the same here are some novice solutions. </p>

<p>-</p>

<p><strong><em>• First Solution:</em></strong></p>

<p>I could update the anaconda version, however, I would prefer not to as as other elements of the anaconda package my depend on this older version of R. For those not yet familiar with Terminal and bash, not such a novice solution. </p>

<p>-</p>

<p><strong><em>• Second Solution:</em></strong></p>

<p>This solution came from mko.  It provides a single use solution.  From the result above, and checking the directory structure a little further to find this R file.  </p>

<p><a href=""https://i.stack.imgur.com/Qjjw6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qjjw6.png"" alt=""icon in finder""></a></p>

<p>Finding the significant R file enables me to edit an extension of the above path shown in the R.app. So add /bin/R to enter </p>

<pre><code>/Library/Frameworks/R.framework/Resources/bin/R
</code></pre>

<p>Entering and pressing return will start R from this version. </p>

<p>Alternatively, one can find this file and icon in the GUI Finder, lead by the above result, and just double click on it, and it will open Terminal and a session with R running for you. Easy!</p>

<p>One could also make an alias of it and put it on your desktop for easy future starts. </p>

<p>-</p>

<p><strong><em>• Third Solution:</em></strong></p>

<p>My last solution I think may be best, adding to mko's solution. Make an alias.</p>

<p>Being in my home directory in Terminal I open .bash_profile using the nano text editor. (If you do not already know how to do this, then best not use this solution.)</p>

<p>I then add the line in this env file.</p>

<pre><code>alias Rv340='/Library/Frameworks/R.framework/Resources/bin/R'
</code></pre>

<p>I then save the changes and exit this terminal session.  I then open a new Terminal window. (This is so the changes to the env above are incorporated in the new terminal session). </p>

<p>Then when I enter the alias:</p>

<pre><code>Rv340
</code></pre>

<p>The version of R I want opens. </p>

<p>You can choose a different alias name to ""Rv340"".</p>

<p>-</p>

<p><strong><em>• Fourth Solution:</em></strong></p>

<p>A second more permanent solution for opening the same version of R in Terminal is as follows. </p>

<p>Copy the path as showing in R.app in response to the R.home() command above, and add that path to PATH in your .bash_profile. (If you do not know already how to do this, then ignore this solution.) Do so as follows. </p>

<pre><code>export PATH=""/Library/Frameworks/R.framework/Resources:$PATH""
</code></pre>

<p>To my understanding, this ensures that bash looks here for R (and anything else), then moves on to the other paths in PATH. Since this adds this path to the beginning of $PATH, an env variable, bash looks here first where it finds the newer version first, and stops looking. </p>

<p>When it comes to understanding PATH in the env set up in .bash_profile the following two links were helpful.</p>

<p><a href=""https://i.stack.imgur.com/Zwptg.png"" rel=""nofollow noreferrer"">About PATH.</a></p>

<p><a href=""https://unix.stackexchange.com/questions/26047/how-to-correctly-add-a-path-to-path"">How to correctly add a path to PATH.</a></p>

<p>This solution may muck with anaconda's invocation of R. I have yet to check this. </p>
"
45057498,r - Length and sum of runs of negative values,1,1,1,"<p>I have a dataframe that contains around 200 columns representing monthly drought measurements from 1999 to 2015. The values in each column can either be positive or negative. Each row in the dataframe represents a focal year that I'm interested in calculating metrics in reference to. The focal year is represented in another column. There can be multiple rows with the same focal year if they represent measurements from different sites (FIPS column). Here's a toy version (updated version!) of the dataframe:</p>

<pre><code>    structure(list(FIPS = c(19045, 48157, 20045, 20027), Year = c(2003, 
2004, 2005, 2005), pdsi_2002.01.15 = c(1.46, 4.38, 0.38, -1.41
), pdsi_2002.02.15 = c(1.6, 3.63, -0.05, -1.66), pdsi_2002.03.15 = c(1.32, 
3, -0.62, -1.93), pdsi_2002.04.15 = c(1.81, 2.68, 0.66, -1.88
), pdsi_2002.05.15 = c(2.03, 1.86, 1.26, -1.7), pdsi_2002.06.15 = c(2.51, 
1.74, -0.5, -2.94), pdsi_2002.07.15 = c(2.79, 1.94, -1.47, -3.82
), pdsi_2002.08.15 = c(3.06, 2.64, -1.99, -4.09), pdsi_2002.09.15 = c(2.08, 
3.02, -2.82, -4.87), pdsi_2002.10.15 = c(2.68, 4.73, -2.02, -3.01
), pdsi_2002.11.15 = c(2, 5.28, -2.55, -3.22), pdsi_2002.12.15 = c(1.55, 
5.94, -3.23, -3.52), pdsi_2003.01.15 = c(0.96, 5.39, -3.58, -3.51
), pdsi_2003.02.15 = c(0.29, 5.24, -3.54, -3.29), pdsi_2003.03.15 = c(-0.15, 
4.41, -3.77, -3.15), pdsi_2003.04.15 = c(-1.13, 3.39, -3.33, 
-2.46), pdsi_2003.05.15 = c(-1.05, 1.91, -3.47, -2.63), pdsi_2003.06.15 = c(-1.5, 
1.45, -2.94, -2.34), pdsi_2003.07.15 = c(-0.85, 1.69, -3.42, 
-3.02), pdsi_2003.08.15 = c(-1.78, 1.48, -2.75, -3.13), pdsi_2003.09.15 = c(-1.55, 
2.31, -2.66, -2.85), pdsi_2003.10.15 = c(-1.87, 2.5, -2.99, -3.16
), pdsi_2003.11.15 = c(-1.19, 2.72, -3.39, -2.73), pdsi_2003.12.15 = c(0.09, 
2.67, -2.96, -2.63), pdsi_2004.01.15 = c(-0.2, 3.2, -2.83, -2.42
), pdsi_2004.02.15 = c(0.07, 3.73, -2.78, -2.21), pdsi_2004.03.15 = c(1.58, 
3.04, -1.66, -0.77), pdsi_2004.04.15 = c(0.37, 3.19, -2, -1.25
), pdsi_2004.05.15 = c(1.7, 3.71, -1.35, -1.41), pdsi_2004.06.15 = c(1.53, 
5.21, -0.84, -1.04), pdsi_2004.07.15 = c(1.14, 4.84, 2.08, 0.93
), pdsi_2004.08.15 = c(1.4, 4.41, 3.22, 0.24), pdsi_2004.09.15 = c(-0.43, 
3.27, 2.39, -0.44), pdsi_2004.10.15 = c(0.77, 2.77, 2.49, -1.11
), pdsi_2004.11.15 = c(0.94, 4.95, 2.94, -1.03), pdsi_2004.12.15 = c(0.62, 
4.41, 2.67, -1.43), pdsi_2005.01.15 = c(1.51, 3.93, 3.55, -1.05
), pdsi_2005.02.15 = c(1.45, 4.54, 3.83, 0.71), pdsi_2005.03.15 = c(0.58, 
4.31, 3.01, 0.24), pdsi_2005.04.15 = c(-0.97, 3.36, 1.97, 0.94
), pdsi_2005.05.15 = c(-1.57, 3.12, 1.54, -0.33), pdsi_2005.06.15 = c(-2.65, 
2.02, 2.33, 1.16), pdsi_2005.07.15 = c(-3.58, 2.07, 2.31, 1.08
), pdsi_2005.08.15 = c(-3.51, 1.56, 3.7, 1.72), pdsi_2005.09.15 = c(-3.96, 
-0.71, 3.62, 0.74), pdsi_2005.10.15 = c(-4.77, -2.13, 3.79, 0.96
), pdsi_2005.11.15 = c(-5.08, -2.32, 3.4, 0.53), pdsi_2005.12.15 = c(-5.63, 
-2.57, 3.27, -0.22)), .Names = c(""FIPS"", ""Year"", ""pdsi_2002.01.15"", 
""pdsi_2002.02.15"", ""pdsi_2002.03.15"", ""pdsi_2002.04.15"", ""pdsi_2002.05.15"", 
""pdsi_2002.06.15"", ""pdsi_2002.07.15"", ""pdsi_2002.08.15"", ""pdsi_2002.09.15"", 
""pdsi_2002.10.15"", ""pdsi_2002.11.15"", ""pdsi_2002.12.15"", ""pdsi_2003.01.15"", 
""pdsi_2003.02.15"", ""pdsi_2003.03.15"", ""pdsi_2003.04.15"", ""pdsi_2003.05.15"", 
""pdsi_2003.06.15"", ""pdsi_2003.07.15"", ""pdsi_2003.08.15"", ""pdsi_2003.09.15"", 
""pdsi_2003.10.15"", ""pdsi_2003.11.15"", ""pdsi_2003.12.15"", ""pdsi_2004.01.15"", 
""pdsi_2004.02.15"", ""pdsi_2004.03.15"", ""pdsi_2004.04.15"", ""pdsi_2004.05.15"", 
""pdsi_2004.06.15"", ""pdsi_2004.07.15"", ""pdsi_2004.08.15"", ""pdsi_2004.09.15"", 
""pdsi_2004.10.15"", ""pdsi_2004.11.15"", ""pdsi_2004.12.15"", ""pdsi_2005.01.15"", 
""pdsi_2005.02.15"", ""pdsi_2005.03.15"", ""pdsi_2005.04.15"", ""pdsi_2005.05.15"", 
""pdsi_2005.06.15"", ""pdsi_2005.07.15"", ""pdsi_2005.08.15"", ""pdsi_2005.09.15"", 
""pdsi_2005.10.15"", ""pdsi_2005.11.15"", ""pdsi_2005.12.15""), row.names = c(13222L, 
18125L, 19543L, 19534L), class = ""data.frame"")
</code></pre>

<p>What I'd like to do is calculate the length and sum of each run of negative values in the focal year (so looking for runs in the same row, across columns), then calculate the mean run length, mean run sum, and mean of each run sum divided by each run length for each row. Adding another layer of difficulty, if the January measurement in the focal year is negative, I'd like to then look back to the years before the focal year to account for a situation where the run of negative numbers started in a previous year. The run could conceivably extend all the way to January 1999. </p>

<p>I've been able to calculate the run length metric using rle(), but haven't been able to figure out how to get run sums.</p>
","<p>I think this may work for what you are looking for, this will generate the 3 required values, for the year specified, and if there is a negative value in January it will continue that run down until a positive value is reached in the previous year.</p>

<pre><code>library(tidyr)
library(dplyr)

select.order &lt;- colnames(drought_data)[3:length(colnames(drought_data))]

drought_data &lt;- drought_data %&gt;% 
                # Gather data by date
                gather(key = date, value = value, -Year, -FIPS) %&gt;% 
                # Separate date into separate columns
                separate(date, into = c(""yr"",""month"", ""day""), sep = ""\\."") %&gt;% 
                # Extract year
                mutate(yr = substr(yr, 6, 9)) %&gt;%
                # Sort data by FIPS number, year, month
                arrange(FIPS, yr, month) %&gt;%
                # Group data by FIPS number, focal year, and data year
                group_by(FIPS, Year, yr) %&gt;%
                # Generate a run number for each run of negative numbers for the focal year
                mutate(run.num = ifelse(Year == yr,
                                 {run.num = rle(ifelse(value &lt; 0, 1, 0)) 
                                  rep(ifelse(run.num$values == 1, cumsum(run.num$values), 0), run.num$lengths)}, NA),
                       # Set run.num to -1 for positive values
                       run.num = ifelse(value &gt;= 0, -1, run.num)) %&gt;%
                # Sort data by FIPS number, descending year, and descending month
                arrange(FIPS, desc(yr), desc(month)) %&gt;%
                # Group data by FIPS number and focal year
                group_by(FIPS, Year) %&gt;%
                # Fill out the run numbers for each run to cross data years
                fill(run.num, .direction = ""down"") %&gt;%
                # Convert all -1 run numbers (Which indicate positive values) to zero
                mutate(run.num = ifelse(run.num == -1, 0, run.num),
                       # Set run.num for negative values that did not qualify as a run for the specified year to 0
                       run.num = ifelse(is.na(run.num), 0, run.num)) %&gt;%
                ungroup %&gt;%
                # mutate(run.num = ifelse(is.na(run.num, 0, run.num))) %&gt;%
                # Group data by FIPS number, focal year, and run number
                group_by(FIPS, Year, run.num) %&gt;%
                # Calculate the length, sum, and rate of each run
                mutate(run.length = ifelse(run.num == 0, 0, n()),
                       run.sum = ifelse(run.num == 0, 0, sum(value)),
                       run.rate = ifelse(run.num == 0, 0, run.sum/run.length)) %&gt;%
                # Group by FIPS number and focal year
                group_by(FIPS, Year) %&gt;%
                # Calculate the mean run length, and mean run sum for the focal year of each FIPS number
                mutate(mean.run.length = sum(ifelse(run.num == 0, 0, 1)) / max(run.num),
                       mean.run.length = ifelse(is.nan(mean.run.length), 0, mean.run.length),
                       mean.run.sum = sum(ifelse(run.num == 0, 0, value) / max(run.num)),
                       mean.run.sum = ifelse(is.nan(mean.run.sum), 0, mean.run.sum)) %&gt;%
                # Combine date parts back to single column
                unite(dt, yr:day, sep = ""."") %&gt;% 
                # Recreate the pdsi_ label format on the date column
                mutate(dt = paste0(""pdsi_"", dt)) %&gt;%
                # Drop the run.sum column
                select(-run.sum) %&gt;% 
                # Spread the data back to a wide view to eliminate duplicate run.rate values
                spread(dt, value) %&gt;% 
                # Group data by FIPS number and focal year
                group_by(FIPS, Year) %&gt;% 
                # Calculate the mean of the sum of run rates over the number of runs
                mutate(mean.run.sum.length = sum(run.rate) / max(run.num),
                       mean.run.sum.length = ifelse(is.nan(mean.run.sum.length), 0, mean.run.sum.length)) %&gt;% 
                # Remove grouping
                ungroup %&gt;% 
                # Drop the run.num, run.length, and run.rate columns 
                select(-run.num, -run.length, -run.rate) %&gt;% 
                # Gather the data into tall view to remove duplicates and NA values
                gather_(""dt"", ""value"", select.order, na.rm = TRUE) %&gt;% 
                # Spread data back to wide view
                spread(dt, value)

# Change the column order
drought_data &lt;- drought_data[,c(""FIPS"",""Year"",""mean.run.length"",""mean.run.sum"",""mean.run.sum.length"", select.order)]
</code></pre>

<p>The final output will be the original dataframe with the three additional calculated columns included. Below is the output of the calculated columns for the provided test dataset.</p>

<pre><code>&gt; drought_data[,c(""FIPS"",""Year"",""mean.run.length"",""mean.run.sum"",""mean.run.sum.length"")]
# A tibble: 4 x 5
   FIPS  Year mean.run.length mean.run.sum mean.run.sum.length
  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;
1 19045  2003        9.000000       -11.07          -1.2300000
2 20027  2005        2.333333        -1.87          -0.5206667
3 20045  2005        0.000000         0.00           0.0000000
4 48157  2004        0.000000         0.00           0.0000000
</code></pre>
"
46261719,in R - crawling with rvest - fail to get the texts in HTML tag using html_text function,2,2,4,"<pre><code>url &lt;-""http://news.chosun.com/svc/content_view/content_view.html?contid=1999080570392""

hh = read_html(GET(url),encoding = ""EUC-KR"")

#guess_encoding(hh)

html_text(html_node(hh, 'div.par'))
#html_text(html_nodes(hh ,xpath='//*[@id=""news_body_id""]/div[2]/div[3]'))
</code></pre>

<p>I'm trying to crawling the news data(just for practice) using rvest in R.</p>

<p>When I tried it on the homepage above, I failed to fetch the text from the page.
(Xpath doesn't work either.)</p>

<p>I do not think I failed to find the link that contain texts that I want to get on the page. But when I try to extract the text from that link using html_text function, it is extracted as """" or blanks. </p>

<p>I can't find why.. I don't have any experience with HTML and crawling.</p>

<p>What I'm guessing is the HTML tag that contain news body contexts, has ""class"" and ""data-dzo""(I don't know what is it). </p>

<p>So If anyone tell me how to solve it or let me know the search keywords that I can find on google to solve this problem.</p>
","<p>It builds quite a bit of the page dynamically. This should help.</p>

<p>The article content is in an XML file. The URL can be constructed from the <code>contid</code> parameter. Either pass in a full article HTML URL (like the one in your example) or just the <code>contid</code> value to this and it'll return an <code>xml2</code> <code>xml_document</code> with the parsed XML results:</p>

<pre><code>#' Retrieve article XML from chosun.com
#' 
#' @param full_url_or_article_id either a full URL like 
#'        `http://news.chosun.com/svc/content_view/content_view.html?contid=1999080570392`
#'        or just the id (e.g. `1999080570392`)
#' @return xml_document
read_chosun_article &lt;- function(full_url_or_article_id) {

  require(rvest)
  require(httr)

  full_url_or_article_id &lt;- full_url_or_article_id[1]

  if (grepl(""^http"", full_url_or_article_id)) {
    contid &lt;- httr::parse_url(full_url_or_article_id)
    contid &lt;- contid$query$contid
  } else {
    contid &lt;- full_url_or_article_id
  }

  # The target article XML URLs are in the following format:
  #
  # http://news.chosun.com/priv/data/www/news/1999/08/05/1999080570392.xml
  #
  # so we need to construct it from substrings in the 'contid'

  sprintf(
    ""http://news.chosun.com/priv/data/www/news/%s/%s/%s/%s.xml"",
    substr(contid, 1, 4), # year
    substr(contid, 5, 6), # month
    substr(contid, 7, 8), # day
    contid
  ) -&gt; contid_xml_url

  res &lt;- httr::GET(contid_xml_url)

  httr::content(res)  

}

read_chosun_article(""http://news.chosun.com/svc/content_view/content_view.html?contid=1999080570392"")
## {xml_document}
## &lt;content&gt;
##  [1] &lt;id&gt;1999080570392&lt;/id&gt;
##  [2] &lt;site&gt;\n  &lt;id&gt;1&lt;/id&gt;\n  &lt;name&gt;&lt;![CDATA[www]]&gt;&lt;/name&gt;\n&lt;/site&gt;
##  [3] &lt;category&gt;\n  &lt;id&gt;3N1&lt;/id&gt;\n  &lt;name&gt;&lt;![CDATA[사람들]]&gt;&lt;/name&gt;\n  &lt;path ...
##  [4] &lt;type&gt;0&lt;/type&gt;
##  [5] &lt;template&gt;\n  &lt;id&gt;2006120400003&lt;/id&gt;\n  &lt;fileName&gt;3N.tpl&lt;/fileName&gt; ...
##  [6] &lt;date&gt;\n  &lt;created&gt;19990805192041&lt;/created&gt;\n  &lt;createdFormated&gt;199 ...
##  [7] &lt;editor&gt;\n  &lt;id&gt;chosun&lt;/id&gt;\n  &lt;email&gt;&lt;![CDATA[webmaster@chosun.com ...
##  [8] &lt;source&gt;&lt;![CDATA[0]]&gt;&lt;/source&gt;
##  [9] &lt;title&gt;&lt;![CDATA[[동정] 이철승, 순국학생 위령제 지내 등]]&gt;&lt;/title&gt;
## [10] &lt;subTitle/&gt;
## [11] &lt;indexTitleList/&gt;
## [12] &lt;authorList/&gt;
## [13] &lt;masterId&gt;1999080570392&lt;/masterId&gt;
## [14] &lt;keyContentId&gt;1999080570392&lt;/keyContentId&gt;
## [15] &lt;imageList count=""0""/&gt;
## [16] &lt;mediaList count=""0""/&gt;
## [17] &lt;body count=""1""&gt;\n  &lt;page no=""0""&gt;\n    &lt;paragraph no=""0""&gt;\n      &lt;t ...
## [18] &lt;copyright/&gt;
## [19] &lt;status&gt;&lt;![CDATA[RL]]&gt;&lt;/status&gt;
## [20] &lt;commentBbs&gt;N&lt;/commentBbs&gt;
## ...

read_chosun_article(""1999080570392"")
## {xml_document}
## &lt;content&gt;
##  [1] &lt;id&gt;1999080570392&lt;/id&gt;
##  [2] &lt;site&gt;\n  &lt;id&gt;1&lt;/id&gt;\n  &lt;name&gt;&lt;![CDATA[www]]&gt;&lt;/name&gt;\n&lt;/site&gt;
##  [3] &lt;category&gt;\n  &lt;id&gt;3N1&lt;/id&gt;\n  &lt;name&gt;&lt;![CDATA[사람들]]&gt;&lt;/name&gt;\n  &lt;path ...
##  [4] &lt;type&gt;0&lt;/type&gt;
##  [5] &lt;template&gt;\n  &lt;id&gt;2006120400003&lt;/id&gt;\n  &lt;fileName&gt;3N.tpl&lt;/fileName&gt; ...
##  [6] &lt;date&gt;\n  &lt;created&gt;19990805192041&lt;/created&gt;\n  &lt;createdFormated&gt;199 ...
##  [7] &lt;editor&gt;\n  &lt;id&gt;chosun&lt;/id&gt;\n  &lt;email&gt;&lt;![CDATA[webmaster@chosun.com ...
##  [8] &lt;source&gt;&lt;![CDATA[0]]&gt;&lt;/source&gt;
##  [9] &lt;title&gt;&lt;![CDATA[[동정] 이철승, 순국학생 위령제 지내 등]]&gt;&lt;/title&gt;
## [10] &lt;subTitle/&gt;
## [11] &lt;indexTitleList/&gt;
## [12] &lt;authorList/&gt;
## [13] &lt;masterId&gt;1999080570392&lt;/masterId&gt;
## [14] &lt;keyContentId&gt;1999080570392&lt;/keyContentId&gt;
## [15] &lt;imageList count=""0""/&gt;
## [16] &lt;mediaList count=""0""/&gt;
## [17] &lt;body count=""1""&gt;\n  &lt;page no=""0""&gt;\n    &lt;paragraph no=""0""&gt;\n      &lt;t ...
## [18] &lt;copyright/&gt;
## [19] &lt;status&gt;&lt;![CDATA[RL]]&gt;&lt;/status&gt;
## [20] &lt;commentBbs&gt;N&lt;/commentBbs&gt;
## ...
</code></pre>

<p>NOTE: I poked around that site to see this violates their terms of service and it does not seem to but I also relied on google translate and it may have made that harder to find. It's important to ensure you can legally (and, ethically, if you care about ethics) scrape this content for whatever use you intend.</p>
"
35729516,replace some value in dataframe from another dataframe,1,1,1,"<p>I have two dataframe:</p>

<pre><code>df1 &lt;- data.frame(id = c(""LABEL1"", ""LABEL2"", ""LABEL3"", ""LABEL4"", ""LABEL5"", ""LABEL6""),matrix(1:60,6,10))
df1[c(4:6), c(2:4)] = NA

df2 = data.frame(id = c( ""LABEL3"", ""LABEL4"", ""LABEL5"", ""LABEL6""),matrix(seq(100,10000, length.out = 32),4,8))
</code></pre>

<p>I would like to look up ONLY the missing values from DF1 in DF2 using a key value = 'id'. Here is the desired output:
<a href=""http://i.stack.imgur.com/vcEjA.png"" rel=""nofollow"">enter image description here</a></p>

<p>Here is the methods I tried:
1. merge: but I get duplicated columns for X1:X3.
2. match:</p>

<pre><code>df1[,2]= df2[,2][match(df1$id, df2$id)] 
</code></pre>

<p>but I will get the lable 3 in DF1 covered. 
3. lookup from qdap package: </p>

<pre><code>library(qdap)
apply(df1, 2, lookup, df2)
</code></pre>

<p>same result as method 2.</p>

<p>Thanks!</p>
","<p>you could use <code>tidyr</code> to work in tidy data form then <code>dplyr</code> to combine table</p>

<pre><code>library(dplyr)
library(tidyr)
</code></pre>

<p><strong>In one way with pipe</strong></p>

<pre><code>df1 %&gt;% 
  mutate(id = as.character(id)) %&gt;%
  gather(key = ""col"", value = ""val"", -id) %&gt;%
  left_join(df2 %&gt;% 
                mutate(id = as.character(id)) %&gt;%
                gather(key = ""col"", value = ""val"", -id), 
            by =c(""id"", ""col"")) %&gt;%
  transmute(id, col, val = ifelse(is.na(val.x), val.y, val.x)) %&gt;%
  spread(col, val) %&gt;%
  select(id, num_range(""X"", 1:10))
#&gt;       id        X1       X2       X3 X4 X5 X6 X7 X8 X9 X10
#&gt; 1 LABEL1    1.0000    7.000   13.000 19 25 31 37 43 49  55
#&gt; 2 LABEL2    2.0000    8.000   14.000 20 26 32 38 44 50  56
#&gt; 3 LABEL3    3.0000    9.000   15.000 21 27 33 39 45 51  57
#&gt; 4 LABEL4  419.3548 1696.774 2974.194 22 28 34 40 46 52  58
#&gt; 5 LABEL5  738.7097 2016.129 3293.548 23 29 35 41 47 53  59
#&gt; 6 LABEL6 1058.0645 2335.484 3612.903 24 30 36 42 48 54  60
</code></pre>

<p><strong>Step by step for explanation</strong></p>

<pre><code># id as character instead of factor
df1 &lt;- df1 %&gt;% mutate(id = as.character(id))
# tidy data
df1 &lt;- df1 %&gt;% gather(key = ""col"", value = ""val"", -id)
# print result as dplyr tbl
df1 %&gt;% as.tbl()
#&gt; Source: local data frame [60 x 3]
#&gt; 
#&gt;        id   col   val
#&gt;     (chr) (chr) (int)
#&gt; 1  LABEL1    X1     1
#&gt; 2  LABEL2    X1     2
#&gt; 3  LABEL3    X1     3
#&gt; 4  LABEL4    X1    NA
#&gt; 5  LABEL5    X1    NA
#&gt; 6  LABEL6    X1    NA
#&gt; 7  LABEL1    X2     7
#&gt; 8  LABEL2    X2     8
#&gt; 9  LABEL3    X2     9
#&gt; 10 LABEL4    X2    NA
#&gt; ..    ...   ...   ...
# idem on df2
df2 &lt;- df2 %&gt;% 
  mutate(id = as.character(id)) %&gt;%
  tidyr::gather(key = ""col"", value = ""val"", -id)
# print result as dplyr tbl
df2 %&gt;% as.tbl()
#&gt; Source: local data frame [32 x 3]
#&gt; 
#&gt;        id   col       val
#&gt;     (chr) (chr)     (dbl)
#&gt; 1  LABEL3    X1  100.0000
#&gt; 2  LABEL4    X1  419.3548
#&gt; 3  LABEL5    X1  738.7097
#&gt; 4  LABEL6    X1 1058.0645
#&gt; 5  LABEL3    X2 1377.4194
#&gt; 6  LABEL4    X2 1696.7742
#&gt; 7  LABEL5    X2 2016.1290
#&gt; 8  LABEL6    X2 2335.4839
#&gt; 9  LABEL3    X3 2654.8387
#&gt; 10 LABEL4    X3 2974.1935
#&gt; ..    ...   ...       ...

# join only id and col level of df1 with df2
new.df &lt;- left_join(df1, df2, by = c(""id"", ""col""))
# print result as dplyr tbl
new.df %&gt;% as.tbl()
#&gt; Source: local data frame [60 x 4]
#&gt; 
#&gt;        id   col val.x     val.y
#&gt;     (chr) (chr) (int)     (dbl)
#&gt; 1  LABEL1    X1     1        NA
#&gt; 2  LABEL2    X1     2        NA
#&gt; 3  LABEL3    X1     3  100.0000
#&gt; 4  LABEL4    X1    NA  419.3548
#&gt; 5  LABEL5    X1    NA  738.7097
#&gt; 6  LABEL6    X1    NA 1058.0645
#&gt; 7  LABEL1    X2     7        NA
#&gt; 8  LABEL2    X2     8        NA
#&gt; 9  LABEL3    X2     9 1377.4194
#&gt; 10 LABEL4    X2    NA 1696.7742
#&gt; ..    ...   ...   ...       ...

#replace NA in col val.x from df1 by value val.y of df2
# and only keep id, col and new column val
new.df &lt;- new.df %&gt;% transmute(id, col, val = ifelse(is.na(val.x), val.y, val.x)) 
new.df %&gt;% as.tbl()
#&gt; Source: local data frame [60 x 3]
#&gt; 
#&gt;        id   col       val
#&gt;     (chr) (chr)     (dbl)
#&gt; 1  LABEL1    X1    1.0000
#&gt; 2  LABEL2    X1    2.0000
#&gt; 3  LABEL3    X1    3.0000
#&gt; 4  LABEL4    X1  419.3548
#&gt; 5  LABEL5    X1  738.7097
#&gt; 6  LABEL6    X1 1058.0645
#&gt; 7  LABEL1    X2    7.0000
#&gt; 8  LABEL2    X2    8.0000
#&gt; 9  LABEL3    X2    9.0000
#&gt; 10 LABEL4    X2 1696.7742
#&gt; ..    ...   ...       ...

# put back data in wide format
new.df %&gt;% 
  spread(col, val) %&gt;%
  select(id, num_range(""X"", 1:10)) # put column in same order as df1
#&gt;       id        X1       X2       X3 X4 X5 X6 X7 X8 X9 X10
#&gt; 1 LABEL1    1.0000    7.000   13.000 19 25 31 37 43 49  55
#&gt; 2 LABEL2    2.0000    8.000   14.000 20 26 32 38 44 50  56
#&gt; 3 LABEL3    3.0000    9.000   15.000 21 27 33 39 45 51  57
#&gt; 4 LABEL4  419.3548 1696.774 2974.194 22 28 34 40 46 52  58
#&gt; 5 LABEL5  738.7097 2016.129 3293.548 23 29 35 41 47 53  59
#&gt; 6 LABEL6 1058.0645 2335.484 3612.903 24 30 36 42 48 54  60
</code></pre>
"
37753452,Can I join more than 4 dataframes in dplyr?,1,1,1,"<p>I am trying to combine the results of 5 models into a single dataframe for presentation purposes using <code>dplyr</code> and <code>left_join</code>. Each model result exists in its own dataframe (dat1 through dat5 for demonstration purposes). </p>

<p>*This is the result of a home-brewed likelihood function, so no summary methods available to exploit through things like <code>mtable</code> in the <code>memisc</code> package or the options available in <code>stargazer</code>. </p>

<pre><code>label1 &lt;- paste0(""var"", 1:10)
beta1 &lt;- 1:10
se1 &lt;- 1:10*.01
p1 &lt;- 1:10*.005

dat1 &lt;- data.frame(label = label1
                   ,beta = beta1
                   ,se = se1
                   ,p = p1)


label2 &lt;- paste0(""var"", 1:4)
beta2 &lt;- 1:4
se2 &lt;- 1:4*.01
p2 &lt;- 1:4*.005

dat2 &lt;- data.frame(label = label2
                   ,beta = beta2
                   ,se = se2
                   ,p = p2)


label3 &lt;- paste0(""var"", 1:3)
beta3 &lt;- 1:3
se3 &lt;- 1:3*.01
p3 &lt;- 1:3*.005

dat3 &lt;- data.frame(label = label3
                   ,beta = beta3
                   ,se = se3
                   ,p = p3)

label4 &lt;- paste0(""var"", 1:2)
beta4 &lt;- 1:2
se4 &lt;- 1:2*.01
p4 &lt;- 1:2*.005

dat4 &lt;- data.frame(label = label4
                   ,beta = beta4
                   ,se = se4
                   ,p = p4)

label5 &lt;- paste0(""var"", 1)
beta5 &lt;- 1
se5 &lt;- 1*.01
p5 &lt;- 1*.005

dat5 &lt;- data.frame(label = label5
                   ,beta = beta5
                   ,se = se5
                   ,p = p5)
</code></pre>

<p>In regular SQL, I would expect a LEFT JOIN function to behave as it does in <code>sqldf</code> as shown below. </p>

<pre><code>sqldf(
""
select * 
from dat1
  left join dat2
    on dat1.label = dat2.label
  left join dat3
    on dat1.label = dat3.label
  left join dat4
    on dat1.label = dat4.label
""
)

#label beta   se     p label beta   se     p label beta   se     p label beta   se     p
#1   var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005
#2   var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010
#3   var3    3 0.03 0.015  var3    3 0.03 0.015  var3    3 0.03 0.015  &lt;NA&gt;   NA   NA    NA
#4   var4    4 0.04 0.020  var4    4 0.04 0.020  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#5   var5    5 0.05 0.025  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#6   var6    6 0.06 0.030  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#7   var7    7 0.07 0.035  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#8   var8    8 0.08 0.040  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#9   var9    9 0.09 0.045  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#10 var10   10 0.10 0.050  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
</code></pre>

<p>Ignoring the repeated column headers from regular SQL, I can replicate the same thing in <code>dplyr</code> as shown below. </p>

<pre><code>dat1 %&gt;% left_join(dat2,  by = c(""label"" = ""label"")) %&gt;%
  left_join(dat3,  by = c(""label"" = ""label"")) %&gt;%
  left_join(dat4,  by = c(""label"" = ""label""))

#label beta.x se.x   p.x beta.y se.y   p.y beta.x se.x   p.x beta.y se.y   p.y
#1   var1      1 0.01 0.005      1 0.01 0.005      1 0.01 0.005      1 0.01 0.005
#2   var2      2 0.02 0.010      2 0.02 0.010      2 0.02 0.010      2 0.02 0.010
#3   var3      3 0.03 0.015      3 0.03 0.015      3 0.03 0.015     NA   NA    NA
#4   var4      4 0.04 0.020      4 0.04 0.020     NA   NA    NA     NA   NA    NA
#5   var5      5 0.05 0.025     NA   NA    NA     NA   NA    NA     NA   NA    NA
#6   var6      6 0.06 0.030     NA   NA    NA     NA   NA    NA     NA   NA    NA
#7   var7      7 0.07 0.035     NA   NA    NA     NA   NA    NA     NA   NA    NA
#8   var8      8 0.08 0.040     NA   NA    NA     NA   NA    NA     NA   NA    NA
#9   var9      9 0.09 0.045     NA   NA    NA     NA   NA    NA     NA   NA    NA
#10 var10     10 0.10 0.050     NA   NA    NA     NA   NA    NA     NA   NA    NA
</code></pre>

<p>In regular SQL, I can add a 5th table to the mix and get the expected result.</p>

<pre><code>sqldf(
  ""
select * 
from dat1
  left join dat2
    on dat1.label = dat2.label
  left join dat3
    on dat1.label = dat3.label
  left join dat4
    on dat1.label = dat4.label
  left join dat5
    on dat1.label = dat5.label
""
)

#label beta   se     p label beta   se     p label beta   se     p label beta   se     p label beta   se     p
#1   var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005
#2   var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  &lt;NA&gt;   NA   NA    NA
#3   var3    3 0.03 0.015  var3    3 0.03 0.015  var3    3 0.03 0.015  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#4   var4    4 0.04 0.020  var4    4 0.04 0.020  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#5   var5    5 0.05 0.025  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#6   var6    6 0.06 0.030  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#7   var7    7 0.07 0.035  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#8   var8    8 0.08 0.040  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#9   var9    9 0.09 0.045  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#10 var10   10 0.10 0.050  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
</code></pre>

<p>In <code>dplyr</code>, although I appear to join <code>dat5</code>, I end up dropping <code>dat3</code> and <code>dat4</code>, and repeating the results of <code>dat1</code> and <code>dat2</code>. </p>

<pre><code>dat1 %&gt;% left_join(dat2,  by = c(""label"" = ""label"")) %&gt;%
  left_join(dat3,  by = c(""label"" = ""label"")) %&gt;%
  left_join(dat4,  by = c(""label"" = ""label"")) %&gt;%
  left_join(dat5,  by = c(""label"" = ""label"")) 

#label beta.x se.x   p.x beta.y se.y   p.y beta.x se.x   p.x beta.y se.y   p.y beta   se     p
#1   var1      1 0.01 0.005      1 0.01 0.005      1 0.01 0.005      1 0.01 0.005    1 0.01 0.005
#2   var2      2 0.02 0.010      2 0.02 0.010      2 0.02 0.010      2 0.02 0.010   NA   NA    NA
#3   var3      3 0.03 0.015      3 0.03 0.015      3 0.03 0.015      3 0.03 0.015   NA   NA    NA
#4   var4      4 0.04 0.020      4 0.04 0.020      4 0.04 0.020      4 0.04 0.020   NA   NA    NA
#5   var5      5 0.05 0.025     NA   NA    NA      5 0.05 0.025     NA   NA    NA   NA   NA    NA
#6   var6      6 0.06 0.030     NA   NA    NA      6 0.06 0.030     NA   NA    NA   NA   NA    NA
#7   var7      7 0.07 0.035     NA   NA    NA      7 0.07 0.035     NA   NA    NA   NA   NA    NA
#8   var8      8 0.08 0.040     NA   NA    NA      8 0.08 0.040     NA   NA    NA   NA   NA    NA
#9   var9      9 0.09 0.045     NA   NA    NA      9 0.09 0.045     NA   NA    NA   NA   NA    NA
#10 var10     10 0.10 0.050     NA   NA    NA     10 0.10 0.050     NA   NA    NA   NA   NA    NA
</code></pre>

<p><strong>Am I porting the join to <code>dat5</code> in <code>dplyr</code> properly?</strong></p>

<p><strong>Is it possible to execute this many joins in <code>dplyr</code>?</strong></p>

<p>EDIT1: I believe this is distinct from (<a href=""https://stackoverflow.com/questions/32066402"">How to perform multiple left joins using dplyr in R</a>). <code>Reduce</code> appeared to solve the problem outlined there. </p>

<p>In my case, <code>Reduce</code> produces the same result shown in my last code chunk above. </p>

<p>EDIT2: To be clear, I am not concerned with the multiple left-join syntax. I am trying to determine why greater than 4 joins do not behave as they do in ""regular"" SQL.</p>

<p>EDIT 3: While I had initially accepted an answer from @akrun below, I realized now that the following output: </p>

<pre><code>lst &lt;- lapply(mget(paste0(""dat"", 1:5)), transform, label2 = label)
suppressWarnings( Reduce(function(...) left_join(..., by = ""label""), lst))

#label beta.x se.x   p.x label2.x beta.y se.y   p.y label2.y beta.x se.x   p.x label2.x beta.y se.y   p.y label2.y beta   se     p label2
#1   var1      1 0.01 0.005     var1      1 0.01 0.005     var1      1 0.01 0.005     var1      1 0.01 0.005     var1    1 0.01 0.005   var1
#2   var2      2 0.02 0.010     var2      2 0.02 0.010     var2      2 0.02 0.010     var2      2 0.02 0.010     var2   NA   NA    NA   &lt;NA&gt;
#  3   var3      3 0.03 0.015     var3      3 0.03 0.015     var3      3 0.03 0.015     var3      3 0.03 0.015     var3   NA   NA    NA   &lt;NA&gt;
#  4   var4      4 0.04 0.020     var4      4 0.04 0.020     var4      4 0.04 0.020     var4      4 0.04 0.020     var4   NA   NA    NA   &lt;NA&gt;
#  5   var5      5 0.05 0.025     var5     NA   NA    NA     &lt;NA&gt;      5 0.05 0.025     var5     NA   NA    NA     &lt;NA&gt;   NA   NA    NA   &lt;NA&gt;
#  6   var6      6 0.06 0.030     var6     NA   NA    NA     &lt;NA&gt;      6 0.06 0.030     var6     NA   NA    NA     &lt;NA&gt;   NA   NA    NA   &lt;NA&gt;
#  7   var7      7 0.07 0.035     var7     NA   NA    NA     &lt;NA&gt;      7 0.07 0.035     var7     NA   NA    NA     &lt;NA&gt;   NA   NA    NA   &lt;NA&gt;
#  8   var8      8 0.08 0.040     var8     NA   NA    NA     &lt;NA&gt;      8 0.08 0.040     var8     NA   NA    NA     &lt;NA&gt;   NA   NA    NA   &lt;NA&gt;
#  9   var9      9 0.09 0.045     var9     NA   NA    NA     &lt;NA&gt;      9 0.09 0.045     var9     NA   NA    NA     &lt;NA&gt;   NA   NA    NA   &lt;NA&gt;
#  10 var10     10 0.10 0.050    var10     NA   NA    NA     &lt;NA&gt;     10 0.10 0.050    var10     NA   NA    NA     &lt;NA&gt;   NA   NA    NA   &lt;NA&gt;
</code></pre>

<p>Is still distinct from</p>

<pre><code>sqldf(
  ""
select * 
from dat1
  left join dat2
    on dat1.label = dat2.label
  left join dat3
    on dat1.label = dat3.label
  left join dat4
    on dat1.label = dat4.label
  left join dat5
    on dat1.label = dat5.label
""
)

#label beta   se     p label beta   se     p label beta   se     p label beta   se     p label beta   se     p
#1   var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005
#2   var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  &lt;NA&gt;   NA   NA    NA
#3   var3    3 0.03 0.015  var3    3 0.03 0.015  var3    3 0.03 0.015  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#4   var4    4 0.04 0.020  var4    4 0.04 0.020  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#5   var5    5 0.05 0.025  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#6   var6    6 0.06 0.030  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#7   var7    7 0.07 0.035  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#8   var8    8 0.08 0.040  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#9   var9    9 0.09 0.045  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#10 var10   10 0.10 0.050  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
</code></pre>

<p>I am still losing <code>dat3</code> and <code>dat4</code>...</p>

<p>EDIT 4: Not sure why, but my confusion below may have been partly a version issue. @akrun's final answer works on <code>dplyr_0.4.3</code> and <code>R version 3.3.0</code> (apparently Linux (Ubuntu) and PC).</p>

<p><strong>EDIT 5: I think @akrun and I are both running the dev version of <code>dplyr</code> (which is why I first had problems running @akrun's solution on my windows machine which is using production <code>dplyr</code>.) As noted <a href=""https://github.com/hadley/dplyr/issues/1906"" rel=""nofollow noreferrer"">here</a> this problem has been addressed in the most recent dev version of <code>dplyr</code>.</strong></p>
","<p>We can use <code>Reduce</code> with <code>left_join</code> after placing the datasets in a <code>list</code>.</p>

<pre><code>Reduce(function(...) left_join(..., by = ""label""), mget(paste0(""dat"", 1:5)))
</code></pre>

<hr>

<p>If we need the <code>label</code> columns, we can create one more column</p>

<pre><code>lst &lt;- lapply(mget(paste0(""dat"", 1:5)), transform, label2 = label)
lst[[1]][""label2""] &lt;- NULL
res1 &lt;- suppressWarnings( Reduce(function(...) left_join(..., by = ""label""), lst))
res1
#   label beta.x se.x   p.x beta.y se.y   p.y label2.x beta.x.x se.x.x p.x.x label2.y beta.y.y se.y.y p.y.y label2.x.x beta   se     p
#1   var1      1 0.01 0.005      1 0.01 0.005     var1        1   0.01 0.005     var1        1   0.01 0.005       var1    1 0.01 0.005
#2   var2      2 0.02 0.010      2 0.02 0.010     var2        2   0.02 0.010     var2        2   0.02 0.010       var2   NA   NA    NA
#3   var3      3 0.03 0.015      3 0.03 0.015     var3        3   0.03 0.015     var3       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#4   var4      4 0.04 0.020      4 0.04 0.020     var4       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#5   var5      5 0.05 0.025     NA   NA    NA     &lt;NA&gt;       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#6   var6      6 0.06 0.030     NA   NA    NA     &lt;NA&gt;       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#7   var7      7 0.07 0.035     NA   NA    NA     &lt;NA&gt;       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#8   var8      8 0.08 0.040     NA   NA    NA     &lt;NA&gt;       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#9   var9      9 0.09 0.045     NA   NA    NA     &lt;NA&gt;       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#10 var10     10 0.10 0.050     NA   NA    NA     &lt;NA&gt;       NA     NA    NA     &lt;NA&gt;       NA     NA    NA       &lt;NA&gt;   NA   NA    NA
#   label2.y.y
#1        var1
#2        &lt;NA&gt;
#3        &lt;NA&gt;
#4        &lt;NA&gt;
#5        &lt;NA&gt;
#6        &lt;NA&gt;
#7        &lt;NA&gt;
#8        &lt;NA&gt;
#9        &lt;NA&gt;
#10       &lt;NA&gt;
</code></pre>

<p>Here is the OP's output from the second <code>sqldf</code> code block</p>

<pre><code>res2
#   label beta   se     p label beta   se     p label beta   se     p label beta   se     p label beta   se     p
#1   var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005  var1    1 0.01 0.005
#2   var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  var2    2 0.02 0.010  &lt;NA&gt;   NA   NA    NA
#3   var3    3 0.03 0.015  var3    3 0.03 0.015  var3    3 0.03 0.015  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#4   var4    4 0.04 0.020  var4    4 0.04 0.020  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#5   var5    5 0.05 0.025  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#6   var6    6 0.06 0.030  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#7   var7    7 0.07 0.035  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#8   var8    8 0.08 0.040  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#9   var9    9 0.09 0.045  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA
#10 var10   10 0.10 0.050  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA  &lt;NA&gt;   NA   NA    NA


dim(res1)
#[1] 10 20
dim(res2)
#[1] 10 20
</code></pre>
"
40563479,"Relationship between R Markdown, Knitr, Pandoc, and Bookdown",2,2,2,"<p>What is the relationship between the functionality of R Markdown, Knitr, Pandoc, and Bookdown?</p>

<p>Specifically what is the 'division of labour' between these packages in converting markup documents with embedded R code (e.g. <code>.Rnw</code> or <code>.Rmd</code>) into final outputs (e.g. <code>.pdf</code> or <code>.html</code>)?  And if Knitr is used to process RMarkdown, what does the <code>rmarkdown</code> package do and how is it different to the <code>markdown package</code>?</p>
","<h1>Pandoc</h1>

<p>Pandoc is a document converter. It can convert from a number of different markup formats to many other formats, such as <code>.doc</code>, <code>.pdf</code> etc.  </p>

<p>Pandoc is a command line tool with no GUI. It is an independent piece of software, separate from R.  However, it comes bundled with R Studio because <code>rmarkdown</code> relies on it for document conversion. </p>

<p>Pandoc not only converts documents, but it also adds functionality on top of the base markdown language to enable it to support more complex outputs.</p>

<h1>R Markdown</h1>

<p>R Markdown is based on markdown:</p>

<h3>Markdown (markup language)</h3>

<p>Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension <code>.md</code>.</p>

<p>Like other markup languages like HTML and Latex, it is completely independent from R.</p>

<p>There is no clearly defined Markdown standard. This has led to fragmentation as different vendors write their own variants of the language to correct flaws or add missing features.</p>

<h3>Markdown (R package)</h3>

<p><a href=""https://cran.r-project.org/web/packages/markdown/index.html"" rel=""noreferrer""><code>markdown</code></a> is an R package which converts <code>.Rmd</code> files into HTML.  It is the predecessor of <a href=""https://cran.r-project.org/web/packages/rmarkdown/index.html"" rel=""noreferrer""><code>rmarkdown</code></a>, which offers much more functionality.  It is no longer recommended for use.</p>

<h3>R Markdown (markup language)</h3>

<p>R Markdown is an extension of the markdown syntax.  R Markdown files are plain text files that typically have the file extension <code>.Rmd</code>.  They are written using an extension of markdown syntax that enables R code to be embedded in them in a way which can later be executed.  </p>

<p>Because they are expected to be processed by the <code>rmarkdown</code> package, it is possible to use <a href=""http://pandoc.org/MANUAL.html#pandocs-markdown"" rel=""noreferrer"">Pandoc markdown syntax</a> as part of a R markdown file.  This is an extension to the original markdown syntax that provides additional functionality like raw HTML/Latex and tables.</p>

<h3>R Markdown (package)</h3>

<p>The R package <code>rmarkdown</code> is a library which proceses and converts <code>.Rmd</code> files into a number of different formats.</p>

<p>The core function is <code>rmarkdown::render</code> which <a href=""https://blog.rstudio.org/2014/06/18/r-markdown-v2/"" rel=""noreferrer"">stands on the shoulders of pandoc</a>.   This function <a href=""https://www.rdocumentation.org/packages/rmarkdown/versions/0.1.2/topics/render?"" rel=""noreferrer"">'renders the input file to the specified output format using pandoc. If the input requires knitting then <code>knitr::knit</code> is called prior to pandoc.</a></p>

<p>The RMarkdown package's aim is simply <a href=""https://blog.rstudio.org/2014/06/18/r-markdown-v2/"" rel=""noreferrer"">to provide reasonably good defaults and an R-friendly interface to customize Pandoc options.</a>.</p>

<p>The YAML metadata seen at the top of RMarkdown files is specificially to pass options to <code>rmarkdown::render</code>, to guide the build process.</p>

<p>Note that RMarkdown only deals with markdown syntax.  If you want to convert a <code>.Rhtml</code> or a <code>.Rnw</code> file, you should use the convenience functions built into <code>Knitr</code>, such as <code>knitr::knit2html</code> and <code>knitr:knit2pdf</code></p>

<h1>Knitr</h1>

<p>Knitr takes a plain text document with embedded code, executes the code and 'knits' the results back into the document.</p>

<p>For for example, it converts </p>

<ul>
<li>An <a href=""https://github.com/yihui/knitr-examples/blob/master/001-minimal.Rmd"" rel=""noreferrer"">R Markdown (<code>.Rmd</code>)</a> file into a standard markdown file (<code>.md</code>) </li>
<li>An <a href=""https://github.com/yihui/knitr/blob/master/inst/examples/knitr-minimal.Rnw"" rel=""noreferrer""><code>.Rnw</code> (Sweave)</a> file into to <code>.tex</code> format.  </li>
<li>An <a href=""https://github.com/yihui/knitr-examples/blob/master/003-minimal.Rhtml"" rel=""noreferrer""><code>.Rhtml</code></a> file into to html.</li>
</ul>

<p>The core function is <code>knitr::knit</code> and by default this will look at the input document and try and guess what type it is - Rnw, Rmd etc.</p>

<p>This core function performs three roles:
- A source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.
- A code evaluator, which evaluates this code
- An output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type.  For instance, if the input file is an <code>.Rmd</code>, the output render marks up the output of code evaluation in <code>.md</code> format.</p>

<h3>Converting between document formats</h3>

<p>Knitr does <em>not</em> convert between document formats - such as converting a <code>.md</code> into a <code>.html</code>.  It does, however, provide some convenience functions to help you use other libraries to do this.  <em>If you are using the <code>rmarkdown</code> package, you should ignore this functionality because it has been superceded by <code>rmarkdown::render</code>.</em></p>

<p>An example is <code>knitr:knit2pdf</code> which will: <a href=""https://www.rforge.net/doc/packages/knitr/knit2pdf.html"" rel=""noreferrer"">'Knit the input Rnw or Rrst document, and compile to PDF using texi2pdf or rst2pdf'.</a></p>

<p>A potential source of confusion is <code>knitr::knit2html</code>, which <a href=""https://rforge.net/doc/packages/knitr/knit2html.html"" rel=""noreferrer"">""is a convenience function to knit the input markdown source and call <code>markdown::markdownToHTML</code> to convert the result to HTML.""</a>  This is now legacy functionality because the <code>markdown</code> package has been superceded by the <code>rmarkdown</code> package.  See <a href=""https://rforge.net/doc/packages/knitr/knit2html.html"" rel=""noreferrer"">this note</a>.</p>

<h1>Bookdown</h1>

<p>The bookdown package is built on top of R Markdown, and inherits the simplicity of the Markdown syntax , as well as the possibility of multiple types of output formats (PDF/HTML/Word/…). </p>

<p>It offers features like multi-page HTML output, numbering and cross-referencing figures/tables/sections/equations, inserting parts/appendices, and imported the GitBook style (<a href=""https://www.gitbook.com"" rel=""noreferrer"">https://www.gitbook.com</a>) to create elegant and appealing HTML book pages. </p>
"
42757647,want to expand a large bipartite network plot avoid vertices overlapped,1,3,3,"<p>I was plotting a bipartite graph using igraph package with R. There are about 10,000 edges, I want to expand the width of the whole plot to avoid state vertices overlapped. </p>

<p>my data looks like this:</p>

<pre><code>&gt; test2 
                 user_id state  meanlat    meanlon countUS countS degState
                   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;int&gt;  &lt;int&gt;    &lt;int&gt;
1 -_1ctLaz3jhPYc12hKXsEQ    NC 35.19401  -80.83235     909      3    18487
2 -_1ctLaz3jhPYc12hKXsEQ    NV 36.11559 -115.18042      29      3    37884
3 -_1ctLaz3jhPYc12hKXsEQ    SC 35.05108  -80.96166       4      3      665
4 -0wUMy3vgInUD4S6KJInnw    IL 40.11227  -88.22955       2      3     1478
5 -0wUMy3vgInUD4S6KJInnw    NV 36.11559 -115.18042      23      3    37884
6 -0wUMy3vgInUD4S6KJInnw    WI 43.08051  -89.39835      20      3     3963
</code></pre>

<p>and below is my code on graph creating and setting.</p>

<pre><code>g2 &lt;- graph_from_data_frame(test2,directed = F)
V(g2)$type &lt;- ifelse(names(V(g2)) %in% UserStateR$user_id, 'user', 'state')
V(g2)$label &lt;- ifelse(V(g2)$type == 'user', "" "", paste(names(V(g2)),""\n"",as.character(test2$degState),sep=""""))
V(g2)$size &lt;- ifelse(V(g2)$type == 'user', 3, 20)
V(g2)$color &lt;- ifelse(V(g2)$type == 'user', 'wheat', 'salmon')
V(g2)$type &lt;- ifelse(names(V(g2)) %in% UserStateR$user_id, T, F )
E(g2)$color &lt;- heat.colors(8)[test2$countS]
plot(g2,layout=layout.bipartite(g2, types = names(V(g2)) %in% UserStateR$state, hgap = 50, vgap = 50))
</code></pre>

<p>as you can see, I have tried to change the <code>hgap</code> and <code>vgap</code> arguments, but it doesn't work apparently. I have also tried <code>asp</code> argument, but that is not what I want.</p>

<p><a href=""https://i.stack.imgur.com/SspeY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SspeY.png"" alt=""enter image description here""></a></p>
","<p>I know this might be too late for @floatsd but I was struggling with this today and had a really hard time finding an answer, so this might help others out.</p>

<p>First, in general, there is a an attribute to <code>iplot.graph</code> called <code>asp</code> that very simply controls how rectangular your plot is. Simply do</p>

<pre><code>l=layout.bipartite(CCM_net)    
plot(CCM_net, layout=l, asp=0.65)
</code></pre>

<p>for a wide plot. <code>asp</code> smaller than 1 gives you a wide plot, <code>asp</code> larger than 1 a tall plot.</p>

<p>However, this might still not give you the layout you want. The bipartite command basically generates a matrix with coordinates for your vertices, and I actually don't understand yet how it comes up with the x-coordinates, so I ended up changing them myself.
Below the example (I am assuming you know how to turn your data into data frames with the edge list and edge/vertex attributes for making graphs so am skipping that).
My data is <code>CCM_data_sign</code> and is</p>

<pre><code>    from   to value
2    EVI MAXT  0.67
4    EVI MINT  0.81
5    EVI    P  0.70
7    EVI   SM  0.79
8    EVI  AMO  0.86
11  MAXT  EVI  0.81
18  MAXT  AMO  0.84
21 MEANT  EVI  0.88
28 MEANT  AMO  0.83
29 MEANT  PDO  0.71
31  MINT  EVI  0.96
39  MINT  PDO  0.78
40  MINT  MEI  0.66
41     P  EVI  0.91
49     P  PDO  0.77
50     P  MEI  0.71
51   PET  EVI  0.90
58   PET  AMO  0.89
59   PET  PDO  0.70
61    SM  EVI  0.94
68    SM  AMO  0.90
69    SM  PDO  0.81
70    SM  MEI  0.73
74   AMO MINT  0.93
76   AMO  PET  0.66
79   AMO  PDO  0.71
80   AMO  MEI  0.83
90   PDO  MEI  0.82
</code></pre>

<p>The data frame I generated for graphing is called <code>CCM_net</code>.
First a bipartite plot without any layout adjustments</p>

<pre><code>V(CCM_net)$size&lt;-30
l=layout.bipartite(CCM_net)
plot(CCM_net,
     layout=l,
     edge.arrow.size=1,
     edge.arrow.width=2,
     vertex.label.family=""Helvetica"",
     vertex.label.color=""black"",
     vertex.label.cex=2,
     vertex.label.dist=c(3,3,3,3,3,3,3,3,3,3,3),
     vertex.label.degree=c(pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,pi/2,pi/2,pi/2), #0 is right, “pi” is left, “pi/2” is below, and “-pi/2” is above
     edge.lty=1)
</code></pre>

<p>This gives you the following
<a href=""https://i.stack.imgur.com/ZboK9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZboK9.jpg"" alt=""regualt_bipartite""></a></p>

<p>If I use <code>asp</code> I get the following</p>

<pre><code>plot(CCM_net,
     layout=l,
     edge.arrow.size=1,
     vertex.label.family=""Helvetica"",
     vertex.label.color=""black"",
     vertex.label.cex=2,
     vertex.label.dist=c(3,3,3,3,3,3,3,3,3,3,3),
     vertex.label.degree=c(pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,pi/2,pi/2,pi/2), #0 is right, “pi” is left, “pi/2” is below, and “-pi/2” is above
     edge.arrow.width=2,
     edge.lty=1,
     asp=0.6) # controls how rectangular the plot is. &lt;1 = wide, &gt;1 = tall
dev.off()
</code></pre>

<p><a href=""https://i.stack.imgur.com/kvnxH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kvnxH.jpg"" alt=""bipartite_with asp""></a></p>

<p>This is looking better, but still not really what I want - see how some vertices are closer to each other than others?
So eventually I took the following approach. Setting the coordinates as bipartite looks like this</p>

<pre><code>coords &lt;- layout_as_bipartite(CCM_net)
coords
        [,1] [,2]
 [1,]  3.0    0
 [2,]  0.0    1
 [3,]  2.0    1
 [4,]  3.5    1
 [5,]  6.0    1
 [6,]  1.0    1
 [7,]  5.0    1
 [8,]  7.0    1
 [9,]  1.0    0
[10,]  4.5    0
[11,]  5.5    0
</code></pre>

<p>This matrix shows the x coordinates of your vertices in the first columns and the y coordinates in the second column, ordered according to your list with names. My list with names is</p>

<pre><code>     id  name
1    EVI   EVI
2   MAXT  MAXT
3  MEANT MEANT
4   MINT  MINT
5      P     P
6    PET   PET
7     SM    SM
8     SR    SR
9    AMO   AMO
10   PDO   PDO
11   MEI   MEI 
</code></pre>

<p>In my graph, EVI, AMO and PDO are on the bottom, but note their x coordinates: 3.0, 1.0, 4.5 and 5.5. I haven't figured out yet how the code comes up with that, but I don't like it so I simply changed it.</p>

<pre><code>coords[,1]=c(2,0,4,8,12,16,20,24,9,16,24) 
</code></pre>

<p>Now the plotting code (also with <code>asp</code>) and the output becomes</p>

<pre><code>plot(CCM_net,
 layout=coords,
 edge.arrow.size=1,
 vertex.label.family=""Helvetica"",
 vertex.label.color=""black"",
 vertex.label.cex=1,
 vertex.label.dist=c(4,4,4,4,4,4,4,4,4,4,4),
 vertex.label.degree=c(pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,-pi/2,pi/2,pi/2,pi/2), #0 is right, “pi” is left, “pi/2” is below, and “-pi/2” is above
 edge.arrow.width=2,
 edge.lty=1,
 asp=0.6) # controls how rectangular the plot is. &lt;1 = wide, &gt;1 = tall
</code></pre>

<p><a href=""https://i.stack.imgur.com/rRX4c.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rRX4c.jpg"" alt=""bipartite_final""></a></p>

<p>Now the vertices are nicely spaced in a rectangular plot!</p>

<p>Note - I also decreased the size of the vertices, the size of the labels and their positioning, for better readability.</p>
"
14860313,hierarchy in voronoi tessellations,1,3,3,"<p>I am working with voronoi tessellations. I have different polygons representing regions in the tessellations. </p>

<p>The points below are used to draw the tessellation in the figure.</p>

<pre><code>tessdata
    [,1]       [,2]
1  -0.4960583 -0.3529047
2  -2.4986929  0.8897895
3   3.6514561 -1.3533369
4  -1.7263101 -5.5341202
5   2.2140143  0.3883696
6  -2.5208933 -1.4881461
7  -3.2556913  4.4535629
8   0.6423109 -2.8350062
9  -0.4160715  1.2676151
10  4.4059361  4.5641771
</code></pre>

<p>Using <code>tessdata</code> as input to draw the tessellation as below:</p>

<pre><code>library(deldir)
dd&lt;-deldir(tessdata[,1], tessdata[,2])
plot(dd,wlines=""tess"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/119dJ.png"" alt=""enter image description here""></p>

<p>Sammon coordinates are below.</p>

<pre><code>       [,1]        [,2]
1   3.14162704 -1.45728604
2   2.35422623  2.46437927
3  -0.85051049  2.71503294
4   1.94310458 -0.45936958
5   0.08737757  3.74324701
6   1.23007799  1.34443842
7   0.01571924  2.19322032
8   1.43320754  2.64818631
9  -0.05463431  0.66980876
10   1.51344967  5.03351176
</code></pre>

<p>I want to construct the tessellations for which the sammon coordinate points are input. The tessellation using these points should be within one of the regions in the figure shown and for that, the above points should be scaled or we can restrict the plot of the tessellation within one of the regions in the above figure.</p>

<p>Hope i have covered all the necessary data.</p>

<p>P.S: </p>

<p>sammon's projection comes in ""MASS"" package.
voronoi tessellations from ""deldir"" package.</p>

<p>dirsgs argument of the deldir function output will give the coordinates of the points forming the lines in the tessellations.</p>

<p>segments function of package graphics can be used to join the 2 points whose coordinates are extracted from dirsgs.</p>
","<p>If you want to restrict the second set of points
to one of the tiles of the tessellation,
you can use <code>tile.list</code> to have a description of each tile,
and then check which points are in this tile
(there are many functions to do so:
in the following example, I use <code>secr::pointsInPolygon</code>).</p>

<pre><code># Sample data
x &lt;- matrix( rnorm(20), nc = 2 )
y &lt;- matrix( rnorm(1000), nc=2 )

# Tessellation
library(deldir)
d &lt;- deldir(x[,1], x[,2])
plot(d, wlines=""tess"")

# Pick a cell at random 
cell &lt;- sample( tile.list(d), 1 )[[1]]
points( cell$pt[1], cell$pt[2], pch=16 )
polygon( cell$x, cell$y, lwd=3 )

# Select the points inside that cell
library(secr)
i &lt;- pointsInPolygon(
  y, 
  cbind( 
    c(cell$x,cell$x[1]), 
    c(cell$y,cell$y[1])
  )
)
points(y[!i,], pch=""."")
points(y[i,], pch=""+"")

# Compute a tessellation of those points
dd &lt;- deldir(y[i,1], y[i,2])
plot(dd, wlines=""tess"", add=TRUE)
</code></pre>

<p><img src=""https://i.stack.imgur.com/yT8Tg.png"" alt=""Tessellation inside a cell of another tessellation""></p>

<p>If, instead, you want to translate and rescale the points 
to fit them into the tile, that is trickier.</p>

<p>We need to somehow estimate how far away from the tile the points are:
to this end, let us define a few auxilliary functions to compute,
first the distance from a point to a segment, 
then the distance from a point to a polygon.</p>

<pre><code>distance_to_segment &lt;- function(M, A, B) {
  norm &lt;- function(u) sqrt(sum(u^2))
  lambda &lt;- sum( (B-A) * (M-A) ) / norm(B-A)^2
  if( lambda &lt;= 0 ) {
    norm(M-A)
  } else if( lambda &gt;= 1 ) {
    norm(M-B)
  } else {
    N &lt;- A + lambda * (B-A)
    norm(M-N)
  }
}
A &lt;- c(-.5,0)
B &lt;- c(.5,.5)
x &lt;- seq(-1,1,length=100)
y &lt;- seq(-1,1,length=100)
z &lt;- apply(
  expand.grid(x,y), 
  1, 
  function(u) distance_to_segment( u, A, B )
)
par(las=1)
image(x, y, matrix(z,nr=length(x)))
box()
segments(A[1],A[2],B[1],B[2],lwd=3)

library(secr)
distance_to_polygon &lt;- function(x, poly) {
  closed_polygon &lt;- rbind(poly, poly[1,])
  if( pointsInPolygon( t(x), closed_polygon ) )
    return(0)
  d &lt;- rep(Inf, nrow(poly))
  for(i in 1:nrow(poly)) {
    A &lt;- closed_polygon[i,]
    B &lt;- closed_polygon[i+1,]
    d[i] &lt;- distance_to_segment(x,A,B)
  }
  min(d)
}
x &lt;- matrix(rnorm(20),nc=2)
poly &lt;- x[chull(x),]
x &lt;- seq(-5,5,length=100)
y &lt;- seq(-5,5,length=100)
z &lt;- apply(
  expand.grid(x,y), 
  1, 
  function(u) distance_to_polygon( u, poly )
)
par(las=1)
image(x, y, matrix(z,nr=length(x)))
box()
polygon(poly, lwd=3)
</code></pre>

<p>We can now look for a transformation of the form </p>

<pre><code>x --&gt; lambda * x + a
y --&gt; lambda * y + b
</code></pre>

<p>that minimizes the (sum of the squared) distances to the polygon.
That is actually not sufficient: we are likely to end up with scaling factor 
lambda equal to (or close to) zero. 
To avoid this, we can add a penalty if lambda is small.</p>

<pre><code># Sample data 
x &lt;- matrix(rnorm(20),nc=2)
x &lt;- x[chull(x),]
y &lt;- matrix( c(1,2) + 5*rnorm(20), nc=2 )
plot(y, axes=FALSE, xlab="""", ylab="""")
polygon(x)

# Function to minimize:
# either the sum of the squares of the distances to the polygon, 
# if at least one point is outside, 
# or minus the square of the scaling factor.
# It is not continuous, but (surprisingly) that does not seem to be a problem.
f &lt;- function( p ) {
  lambda &lt;- log( 1 + exp(p[1]) )
  a &lt;- p[2:3]
  y0 &lt;- colMeans(y)
  transformed_points &lt;- t( lambda * (t(y)-y0) + a )
  distances &lt;- apply(
    transformed_points, 
    1, 
    function(u) distance_to_polygon(u, x)
  )
  if( all(distances == 0) ) - lambda^2
  else                      sum( distances^2 )
}
# Minimize this function
p &lt;- optim(c(1,0,0), f)$par
# Compute the optimal parameters
lambda &lt;- log( 1 + exp(p[1]) )
a &lt;- p[2:3]
y0 &lt;- colMeans(y)
# Compute the new coordinates
transformed_points &lt;- t( lambda * (t(y)-y0) + a )
# Plot them
segments( y[,1], y[,2], transformed_points[,1], transformed_points[,2], lty=3 )
points( transformed_points, pch=3 )
library(deldir)
plot( 
  deldir( transformed_points[,1], transformed_points[,2] ), 
  wlines=""tess"", add=TRUE 
)
</code></pre>

<p><img src=""https://i.stack.imgur.com/Ym5VG.png"" alt=""Shifting and rescaling a set of points to put them inside a polygon""></p>
"
10525957,How to draw lines outside of plot area in ggplot2?,1,3,1,"<p>I created this plot with <strong>ggplot2</strong>:</p>

<p><img src=""https://i.stack.imgur.com/jmnzc.jpg"" alt=""enter image description here""></p>

<p>The outside lines need to correspond to the Y scale, (i.e the Y position of the lines for Text1 should be 100 and 85). The only way I can do it by drawing a blank plot to the right of the figure with the same scale as the barchart and then using annotate function to draw the lines. Another approach is to simply ""manually"" draw the lines with <code>grid.lines</code>, however the coordinates of <code>grid.lines</code> will not correspond to the Y scale of the plot.</p>

<p>Is it possible to somehow draw these lines using a different approach? I assume it would have to be done with <code>grid.lines</code>. How could I pass Y coordindates of the barchart to <code>grid.lines</code>?</p>

<p>Below is the minimal code used to create this figure:</p>

<pre><code>library (ggplot2)
test= data.frame(
  group=c(rep(1,6), rep(2,6)),
  subgroup=c( 1,1,1,2,2,2,1,1,1,2,2,2),
  category=c( rep(1:3, 4)),
  count=c( 10,80,10,5,90,5,  10,80,10,5,90,5   )
  )

qplot(subgroup, 
      count, 
      data=test, 
      geom=""bar"", 
      stat=""identity"",
      fill =category,  
      facets =  .~ group,  width=0.9)+
      opts(legend.position=""none"",
           plot.margin = unit(c(0,9,2,0), ""lines""))
</code></pre>

<p><img src=""https://i.stack.imgur.com/WslAN.jpg"" alt=""enter image description here""></p>

<p>How can I draw the lines to the right of the bars?</p>

<p>I recently asked a question about drawing text outside of plot area in <strong>ggplot2</strong> and the solution was to use <code>gt$layout</code> and <code>grid.draw</code>. </p>

<p><a href=""https://stackoverflow.com/questions/10014187/displaying-text-below-the-plot-generated-by-ggplot"">Displaying text below the plot generated by ggplot2</a></p>

<p>Could the similar approach be used here? It is my understanding that annotation_custom is for text only and won't work with other graphical elements.
Thanks</p>
","<p><strong>Update</strong> </p>

<p>The original solution used <code>annotation_custom</code>, but a problem with <code>annotation_custom</code> is that it draws the annotation in all panels. However, with a simple modification, <code>annotation_custom</code> can be made to draw in one panel only (taken from Baptiste's answer <a href=""https://stackoverflow.com/questions/32807665/removing-one-tablegrob-when-applied-to-a-box-plot-with-a-facet-wrap/32832732#32832732"">here</a>)</p>

<pre><code>annotation_custom2 &lt;- 
function (grob, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, data) 
{
  layer(data = data, stat = StatIdentity, position = PositionIdentity, 
        geom = ggplot2:::GeomCustomAnn,
        inherit.aes = TRUE, params = list(grob = grob, 
                                          xmin = xmin, xmax = xmax, 
                                          ymin = ymin, ymax = ymax))
}

library(ggplot2)
library(grid)

 #Some data
test = data.frame(
  group=c(rep(1,6), rep(2,6)),
  subgroup=c( 1,1,1,2,2,2,1,1,1,2,2,2),
  category=c( rep(1:3, 4)),
  count=c( 10,80,10,5,90,5,  10,80,10,5,90,5   )
  )

# base plot
p &lt;- ggplot(test) +
   geom_bar(aes(subgroup, count, fill = category), stat = ""identity"") +
   facet_grid(. ~ group) +
  theme(legend.position = ""none"",  
        plot.margin = unit(c(1,5,1,1), ""lines""))

# Create the text Grobs
Text1 = textGrob(""Text 1"")
Text2 = textGrob(""Text 2"")
Text4 = textGrob(""Text 4"")

## Add the annotations
# Which panel to attach the annotations
data = data.frame(group=2)

# Text 1
p1 = p + annotation_custom2(Text1,  xmin = 3., xmax = 3., ymin = 85, ymax = 100, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 100, ymax = 100, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 85, ymax = 85, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.75, xmax = 2.75, ymin = 85, ymax = 100, data = data)

# Text 2
p1 = p1 + annotation_custom2(Text2,  xmin = 3, xmax = 3, ymin = 20, ymax = 80, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 80, ymax = 80, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 20, ymax = 20, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.75, xmax = 2.75, ymin = 20, ymax = 80, data = data)

# Text 4
p1 = p1 + annotation_custom2(Text4,  xmin = 3, xmax = 3, ymin = 0, ymax = 15, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 15, ymax = 15, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 0, ymax = 0, data = data) +
    annotation_custom2(linesGrob(), xmin = 2.75, xmax = 2.75, ymin = 0, ymax = 15, data = data)


# Code to override clipping
gt &lt;- ggplotGrob(p1)
gt$layout[grepl(""panel"", gt$layout$name), ]$clip &lt;- ""off""

# Draw the plot
grid.newpage()
grid.draw(gt)
</code></pre>

<p><strong>Original Solution</strong></p>

<p>I think almost any Grob created using <code>grid()</code> can be used in <code>annotation_custom()</code>. 
There might be neater ways to do this, but here's a way using <code>grid</code>, <code>annotation_custom</code> and @baptiste's code <a href=""https://stackoverflow.com/questions/9690648/point-clipped-on-x-axis-in-ggplot"">from here</a> to override the clipping (as in the earlier post).</p>

<pre><code>library (ggplot2)
library(grid)

test= data.frame(
  group=c(rep(1,6), rep(2,6)),
  subgroup=c( 1,1,1,2,2,2,1,1,1,2,2,2),
  category=c( rep(1:3, 4)),
  count=c( 10,80,10,5,90,5,  10,80,10,5,90,5   )
  )

## EDIT:  Updated qplot() command
p &lt;- qplot(subgroup, count, 
  data = test, geom = ""bar"",  stat = ""identity"",
  fill = category,  
  facets = .~ group,  width = 0.9)+
  theme(legend.position=""none"",  plot.margin = unit(c(0,9,2,0), ""lines""))



# Create the text Grobs
Text1 = textGrob(""Text 1"")
Text2 = textGrob(""Text 2"")
Text4 = textGrob(""Text 4"")

# Draw the plot
# Text 1
p1 = p + annotation_custom(grob = Text1,  xmin = 3., xmax = 3., ymin = 85, ymax = 100) +
    annotation_custom(grob = linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 100, ymax = 100) +
    annotation_custom(grob = linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 85, ymax = 85) +
    annotation_custom(grob = linesGrob(), xmin = 2.75, xmax = 2.75, ymin = 85, ymax = 100)

# Text 2
p1 = p1 + annotation_custom(grob = Text2,  xmin = 3, xmax = 3, ymin = 20, ymax = 80) +
    annotation_custom(grob = linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 80, ymax = 80) +
    annotation_custom(grob = linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 20, ymax = 20) +
    annotation_custom(grob = linesGrob(), xmin = 2.75, xmax = 2.75, ymin = 20, ymax = 80)

# Text 4
p1 = p1 + annotation_custom(grob = Text4,  xmin = 3, xmax = 3, ymin = 0, ymax = 15) +
    annotation_custom(grob = linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 15, ymax = 15) +
    annotation_custom(grob = linesGrob(), xmin = 2.6, xmax = 2.75, ymin = 0, ymax = 0) +
    annotation_custom(grob = linesGrob(), xmin = 2.75, xmax = 2.75, ymin = 0, ymax = 15)

p1

# Code to override clipping
gt &lt;- ggplot_gtable(ggplot_build(p1))
gt$layout$clip[gt$layout$name==""panel""] &lt;- ""off""
grid.draw(gt)
</code></pre>

<p><img src=""https://i.stack.imgur.com/LBSKu.png"" alt=""enter image description here""></p>
"
41666580,Show layer in leaflet map in Shiny only when zoom level > 8 with LayersControl?,2,2,2,"<p>I want to show a layer only when its clicked in the LayersControl and the zoom level is greater than a certain number, e.g. 8. One of the reasons is, that some expensive computations must be performed to get the layer coordinates. I want to use the layerscontrol and not an extra input button (for optical reasons).</p>

<p>Is there a way to retrieve the value, if the layer button is clicked in the layerscontrol?</p>

<p>Here is a simple example (not working):</p>

<pre><code>library(leaflet) 
library(shiny)

ui &lt;- fluidPage(
  leafletOutput(""map"", width = ""100%"", height = ""700"")
)

server &lt;- function(input, output){
  output$map &lt;- renderLeaflet({
    leaflet() %&gt;% addTiles() %&gt;% setView(10.4, 50.3, 7) %&gt;%
      addLayersControl(overlayGroups = c(""marker""),
                       options = layersControlOptions(collapsed = FALSE))
  })

  observe({
   # if (input$marker == TRUE){ # how to get value if layercontrol is clicked?
      if (input$map_zoom &gt; 8) {
        leafletProxy(""map"") %&gt;% addMarkers(lng = 10.5, lat = 50, group = ""marker"")
      }
  #  }
  })
}

shinyApp(ui = ui, server = server)
</code></pre>
","<p>Here is a first running version. Maybe smdy comes up with sthg ""cleaner"" :).</p>

<p>Here a small explanation:</p>

<p><strong>Challenge 1: input$marker does not exist as shiny input.</strong>
Open your app (in a browser), make a right click on the marker input you are interested in and select ""Inspect Element"" or the equivilant label in your browser. You will see the code of that input.
So why cant you access it. To see the difference to the kind of input you know from shiny, create a <code>textinput</code> or sthg and make ""inspect element"" as well. You see that the shiny-inputs have an id,....the marker input does not</p>

<p><strong>Challenge 2: Access input that does not have an id:</strong>
(From here on you should know how to send messages from JS to R and back: A very good article you will find here: <a href=""https://ryouready.wordpress.com/2013/11/20/sending-data-from-client-to-server-and-back-using-shiny/"" rel=""nofollow noreferrer"">https://ryouready.wordpress.com/2013/11/20/sending-data-from-client-to-server-and-back-using-shiny/</a>)
How to access the input: Well, thats basically just finding the right snippet via google. In the end this: <code>document.getElementsByTagName(""input"")</code>.
(Attention: From here on I assume you only have one input)
And know it gets a bit tricky. Try to access
this input. Via <code>console.log()</code> you can print to javascript console (and open it in the running app via ""F12"" --> Console (JS).)
You can print this input as HtMLCollection but can not access it, which can be very confusing.</p>

<p><strong>Challenge 3: Access HTMLCollection</strong></p>

<p>The reason (in short) why you can not access it is that the JS code is called before the ""DOM"" is build. It would work totally fine if the script is called after ""<code>&lt;body&gt;&lt;/body&gt;</code>"". But thats not that easy with plain vanilla shiny. You can try <code>window.onload()</code> or <code>document.ready()</code>.
What is the most reliable for me so far is to use: <strong>session$onFlushed()</strong> and trigger to send the JSCode within that function from R to ""JS"".
(And then send the value as an input back to R via <code>Shiny.onInputChange(""marker"", inputs[0].checked)</code>; ) --> This will produce the desired ""input$marker"".
However, this function only fires once, which is totally right behaviour. But you wont have updates when you click the button. </p>

<p><strong>Challenge 4: Update input$marker</strong>
Well the pretty version would be to have a function <code>.onclicked()</code>/ a listener for the input. Maybe somebody could find a solution. I tried a workaround in shiny, that i tell shiny to constantly get value of the input via <code>autoInvalidate()</code>. </p>

<p><strong>Challenge 5:</strong>
Well, not that difficult, because it is shiny only, but for sake of completeness. Given the provided code in the question, the marker will stay when loaded once. Not sure if you want it to stay or to be removed once your zooming criteria is not met.
Anyway, if you want it to disappear, <code>%&gt;% clearMarkers()</code> is your friend.</p>

<pre><code>library(leaflet)
library(shiny)

getInputwithJS &lt;- '
Shiny.addCustomMessageHandler(""findInput"",
  function(message) {
  var inputs = document.getElementsByTagName(""input"");
  Shiny.onInputChange(""marker"", inputs[0].checked);
}
);
'

ui &lt;- fluidPage(

  leafletOutput(""map"", width = ""100%"", height = ""700""),
  tags$head(tags$script(HTML(getInputwithJS)))
)

server &lt;- function(input, output, session){
  global &lt;- reactiveValues(DOMRdy = FALSE)
  output$map &lt;- renderLeaflet({
    leaflet() %&gt;% addTiles() %&gt;% setView(10.4, 50.3, 7) %&gt;%
      addLayersControl(overlayGroups = c(""marker""),
                       options = layersControlOptions(collapsed = FALSE))
  })

  autoInvalidate &lt;- reactiveTimer(1)

  observe({
    autoInvalidate()
    if(global$DOMRdy){
      session$sendCustomMessage(type = ""findInput"", message = """")      
    }
  })

  session$onFlushed(function() {
    global$DOMRdy &lt;- TRUE
  })

  observe({
    if (!is.null(input$marker)){
      if (input$marker == TRUE){ # how to get value if layercontrol is clicked?
        if (input$map_zoom &gt; 8) {
          leafletProxy(""map"") %&gt;% addMarkers(lng = 10.5, lat = 50, group = ""marker"")
        }else{
          leafletProxy(""map"") %&gt;% clearMarkers()
        }
      }
    }
  })
}

shinyApp(ui = ui, server = server)
</code></pre>
"
22410606,Violin plot with list input,1,3,3,"<p>I am using the vioplot funciton form the vioplot package, and I would like to feed a list as an input. So this is what I have:</p>

<pre><code># Violin Plots
library(vioplot)
x1 &lt;- mtcars$mpg[mtcars$cyl==4]
x2 &lt;- mtcars$mpg[mtcars$cyl==6]
x3 &lt;- mtcars$mpg[mtcars$cyl==8]
vioplot(x1, x2, x3, names=c(""4 cyl"", ""6 cyl"", ""8 cyl""),
   col=""gold"")
title(""Violin Plots of Miles Per Gallon"")
</code></pre>

<p>And this is what I would like to do:</p>

<pre><code># Violin Plots
library(vioplot)
x1 &lt;- mtcars$mpg[mtcars$cyl==4]
x2 &lt;- mtcars$mpg[mtcars$cyl==6]
x3 &lt;- mtcars$mpg[mtcars$cyl==8]
l&lt;-list(x1,x2,x3)
vioplot(l, names=c(""4 cyl"", ""6 cyl"", ""8 cyl""),
   col=""gold"")
title(""Violin Plots of Miles Per Gallon"")
</code></pre>

<p>But I get this error:</p>

<pre><code>Error in min(data) : invalid 'type' (list) of argument
</code></pre>

<p>Can you help? Thanks!</p>
","<p>I have modified the vioplot function to accept a list as an input, you can use this <em>vioplot2</em>:</p>

<pre><code>vioplot2&lt;-function (x, ..., range = 1.5, h = NULL, ylim = NULL, names = NULL, 
    horizontal = FALSE, col = ""magenta"", border = ""black"", lty = 1, 
    lwd = 1, rectCol = ""black"", colMed = ""white"", pchMed = 19, 
    at, add = FALSE, wex = 1, drawRect = TRUE) 
{
    if(!is.list(x)){
        datas &lt;- list(x, ...)
    } else{
        datas&lt;-x
    }
    n &lt;- length(datas)
    if (missing(at)) 
        at &lt;- 1:n
    upper &lt;- vector(mode = ""numeric"", length = n)
    lower &lt;- vector(mode = ""numeric"", length = n)
    q1 &lt;- vector(mode = ""numeric"", length = n)
    q3 &lt;- vector(mode = ""numeric"", length = n)
    med &lt;- vector(mode = ""numeric"", length = n)
    base &lt;- vector(mode = ""list"", length = n)
    height &lt;- vector(mode = ""list"", length = n)
    baserange &lt;- c(Inf, -Inf)
    args &lt;- list(display = ""none"")
    if (!(is.null(h))) 
        args &lt;- c(args, h = h)
    for (i in 1:n) {
        data &lt;- datas[[i]]
        data.min &lt;- min(data)
        data.max &lt;- max(data)
        q1[i] &lt;- quantile(data, 0.25)
        q3[i] &lt;- quantile(data, 0.75)
        med[i] &lt;- median(data)
        iqd &lt;- q3[i] - q1[i]
        upper[i] &lt;- min(q3[i] + range * iqd, data.max)
        lower[i] &lt;- max(q1[i] - range * iqd, data.min)
        est.xlim &lt;- c(min(lower[i], data.min), max(upper[i], 
            data.max))
        smout &lt;- do.call(""sm.density"", c(list(data, xlim = est.xlim), 
            args))
        hscale &lt;- 0.4/max(smout$estimate) * wex
        base[[i]] &lt;- smout$eval.points
        height[[i]] &lt;- smout$estimate * hscale
        t &lt;- range(base[[i]])
        baserange[1] &lt;- min(baserange[1], t[1])
        baserange[2] &lt;- max(baserange[2], t[2])
    }
    if (!add) {
        xlim &lt;- if (n == 1) 
            at + c(-0.5, 0.5)
        else range(at) + min(diff(at))/2 * c(-1, 1)
        if (is.null(ylim)) {
            ylim &lt;- baserange
        }
    }
    if (is.null(names)) {
        label &lt;- 1:n
    }
    else {
        label &lt;- names
    }
    boxwidth &lt;- 0.05 * wex
    if (!add) 
        plot.new()
    if (!horizontal) {
        if (!add) {
            plot.window(xlim = xlim, ylim = ylim)
            axis(2)
            axis(1, at = at, label = label)
        }
        box()
        for (i in 1:n) {
            polygon(c(at[i] - height[[i]], rev(at[i] + height[[i]])), 
                c(base[[i]], rev(base[[i]])), col = col, border = border, 
                lty = lty, lwd = lwd)
            if (drawRect) {
                lines(at[c(i, i)], c(lower[i], upper[i]), lwd = lwd, 
                  lty = lty)
                rect(at[i] - boxwidth/2, q1[i], at[i] + boxwidth/2, 
                  q3[i], col = rectCol)
                points(at[i], med[i], pch = pchMed, col = colMed)
            }
        }
    }
    else {
        if (!add) {
            plot.window(xlim = ylim, ylim = xlim)
            axis(1)
            axis(2, at = at, label = label)
        }
        box()
        for (i in 1:n) {
            polygon(c(base[[i]], rev(base[[i]])), c(at[i] - height[[i]], 
                rev(at[i] + height[[i]])), col = col, border = border, 
                lty = lty, lwd = lwd)
            if (drawRect) {
                lines(c(lower[i], upper[i]), at[c(i, i)], lwd = lwd, 
                  lty = lty)
                rect(q1[i], at[i] - boxwidth/2, q3[i], at[i] + 
                  boxwidth/2, col = rectCol)
                points(med[i], at[i], pch = pchMed, col = colMed)
            }
        }
    }
    invisible(list(upper = upper, lower = lower, median = med, 
        q1 = q1, q3 = q3))
}
</code></pre>

<p>What you get with your example is this:
<img src=""https://i.stack.imgur.com/x3Z4F.png"" alt=""enter image description here""></p>
"
15373988,R and looping across columns,1,1,1,"<p>I have ""Sec1OSp"", which looks like this:</p>

<pre><code>TIMEBLOCK   ep.HRC_E    ep.HRC_D    ep.HRCcm_E  ep.HRCcm_D
   1           NA          NA          NA          NA
   2           NA          NA          NA          NA
   3           NA          NA          NA          NA
   4           NA          NA          NA          NA
   5           NA          NA          NA          NA
   6           NA          5           5           5
   7           NA          5           10          5
   8           NA          5           20          5
   9           NA          5           20          5 
   10          NA          10          20          10
   11          20          10          20          10
   12          20          10          20          10
   13          20          NA          20          10
   14          20          NA          20          10
   15          10          NA          10          10
   16          10          NA          10          5
   17          NA          NA          NA          NA
   18          NA          NA          NA          NA
   19          NA          NA          NA          NA
   20          NA          NA          NA          NA
   21          NA          NA          NA          NA
   22          NA          NA          NA          NA
   23          NA          NA          NA          NA
   24          NA          NA          NA          NA
</code></pre>

<p>I have ""data.EP.S1p"", which looks like this:</p>

<pre><code>TIMEBLOCK   ep.HRC_E    ep.HRC_D    ep.HRCcm_E  ep.HRCcm_D
   1           NA          NA          NA          NA
   2           NA          NA          NA          NA
   3           NA          NA          NA          NA
   4           NA          NA          NA          NA
   5           NA          NA          NA          NA
   6           NA          NA          NA          NA
   7           NA          NA          NA          NA
   8           NA          NA          NA          NA
   9           NA          NA          NA          NA
   10          NA          NA          NA          NA
   11          20          NA          NA          NA
   12          20          NA          NA          NA
   13          20          NA          NA          NA
   14          20          NA          NA          NA
   15          10          NA          NA          NA
   16          10          NA          NA          NA
   17          NA          NA          NA          NA
   18          NA          NA          NA          NA
   19          NA          NA          NA          NA
   20          NA          NA          NA          NA
   21          NA          NA          NA          NA
   22          NA          NA          NA          NA
   23          NA          NA          NA          NA
   24          NA          NA          NA          NA
</code></pre>

<p>This code (below) currently uses the data from Sec1OSp$ep.HRC_E to fill data.EP.S1p$ep.HRC_E.</p>

<pre><code>  for(t in 1:24) {
    Sec1OSpt &lt;- subset(Sec1OSp, TIMEBLOCK==t)
    Sec1OSptnonNArows &lt;- Sec1OSpt[!is.na(Sec1OSpt$ep.HRC_E),]
    if(nrow(Sec1OSptnonNArows) &gt; 0) {
      if(sum(Sec1OSptnonNArows$ep.HRC_E, na.rm=TRUE) &gt; 0) {
        data.EP.S1p$ep.HRC_E[t] &lt;- (sum(Sec1OSptnonNArows$ep.HRC_E, na.rm=TRUE)) / nrow(Sec1OSptnonNArows)
      }
      else {
        data.EP.S1p$ep.HRC_E[t] &lt;- NA
      }
    }
</code></pre>

<p>I would like be able to loop this code so that it is applied to all 4 columns, thereby using the 4 columns in Sec1OSp to fill the 4 columns in data.EP.S1p
Does anyone have a solution for me?
Thanks.</p>
","<p>If I understand you correctly , you are first finding average values for 4 columns for each value of <code>TIMEBLOCK</code>. You can do that by using <code>aggregate</code> function. After that you can fill in those values in <code>data.EP.S1p</code> using <code>merge</code></p>

<pre><code>Sec1OSp &lt;- read.table(textConnection(""TIMEBLOCK   ep.HRC_E    ep.HRC_D    ep.HRCcm_E  ep.HRCcm_D\n1           NA          NA          NA          NA\n2           NA          NA          NA          NA\n3           NA          NA          NA          NA\n4           NA          NA          NA          NA\n5           NA          NA          NA          NA\n6           NA          5           5           5\n7           NA          5           10          5\n8           NA          5           20          5\n9           NA          5           20          5\n10          NA          10          20          10\n11          20          10          20          10\n12          20          10          20          10\n13          20          NA          20          10\n14          20          NA          20          10\n15          10          NA          10          10\n16          10          NA          10          5\n17          NA          NA          NA          NA\n18          NA          NA          NA          NA\n19          NA          NA          NA          NA\n20          NA          NA          NA          NA\n21          NA          NA          NA          NA\n22          NA          NA          NA          NA\n23          NA          NA          NA          NA\n24          NA          NA          NA          NA""), 
    header = TRUE)

data.EP.S1p &lt;- read.table(textConnection(""TIMEBLOCK   ep.HRC_E    ep.HRC_D    ep.HRCcm_E  ep.HRCcm_D\n1           NA          NA          NA          NA\n2           NA          NA          NA          NA\n3           NA          NA          NA          NA\n4           NA          NA          NA          NA\n5           NA          NA          NA          NA\n6           NA          NA          NA          NA\n7           NA          NA          NA          NA\n8           NA          NA          NA          NA\n9           NA          NA          NA          NA\n10          NA          NA          NA          NA\n11          20          NA          NA          NA\n12          20          NA          NA          NA\n13          20          NA          NA          NA\n14          20          NA          NA          NA\n15          10          NA          NA          NA\n16          10          NA          NA          NA\n17          NA          NA          NA          NA\n18          NA          NA          NA          NA\n19          NA          NA          NA          NA\n20          NA          NA          NA          NA\n21          NA          NA          NA          NA\n22          NA          NA          NA          NA\n23          NA          NA          NA          NA\n24          NA          NA          NA          NA""), 
    header = TRUE)


avgdata &lt;- aggregate(Sec1OSp[, 2:5], by = list(Sec1OSp$TIMEBLOCK), FUN = function(x) mean(x, na.rm = TRUE))
names(avgdata)[1] &lt;- ""TIMEBLOCK""
avgdata
##    TIMEBLOCK ep.HRC_E ep.HRC_D ep.HRCcm_E ep.HRCcm_D
## 1          1      NaN      NaN        NaN        NaN
## 2          2      NaN      NaN        NaN        NaN
## 3          3      NaN      NaN        NaN        NaN
## 4          4      NaN      NaN        NaN        NaN
## 5          5      NaN      NaN        NaN        NaN
## 6          6      NaN        5          5          5
## 7          7      NaN        5         10          5
## 8          8      NaN        5         20          5
## 9          9      NaN        5         20          5
## 10        10      NaN       10         20         10
## 11        11       20       10         20         10
## 12        12       20       10         20         10
## 13        13       20      NaN         20         10
## 14        14       20      NaN         20         10
## 15        15       10      NaN         10         10
## 16        16       10      NaN         10          5
## 17        17      NaN      NaN        NaN        NaN
## 18        18      NaN      NaN        NaN        NaN
## 19        19      NaN      NaN        NaN        NaN
## 20        20      NaN      NaN        NaN        NaN
## 21        21      NaN      NaN        NaN        NaN
## 22        22      NaN      NaN        NaN        NaN
## 23        23      NaN      NaN        NaN        NaN
## 24        24      NaN      NaN        NaN        NaN

data.EP.S1p &lt;- merge(data.EP.S1p[, ""TIMEBLOCK"", drop = FALSE], avgdata, all.x = TRUE)
data.EP.S1p
##    TIMEBLOCK ep.HRC_E ep.HRC_D ep.HRCcm_E ep.HRCcm_D
## 1          1      NaN      NaN        NaN        NaN
## 2          2      NaN      NaN        NaN        NaN
## 3          3      NaN      NaN        NaN        NaN
## 4          4      NaN      NaN        NaN        NaN
## 5          5      NaN      NaN        NaN        NaN
## 6          6      NaN        5          5          5
## 7          7      NaN        5         10          5
## 8          8      NaN        5         20          5
## 9          9      NaN        5         20          5
## 10        10      NaN       10         20         10
## 11        11       20       10         20         10
## 12        12       20       10         20         10
## 13        13       20      NaN         20         10
## 14        14       20      NaN         20         10
## 15        15       10      NaN         10         10
## 16        16       10      NaN         10          5
## 17        17      NaN      NaN        NaN        NaN
## 18        18      NaN      NaN        NaN        NaN
## 19        19      NaN      NaN        NaN        NaN
## 20        20      NaN      NaN        NaN        NaN
## 21        21      NaN      NaN        NaN        NaN
## 22        22      NaN      NaN        NaN        NaN
## 23        23      NaN      NaN        NaN        NaN
## 24        24      NaN      NaN        NaN        NaN
</code></pre>
"
20637248,shiny 4 small textInput boxes side-by-side,2,2,2,"<p>I've got a shiny server version 0.4.0 and I want to have 4 small textInput boxes to look like this:</p>

<pre><code>x-min x-max y-min y-max
[...] [...] [...] [...]
</code></pre>

<p>They now look like this:</p>

<pre><code>x-min 
[...................]
x-max
[...................]
y-min 
[...................]
y-max 
[...................]
</code></pre>

<p>With this code:</p>

<pre><code>textInput(inputId=""xlimitsmin"", label=""x-min"", value = 0.0),
textInput(inputId=""xlimitsmax"", label=""x-max"", value = 0.5),
textInput(inputId=""ylimitsmin"", label=""y-min"", value = 0.5),
textInput(inputId=""ylimitsmax"", label=""y-max"", value = 1.0),
</code></pre>

<p>Any ideas how to achieve this?</p>

<p>EDITED: I've successfully changed things like this elsewhere in the code:</p>

<pre><code>&lt;style type=""text/css""&gt;select#yaxis4 { height: 280px; width: 500px; }&lt;/style&gt;
[... which links to this later on in the page...]
          &lt;label class=""control-label"" for=""yaxis4""&gt;Y-Axis&lt;/label&gt;
          &lt;select id=""yaxis4"" multiple=""multiple""&gt;
</code></pre>

<p>And this is what it looks like for the ones that don't work:</p>

<pre><code>&lt;style type=""text/css""&gt;select#xlimitsmax { display: inline-block; max-width: 50px; }&lt;/style&gt;
[... which links to...]
          &lt;label&gt;x-max&lt;/label&gt;
          &lt;input id=""xlimitsmax"" type=""text"" value=""0.5""/&gt;
</code></pre>

<p>EDITED:</p>

<p>Here is a self contained example <code>ui.R</code> that doesn't work:</p>

<pre><code>library(shiny)
shinyUI(
pageWithSidebar(
  # application title
  headerPanel(""test01""),
  sidebarPanel(
    tags$head(
      tags$style(type=""text/css"", ""select { max-width: 360px; }""),
      tags$style(type=""text/css"", "".span4 { max-width: 360px; }""),
      tags$style(type=""text/css"",  "".well { max-width: 360px; }"")
              ),
    wellPanel(
      p(strong(""Side Panel:""))
             )
   ),
  mainPanel(
    textInput(inputId=""xlimitsmin"", label=""x-min"", value = 0.0),
    tags$head(tags$style(type=""text/css"", ""select#xlimitsmin { max-width: 50px }"")),
    textInput(inputId=""xlimitsmax"", label=""x-max"", value = 0.5),
    tags$head(tags$style(type=""text/css"", ""select#xlimitsmax { display: inline-block; max-width: 50px; }""))
    )
))
</code></pre>

<p>Resulting page:</p>

<p><img src=""https://i.stack.imgur.com/vXOaO.png"" alt=""enter image description here""></p>
","<p>to paraphrase (and to simplify to the case of 2 inputs), your problem is that:</p>

<pre><code>runApp(list(
    ui = bootstrapPage(
        textInput(inputId=""xlimitsmin"", label=""x-min"", value = 0.0),
        textInput(inputId=""xlimitsmax"", label=""x-max"", value = 0.5)
    ),
    server = function(input, output) {}
))
</code></pre>

<p>shows</p>

<p><img src=""https://i.stack.imgur.com/Pr0zb.png"" alt=""enter image description here""></p>

<p>But you want side-by-side small inputs, like so:</p>

<p><img src=""https://i.stack.imgur.com/RvIei.png"" alt=""small row""></p>

<h1>The short answer</h1>

<pre><code>textInputRow&lt;-function (inputId, label, value = """") 
{
    div(style=""display:inline-block"",
        tags$label(label, `for` = inputId), 
        tags$input(id = inputId, type = ""text"", value = value,class=""input-small""))
}
runApp(list(
    ui = bootstrapPage(
        textInputRow(inputId=""xlimitsmin"", label=""x-min"", value = 0.0),
        textInputRow(inputId=""xlimitsmax"", label=""x-max"", value = 0.5)
    ),
    server = function(input, output) {}
))
</code></pre>

<p>gives:</p>

<p><img src=""https://i.stack.imgur.com/RvIei.png"" alt=""enter image description here""></p>

<h1>The long answer</h1>

<h2>Side-by-side inputs</h2>

<p>Let's do side-by-side first:</p>

<p>Currently textInput generates two separate tags - the <code>label</code>, and the <code>input</code>, each of which is configured by CSS as <code>display:block</code>, which means it's a rectangle that will break to the left side of the container.  We need to wrap each <code>textInput</code>'s field in new container (div) and tell that container that the container that follows it (the next <code>textInput</code>) is allowed to be on the same horizontal row on the page, using CSS's <code>display:inline-block</code>.</p>

<p>So we add a div with a style around each <code>textInput</code>:</p>

<pre><code>runApp(list(
    ui = bootstrapPage(
        div(style=""display:inline-block"",textInput(inputId=""xlimitsmin"", label=""x-min"", value = 0.0)),
        div(style=""display:inline-block"",textInput(inputId=""xlimitsmax"", label=""x-max"", value = 0.5))
    ),
    server = function(input, output) {}
))
</code></pre>

<p><img src=""https://i.stack.imgur.com/nf4dQ.png"" alt=""row""></p>

<h2>Small inputs</h2>

<p>Now let's deal with small.  There are a few ways to do small, </p>

<ol>
<li>make the font smaller, </li>
<li>make the input box have fewer characters in it.  </li>
<li>tell css or (here) bootstrap to draw a smaller box</li>
</ol>

<p>Since <code>bootstrap.js</code> is really in control of layout when we use shiny, only 3 will reliably work, so let's use that.</p>

<p>Input sizes are documented in <a href=""http://getbootstrap.com/2.3.2/base-css.html#forms"">Bootstrap 2.3.2's CSS Forms documentation, under 'Control Sizing'</a>.  It includes a variety of sizes from mini, small, medium, large, xlarge, and xxlarge, and the default is probably medium.  Let's try small, instead.</p>

<p>To set the size, we need to change the class of the <code>input</code> tag generated by <code>textInput</code>.</p>

<p>Now <code>textInput</code> is just a convenience function around the more powerful <code>tags</code> functions such as <code>tags$label</code> and <code>tags$input</code>.  We can build a more powerful version of <code>textInput</code> that allows us to configure the elements, specifically the class of the <code>input</code> node:</p>

<pre><code>textInput2&lt;-function (inputId, label, value = """",...) 
{
    tagList(tags$label(label, `for` = inputId), tags$input(id = inputId, 
                                                           type = ""text"", value = value,...))
}
runApp(list(
    ui = bootstrapPage(
        div(style=""display:inline-block"",textInput2(inputId=""xlimitsmin"", label=""x-min"", value = 0.0, class=""input-small"")),
        div(style=""display:inline-block"",textInput2(inputId=""xlimitsmax"", label=""x-max"", value = 0.5, class=""input-small""))
    ),
    server = function(input, output) {}
))
</code></pre>

<p><img src=""https://i.stack.imgur.com/RvIei.png"" alt=""small row""></p>

<p>And we are done - but we can roll some of this functionality up by having <code>textInput3</code> generate the div tag.  It could also set the class by itself, but I'll leave that for you to write.</p>

<h2>Wrapping it up</h2>

<pre><code>textInput3&lt;-function (inputId, label, value = """",...) 
{
    div(style=""display:inline-block"",
        tags$label(label, `for` = inputId), 
        tags$input(id = inputId, type = ""text"", value = value,...))
}
runApp(list(
    ui = bootstrapPage(
        textInput3(inputId=""xlimitsmin"", label=""x-min"", value = 0.0, class=""input-small""),
        textInput3(inputId=""xlimitsmax"", label=""x-max"", value = 0.5, class=""input-small"")
    ),
    server = function(input, output) {}
))
</code></pre>

<p>For interest's sake, here's the version using class <code>input-mini</code>:</p>

<p><img src=""https://i.stack.imgur.com/DRW4i.png"" alt=""enter image description here""></p>
"
23019532,Can I use two character vectors in a sqldf join statement?,1,1,1,"<p>I am conducting a <code>sqldf</code> join of 3 different data.tables. My current working code looks like this:</p>

<pre><code>AltSuitRaw &lt;- data.table(sqldf('select RealAlt.*, SpdSpSuit * SpdSpT as SpdSpSuitT, SpdIncSuit * SpdIncT as SpdIncSuitT, SpdGrowSuit * SpdGrT as SpdGrowSuitT,
        RzbSpSuit * RzbSpT as RzbSpSuitT, RzbIncSuit * RzbIncT as RzbIncSuitT, RzbGrowSuit * RzbGrT as RzbGrowSuitT,
        FMSSpSuit * FmsSpT as FmsSpSuitT, FMSIncSuit * FmsIncT as FmsIncSuitT, FMSGrowSuit * FMSGrT as FmsGrowSuitT,
        BhsSpSuit * BhsSpT as BhsSpSuitT, BhsIncSuit * BhsIncT as BhsIncSuitT, BhsGrowSuit * BhsGrT as BhsGrowSuitT,
        BrtSpSuit * BRTsp as BrtSpSuitT, BrtIncSuit * BRTinc as BrtIncSuitT, BrtGrSuit * BRTgr as BrtGrowSuitT,
        CcfSpSuit * CCFsp as CcfSpSuitT, CcfIncSuit * CCFinc as CcfIncSuitT, CcfGrSuit * Ccfgr as CcfGrowSuitT,
        GsfSpSuit * GSFsp as GsfSpSuitT, GsfIncSuit * GSFinc as GsfIncSuitT, GsfGrSuit * GSFgr as GsfGrowSuitT,
        RbtSpSuit * RBTsp as RbtSpSuitT, RbtIncSuit * RBTinc as RbtIncSuitT, RbtGrSuit * RBTgr as RbtGrowSuitT,
        SmbSpSuit * SMBsp as SmbSpSuitT, SmbIncSuit * SMBinc as SmbIncSuitT, SmbGrSuit * SMBgr as SmbGrowSuitT,
        StbSpSuit * STBsp as StbSpSuitT, StbIncSuit * STBinc as StbIncSuitT, StbGrSuit * STBgr as StbGrowSuitT,
        HbcSpSuit * HBCsp as HbcSpSuitT, HbcIncSuit * HBCinc as HbcIncSuitT, HbcGrSuit * HBCgr as HbcGrowSuitT,
        AtActSuit * ATha as AtActSuitT, AtInfSuit * ATinf as AtInfSuitT,
        LcActSuit * LCha as LcActSuitT, LcInfSuit * LCha as LcInfSuitT,
        TnActSuit * TNha as TnActSuitT, TnInfSuit * TNinf as TnInfSuitT,
        WdActSuit * WDha as WdActSuitT, WdInfSuit * WDinf as WdInfSuitT
        from RealAlt 
        left join SpecSuitTemp using (Temp)
        left join AltSuitDates using (Month)'))
</code></pre>

<p>As you can see this is very large, and in my opinion, unwieldy chunk of code. A small change in the column names of either SpecSuitTemp or AltSuitDates would break the above code. What I was hoping to do is to do the join using a character vector of the column names so that if they change it doesn't matter, something like the following:</p>

<pre><code>AltSuitRawt &lt;- data.table(sqldf('select RealAlt.*, SpecSuitTemp[tcoln] * AltSuitDates[dcoln] as newcn from RealAlt left join SpecSuitTemp using (Temp) left join AltSuitDates using (Month)'))
</code></pre>

<p>Where</p>

<pre><code>newcn &lt;- paste0(names(SpecSuitTemp),""T""); newcn &lt;- newcn[-1]
tcoln &lt;- names(SpecSuitTemp); tcoln &lt;- tcoln[-1]
dcoln &lt;- names(AltSuitDates); dcoln &lt;- dcoln[-1]
</code></pre>

<p>However this statement doesn't work. I do not know <code>sql</code> but was able to get the large statement above to work, with a lot of help from a coworker.</p>

<p>My question is, can I conduct a <code>sqldf</code> join statement using character vectors of the column names? I was thinking maybe I should wrap my statement with an <code>lapply</code> but not sure if that is possible. Any help, resources, or suggestions are appreciated.</p>

<p>Dput of my data:</p>

<pre><code>RealAlt &lt;- structure(list(Alternative = c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A""), TraceID = c(""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1""), WaterYear = c(2013L, 2013L, 2013L, 2013L, 2013L, 2013L, 2013L, 2013L, 2013L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L, 2014L), Month = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L), Location = c(""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD""), Temp = c(9.8, 8.7, 8.5, 8.5, 8.9, 9.3, 9.8, 10.8, 11.2, 11.6, 11.3, 11.3, 10.1, 8.6, 8.1, 9.1, 9.5, 10.1, 10.7, 11.6)), .Names = c(""Alternative"", ""TraceID"", ""WaterYear"", ""Month"", ""Location"", ""Temp""), class = c(""data.table"", ""data.frame""), row.names = c(NA, -20L))
SpecSuitTemp &lt;- structure(list(Temp = c(8, 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9, 9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9, 10, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.8, 10.9, 11, 11.1, 11.2, 11.3, 11.4, 11.5, 11.6, 11.7, 11.8, 11.9, 12), SpdSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), SpdIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), SpdGrowSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), RzbSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), RzbIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), RzbGrowSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), FMSSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), FMSIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), FMSGrowSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BhsSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BhsIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BhsGrowSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BrtSpSuit = c(0.333333333, 0.366666667, 0.4, 0.433333333, 0.466666667, 0.5, 0.533333333, 0.566666667, 0.6, 0.633333333, 0.666666667, 0.7, 0.733333333, 0.766666667, 0.8, 0.833333333, 0.866666667, 0.9, 0.933333333, 0.966666667, 1, 0.975, 0.95, 0.925, 0.9, 0.875, 0.85, 0.825, 0.8, 0.775, 0.75, 0.725, 0.7, 0.675, 0.65, 0.625, 0.6, 0.575, 0.55, 0.525, 0.5), BrtIncSuit = c(0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94, 0.93, 0.92, 0.91, 0.9, 0.89, 0.88, 0.87, 0.86, 0.85, 0.84, 0.83, 0.82, 0.81, 0.8), BrtGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), CcfSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), CcfIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), CcfGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), GsfSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), GsfIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), GsfGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), RbtSpSuit = c(0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1, 0.966666667, 0.933333333, 0.9, 0.866666667, 0.833333333, 0.8, 0.766666667, 0.733333333, 0.7, 0.666666667, 0.633333333, 0.6, 0.566666667, 0.533333333, 0.5, 0.466666667, 0.433333333, 0.4, 0.366666667, 0.333333333), RbtIncSuit = c(0.333333333, 0.366666667, 0.4, 0.433333333, 0.466666667, 0.5, 0.533333333, 0.566666667, 0.6, 0.633333333, 0.666666667, 0.7, 0.733333333, 0.766666667, 0.8, 0.833333333, 0.866666667, 0.9, 0.933333333, 0.966666667, 1, 0.98, 0.96, 0.94, 0.92, 0.9, 0.88, 0.86, 0.84, 0.82, 0.8, 0.78, 0.76, 0.74, 0.72, 0.7, 0.68, 0.66, 0.64, 0.62, 0.6), RbtGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), SmbSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), SmbIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), SmbGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), StbSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), StbIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), StbGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), HbcSpSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), HbcIncSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), HbcGrSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), AtActSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), AtInfSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), LcActSuit = c(0,  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), LcInfSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), TnActSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), TnInfSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), WdActSuit = c(1, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), WdInfSuit = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.033333333, 0.066666667, 0.1, 0.133333333, 0.166666667, 0.2, 0.233333333, 0.266666667, 0.3, 0.333333333)), .Names = c(""Temp"", ""SpdSpSuit"", ""SpdIncSuit"", ""SpdGrowSuit"", ""RzbSpSuit"", ""RzbIncSuit"", ""RzbGrowSuit"", ""FMSSpSuit"", ""FMSIncSuit"", ""FMSGrowSuit"", ""BhsSpSuit"", ""BhsIncSuit"", ""BhsGrowSuit"", ""BrtSpSuit"", ""BrtIncSuit"", ""BrtGrSuit"", ""CcfSpSuit"", ""CcfIncSuit"", ""CcfGrSuit"", ""GsfSpSuit"", ""GsfIncSuit"", ""GsfGrSuit"", ""RbtSpSuit"", ""RbtIncSuit"", ""RbtGrSuit"", ""SmbSpSuit"", ""SmbIncSuit"", ""SmbGrSuit"", ""StbSpSuit"", ""StbIncSuit"", ""StbGrSuit"", ""HbcSpSuit"", ""HbcIncSuit"", ""HbcGrSuit"", ""AtActSuit"", ""AtInfSuit"", ""LcActSuit"", ""LcInfSuit"", ""TnActSuit"", ""TnInfSuit"", ""WdActSuit"", ""WdInfSuit""), class = c(""data.table"", ""data.frame""), row.names = c(NA, -41L))
AltSuitDates &lt;- data.table(structure(list(Month = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 10L, 11L,12L), SpdSpT = c(NA, NA, NA, NA, NA, 1L, 1L, NA, NA, NA), SpdIncT = c(NA,NA, NA, NA, NA, 1L, 1L, NA, NA, NA), SpdGrT = c(1L, 1L, 1L, 1L,1L, 1L, 1L, 1L, 1L, 1L), RzbSpT = c(NA, NA, NA, 1L, 1L, 1L, NA,NA, NA, NA), RzbIncT = c(NA, NA, NA, 1L, 1L, 1L, NA, NA, NA,NA), RzbGrT = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), FmsSpT = c(NA,NA, 1L, 1L, NA, NA, NA, NA, NA, NA), FmsIncT = c(NA, NA, 1L,1L, 1L, NA, NA, NA, NA, NA), FMSGrT = c(1L, 1L, 1L, 1L, 1L, 1L,1L, 1L, 1L, 1L), BhsSpT = c(NA, NA, NA, 1L, 1L, 1L, NA, NA, NA,NA), BhsIncT = c(NA, NA, NA, 1L, 1L, 1L, NA, NA, NA, NA), BhsGrT = c(1L,1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), BRTsp = c(1L, 1L, 1L, NA,NA, NA, NA, 1L, 1L, 1L), BRTinc = c(1L, 1L, 1L, 1L, NA, NA, NA,1L, 1L, 1L), BRTgr = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), CCFsp = c(NA, NA, NA, NA, 1L, 1L, 1L, NA, NA, NA), CCFinc = c(NA,NA, NA, NA, 1L, 1L, 1L, NA, NA, NA), CCFgr = c(1L, 1L, 1L, 1L,1L, 1L, 1L, 1L, 1L, 1L), GSFsp = c(NA, NA, NA, NA, 1L, 1L, 1L,NA, NA, NA), GSFinc = c(NA, NA, NA, NA, 1L, 1L, 1L, NA, NA, NA), GSFgr = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), RBTsp = c(1L,1L, 1L, 1L, 1L, 1L, 1L, NA, NA, NA), RBTinc = c(1L, 1L, 1L, 1L,1L, 1L, 1L, NA, NA, NA), RBTgr = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,1L, 1L, 1L), SMBsp = c(NA, NA, NA, 1L, 1L, 1L, 1L, NA, NA, NA), SMBinc = c(NA, NA, NA, 1L, 1L, 1L, 1L, NA, NA, NA), SMBgr = c(1L,1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), STBsp = c(NA, NA, NA, NA,NA, 1L, 1L, NA, NA, NA), STBinc = c(NA, NA, NA, NA, NA, 1L, 1L,NA, NA, NA), STBgr = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), HBCsp = c(NA, NA, NA, 1L, 1L, 1L, NA, NA, NA, NA), HBCinc = c(NA,NA, NA, 1L, 1L, 1L, NA, NA, NA, NA), HBCgr = c(1L, 1L, 1L, 1L,1L, 1L, 1L, 1L, 1L, 1L), ATha = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,1L, 1L, 1L), ATinf = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), LCha = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), LCinf = c(1L,1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), TNha = c(1L, 1L, 1L, 1L,1L, 1L, 1L, 1L, 1L, 1L), TNinf = c(1L, 1L, 1L, 1L, 1L, 1L, 1L,1L, 1L, 1L), WDha = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L),WDinf = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, NA, NA, NA)), .Names = c(""Month"",""SpdSpT"", ""SpdIncT"", ""SpdGrT"", ""RzbSpT"", ""RzbIncT"", ""RzbGrT"",""FmsSpT"", ""FmsIncT"", ""FMSGrT"", ""BhsSpT"", ""BhsIncT"", ""BhsGrT"",""BRTsp"", ""BRTinc"", ""BRTgr"", ""CCFsp"", ""CCFinc"", ""CCFgr"", ""GSFsp"",""GSFinc"", ""GSFgr"", ""RBTsp"", ""RBTinc"", ""RBTgr"", ""SMBsp"", ""SMBinc"",""SMBgr"", ""STBsp"", ""STBinc"", ""STBgr"", ""HBCsp"", ""HBCinc"", ""HBCgr"",""ATha"", ""ATinf"", ""LCha"", ""LCinf"", ""TNha"", ""TNinf"", ""WDha"", ""WDinf""), class = c(""data.table"", ""data.frame""), row.names = c(NA, -10L))
</code></pre>

<p><strong>Edit: Added Expected output data</strong></p>

<pre><code>AltSuitRaw &lt;- data.table(structure(list(Alternative = c(""A"", ""A"", ""A"", ""A"", ""A"", ""A""),TraceID = c(""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"",""h00s1""), WaterYear = c(2013L, 2013L, 2013L, 2013L, 2013L,2013L), Month = 1:6, Location = c(""GCD"", ""GCD"", ""GCD"", ""GCD"",""GCD"", ""GCD""), Temp = c(9.8, 8.7, 8.5, 8.5, 8.9, 9.3), SpdSpSuitT = c(NA,NA, NA, NA, NA, ""0.0""), SpdIncSuitT = c(NA, NA, NA, NA, NA,""0.0""), SpdGrowSuitT = c(0, 0, 0, 0, 0, 0), RzbSpSuitT = c(NA,NA, NA, ""0.0"", ""0.0"", ""0.0""), RzbIncSuitT = c(NA, NA, NA,""0.0"", ""0.0"", ""0.0""), RzbGrowSuitT = c(0, 0, 0, 0, 0, 0),FmsSpSuitT = c(NA, NA, ""0.0"", ""0.0"", NA, NA), FmsIncSuitT = c(NA,NA, ""0.0"", ""0.0"", ""0.0"", NA), FmsGrowSuitT = c(0, 0, 0, 0,0, 0), BhsSpSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0""), BhsIncSuitT = c(NA,NA, NA, ""0.0"", ""0.0"", ""0.0""), BhsGrowSuitT = c(0, 0, 0, 0,0, 0), BrtSpSuitT = c(0.933333333, 0.566666667, 0.5, NA,NA, NA), BrtIncSuitT = c(0.9, 0.35, 0.25, 0.25, NA, NA),BrtGrowSuitT = c(0, 0, 0, 0, 0, 0), CcfSpSuitT = c(NA, NA,NA, NA, ""0.0"", ""0.0""), CcfIncSuitT = c(NA, NA, NA, NA, ""0.0"",""0.0""), CcfGrowSuitT = c(0, 0, 0, 0, 0, 0), GsfSpSuitT = c(NA,NA, NA, NA, ""0.0"", ""0.0""), GsfIncSuitT = c(NA, NA, NA, NA,""0.0"", ""0.0""), GsfGrowSuitT = c(0, 0, 0, 0, 0, 0), RbtSpSuitT = c(0.9,0.35, 0.25, 0.25, 0.45, 0.65), RbtIncSuitT = c(0.933333333,0.566666667, 0.5, 0.5, 0.633333333, 0.766666667), RbtGrowSuitT = c(0,0, 0, 0, 0, 0), SmbSpSuitT = c(NA, NA, NA, ""0.0"", ""0.0"",""0.0""), SmbIncSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0""),SmbGrowSuitT = c(0, 0, 0, 0, 0, 0), StbSpSuitT = c(NA, NA,NA, NA, NA, ""0.0""), StbIncSuitT = c(NA, NA, NA, NA, NA, ""0.0""), StbGrowSuitT = c(0, 0, 0, 0, 0, 0), HbcSpSuitT = c(NA,NA, NA, ""0.0"", ""0.0"", ""0.0""), HbcIncSuitT = c(NA, NA, NA,""0.0"", ""0.0"", ""0.0""), HbcGrowSuitT = c(0, 0, 0, 0, 0, 0),AtActSuitT = c(0, 0, 0, 0, 0, 0), AtInfSuitT = c(0, 0, 0,0, 0, 0), LcActSuitT = c(0, 0, 0, 0, 0, 0), LcInfSuitT = c(0,0, 0, 0, 0, 0), TnActSuitT = c(0, 0, 0, 0, 0, 0), TnInfSuitT = c(0,0, 0, 0, 0, 0), WdActSuitT = c(0.1, 0.65, 0.75, 0.75, 0.55,0.35), WdInfSuitT = c(0, 0, 0, 0, 0, 0)), .Names = c(""Alternative"",""TraceID"", ""WaterYear"", ""Month"", ""Location"", ""Temp"", ""SpdSpSuitT"",""SpdIncSuitT"", ""SpdGrowSuitT"", ""RzbSpSuitT"", ""RzbIncSuitT"", ""RzbGrowSuitT"",""FmsSpSuitT"", ""FmsIncSuitT"", ""FmsGrowSuitT"", ""BhsSpSuitT"", ""BhsIncSuitT"",""BhsGrowSuitT"", ""BrtSpSuitT"", ""BrtIncSuitT"", ""BrtGrowSuitT"",""CcfSpSuitT"", ""CcfIncSuitT"", ""CcfGrowSuitT"", ""GsfSpSuitT"", ""GsfIncSuitT"",""GsfGrowSuitT"", ""RbtSpSuitT"", ""RbtIncSuitT"", ""RbtGrowSuitT"",""SmbSpSuitT"", ""SmbIncSuitT"", ""SmbGrowSuitT"", ""StbSpSuitT"", ""StbIncSuitT"",""StbGrowSuitT"", ""HbcSpSuitT"", ""HbcIncSuitT"", ""HbcGrowSuitT"",""AtActSuitT"", ""AtInfSuitT"", ""LcActSuitT"", ""LcInfSuitT"", ""TnActSuitT"",""TnInfSuitT"", ""WdActSuitT"", ""WdInfSuitT""), class = c(""data.table"",""data.frame""), row.names = c(NA, -6L))
</code></pre>
","<p><strong>Thanks to G. Grothendieck I was able to answer this question fully</strong></p>

<pre><code>tcoln &lt;- names(SpecSuitTemp)[-1]
dcoln &lt;- names(AltSuitDates)[-1]
newcn &lt;- toString(paste0(tcoln, "" * "", dcoln, "" as "", tcoln, ""T""))

AltSuitRaw &lt;- data.table(fn$sqldf('select RealAlt.*, $newcn from RealAlt left join SpecSuitTemp using (Temp) left join AltSuitDates using (Month)'))
</code></pre>

<p>Adding in the statement <code>RealAlt.*,</code> before <code>$newcn</code> gives me this output:</p>

<pre><code>AltSuitRawt &lt;- data.table(structure(list(Alternative = c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"",""A"", ""A"", ""A"", ""A""), TraceID = c(""h00s1"", ""h00s1"", ""h00s1"", ""h00s1"",""h00s1"", ""h00s1"", ""h00s1"", ""h20s3"", ""h20s3"", ""h20s3""), WaterYear = c(2013L,2013L, 2013L, 2013L, 2013L, 2013L, 2013L, 2034L, 2034L, 2034L), Month = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 10L, 11L, 12L), Location = c(""GCD"",""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""GCD"", ""RM225"", ""RM225"", ""RM225""), Temp = c(9.8, 8.7, 8.5, 8.5, 8.9, 9.3, 9.8, 14.7, 12.9, 10.9), SpdSpSuitT = c(NA, NA, NA, NA, NA, ""0.0"", ""0.0"", NA, NA, NA), SpdIncSuitT = c(NA, NA, NA, NA, NA, ""0.0"", ""0.0"", NA, NA,NA), SpdGrowSuitT = c(0, 0, 0, 0, 0, 0, 0, 0.3375, 0.1125, 0),RzbSpSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", NA, NA, NA,NA), RzbIncSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", NA,NA, NA, NA), RzbGrowSuitT = c(0, 0, 0, 0, 0, 0, 0, 0.116666667,0, 0), FMSSpSuitT = c(NA, NA, ""0.0"", ""0.0"", NA, NA, NA, NA,NA, NA), FMSIncSuitT = c(NA, NA, ""0.0"", ""0.0"", ""0.0"", NA,NA, NA, NA, NA), FMSGrowSuitT = c(0, 0, 0, 0, 0, 0, 0, 0.116666667,0, 0), BhsSpSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", NA,NA, NA, NA), BhsIncSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"",NA, NA, NA, NA), BhsGrowSuitT = c(0, 0, 0, 0, 0, 0, 0, 0,0, 0), BrtSpSuitT = c(0.933333333, 0.566666667, 0.5, NA,NA, NA, NA, 0, 0.275, 0.775), BrtIncSuitT = c(0.9, 0.35,0.25, 0.25, NA, NA, NA, 0.53, 0.71, 0.91), BrtGrSuitT = c(0,0, 0, 0, 0, 0, 0, 0.9, 0.3, 0), CcfSpSuitT = c(NA, NA, NA,NA, ""0.0"", ""0.0"", ""0.0"", NA, NA, NA), CcfIncSuitT = c(NA,NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", NA, NA, NA), CcfGrSuitT = c(0,0, 0, 0, 0, 0, 0, 0, 0, 0), GsfSpSuitT = c(NA, NA, NA, NA,""0.0"", ""0.0"", ""0.0"", NA, NA, NA), GsfIncSuitT = c(NA, NA,NA, NA, ""0.0"", ""0.0"", ""0.0"", NA, NA, NA), GsfGrSuitT = c(0,0, 0, 0, 0, 0, 0, 0, 0, 0), RbtSpSuitT = c(0.9, 0.35, 0.25,0.25, 0.45, 0.65, 0.9, NA, NA, NA), RbtIncSuitT = c(0.933333333,0.566666667, 0.5, 0.5, 0.633333333, 0.766666667, 0.933333333,NA, NA, NA), RbtGrSuitT = c(0, 0, 0, 0, 0, 0, 0, 0.675, 0.225,0), SmbSpSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", ""0.0"",NA, NA, NA), SmbIncSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"",""0.0"", NA, NA, NA), SmbGrSuitT = c(0, 0, 0, 0, 0, 0, 0, 0,0, 0), StbSpSuitT = c(NA, NA, NA, NA, NA, ""0.0"", ""0.0"", NA,NA, NA), StbIncSuitT = c(NA, NA, NA, NA, NA, ""0.0"", ""0.0"",NA, NA, NA), StbGrSuitT = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),HbcSpSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", NA, NA, NA,NA), HbcIncSuitT = c(NA, NA, NA, ""0.0"", ""0.0"", ""0.0"", NA,NA, NA, NA), HbcGrSuitT = c(0, 0, 0, 0, 0, 0, 0, 0.175, 0,0), AtActSuitT = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), AtInfSuitT = c(0,0, 0, 0, 0, 0, 0, 0, 0, 0), LcActSuitT = c(0, 0, 0, 0, 0,0, 0, 0, 0, 0), LcInfSuitT = c(0, 0, 0, 0, 0, 0, 0, 0, 0,0), TnActSuitT = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), TnInfSuitT = c(0,0, 0, 0, 0, 0, 0, 0, 0, 0), WdActSuitT = c(0.1, 0.65, 0.75,0.75, 0.55, 0.35, 0.1, 0, 0, 0), WdInfSuitT = c(0, 0, 0,0, 0, 0, 0, NA, NA, NA)), .Names = c(""Alternative"", ""TraceID"",""WaterYear"", ""Month"", ""Location"", ""Temp"", ""SpdSpSuitT"", ""SpdIncSuitT"",""SpdGrowSuitT"", ""RzbSpSuitT"", ""RzbIncSuitT"", ""RzbGrowSuitT"",""FMSSpSuitT"", ""FMSIncSuitT"", ""FMSGrowSuitT"", ""BhsSpSuitT"", ""BhsIncSuitT"",""BhsGrowSuitT"", ""BrtSpSuitT"", ""BrtIncSuitT"", ""BrtGrSuitT"", ""CcfSpSuitT"",""CcfIncSuitT"", ""CcfGrSuitT"", ""GsfSpSuitT"", ""GsfIncSuitT"", ""GsfGrSuitT"",""RbtSpSuitT"", ""RbtIncSuitT"", ""RbtGrSuitT"", ""SmbSpSuitT"", ""SmbIncSuitT"",""SmbGrSuitT"", ""StbSpSuitT"", ""StbIncSuitT"", ""StbGrSuitT"", ""HbcSpSuitT"",""HbcIncSuitT"", ""HbcGrSuitT"", ""AtActSuitT"", ""AtInfSuitT"", ""LcActSuitT"",""LcInfSuitT"", ""TnActSuitT"", ""TnInfSuitT"", ""WdActSuitT"", ""WdInfSuitT""), class = c(""data.table"", ""data.frame""), row.names = c(NA, -10L))
</code></pre>

<p>Using <code>all.equal</code> the two <code>data.tables</code> are equal except for some column header name mismatches based on whether or not some of the name is capitalized or not.</p>
"
30637803,How to generate random numbers in a data.frame with range,1,3,3,"<p>I have a <code>data.frame</code> which I want to generate random numbers each list by a sequence.</p>

<p>I used <code>sample</code> function to create random numbers but even I created random numbers for list <code>[[1]]</code>, for set  <code>[[2]]</code> same numbers produced again. So, here how can I create different random numbers for the set <code>[[2]]</code>. </p>

<p>here is the simple code;</p>

<pre><code>data.list &lt;- lapply(1:2, function(x) {
nrep &lt;- 1
time &lt;- rep(seq(90,54000,by=90),times=nrep) 
Mx &lt;- rep(sort(sample(seq(0.012,-0.014,length.out = 600),replace=TRUE)), times=nrep)
My &lt;- rep(sort(sample(seq(0.02,-0.02,length.out = 600),replace=TRUE)), times=nrep)
Mz &lt;- rep(sort(sample(seq(-1,1,length.out=600),replace=TRUE)), times=nrep)
data.frame(time,Mx,My,Mz,set_nbr=x)
})
</code></pre>

<p>this is provide the 5 first lines of each of datasets</p>

<pre><code>[[1]]
      time       Mx            My           Mz       set_nbr
1      90 -1.391319e-02 -2.000000e-02 -1.000000000       1
2     180 -1.386978e-02 -1.986644e-02 -1.000000000       1
3     270 -1.386978e-02 -1.973289e-02 -0.996661102       1
4     360 -1.382638e-02 -1.973289e-02 -0.993322204       1
5     450 -1.382638e-02 -1.973289e-02 -0.979966611       1  
..     ..  ....            ....         ....           ...

[[2]]

      time       Mx            My           Mz       set_nbr
1      90 -1.395659e-02 -0.0200000000 -1.000000000       2
2     180 -1.391319e-02 -0.0199332220 -0.993322204       2
3     270 -1.386978e-02 -0.0199332220 -0.993322204       2
4     360 -1.386978e-02 -0.0199332220 -0.993322204       2
5     450 -1.382638e-02 -0.0199332220 -0.986644407       2
..     ..  ....            ....         ....           ...
</code></pre>

<p>EDIT 1:</p>

<p>regarding to @bgoldst answer now I can produce different numbers</p>

<pre><code>set.seed(1);
data.list &lt;- lapply(1:2, function(x) {
nrep &lt;- 1;
time &lt;- rep(seq(90,54000,by=90),times=nrep);
Mx &lt;- rep(sort(runif(600,-0.014,0.012)),times=nrep);
My &lt;- rep(sort(runif(600,-0.02,0.02)),times=nrep);
Mz &lt;- rep(sort(runif(600,-1,1)),times=nrep);
data.frame(time,Mx,My,Mz,set_nbr=x);
});
</code></pre>

<p>On the other hand when I change <code>nrep &lt;- 3;</code> same numbers are created for each <code>nrep</code>. This is the thing I want to avoid from the beginning. </p>

<p>EDIT 2:</p>

<p>@bgoldst showed that <code>replicate</code> does the job!</p>
","<p>I think you may have some confusion about how <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/sample.html"" rel=""nofollow""><code>sample()</code></a> works.</p>

<p>First, let's examine <code>sample()</code>'s behavior with respect to this simple vector:</p>

<pre><code>1:5;
## [1] 1 2 3 4 5
</code></pre>

<p>When you pass a multi-element vector to <code>sample()</code> it basically just randomizes the order. This means you'll get a different result every time, or rather, to state it more precisely, the longer the vector is, the less likely you are to get the same result twice:</p>

<pre><code>set.seed(1); sample(1:5); sample(1:5); sample(1:5);
## [1] 2 5 4 3 1
## [1] 5 4 2 3 1
## [1] 2 1 3 4 5
</code></pre>

<p>This means if you sort it immediately after sampling, then you'll get the same result every time. And if the original vector was itself sorted, then the result will also be equal to that original vector. This will be true regardless how <code>sample()</code> randomized the order, because the order is always restored by <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/sort.html"" rel=""nofollow""><code>sort()</code></a>:</p>

<pre><code>set.seed(1); sort(sample(1:5)); sort(sample(1:5)); sort(sample(1:5));
## [1] 1 2 3 4 5
## [1] 1 2 3 4 5
## [1] 1 2 3 4 5
</code></pre>

<p>Now if you add <code>replace=T</code> (or just <code>rep=T</code> if you like to take advantage of partial matching for concision, which I do), then you're <em>not</em> just randomizing the order, you're selecting <code>size</code> elements with replacement, where <code>size</code> is the vector length if you didn't provide <code>size</code> explicitly. This means you can get repeated elements in the result:</p>

<pre><code>set.seed(1); sample(1:5,rep=T); sample(1:5,rep=T); sample(1:5,rep=T);
## [1] 2 2 3 5 2
## [1] 5 5 4 4 1
## [1] 2 1 4 2 4
</code></pre>

<p>And so, if you sort the result, you (likely) <em>won't</em> get back the original vector, because some elements will have been repeated, and some elements will have been omitted:</p>

<pre><code>set.seed(1); sort(sample(1:5,rep=T)); sort(sample(1:5,rep=T)); sort(sample(1:5,rep=T));
## [1] 2 2 2 3 5
## [1] 1 4 4 5 5
## [1] 1 2 2 4 4
</code></pre>

<p>That's exactly what is happening with your code. Your output vectors <em>are</em> different between the two list components, because you're sampling with replacement before sorting, which means different repetitions and omissions of the elements will occur for each list component. But since you're sampling from the same sequence and you're sorting the result, you're bound to get similar-looking results for each list component, even though they're not identical.</p>

<p>I think what you might be looking for is random deviates from a uniform distribution. You can get these from <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Uniform.html"" rel=""nofollow""><code>runif()</code></a>:</p>

<pre><code>set.seed(1); runif(5,-0.014,0.012);
## [1] -0.0070967748 -0.0043247786  0.0008941874  0.0096134025 -0.0087562698
set.seed(1); runif(5,-0.02,0.02);
## [1] -0.009379653 -0.005115044  0.002914135  0.016328312 -0.011932723
set.seed(1); runif(5,-1,1);
## [1] -0.4689827 -0.2557522  0.1457067  0.8164156 -0.5966361
</code></pre>

<p>Thus, your code would become:</p>

<pre><code>set.seed(1);
data.list &lt;- lapply(1:2, function(x) {
    nrep &lt;- 1;
    time &lt;- rep(seq(90,54000,by=90),times=nrep);
    Mx &lt;- rep(sort(runif(600,-0.014,0.012)),times=nrep);
    My &lt;- rep(sort(runif(600,-0.02,0.02)),times=nrep);
    Mz &lt;- rep(sort(runif(600,-1,1)),times=nrep);
    data.frame(time,Mx,My,Mz,set_nbr=x);
});
</code></pre>

<p>Which gives:</p>

<pre><code>lapply(data.list,head);
## [[1]]
##   time          Mx          My         Mz set_nbr
## 1   90 -0.01395224 -0.01994741 -0.9967155       1
## 2  180 -0.01394975 -0.01991923 -0.9933909       1
## 3  270 -0.01378866 -0.01980934 -0.9905714       1
## 4  360 -0.01371306 -0.01977090 -0.9854065       1
## 5  450 -0.01371011 -0.01961713 -0.9850108       1
## 6  540 -0.01365998 -0.01960718 -0.9846628       1
##
## [[2]]
##   time          Mx          My         Mz set_nbr
## 1   90 -0.01398426 -0.01997718 -0.9970438       2
## 2  180 -0.01398293 -0.01989651 -0.9931286       2
## 3  270 -0.01397330 -0.01988715 -0.9923425       2
## 4  360 -0.01396455 -0.01957807 -0.9913645       2
## 5  450 -0.01384501 -0.01939597 -0.9892001       2
## 6  540 -0.01382531 -0.01931913 -0.9889356       2
</code></pre>

<hr>

<p><strong>Edit:</strong> It looked from your question like you wanted the random numbers to be different <em>between list components</em>, that is to say, between the components generated from the 1:2 passed as the first argument to <code>lapply()</code>. The repetition of each random vector <code>nrep</code> times <em>within</em> each list component didn't appear to be relevant, partly because you set <code>nrep</code> to 1, so there wasn't any actual repetition.</p>

<p>But that's ok, we can achieve this requirement by using <code>replicate()</code> instead of <code>rep()</code>, because <code>replicate()</code> actual runs its expression argument once for every repetition. We also have to flatten the result, because <code>replicate()</code> by default returns a matrix, and we want a straight vector:</p>

<pre><code>set.seed(1);
data.list &lt;- lapply(1:2, function(x) {
    nrep &lt;- 2;
    time &lt;- rep(seq(90,54000,by=90),times=nrep);
    Mx &lt;- c(replicate(nrep,sort(runif(600,-0.014,0.012))));
    My &lt;- c(replicate(nrep,sort(runif(600,-0.02,0.02))));
    Mz &lt;- c(replicate(nrep,sort(runif(600,-1,1))));
    data.frame(time,Mx,My,Mz,set_nbr=x);
});
lapply(data.list,function(x) x[c(1:6,601:606),]);
## [[1]]
##     time          Mx          My         Mz set_nbr
## 1     90 -0.01395224 -0.01993431 -0.9988590       1
## 2    180 -0.01394975 -0.01986782 -0.9948254       1
## 3    270 -0.01378866 -0.01981143 -0.9943576       1
## 4    360 -0.01371306 -0.01970813 -0.9789037       1
## 5    450 -0.01371011 -0.01970022 -0.9697986       1
## 6    540 -0.01365998 -0.01969326 -0.9659567       1
## 601   90 -0.01396582 -0.01997579 -0.9970438       1
## 602  180 -0.01394750 -0.01997375 -0.9931286       1
## 603  270 -0.01387607 -0.01995893 -0.9923425       1
## 604  360 -0.01385108 -0.01994546 -0.9913645       1
## 605  450 -0.01375113 -0.01976155 -0.9892001       1
## 606  540 -0.01374467 -0.01973125 -0.9889356       1
##
## [[2]]
##     time          Mx          My         Mz set_nbr
## 1     90 -0.01396979 -0.01999198 -0.9960861       2
## 2    180 -0.01390373 -0.01995219 -0.9945237       2
## 3    270 -0.01390252 -0.01991559 -0.9925640       2
## 4    360 -0.01388905 -0.01978123 -0.9890171       2
## 5    450 -0.01386718 -0.01967644 -0.9835435       2
## 6    540 -0.01384351 -0.01958008 -0.9822988       2
## 601   90 -0.01396739 -0.01989328 -0.9971255       2
## 602  180 -0.01396433 -0.01985785 -0.9954987       2
## 603  270 -0.01390700 -0.01984074 -0.9903196       2
## 604  360 -0.01376890 -0.01982715 -0.9902251       2
## 605  450 -0.01366110 -0.01979802 -0.9829480       2
## 606  540 -0.01364868 -0.01977278 -0.9812671       2
</code></pre>
"
39372372,Count number of occurrences of vector in list,1,1,3,"<p>I have a list of vectors of variable length, for example:</p>

<pre><code>q &lt;- list(c(1,3,5), c(2,4), c(1,3,5), c(2,5), c(7), c(2,5))
</code></pre>

<p>I need to count the number of occurrences for each of the vectors in the list, for example (any other suitable datastructure acceptable):</p>

<pre><code>list(list(c(1,3,5), 2), list(c(2,4), 1), list(c(2,5), 2), list(c(7), 1))
</code></pre>

<p>Is there an efficient way to do this? The actual list has tens of thousands of items so quadratic behaviour is not feasible.</p>
","<p><code>match</code> and <code>unique</code> accept and handle ""list""s too (<code>?match</code> warns for being slow on ""list""s). So, with:</p>

<pre><code>match(q, unique(q))
#[1] 1 2 1 3 4 3
</code></pre>

<p>each element is mapped to a single integer. Then:</p>

<pre><code>tabulate(match(q, unique(q)))
#[1] 2 1 2 1
</code></pre>

<p>And find a structure to present the results:</p>

<pre><code>as.data.frame(cbind(vec = unique(q), n = tabulate(match(q, unique(q)))))
#      vec n
#1 1, 3, 5 2
#2    2, 4 1
#3    2, 5 2
#4       7 1
</code></pre>

<p>Alternatively to <code>match(x, unique(x))</code> approach, we could map each element to a single value with <code>deparse</code>ing:</p>

<pre><code>table(sapply(q, deparse))
#
#         7 c(1, 3, 5)    c(2, 4)    c(2, 5) 
#         1          2          1          2
</code></pre>

<p>Also, since this is a case with unique integers, and assuming in a small range, we could map each element to a single integer after transforming each element to a binary representation:</p>

<pre><code>n = max(unlist(q))
pow2 = 2 ^ (0:(n - 1))
sapply(q, function(x) tabulate(x, nbins = n))  # 'binary' form
sapply(q, function(x) sum(tabulate(x, nbins = n) * pow2))
#[1] 21 10 21 18 64 18
</code></pre>

<p>and then <code>tabulate</code> as before.</p>

<p>And just to compare the above alternatives:</p>

<pre><code>f1 = function(x) 
{
    ux = unique(x)
    i = match(x, ux)
    cbind(vec = ux, n = tabulate(i))
}   

f2 = function(x)
{
    xc = sapply(x, deparse)
    i = match(xc, unique(xc))
    cbind(vec = x[!duplicated(i)], n = tabulate(i))
}  

f3 = function(x)
{
    n = max(unlist(x))
    pow2 = 2 ^ (0:(n - 1))
    v = sapply(x, function(X) sum(tabulate(X, nbins = n) * pow2))
    i = match(v, unique(v))
    cbind(vec = x[!duplicated(v)], n = tabulate(i))
}

q2 = rep_len(q, 1e3)

all.equal(f1(q2), f2(q2))
#[1] TRUE
all.equal(f2(q2), f3(q2))
#[1] TRUE

microbenchmark::microbenchmark(f1(q2), f2(q2), f3(q2))
#Unit: milliseconds
#   expr       min        lq      mean    median        uq       max neval cld
# f1(q2)  7.980041  8.161524 10.525946  8.291678  8.848133 178.96333   100  b 
# f2(q2) 24.407143 24.964991 27.311056 25.514834 27.538643  45.25388   100   c
# f3(q2)  3.951567  4.127482  4.688778  4.261985  4.518463  10.25980   100 a 
</code></pre>

<p>Another interesting alternative is based on ordering. R > 3.3.0 has a <code>grouping</code> function, built off data.table, which, along with the ordering, provides some attributes for further manipulation:</p>

<p>Make all elements of equal length and ""transpose"" (probably the most slow operation in this case, though I'm not sure how else to feed <code>grouping</code>):</p>

<pre><code>n = max(lengths(q))
qq = .mapply(c, lapply(q, ""["", seq_len(n)), NULL)
</code></pre>

<p>Use ordering to group similar elements mapped to integers:</p>

<pre><code>gr = do.call(grouping, qq)
e = attr(gr, ""ends"") 
i = rep(seq_along(e), c(e[1], diff(e)))[order(gr)]
i
#[1] 1 2 1 3 4 3
</code></pre>

<p>then, tabulate as before.
To continue the comparisons:</p>

<pre><code>f4 = function(x)
{
    n = max(lengths(x))
    x2 = .mapply(c, lapply(x, ""["", seq_len(n)), NULL)
    gr = do.call(grouping, x2)
    e = attr(gr, ""ends"") 
    i = rep(seq_along(e), c(e[1], diff(e)))[order(gr)]
    cbind(vec = x[!duplicated(i)], n = tabulate(i))
}

all.equal(f3(q2), f4(q2))
#[1] TRUE

microbenchmark::microbenchmark(f1(q2), f2(q2), f3(q2), f4(q2))
#Unit: milliseconds
#   expr       min        lq      mean    median        uq        max neval cld
# f1(q2)  7.956377  8.048250  8.792181  8.131771  8.270101  21.944331   100  b 
# f2(q2) 24.228966 24.618728 28.043548 25.031807 26.188219 195.456203   100   c
# f3(q2)  3.963746  4.103295  4.801138  4.179508  4.360991  35.105431   100 a  
# f4(q2)  2.874151  2.985512  3.219568  3.066248  3.186657   7.763236   100 a
</code></pre>

<p>In this comparison <code>q</code>'s elements are of small length to accomodate for <code>f3</code>, but <code>f3</code> (because of large exponentiation) and <code>f4</code> (because of <code>mapply</code>) will suffer, in performance, if ""list""s of larger elements are used.</p>
"
27126609,copy values of a column into another column based on a condition using a loop,1,1,1,"<p>I need to create a complicated ""for"" loop, but after reading some examples I'm still clueless of how to write it in a proper R way and therefore I'm not sure whether it will work or not. I'm still an R beginner :(</p>

<p>I have a dataset in the long format, with different occasions, however, some occasions are not truly new ones since the date of start is the same, but have a different offence that I need to copy in a new column called ""offence2"", after this I need to drop the false new occasion, in order to keep only rows that represent new occasions. My real data have up to 8 different offences for a single date, but I made a simpler example. </p>

<p>This are an example of how my data looks like </p>

<pre><code>    id&lt;-c(1,1,1,2,2,3,3,3,4,4,4,4,5,5,5)
    dstart&lt;-c(""25/11/2006"", ""13/12/2006"",""13/12/2006"",""07/02/2006"",""07/02/2006"",
     ""15/01/2006"", ""22/03/2006"",""18/09/2006"", ""04/03/2006"",""04/03/2006"",
     ""22/08/2006"",""22/08/2006"",""11/04/2006"", ""11/04/2006"", ""19/10/2006"") 
    dstart1&lt;-as.Date(dstart, ""%d/%m/%Y"")

    offence&lt;-c(""a"",""b"",""c"",""b"",""d"",""a"",""a"",""e"",""b"",""a"",""c"",""a"",""a"",""b"",""a"")
    cod_offence&lt;-c(25, 26,27,26,28,25,25,29,26,25,27,25,25,26,25)

    mydata&lt;-data.frame(id, dstart1, offence, cod_offence)
</code></pre>

<p>Data</p>

<pre><code>       id    dstart1   offence  cod_offence
   1   1   2006-11-25       a          25
   2   1   2006-12-13       b          26
   3   1   2006-12-13       c          27
   4   2   2006-02-07       b          26
   5   2   2006-02-07       d          28
   6   3   2006-01-15       a          25
   7   3   2006-03-22       a          25
   8   3   2006-09-18       e          29
   9   4   2006-03-04       b          26
   10  4   2006-03-04       a          25
   11  4   2006-08-22       c          27
   12  4   2006-08-22       a          25
   13  5   2006-04-11       a          25
   14  5   2006-04-11       b          26
   15  5   2006-10-19       a          25
</code></pre>

<p>I need something like this:</p>

<pre><code>      id    dstart1   offence  cod_offence   offence2
   1   1   2006-11-25       a          25       NA
   2   1   2006-12-13       b          26       c
   3   1   2006-12-13       c          27       NA
   4   2   2006-02-07       b          26       d
   5   2   2006-02-07       d          28       NA
   6   3   2006-01-15       a          25       NA
   7   3   2006-03-22       a          25       NA
   8   3   2006-09-18       e          29       NA
   9   4   2006-03-04       b          26       a
   10  4   2006-03-04       a          25       NA
   11  4   2006-08-22       c          27       a
   12  4   2006-08-22       a          25       NA
   13  5   2006-04-11       a          25       b
   14  5   2006-04-11       b          26       NA
   15  5   2006-10-19       a          25       NA
</code></pre>

<p>I think that I need to do something like this:
given i=individual
      j=observation within individual</p>

<pre><code>for each individual I need to check whether mydata$dstart1(j) = mydata$dstart1(j+1)
if this is true, then copy mydata$offence2(j)=mydata$offence(j+1), otherwise keep the same value
This has to stop if id(j) != id(j+1) and re-start with the new id.
</code></pre>

<p>My problem is that I don't know how to put this in a loop.</p>

<p>Thank you!!</p>

<p>Update</p>

<p>Yes, it'w works fine with the example, but not yet with my real data, since they are a little bit more complex
What happen if instead of two repeated dates I have three or more? each one of them with different offences. Following @CathG solution, I need to create more variables according to the number of offences (in my case 8), I guess I would need a new vector that identify the position of the observation within id and a new ""instruction"" that tell R that depending of the position of the mydata$dstart1, the value need to be copied in a different column. But then again, I don't know how to do it.</p>

<pre><code>     id    dstart1   offence  cod_offence   offence2   offence3  offence4
   1   1   2006-11-25       a          25       NA        NA       NA
   2   1   2006-12-13       b          26       c         NA       NA
   3   1   2006-12-13       c          27       NA        NA       NA
   4   2   2006-02-07       b          26       d         NA       NA
   5   2   2006-02-07       d          28       NA        NA       NA
   6   2   2006-04-12       b          26       d         c        a
   7   2   2006-04-12       d          28       NA        NA       NA
   8   2   2006-04-12       c          27       NA        NA       NA
   9   2   2006-04-12       a          25       NA        NA       NA
</code></pre>

<p>Thanks again!!!</p>
","<p>You can use <code>base R</code></p>

<pre><code>indx &lt;- with(mydata, ave(as.numeric(dstart1), id,
           FUN=function(x) c(x[-1]==x[-length(x)], FALSE)))

 transform(mydata, offence2=ifelse(!!indx, 
            c(as.character(offence[-1]), NA), NA))
</code></pre>

<p>Or using <code>dplyr</code></p>

<pre><code>library(dplyr)
mydata %&gt;%
      group_by(id) %&gt;% 
      mutate(offence2= dstart1==lead(dstart1), 
       offence2= ifelse(!is.na(offence2)&amp;offence2,
         as.character(lead(offence)), NA_character_))
#     id    dstart1 offence cod_offence offence2
#1   1 2006-11-25       a          25       NA
#2   1 2006-12-13       b          26        c
#3   1 2006-12-13       c          27       NA
#4   2 2006-02-07       b          26        d
#5   2 2006-02-07       d          28       NA
#6   3 2006-01-15       a          25       NA
#7   3 2006-03-22       a          25       NA
#8   3 2006-09-18       e          29       NA
#9   4 2006-03-04       b          26        a
#10  4 2006-03-04       a          25       NA
#11  4 2006-08-22       c          27        a
#12  4 2006-08-22       a          25       NA
#13  5 2006-04-11       a          25        b
#14  5 2006-04-11       b          26       NA
#15  5 2006-10-19       a          25       NA
</code></pre>

<p>or using <code>data.table</code></p>

<pre><code>library(data.table)
setDT(mydata)[, indx:=c(dstart1[-1]==dstart1[-.N], FALSE), by=id][,
      offence2:=ifelse(indx, as.character(offence)[which(indx)+1],
                                 NA_character_), by=id][,indx:=NULL]

mydata
 #    id    dstart1 offence cod_offence offence2
 #1:  1 2006-11-25       a          25       NA
 #2:  1 2006-12-13       b          26        c
 #3:  1 2006-12-13       c          27       NA
 #4:  2 2006-02-07       b          26        d
 #5:  2 2006-02-07       d          28       NA
 #6:  3 2006-01-15       a          25       NA
 #7:  3 2006-03-22       a          25       NA
 #8:  3 2006-09-18       e          29       NA
 #9:  4 2006-03-04       b          26        a
#10:  4 2006-03-04       a          25       NA
#11:  4 2006-08-22       c          27        a
#12:  4 2006-08-22       a          25       NA
#13:  5 2006-04-11       a          25        b
#14:  5 2006-04-11       b          26       NA
#15:  5 2006-10-19       a          25       NA
</code></pre>

<h3>Update</h3>

<p>Using the new dataset <code>mydata2</code> and if you use the first method, we get <code>d1</code></p>

<pre><code> indx &lt;- with(mydata2, ave(as.numeric(dstart1), id,
       FUN=function(x) c(x[-1]==x[-length(x)], FALSE)))

 d1 &lt;-  transform(mydata2, offence2=ifelse(!!indx, 
                  c(as.character(offence[-1]), NA), NA))
</code></pre>

<p>From <code>d1</code>, we can create an <code>indx</code> column and then use <code>dcast</code> to convert from <code>long</code> form to <code>wide</code> for the column <code>offence2</code>.  If there are columns with all <code>NAs</code>, we can remove that by using <code>colSums(is.na(</code>.  Rename the columns, and then use <code>mutate_each</code> from <code>dplyr</code> to sort the columns, and finally <code>cbind</code> it with <code>mydata2</code></p>

<pre><code> d1$indx &lt;- with(d1, ave(seq_along(id), id, dstart1, FUN=seq_along))
 library(reshape2)

 d2 &lt;- dcast(d1, id + dstart1+indx~indx, value.var='offence2')
 d2New &lt;- d2[,colSums(is.na(d2))!=nrow(d2)]
 nm1 &lt;-  grep(""^\\d"",colnames(d2New))
 colnames(d2New)[nm1] &lt;- paste0('offence', 2:(length(nm1)+1)) 
 d3 &lt;- d2New[,-3] %&gt;%
                group_by(id, dstart1) %&gt;%
                mutate_each(funs(.[order(.)])) %&gt;%
                ungroup()

 cbind(mydata,d3[,-c(1:2)])
 #    id    dstart1 offence cod_offence offence2 offence3 offence4
 #1  1 2006-11-25       a          25     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;
 #2  1 2006-12-13       b          26        c     &lt;NA&gt;     &lt;NA&gt;
 #3  1 2006-12-13       c          27     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;
 #4  2 2006-02-07       b          26        d     &lt;NA&gt;     &lt;NA&gt;
 #5  2 2006-02-07       d          28     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;
 #6  2 2006-04-12       b          26        d        c        a
 #7  2 2006-04-12       d          28     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;
 #8  2 2006-04-12       c          27     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;
 #9  2 2006-04-12       a          25     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;
</code></pre>

<h3>data</h3>

<pre><code>mydata &lt;- structure(list(id = c(1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 
5, 5), dstart1 = structure(c(13477, 13495, 13495, 13186, 13186, 
13163, 13229, 13409, 13211, 13211, 13382, 13382, 13249, 13249, 
13440), class = ""Date""), offence = structure(c(1L, 2L, 3L, 2L, 
4L, 1L, 1L, 5L, 2L, 1L, 3L, 1L, 1L, 2L, 1L), .Label = c(""a"", 
""b"", ""c"", ""d"", ""e""), class = ""factor""), cod_offence = c(25, 26, 
27, 26, 28, 25, 25, 29, 26, 25, 27, 25, 25, 26, 25)), .Names = c(""id"", 
""dstart1"", ""offence"", ""cod_offence""), row.names = c(NA, -15L), 
class = ""data.frame"")

mydata2 &lt;- structure(list(id = c(1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L),
dstart1 = structure(c(13477, 13495, 13495, 13186, 13186, 13250, 13250,
 13250, 13250), class = ""Date""), offence = c(""a"", ""b"", ""c"", ""b"", ""d"", ""b"",
""d"", ""c"", ""a""), cod_offence = c(25L, 26L, 27L, 26L, 28L, 26L, 28L, 27L, 25L
)), .Names = c(""id"", ""dstart1"", ""offence"", ""cod_offence""), row.names =
 c(""1"",""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""), class = ""data.frame"")
</code></pre>
"
45389673,"function to convert amount into Indian words (Crores, Lakhs, Thousands ...) in R language",2,2,4,"<p>I need a function in R language to convert amount into Indian words in <a href=""https://en.m.wikipedia.org/wiki/Indian_numbering_system#Names_of_numbers"" rel=""nofollow noreferrer"">crore, lakh, thousand etc</a>...</p>

<p>For example:</p>

<ul>
<li><p>function(3257) should generate: Three Thousand Two Hundred and Fifty Seven;</p></li>
<li><p>function(473257) should generate: Four Lakh Seventy Three Thousand Two Hundred and Fifty Seven;</p></li>
<li><p>function(12473257) should generate: One Crore Twenty Four Lakh Seventy Three Thousand Two Hundred and Fifty Seven</p></li>
</ul>

<p>Plenty of working VBA functions can be found on the internet for use with Microsoft Excel and Access but I was unable to find a similar function in R language.</p>

<p><strong>Edit:</strong> Example how to do it in English words: <a href=""https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r"" rel=""nofollow noreferrer"">https://github.com/ateucher/useful_code/blob/master/R/numbers2words.r</a></p>
","<p>Seems that I have figured it out myself. Interested users may test and report bugs, if any.</p>

<pre><code># function to convert number to words in Indian counting system
# https://en.wikipedia.org/wiki/Indian_numbering_system#Names_of_numbers

# number always rounded; multiple numbers also accepted as vector
# currently handles numbers upto 15 digits but can be easily extended

# Credit: A big THANKS to Mr. John Fox.
# His function to convert number to English words can be found at:
# http://tolstoy.newcastle.edu.au/R/help/05/04/2715.html

SpellIndianNumber &lt;- function(x){

  helper &lt;- function(x){

    digits &lt;- rev(strsplit(as.character(x), """")[[1]])
    nDigits &lt;- length(digits)

    # function meant to handle numbers upto 15 digits only
    if(nDigits &gt; 15) stop(""Number is too large!"")

    # single digit cases: 1 to 9
    if (nDigits == 1) as.vector(ones[digits])

    # two digits cases: 10 to 99
    else if (nDigits == 2)
      if (x &lt;= 19) as.vector(teens[digits[1]]) # for 10 to 19
    else trim(paste(tens[digits[2]], # for 20 to 99
                    Recall(as.numeric(digits[1]))))

    # three digits cases: 100 to 999
    else if (nDigits == 3) trim(paste(ones[digits[3]], ""Hundred"", 
                                      Recall(makeNumber(digits[2:1]))))

    # four &amp; five digits cases: handling thousands
    else if (nDigits &lt;= 5){
      thousands &lt;- x %/% 1e3
      remainder &lt;- x %% 1e3
      trim(paste(Recall(thousands), ""Thousand"", Recall(remainder)))
    }

    # six &amp; seven digits cases: handling lakhs
    else if (nDigits &lt;= 7){
      lakhs &lt;- x %/% 1e5
      remainder &lt;- x %% 1e5
      trim(paste(Recall(lakhs), ""Lakh"", Recall(remainder)))
    }

    # eight &amp; nine digits cases: handling crores
    else if (nDigits &lt;= 9){
      crores &lt;- x %/% 1e7
      remainder &lt;- x %% 1e7
      trim(paste(Recall(crores), ""Crore"", Recall(remainder)))
    }

    # ten &amp; eleven digits cases: handling arabs
    else if (nDigits &lt;= 11){
      arabs &lt;- x %/% 1e9
      remainder &lt;- x %% 1e9
      trim(paste(Recall(arabs), ""Arab"", Recall(remainder)))
    }

    # twelve &amp; thirteen digits cases: handling kharabs
    else if (nDigits &lt;= 13){
      kharabs &lt;- x %/% 1e11
      remainder &lt;- x %% 1e11
      trim(paste(Recall(kharabs), ""Kharab"", Recall(remainder)))
    }

    # fourteen &amp; fifteen digits cases: handling neels
    else if (nDigits &lt;= 15){
      neels &lt;- x %/% 1e13
      remainder &lt;- x %% 1e13
      trim(paste(Recall(neels), ""Neel"", Recall(remainder)))
    }
  }

  trim &lt;- function(text){
    gsub(""^\ "", """", gsub(""\ *$"", """", text))
  }

  makeNumber &lt;- function(...) as.numeric(paste(..., collapse=""""))

  opts &lt;- options(scipen=100)
  on.exit(options(opts))

  ones &lt;- c("""", ""One"", ""Two"", ""Three"", ""Four"", ""Five"", ""Six"", ""Seven"",
            ""Eight"", ""Nine"")
  names(ones) &lt;- 0:9

  teens &lt;- c(""Ten"", ""Eleven"", ""Twelve"", ""Thirteen"", ""Fourteen"", ""Fifteen"",
             ""Sixteen"", "" Seventeen"", ""Eighteen"", ""Nineteen"")
  names(teens) &lt;- 0:9

  tens &lt;- c(""Twenty"", ""Thirty"", ""Forty"", ""Fifty"", ""Sixty"", ""Seventy"",
            ""Eighty"", ""Ninety"")
  names(tens) &lt;- 2:9

  x &lt;- round(x)

  if (length(x) &gt; 1) return(sapply(x, helper))
  helper(x)
}

# Examples:

# &gt; SpellIndianNumber(83720834)
# [1] ""Eight Crore Thirty Seven Lakh Twenty Thousand Eight Hundred Thirty Four""

# &gt; SpellIndianNumber(1283720834)
# [1] ""One Arab Twenty Eight Crore Thirty Seven Lakh Twenty Thousand Eight Hundred Thirty Four""

# &gt; SpellIndianNumber(653234532342345)
# [1] ""Sixty Five Neel Thirty Two Kharab Thirty Four Arab Fifty Three Crore Twenty Three Lakh Forty Two Thousand Three Hundred Forty Five""

# &gt; SpellIndianNumber(c(5,10,87))
# [1] ""Five""         ""Ten""          ""Eighty Seven""

# &gt; SpellIndianNumber(11:15)
# [1] ""Eleven""   ""Twelve""   ""Thirteen"" ""Fourteen"" ""Fifteen"" 

# Number Zero will appear as a zero length string
# &gt; SpellIndianNumber(0)
# [1] """"

# &gt; SpellIndianNumber(c(12,0,87))
# [1] ""Twelve""       """"             ""Eighty Seven""
</code></pre>
"
41320139,how can I make my data side by side barplot with dots,1,3,1,"<p>I have read this post <a href=""https://stackoverflow.com/questions/18265941/two-horizontal-bar-charts-with-shared-axis-in-ggplot2-similar-to-population-pyr"">Two horizontal bar charts with shared axis in ggplot2 (similar to population pyramid)</a> and similar ones which could not help me to solve my problem because my data is different. Where I try to explain below </p>

<p>I have a data like below </p>

<pre><code>    dfm&lt;- structure(list(Var1 = structure(1:35, .Label = c(""F10"", ""F11"", 
""F12"", ""F13"", ""F14"", ""F15"", ""F16"", ""F18"", ""F19"", ""F2"", ""F21"", 
""F25"", ""F3"", ""F30"", ""F35"", ""F4"", ""F5"", ""F58"", ""F6"", ""F60"", ""F7"", 
""F79"", ""F8"", ""F9"", ""F97"", ""F117"", ""F17"", ""F22"", ""F23"", ""F26"", 
""F31"", ""F41"", ""F61"", ""F67"", ""F87""), class = ""factor""), Freq.x = c(4L, 
1L, 2L, 3L, 4L, 1L, 2L, 2L, 1L, 252L, 1L, 2L, 106L, 1L, 1L, 56L, 
32L, 1L, 28L, 1L, 17L, 1L, 10L, 7L, 1L, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA), Freq.y = c(12L, 3L, 6L, NA, 7L, NA, 1L, NA, 
2L, 306L, 1L, 2L, 170L, NA, NA, 69L, 45L, NA, 35L, NA, 20L, NA, 
13L, 7L, NA, 1L, 3L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L)), .Names = c(""Var1"", 
""Freq.x"", ""Freq.y""), row.names = c(NA, -35L), class = ""data.frame"")
</code></pre>

<p>I have a mutual column named Var1 and two columns Freq.x and Freq.y 
I want to plot barplot in vertical ways and put a <code>red</code> or <code>blue</code>dot in top of each bar.</p>

<p>I want to plot the Freq.x in the right side, Freq.y in the left side and the F2 to ... in the middle of both as Y axis </p>

<p>Something like this </p>

<p><a href=""https://i.stack.imgur.com/ywxox.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ywxox.png"" alt=""enter image description here""></a></p>

<p>I have tried many things but I could not make it. </p>

<p>The closest I got is with the following code</p>

<pre><code>par(mfrow=c(1,2))
barplot(dfm$Freq.y,axes=F,horiz=T,axisnames=FALSE)
axis(side=1,at=seq(1,252,50))
barplot(dfm$Freq.x,horiz=T,axes=T,las=1)
</code></pre>

<p>If you just plot the figure as the post mentioned above,
you see that it is giving wrong figure for my data
for example</p>

<pre><code>library(grid)
g.mid&lt;-ggplot(dfm,aes(x=1,y=Var1))+geom_text(aes(label=Var1))+
  geom_segment(aes(x=0.94,xend=0.96,yend=Var1))+
  geom_segment(aes(x=1.04,xend=1.065,yend=Var1))+
  ggtitle("""")+
  ylab(NULL)+
  scale_x_continuous(expand=c(0,0),limits=c(0.94,1.065))+
  theme(axis.title=element_blank(),
        panel.grid=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.background=element_blank(),
        axis.text.x=element_text(color=NA),
        axis.ticks.x=element_line(color=NA),
        plot.margin = unit(c(1,-1,1,-1), ""mm""))


g1 &lt;- ggplot(data = dfm, aes(x = Var1, y = Freq.x)) + 
geom_bar(stat = ""identity"") + ggtitle(""Number of sales staff"") + 
theme(axis.title.x = element_blank(), 
axis.title.y = element_blank(), 
axis.text.y = element_blank(), 
axis.ticks.y = element_blank(), 
plot.margin = unit(c(1,-1,1,0), ""mm"")) + 
scale_y_reverse() + coord_flip() 

g2 &lt;- ggplot(data = dfm, aes(x = Var1, y = Freq.y)) +xlab(NULL)+ 
geom_bar(stat = ""identity"") + ggtitle(""Sales (x $1000)"") + 
theme(axis.title.x = element_blank(), axis.title.y = element_blank(), 
axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
plot.margin = unit(c(1,0,1,-1), ""mm"")) + 
coord_flip() 

library(gridExtra) 
gg1 &lt;- ggplot_gtable(ggplot_build(g1)) 
gg2 &lt;- ggplot_gtable(ggplot_build(g2)) 
gg.mid &lt;- ggplot_gtable(ggplot_build(g.mid)) 

grid.arrange(gg1,gg.mid,gg2,ncol=3,widths=c(4/9,1/9,4/9))
</code></pre>

<p><a href=""https://i.stack.imgur.com/bLSHS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bLSHS.png"" alt=""enter image description here""></a>
See the plot
and look at for example the F2 values for both dfs
you see? both are the highest values but in this plot they show the least
about the second answer. they guy Melted the data and it made me crazy to melt my data
so I could not figure it out how to plot it based on that
about the third, it is completely out of my interest because I only need one plot with my data and not splitting it</p>

<p>after assigning the g.mid , it does not take the order into account , it starts from 87 to the end while the order of my <code>dfm</code>starts from 2</p>

<p><a href=""https://i.stack.imgur.com/R4BGr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R4BGr.png"" alt=""enter image description here""></a></p>
","<p>For the red and blue dots. You can add <code>+geom_point(aes(x = Var1, y = Freq.x),shape=19,size=6,color=""red"")+</code>and <code>geom_point(aes(x = Var1, y = Freq.y),shape=19,size=6,color=""blue"")+</code>. Try this code (with @cuttlefish44 's help):</p>

<pre><code>library(dplyr)
ind &lt;- gsub(""F"", """", dfm$Var1) %&gt;% as.numeric() %&gt;% order(decreasing = T)
newlev &lt;- levels(dfm$Var1)[ind]
dfm$Var1 &lt;- factor(dfm$Var1, levels = newlev)
library(grid)
g.mid&lt;-ggplot(dfm,aes(x=1,y=Var1))+geom_text(aes(label=Var1))+
  geom_segment(aes(x=0.94,xend=0.96,yend=Var1))+
  geom_segment(aes(x=1.04,xend=1.065,yend=Var1))+
  ggtitle("""")+
  ylab(NULL)+
  scale_x_continuous(expand=c(0,0),limits=c(0.94,1.065))+
  theme(axis.title=element_blank(),
        panel.grid=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.background=element_blank(),
        axis.text.x=element_text(color=NA),
        axis.ticks.x=element_line(color=NA),
        plot.margin = unit(c(1,-1,1,-1), ""mm""))
g1 &lt;- ggplot(data = dfm, aes(x = Var1, y = Freq.x)) + 
  geom_bar(stat = ""identity"") + ggtitle(""Number of sales staff"") + 
  geom_point(aes(x = Var1, y = Freq.x),shape=19,size=6,color=""red"")+
  theme(axis.title.x = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        plot.margin = unit(c(1,-1,1,0), ""mm"")) + 
  scale_y_reverse() + coord_flip() 
g2 &lt;- ggplot(data = dfm, aes(x = Var1, y = Freq.y)) +xlab(NULL)+ 
  geom_bar(stat = ""identity"") + ggtitle(""Sales (x $1000)"") + 
  geom_point(aes(x = Var1, y = Freq.y),shape=19,size=6,color=""blue"")+
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), 
        axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
        plot.margin = unit(c(1,0,1,-1), ""mm"")) + 
  coord_flip() 
library(gridExtra) 
gg1 &lt;- ggplot_gtable(ggplot_build(g1)) 
gg2 &lt;- ggplot_gtable(ggplot_build(g2)) 
gg.mid &lt;- ggplot_gtable(ggplot_build(g.mid)) 

grid.arrange(gg1,gg.mid,gg2,ncol=3,widths=c(4/9,1/9,4/9))
</code></pre>

<p><a href=""https://i.stack.imgur.com/7HJIh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7HJIh.png"" alt=""enter image description here""></a></p>

<p><strong>UPDATE Nº4</strong></p>

<p>In this example, I modify the labels. Try this code:</p>

<pre><code>library(ggplot2)
library(dplyr)
ind &lt;- gsub(""F"", """", dfm$Var1) %&gt;% as.numeric() %&gt;% order(decreasing = T)
newlev &lt;- levels(dfm$Var1)[ind]
dfm$Var1 &lt;- factor(dfm$Var1, levels = newlev)
#labeling 35 levels to 7 levels
library(plyr)
each5&lt;-levels(dfm$Var1)
interval&lt;-rep(c(rep(c(FALSE),each=4),TRUE),length.out= 35)
dfm$Var2&lt;-c("""")
for (i in 1:length(dfm$Var1)){
  if (dfm$Var1[i] %in% each5[interval])
      dfm$Var2[i]&lt;-as.character(dfm$Var1[i])
}
library(grid)
g.mid&lt;-ggplot(dfm,aes(x=1,y=Var1))+
  geom_text(aes(label= Var2),fontface=2)+
  geom_segment(aes(x=0.94,xend=0.96,yend=Var1))+
  geom_segment(aes(x=1.04,xend=1.065,yend=Var1))+
  ggtitle("""")+
  ylab(NULL)+
  scale_x_continuous(expand=c(0,0),limits=c(0.94,1.065))+
  scale_y_discrete(labels=c(""F2"" = ""Dose 0.5"", ""F10"" = ""Dose 1"",
                            ""F21"" = ""Dose 2""))+
  theme(axis.title=element_blank(),
        panel.grid=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.background=element_blank(),
        axis.text.x=element_text(color=NA),
        axis.ticks.x=element_line(color=NA),
        plot.margin = unit(c(1,-1,1,-1), ""mm""))
g1 &lt;- ggplot(data = dfm, aes(x = Var1, y = Freq.x)) + 
  geom_bar(stat = ""identity"") + ggtitle(""Number of sales staff"") + 
  geom_point(aes(x = Var1, y = Freq.x),shape=19,size=6,color=""red"")+
  expand_limits(y =600) +
  theme(axis.title.x = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        plot.margin = unit(c(1,-1,1,0), ""mm"")) + 
  scale_y_reverse() + coord_flip() 
g2 &lt;- ggplot(data = dfm, aes(x = Var1, y = Freq.y)) +xlab(NULL)+ 
  geom_bar(stat = ""identity"") + ggtitle(""Sales (x $1000)"") + 
  geom_point(aes(x = Var1, y = Freq.y),shape=19,size=6,color=""blue"")+
  expand_limits(y =600) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), 
        axis.text.y = element_blank(), axis.ticks.y = element_blank(), 
        plot.margin = unit(c(1,0,1,-1), ""mm"")) + 
  coord_flip() 
library(gridExtra) 
gg1 &lt;- ggplot_gtable(ggplot_build(g1)) 
gg2 &lt;- ggplot_gtable(ggplot_build(g2)) 
gg.mid &lt;- ggplot_gtable(ggplot_build(g.mid)) 

grid.arrange(gg1,gg.mid,gg2,ncol=3,widths=c(4/9,1/9,4/9))
</code></pre>

<p><a href=""https://i.stack.imgur.com/OBgLj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OBgLj.png"" alt=""enter image description here""></a></p>
"
9627389,XY plot with multiple Y scales,1,3,3,"<p>This is probably not a duplicate as I am not requesting how to plot 2 series in the same plot, but rather how to add 2 (interconnected) y scales on one XY series</p>

<p>Here is the frame</p>

<pre><code>t &lt;- data.frame(x=seq(0,1,0.01), y=exp(seq(0,1,0.01))*500, y2=exp(seq(0,1,0.01))*30)

head(t)
     x        y       y2
1 0.00 500.0000 30.00000
2 0.01 505.0251 30.30151
3 0.02 510.1007 30.60604
4 0.03 515.2273 30.91364
5 0.04 520.4054 31.22432
6 0.05 525.6355 31.53813
</code></pre>

<p>y and y2 are linearly interconnected. I would like to construct a plot with x, y (at left, side 2) and y2 (at right, side 4) so that each point on the plot has one x and 2 y coordinates.</p>

<p>base R only solutions please.</p>

<p>Thank you</p>
","<p>From what I understand, you want <code>y</code> and <code>y2</code> to correspond exactly (in terms of the scales)?</p>

<p>In that case, you can plot y vs x which draws an axis on the left hand side, and then draw an identical axis on the right hand side, <em>except</em> that you change the labels.</p>

<p>In base graphics:</p>

<pre><code># do first plot. Don't draw axis (we'll do it later)
plot(y~x,data=t,axes=F,ylab='y')
# draw y axis (see ?axis, 1=bottom, 2=left, 3=top, 4=right)
axis(2,pretty(range(t$y)))

# tell R to draw over the first plot
par(new=T)

# do second plot. 
plot(y2~x,data=t,axes=F,ylab="""")
# draw second axis on the right
axis(4,pretty(range(t$y2)),ylab='y2')

# draw x axis on the bottom
axis(1,pretty(range(t$x)))

# draw the box if you want
box()
</code></pre>

<p>How this works:</p>

<p>1) plot y vs x. Doesn't draw the axes, because:
2) we draw on the axes. <code>axis(2,pretty(range(t$y)))</code>. <code>pretty</code> takes a start and end point, and generates suitable axis tick marks (usually multiples of 5 or 10)
3) <code>par(new=T)</code> : we tell R that the next plot it does should be drawn over the top of the current - don't wipe away what we've drawn so far!
4) we plot <code>y2</code> vs <code>x</code>. This resets the coordinate system to the <code>y2</code> coordinate system.
5) we draw the right hand axis. This works because since we've just plotted <code>y2</code> vs <code>x</code>, the coordinate system is for <code>y2</code> (and not for <code>y</code> as it was when we first started plotting).
6) draw on the x axis
7) draw the box around the plot (if you like).</p>

<h3>Tweaks</h3>

<p>You may notice that the plot is lop-sided - there's a lot more white space between the left hand side of the plot &amp; the edge of the graphics device than there is on the right. That's because R wants to make space for the y label which is usually drawn on the left.</p>

<p>If you want to even it up, use <code>par(mar=c(top,left,bottom,right))</code>.</p>

<p>Looking at <code>?par</code>, we see the default is <code>c(5,4,4,2)+.1</code>.</p>

<p>So, add this snippet <em>to the front</em> of your code:</p>

<pre><code># get the current margins (top, left, bottom, right)
m &lt;- par('mar')
# make sure the right margin is the same as the left.
m[4]&lt;-m[2]
# set the new margins
par(mar=m)

# .... perform plotting as above.
</code></pre>

<p>Now you can see that there's equal space either side of the plot, so ti doesn't look lopsided any more. However, you may also notice that there is a y label on the left axis but not on the right.</p>

<p>It's a bit ugly - we have to add it in manually using <code>mtext</code> (which draws text on the plot):</p>

<pre><code>mtext('y2',4,line=2)
</code></pre>

<p>The <code>'y2'</code> is the y axis label, the 4 means ""draw on the right hand side of the plot"", and <code>line=2</code> says ""draw the label on line 2, starting from 0 at the axis and counting outwards"".</p>

<h3>Summary</h3>

<pre><code># margin
m &lt;- par('mar')
m[4] &lt;- m[2]
par(mar=m)

# plotting
plot(y~x,data=t,axes=F,ylab='y')
axis(2,pretty(range(t$y)))
par(new=T)
plot(y2~x,data=t,axes=F,ylab="""")
axis(4,pretty(range(t$y2)),ylab='y2')
axis(1,pretty(range(t$x)))
box()

# right-hand ylabel
mtext('y2',4,line=2)
</code></pre>

<p><img src=""https://i.stack.imgur.com/zO19G.png"" alt=""enter image description here""></p>
"
46249272,R: use read.table to load data with doublequotes,2,2,4,"<p>My csv version data is like:</p>

<pre><code>name,words,name
John, ""He says:""I love it!"""", 18
</code></pre>

<p>At first, I tried to load data with</p>

<pre><code>data &lt;- read.table(""data.csv"",header = T,sep = ',',quote = """",stringsAsFactors = FALSE)
</code></pre>

<p>And error is:</p>

<pre><code>Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  : 
  line 1 did not have 3 elements
</code></pre>

<p>Well, I can understand that, since R messes up with many doublequotes.</p>

<p>And I fixed it with</p>

<pre><code>data &lt;- read.table(""data.csv"",header = T,sep = ',',quote = ""\"""",,stringsAsFactors = FALSE) #change the name of the output file 
</code></pre>

<p>However, I can't figure why is it so, how does R know which doublquotes he should stop at?</p>
","<p>Well, that's an interesting data format -- and interesting behavior. The help page says ""See scan for the behaviour on quotes embedded in quotes,"" but I didn't see anything useful in that help page, so I tried some things.</p>

<p>What I believe the <code>quote</code> argument does is to tell R to ignore any <code>sep</code> elements that occur between quotes, and also to remove any <code>quote</code> elements (because that's meant to be used only for delimiting columns, not as data). So this works for you only because you don't have any commas after the second quote in your <code>words</code> column.</p>

<p>Here are four examples.</p>

<h3>no commas in the quotes (your example)</h3>

<pre><code>name,words,name
John, ""He says:""I love it!"""", 18
</code></pre>

<p>Interestingly, this example works for me in both versions of your code. The first leaves in all the quotes and the second removes them.</p>

<pre><code>read.table(""data.csv"", header = TRUE, sep = ',', quote = """", stringsAsFactors = FALSE)
##   name                   words name.1
## 1 John  ""He says:""I love it!""""     18
read.table(""data.csv"", header = TRUE, sep = ',', quote = ""\"""", stringsAsFactors = FALSE)
##   name               words name.1
## 1 John  He says:I love it!     18
</code></pre>

<h3>comma after the first quote</h3>

<pre><code>name,words,name
John, ""He says, ""I love it!"""", 18
</code></pre>

<p>Here the first version (<code>quote=""""</code>) separates the row into four columns, not three, based on the commas, and uses the extra column as the rownames. The second version ignores the added comma, but also removes the quotes around the actual quotation.</p>

<pre><code>read.table(""text.csv"", header = TRUE, sep = ',', quote = """", stringsAsFactors = FALSE)
##           name          words name.1
## John  ""He says  ""I love it!""""     18
read.table(""text.csv"", header = TRUE, sep = ',', quote = ""\"""", stringsAsFactors = FALSE)
##   name                words name.1
## 1 John  He says, I love it!     18
</code></pre>

<h3>comma after the second quote</h3>

<pre><code>name,words,name
John, ""He says: ""I love it, do you?"""", 18
</code></pre>

<p>Here both versions do almost the same thing (four columns) because the comma isn't between a paired quote. The first keeps the quotes, the second doesn't.</p>

<pre><code>read.table(""text.csv"", header = TRUE, sep = ',', quote = """", stringsAsFactors = FALSE)
##                       name      words name.1
## John  ""He says: ""I love it  do you?""""     18
read.table(""text.csv"", header = TRUE, sep = ',', quote = ""\"""", stringsAsFactors = FALSE)
##                     name    words name.1
## John  He says: I love it  do you?     18
</code></pre>

<h3>commas in between both quotes</h3>

<pre><code>name,words,name
John, ""He says, ""I love it, do you?"""", 18
</code></pre>

<p>Here the first one doesn't work, as it finds three column names but five columns in the first row. The second skips the first comma, but not the second, so again separates it into four columns, and uses the extra as the row name.</p>

<pre><code>read.table(""text.csv"", header = TRUE, sep = ',', quote = """", stringsAsFactors = FALSE)
## Error in read.table(""text.csv"", header = TRUE, sep = "","", quote = """",  : 
##  more columns than column names
read.table(""text.csv"", header = TRUE, sep = ',', quote = ""\"""", stringsAsFactors = FALSE)
##                     name    words name.1
## John  He says, I love it  do you?     18
</code></pre>

<p>Finally, all of these examples only have one line; if you have more than one line and they parse into different numbers of columns, you'll get an error like the one you got, except for the first line at which the number of columns differ.</p>

<p>What surprises me about your error is that it happens on line 1; you'd get this error if R thought you had less than three columns in that line (the number it found in the header row), but on my system, anyway, it finds three elements in that line.</p>
"
22064611,How to draw rotated axes in R?,1,3,3,"<p>I want to plot the results from a six factor personality test as a circumplex.</p>

<p>The test in question is the <em>Allgemeiner Interessen-Struktur-Test</em> (AIST-R; Bergmann &amp; Eder, 2005) [General Interest Structure Test], which measures vocational choice based on the theory of J. L. Holland (<a href=""http://en.wikipedia.org/wiki/Holland_Codes"" rel=""nofollow noreferrer"">Holland codes</a>, RIASEC). You can use the answers below to plot the ""Felddarstellung"" [field representation] recommended in the manual in stead of the interest profile to better visualize the vector of differentiation.</p>

<p>The resulting graphic should look similar to this:</p>

<p><img src=""https://i.stack.imgur.com/okWjR.png"" alt=""enter image description here""></p>

<p>The test results are given as angles and lengths.</p>

<ul>
<li><p><strong>How can I draw an axis or geometric vector in R from a starting point with a specific length, without defining the end coordinates (as required by <code>arrows</code>)?</strong></p></li>
<li><p>How can I add tickmarks to such a vector?</p></li>
<li><p>How can I define the points of a polygon (here in grey) in a similar manner, i.e. by providing an angle and a distance from the origin, instead of coordinates)?</p></li>
</ul>

<p>I can of course calculate the endpoints, but I would like to avoid this. Also, I wouldn't know how to add tick marks to an arrow.</p>

<hr>

<p>My attempts that did not work:</p>

<pre><code>par(pin = c(4, 4))
plot(0, 0, type = ""n"", xlim = c(-60, 60), ylim = c(-60, 60))
symbols(c(0, 0, 0), c(0, 0, 0), circles = c(60, 1.5, 1.5), inches = FALSE, add = TRUE, fg = c(""black"", ""black"", ""white""), bg = c(""transparent"", ""#000000"", ""transparent""))
arrows(0, 0, length = c(60, 60, 60, 60, 60, 60), angle = c(0, 60, 120, 180, 240, 300))
</code></pre>
","<p>The following uses <code>base</code> functions and a couple of functions that we define ourselves. </p>

<p>While you requested a method that doesn't require calculating coordinates of segments' end points, I think this is impossible. However, we can define a simple helper function that uses some basic trigonometry to calculate the coordinates given the angle (clockwise from the positive y-axis) and the segment length. We do this below, as well as defining a function that plots a rotated axis.</p>

<pre><code>get.coords &lt;- function(a, d, x0, y0) {
  a &lt;- ifelse(a &lt;= 90, 90 - a, 450 - a)
  data.frame(x = x0 + d * cos(a / 180 * pi), 
             y = y0+ d * sin(a / 180 * pi))
}

rotatedAxis &lt;- function(x0, y0, a, d, symmetrical=FALSE, tickdist, ticklen, ...) {
  if(isTRUE(symmetrical)) {
    axends &lt;- get.coords(c(a, a + 180), d, x0, y0)    
    tick.d &lt;- c(seq(0, d, tickdist), seq(-tickdist, -d, -tickdist))      
  } else {
    axends &lt;- rbind(get.coords(a, d, x0, y0), c(x0, y0))
    tick.d &lt;- seq(0, d, tickdist)
  }
  invisible(lapply(apply(get.coords(a, d=tick.d, x0, y0), 1, function(x) {
    get.coords(a + 90, c(-ticklen, ticklen), x[1], x[2])
  }), function(x) lines(x$x, x$y, ...)))
  lines(axends$x, axends$y, ...)
}
</code></pre>

<p><code>get.coords</code> takes arguments <code>a</code> (a vector of angles), <code>d</code> (a vector of segment lengths), and <code>x0</code> and <code>y0</code>, the coordinates of the known point. Vectors <code>a</code> and <code>d</code> are recycled as necessary. The function returns a <code>data.frame</code> with elements <code>x</code> and <code>y</code> giving the coordinates corresponding to each angle/length pair.</p>

<p><code>rotatedAxis</code> plots an axis between <code>x0, y0</code> and the point <code>d</code> units away along the line at angle <code>a</code>. If <code>symmetrical</code> is <code>TRUE</code>, the axis extends <code>d</code> units in opposite directions. Tick marks, of height <code>ticklen</code> are plotted <code>tickdist</code> units apart.</p>

<p>Plotting of the circle uses <code>get.coords</code> to calculate coordinates along the circumference, and plots the line connecting these with <code>polygon</code> (<a href=""https://stackoverflow.com/a/9838248/489704"">inspired by @timriffe</a>).</p>

<p>Below we use these functions to replicate the plot provided by the OP.</p>

<pre><code># Set up plotting device
plot.new()
plot.window(xlim=c(-70, 70), ylim=c(-70, 70), asp=1)

# Plot circle with radius = 60 units and centre at the origin.
polygon(get.coords(seq(0, 360, length.out=1000), 60, 0, 0), lwd=2)

# Plot a polygon with vertices along six axes, at distances of 17, 34, 44, 40,
# 35, and 10 units from the centre.
poly.pts &lt;- get.coords(seq(0, 300, 60), c(17, 34, 44, 40, 35, 10), 0, 0)
polygon(poly.pts$x, poly.pts$y, col='gray', lwd=2)

# Plot the rotated axes
rotatedAxis(0, 0, a=60, d=60, symmetrical=TRUE, tickdist=10, ticklen=1)
rotatedAxis(0, 0, a=120, d=60, symmetrical=TRUE, tickdist=10, ticklen=1)
rotatedAxis(0, 0, a=180, d=60, symmetrical=TRUE, tickdist=10, ticklen=1)

# Add text labels to circumference
text.coords &lt;- get.coords(seq(0, 300, 60), 65, 0, 0)
text(text.coords$x, text.coords$y, c('I', 'A', 'S', 'E', 'C', 'R'))    

# Plot a second point and connect to centre by a line
point2 &lt;- get.coords(145, 50, 0, 0)
points(point2, pch=20, cex=2)
segments(0, 0, point2$x, point2$y, lwd=3)

# Plot central point
points(0, 0, pch=21, bg=1, col=0, lwd=2, cex=2)
</code></pre>

<p>(<strong>Edit:</strong> I heavily edited this post - without changing it's general message drastically - in order to make it easier to read and more generally applicable. Additions/changes include that I now define a function to plot rotated axes, plot the circle by calculating coordinates of vertices along the circumference and plotting with <code>polygon</code>, as inspired by @timriffe.)</p>

<p><img src=""https://i.stack.imgur.com/XWezV.png"" alt=""enter image description here""></p>
"
21755579,Issues with calling for specific info from symbols held in a list in R,2,2,4,"<pre><code>START &lt;- '2013-09-03'    
symbolList &lt;- list(""AAPL"", ""MSFT"", ""TSLA"", ""GOOG"", ""IBM"")
for (ii in 1:length(symbolList))
  {
  getSymbols(paste(symbolList[ii]), src='yahoo', from=START)
  }
</code></pre>

<p>This will generate 5 xts objects, each with 6 columns. I want to create <strong><em>mergedData</em></strong>, but by referencing <strong><em>symbolList</em></strong>, and not the brute force way I am doing it below.</p>

<pre><code>mergedData &lt;- merge(AAPL[,6], MSFT[,6], TSLA[,6], GOOG[,6], IBM[,6])
colnames(mergedData) &lt;- c(""Apple"", ""Microsoft"", ""Tesla"", ""Google"", ""IBM"")
</code></pre>

<p>How would I rewrite the last 2 lines above using a method that loops through <strong><em>symbolList</em></strong>? Also, I would bet the original list having quotes might be part of my problem, as the created xts's are AAPL, MSFT, etc w/out the quotes. How do I get around that?</p>

<p>Thanks!</p>
","<p>You <em>could</em> write those last two lines using <code>get</code> (which converts symbol names to symbols), <code>do.call</code> (which applies a function to a list), and <code>lapply</code> (which creates a list from the results of a [different] function):</p>

<pre><code>mergedData &lt;- do.call(merge, lapply(symbolList, function(sym) {  get(sym)[,6] }) )
colnames(mergedData) &lt;- c(""Apple"", ""Microsoft"", ""Tesla"", ""Google"", ""IBM"")
</code></pre>

<p>But you could also simplify your whole process considerably by using <code>auto.assign=FALSE</code> in <code>getSymbols</code>. (I've also changed symbolList from a list to a named vector, which is really all you need.):</p>

<pre><code>library(quantmod)
START &lt;- '2013-09-03'    
symbolList &lt;- c(Apple=""AAPL"", Microsoft=""MSFT"", Tesla=""TSLA"", Google=""GOOG"", IBM=""IBM"")
dataList &lt;- lapply(symbolList, function(sym) {
  setNames(getSymbols(sym, src='yahoo', from=START, auto.assign=FALSE)[,6], names(sym))
})
mergedData &lt;- do.call(merge, dataList)
</code></pre>

<p>Or if you want <em>really</em> dense code:</p>

<pre><code>mergedData &lt;- do.call(merge, lapply(symbolList, function(sym) {
  setNames(getSymbols(sym, src='yahoo', from=START, auto.assign=FALSE)[,6], names(sym))
}))
</code></pre>

<hr>

<p>Appendix - An explanation of the lapply call (in response to your comment)</p>

<p>In this line:</p>

<pre><code>dataList &lt;- lapply(symbolList, function(sym) {
  setNames(getSymbols(sym, src='yahoo', from=START, auto.assign=FALSE)[,6], names(sym))
})
</code></pre>

<p>I've passed two arguments to lapply: the vector <code>symbolList</code> (could have been a list) and a function <code>function(sym) { ... }</code>. I could have passed a pre-defined function -- for example, if I wanted to count the number of characters in each symbol, I could have passed in <code>nchar</code>:</p>

<pre><code>lapply(symbolList, nchar)
</code></pre>

<p>and that would have been the same as calling </p>

<pre><code>list(Apple=nchar(symbolList[[1]]), Microsoft=nchar(symbolList[[2]]), Tesla=nchar(symbolList[[3]]), Google=nchar(symbolList[[4]]), IBM=nchar(symbolList[[5]]))
</code></pre>

<p>where both return a list of the form</p>

<pre><code>list(Apple=4, Microsoft=4, Tesla=4, Google=4, IBM=3)
</code></pre>

<p>But instead of using <code>nchar</code>, I wrote my own function. This function returns things I want in place of the 4, 4, 4, 4, and 3 in the <code>nchar</code> example. The function could be defined separately:</p>

<pre><code>myfun &lt;- function(sym) {
  setNames(getSymbols(sym, src='yahoo', from=START, auto.assign=FALSE)[,6], names(sym))
}
</code></pre>

<p>And we could then have passed <code>myfun</code> to <code>lapply</code> to get the result we want:</p>

<pre><code>dataList &lt;- lapply(symbolList, myfun)
</code></pre>

<p>In this case, rather than 4, 4, 4, 4, and 3, we get a list of xts objects. We also could have constructed that same list this way:</p>

<pre><code>dataList2 &lt;- list(Apple=myfun(symbolList[[1]]), Microsoft=myfun(symbolList[[2]]), Tesla=myfun(symbolList[[3]]), Google=myfun(symbolList[[4]]), IBM=myfun(symbolList[[5]]))
</code></pre>

<p>but of course the <code>lapply</code> version is cleaner and more flexible about the number and names of the list elements. </p>

<p>In practice, it's often a little clunky to first assign the function to some variable (<code>myfun</code> in this case) and then pass the function to <code>lapply</code>. So in my original answer I chose instead to define the function within the <code>lapply</code> call, without ever giving it a name. <code>myfun</code> and <code>function(sym) { ... }</code> are equivalent function objects. Given that information, I hope you can now understand (or even modify) the <code>lapply</code> call from my original answer, copied here:</p>

<pre><code>dataList &lt;- lapply(symbolList, function(sym) {
  setNames(getSymbols(sym, src='yahoo', from=START, auto.assign=FALSE)[,6], names(sym))
})
</code></pre>
"
34928972,Convert matrix values to number of cells with a given value,1,1,1,"<p>I have a <code>data.frame</code> containing 5 columns, each of which holds a proportion of the whole. Here's what it looks like:</p>

<pre><code>Sample    Type_A    Type_B    Type_C    Type_D    Type_E    Sum
00001      54        13         24        3          6      100
00002      5         2          15        54        24      100
00003      10        10         23        37        20      100
</code></pre>

<p>I want to create a 100-column <code>matrix</code> and populate the cells proportionate to their values in my <code>data.frame</code>. Row 00001 would look have <code>A</code> in the first 50 cells, then 13 cells with <code>B</code> in them, then 24 cells with <code>C</code> in them, etc.</p>

<p>The desired matrix would look something like this:</p>

<pre><code>00001  A  A  A  A  A  A  A  A  A  A  A  A  A  A .....
00002  A  A  A  A  A  B  B  C  C  C  C  C  C  C .....
00003  A  A  A  A  A  A  A  A  A  A  B  B  B  B .....
</code></pre>
","<p>Here is another option with <code>data.table</code> (Assuming that the values in the ""Type"" columns sum to 100 for all the rows).</p>

<pre><code>library(data.table)
nm1 &lt;- sub("".*_"", """", grep(""_"", names(df1), value=TRUE))
setDT(df1)[, transpose(list(rep(nm1, unlist(.SD)))),
    by = Sample ,.SDcols = Type_A:Type_E]
# Sample V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48
#1:  00001  A  A  A  A  A  A  A  A  A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A   A
#2:  00002  A  A  A  A  A  B  B  C  C   C   C   C   C   C   C   C   C   C   C   C   C   C   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D
#3:  00003  A  A  A  A  A  A  A  A  A   A   B   B   B   B   B   B   B   B   B   B   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   D   D   D   D   D
#   V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95
#1:   A   A   A   A   A   A   B   B   B   B   B   B   B   B   B   B   B   B   B   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   C   D   D   D   E
#2:   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   E   E   E   E   E   E   E   E   E   E   E   E   E   E   E   E   E   E   E
#3:   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   D   E   E   E   E   E   E   E   E   E   E   E   E   E   E   E
#   V96 V97 V98 V99 V100
#1:   E   E   E   E    E
#2:   E   E   E   E    E
#3:   E   E   E   E    E
</code></pre>
"
45702886,Can we use Base R to find the 95% of the area under a curve?,2,3,3,"<p>Using Base R, I was wondering if I could determine the 95% area under the curve denoted as <code>posterior</code> below?</p>

<p>More specifically, I want to move from the <code>mode</code> (the green dashed line) toward the tails and then stop when I have covered 95% of the curve area. Desired are the x-axis values that are the limits of this 95% area as shown in the picture below?</p>

<pre><code>     prior = function(x) dbeta(x, 15.566, 7.051) 
likelihood = function(x) dbinom(55, 100, x)
 posterior = function(x) prior(x)*likelihood(x)

mode = optimize(posterior, interval = c(0, 1), maximum = TRUE, tol = 1e-12)[[1]]

curve(posterior, n = 1e4)
</code></pre>

<p><strong>P.S</strong> In other words, it is highly desirable if such an Interval be the shortest 95% interval possible.</p>

<p><a href=""https://i.stack.imgur.com/azwgX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/azwgX.png"" alt=""enter image description here""></a></p>
","<h1>Symmetric distribution</h1>

<p>Even though OP's example was not exactly symmetric, it is close enough - and useful to start there since the solution is much simpler.</p>

<p>You can use a combination of <code>integrate</code> and <code>optimize</code>. I wrote this as a custom function, but note that if you use this in other situations you may have to rethink the bounds for searching the quantile.</p>

<pre><code># For a distribution with a single peak, find the symmetric!
# interval that contains probs probability. Search over 'range'.
f_quan &lt;- function(fun, probs, range=c(0,1)){

  mode &lt;- optimize(fun, interval = range, maximum = TRUE, tol = 1e-12)[[1]]

  total_area &lt;- integrate(fun, range[1], range[2])[[1]]

  O &lt;- function(d){
    parea &lt;- integrate(fun, mode-d, mode+d)[[1]] / total_area
    (probs - parea)^2
  }
  # Bounds for searching may need some adjustment depending on the problem!
  o &lt;- optimize(O, c(0,range[2]/2 - 1E-02))[[1]]

return(c(mode-o, mode+o))
}
</code></pre>

<p>Use it like this,</p>

<pre><code>f &lt;- f_quan(posterior, 0.95)
curve(posterior, n = 1e4)
abline(v=f, col=""blue"", lwd=2, lty=3)
</code></pre>

<p>gives</p>

<p><a href=""https://i.stack.imgur.com/AG7TE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AG7TE.png"" alt=""enter image description here""></a></p>

<h1>Asymmetric distribution</h1>

<p>In the case of an asymmetric distribution, we have to search two points that meet the criterium that P(a &lt; x &lt; b) = Prob, where Prob is some desired probability. Since there are infinitely many intervals (a,b) that meet this, OP suggested finding the shortest one. </p>

<p>Important in the solution is the definition of a <code>domain</code>, the region where we want to search (we cannot use <code>-Inf, Inf</code>, so the user has to set this to reasonable values).</p>

<pre><code># consider interval (a,b) on the x-axis
# integrate our function, normalize to total area, to 
# get the total probability in the interval
prob_ab &lt;- function(fun, a, b, domain){
  totarea &lt;- integrate(fun, domain[1], domain[2])[[1]]
  integrate(fun, a, b)[[1]] / totarea
}

# now given a and the probability, invert to find b
invert_prob_ab &lt;- function(fun, a, prob, domain){

  O &lt;- function(b, fun, a, prob){
    (prob_ab(fun, a, b, domain=domain) - prob)^2
  }

  b &lt;- optimize(O, c(a, domain[2]), a = a, fun=fun, prob=prob)$minimum

return(b)
}

# now find the shortest interval by varying a
# Simplification: don't search past the mode, otherwise getting close
# to the right-hand side of domain will give serious trouble!
prob_int_shortest &lt;- function(fun, prob, domain){

  mode &lt;- optimize(fun, interval = domain, maximum = TRUE, tol = 1e-12)[[1]]

  # objective function to be minimized: the width of the interval
  O &lt;- function(a, fun, prob, domain){
    b &lt;- invert_prob_ab(fun, a, prob, domain)

    b - a
  }

  # shortest interval that meets criterium
  abest &lt;- optimize(O, c(0,mode), fun=fun, prob=prob, domain=domain)$minimum

  # now return the interval
  b &lt;- invert_prob_ab(fun, abest, prob, domain)

return(c(abest,b))
}
</code></pre>

<p>Now use the above code like this. I use a very asymmetric function (just assume mydist is actually some complicated pdf, not the dgamma).</p>

<pre><code>mydist &lt;- function(x)dgamma(x, shape=2)
curve(mydist(x), from=0,  to=10)
abline(v=prob_int_shortest(mydist, 0.9, c(0,10)), lty=3, col=""blue"", lwd=2)
</code></pre>

<p>In this example I set domain to (0,10), since clearly the interval must be in there somewhere. Note that using a very large value like (0, 1E05) does not work, because <code>integrate</code> has trouble with long sequences of near-zeroes. Again, for your situation, you will have to adjust the domain (unless someone has a better idea!).</p>

<p><a href=""https://i.stack.imgur.com/7wcJj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7wcJj.png"" alt=""enter image description here""></a></p>
"
37111471,What roxygen should I put when I use a function of another package in my function,2,2,2,"<p>I am writing many functions and I am trying to document using <a href=""http://roxygen.org/roxygen2-manual.pdf"" rel=""nofollow"">roxygen2</a></p>

<p>I use the <a href=""https://cran.r-project.org/web/packages/futile.logger/index.html"" rel=""nofollow"">futile.logger</a> package a lot, say I use the <code>flog.debug</code> function in a function. What <code>@*</code> should I use to document it ?</p>
","<p>First, being aware of your</p>

<pre><code>sessionInfo() 
getwd() # your R's working directory
.libPaths() # your R's library location
</code></pre>

<p><strong>Step0</strong> Download and install the necessary packages:</p>

<pre><code>library(roxygen2)
library(devtools)
library(digest)
</code></pre>

<p><strong>Step1</strong> Put all your related "".R"" files (yourfunction1.R, yourfunction2.R, yourfunction3.R) to your R's working directory.</p>

<p><strong>Step2</strong> Create your package skeleton in your R's working directory: 
Be sure that there is no folder named ""yourpackage"" in your R's working directory before running the following command. (from R's console) </p>

<pre><code>package.skeleton(name = ""yourpackage"", code_files = c(""yourfunction1.R"", ""yourfunction2.R"", ""yourfunction3.R""), path = ""."")
</code></pre>

<p>After running <code>package.skeleton</code>, the folder yourpackage is created in your R's Working Directory.</p>

<p>Delete <code>Read-and-delete-me</code> file from Windows Explorer.<br>
Delete ""yourpackage-package.Rd"" file in YourR'sWorkingDirectory\yourpackage\man folder<br>
(Do NOT delete ""yourpackage.Rd"" file in YourR'sWorkingDirectory\yourpackage\man folder!)</p>

<p><strong>Step3</strong> At the end of the preamble of your "".R"" file (yourfunction.R), put the following (if you had not done it so in Step1):</p>

<pre><code>#' @importFrom futile.logger flog.debug
#' @export
yourfunction &lt;- function(...) {...
</code></pre>

<p><strong>Step4</strong> In the <code>DESCRIPTION</code> file of your package, in the <code>Imports</code> part, add:</p>

<pre><code>Imports:
    futile.logger(&gt;= VersionNumber)
</code></pre>

<p>where VersionNumber is the version number of the futile.logger package you are using. You can find the version number by right-click any function (from yourpackage) in Object Browser of RevolutionREnterprise; and going the bottom of the resultant .html help file. There, the version number of the package is shown.</p>

<p>In Step2, package.skeleton automatically produced a NAMESPACE file whose content is:</p>

<pre><code>exportPattern(""^[[:alpha:]]+"")
</code></pre>

<p>Do not handle this NAMESPACE file manually.</p>

<p><strong>Step5</strong> roxygenize the package you wanna create (""yourpackage"")</p>

<pre><code>library(roxygen2)
roxygenize(""yourpackage"")
</code></pre>

<p>Upon roxygenization, the content of the NAMESPACE file of yourpackage is automatically converted from <code>exportPattern(""^[[:alpha:]]+"")</code> to </p>

<pre><code># Generated by roxygen2: do not edit by hand

export(yourfunction)
importFrom(futile.logger,flog.debug)
</code></pre>

<p><strong>Step6</strong> Build your package:<br>
(first, delete ""src-i386"" and ""src-x64"" folders (if any) in YourR'sWorkingDirectoryFolder\yourpackage folder from Windows Explorer)<br>
(Be sure again that there is no ""yourpackage-package.Rd"" file in YourR'sWorkingDirectory\yourpackage\man folder. If there is, delete it before building)</p>

<pre><code>build(""yourpackage"")
</code></pre>

<p><strong>Step7</strong> Install your package:</p>

<pre><code>install(""yourpackage"")
</code></pre>

<p><strong>Step8</strong> Check that all is going well by loading your package and running a function in the package.</p>

<pre><code>library(yourpackage)
yourfunction(6,1,2) # ""yourfunction"" is a function in the package ""yourpackage""
</code></pre>

<p><strong>Step9</strong> Check that your package is loadable to CRAN (Comprehensive R Archieve Network) (if you wanna share your package):</p>

<p>(first, delete ""src-i386"" and ""src-x64"" folders (if any) in YourR'sWorkingDirectoryFolder\yourpackage folder from Windows Explorer)<br>
(Be sure again that there is no ""yourpackage-package.Rd"" file in YourR'sWorkingDirectory\yourpackage\man folder. If there is, delete it before checking)</p>

<p>From DOS Command Prompt:<br>
Start – cmd  - Enter. Pass to R's working directory (your R's working directory is known via getwd()) and do CRAN check:</p>

<pre><code>cd C:\Users\User\Documents\Revolution
R CMD check yourpackage
</code></pre>

<p>From R's console:</p>

<pre><code>devtools::check(""C:/Users/User/Documents/Revolution/yourpackage"")
</code></pre>
"
30053405,Change the value in a column of a dataframe depending on how many of each possible value there are,1,1,1,"<p>I have a dataframe looking like this:</p>

<pre><code>chr &lt;- c(1,1,1,1,1)
b1 &lt;- c('HP', 'HP', 'CP', 'CP', 'KP')
b2 &lt;- c('HP', 'HP', 'CP', 'CP', 'KP')
b3 &lt;- c('CP', 'KP', 'CP', 'HP', 'CP')
b4 &lt;- c('CP', 'KP', 'CP', 'HP', 'CP')
b5 &lt;- c('CP', 'CP', 'KP', 'KP', 'HP')
b6 &lt;- c('CP', 'CP', 'KP', 'KP', 'HP')
b7 &lt;- c('CP', 'KP', 'HP', 'CP', 'CP')
b8 &lt;- c('CP', 'KP', 'HP', 'CP', 'CP')
df &lt;- data.frame(chr, b1,b2,b3,b4,b5,b6,b7,b8)
</code></pre>

<p>I want to write a function that looks at each 'b' column and asks if it contains the value 'HP'. If it does, and the other six 'b' columns contain 'CP' or 'KP', I want to change the value 'HP' into 'CP' or 'KP' depending on which is the majority. If CP is the majority, change the HP to CP. If KP is the majority, change HP to KP.</p>

<p>(note that the value of b1 and b2, b3 and b4 etc is always the same, so really only 4 columns need to be looked at, b1, b3, b5, and b7).</p>

<p>To clarify, if the columns are e.g. HP HP CP CP CP CP KP KP, I want to change the two HPs into CPs (and leave the other columns the same). </p>

<p>So, the example I gave would become:</p>

<pre><code> chr &lt;- c(1,1,1,1,1)
    b1 &lt;- c('CP', 'KP', 'CP', 'CP', 'KP')
    b2 &lt;- c('CP', 'KP', 'CP', 'CP', 'KP')
    b3 &lt;- c('CP', 'KP', 'CP', 'CP', 'CP')
    b4 &lt;- c('CP', 'KP', 'CP', 'CP', 'CP')
    b5 &lt;- c('CP', 'CP', 'KP', 'KP', 'CP')
    b6 &lt;- c('CP', 'CP', 'KP', 'KP', 'CP')
    b7 &lt;- c('CP', 'KP', 'CP', 'CP', 'CP')
    b8 &lt;- c('CP', 'KP', 'CP', 'CP', 'CP')
    df &lt;- data.frame(chr, b1,b2,b3,b4,b5,b6,b7,b8)
    df
</code></pre>

<p>I have written a function (just for df$b1) with if statements, but it doesn't work.
(note the rules for whether the HP changes to KP or CP depend on how many other CPs or KPs there are):</p>

<pre><code>fun &lt;- function(df){

if(df$b1 == 'HP' &amp;&amp; df$b3 == 'CP' &amp;&amp; df$b5 == 'CP' &amp;&amp; df$b7 == 'CP') {df$b1 &lt;- 'KP'} 
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'KP' &amp;&amp; df$b5 == 'CP' &amp;&amp; df$b7 == 'CP') {df$b1 &lt;- 'CP'}
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'CP' &amp;&amp; df$b5 == 'KP' &amp;&amp; df$b7 == 'CP') {df$b1 &lt;- 'CP'}
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'CP' &amp;&amp; df$b5 == 'CP' &amp;&amp; df$b7 == 'KP') {df$b1 &lt;- 'CP'}
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'KP' &amp;&amp; df$b5 == 'KP' &amp;&amp; df$b7 == 'CP') {df$b1 &lt;- 'KP'} 
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'KP' &amp;&amp; df$b5 == 'CP' &amp;&amp; df$b7 == 'KP') {df$b1 &lt;- 'KP'} 
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'CP' &amp;&amp; df$b5 == 'KP' &amp;&amp; df$b7 == 'KP') {df$b1 &lt;- 'KP'} 
if(df$b1 == 'HP' &amp;&amp; df$b3 == 'KP' &amp;&amp; df$b5 == 'KP' &amp;&amp; df$b7 == 'KP') {df$b1 &lt;- 'CP'}

df$b2 &lt;-df$b1

}
</code></pre>

<p>Thanks very much for any help. I'm really stuck on this one.</p>

<p>EDIT: This is a sample of my actual data which is more complex than the example I gave above.</p>

<pre><code>structure(list(chr = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L), pos_c = c(2373L, 2406L, 2418L, 2419L, 
2447L, 2450L, 2468L, 2524L, 2533L, 2535L, 2536L, 2542L, 2623L, 
2709L, 3942L, 11716L, 11893L, 11898L, 12190L, 12396L, 26639L, 
26640L, 26643L, 26646L, 26655L, 26657L, 26661L, 26667L, 26670L, 
26676L, 26679L, 26684L, 26685L, 26688L, 26694L, 26703L, 26710L, 
26712L, 26713L, 26723L, 26733L, 26737L, 26738L, 26739L, 26742L, 
26743L, 26748L, 26761L, 26765L, 26766L, 26778L, 26781L, 26790L, 
26792L, 26796L, 26802L, 26805L, 26811L, 26814L, 26819L, 26820L, 
26823L, 26829L, 26838L, 26846L, 26847L, 26848L, 26872L, 26873L, 
26874L, 26877L, 26878L, 26883L, 26889L, 26901L, 26904L, 26907L, 
26916L, 26923L, 26925L, 26927L, 26931L, 26937L, 26940L, 26946L, 
26954L, 26958L, 26961L, 26963L, 26964L, 26970L, 26981L, 26982L, 
26983L, 26991L, 26994L, 26997L, 27007L, 27008L, 27009L, 27012L, 
27015L, 27018L, 27027L, 202471L, 203660L, 203668L, 203669L, 203670L, 
203672L, 203678L, 203683L, 203686L, 203687L, 203690L, 203704L, 
203705L, 203711L, 203714L, 203732L, 203749L, 203752L, 203754L, 
203755L, 203903L, 203910L, 203911L, 203912L, 203913L, 203914L, 
203915L, 203922L, 203924L, 203933L, 203937L, 203939L, 203945L, 
203948L, 203951L, 203957L, 203960L, 203961L, 203963L, 203969L, 
203972L, 203973L, 203974L, 203975L, 203981L, 203991L, 204220L, 
204227L, 204230L, 204232L, 204242L, 204245L, 204262L, 204272L, 
204278L, 204282L, 204290L), c1 = c(101L, 60L, 63L, 64L, 100L, 
97L, 94L, 83L, 80L, 48L, 46L, 51L, 69L, 46L, 23L, 79L, 63L, 59L, 
53L, 85L, 13L, 12L, 1L, 9L, 11L, 13L, 9L, 14L, 14L, 12L, 15L, 
9L, 15L, 14L, 14L, 2L, 2L, 8L, 3L, 0L, 0L, 4L, 2L, 1L, 4L, 4L, 
8L, 39L, 7L, 5L, 2L, 41L, 69L, 79L, 89L, 120L, 128L, 90L, 134L, 
107L, 169L, 120L, 103L, 48L, 58L, 132L, 62L, 19L, 9L, 13L, 12L, 
12L, 17L, 251L, 8L, 367L, 367L, 264L, 5L, 170L, 113L, 234L, 134L, 
143L, 189L, 224L, 255L, 296L, 448L, 239L, 169L, 80L, 312L, 84L, 
403L, 397L, 430L, 529L, 544L, 556L, 565L, 549L, 555L, 4L, 11L, 
0L, 18L, 18L, 19L, 19L, 18L, 18L, 17L, 17L, 15L, 15L, 16L, 15L, 
13L, 14L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 2L, 3L, 2L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 13L, 2L, 10L, 4L, 10L, 24L, 33L, 33L, 63L, 42L), c2 = c(101L, 
60L, 63L, 64L, 100L, 97L, 94L, 83L, 80L, 48L, 46L, 51L, 69L, 
46L, 23L, 79L, 63L, 59L, 53L, 85L, 13L, 12L, 1L, 9L, 11L, 13L, 
9L, 14L, 14L, 12L, 15L, 9L, 15L, 14L, 14L, 2L, 2L, 8L, 3L, 0L, 
0L, 4L, 2L, 1L, 4L, 4L, 8L, 39L, 7L, 5L, 2L, 41L, 69L, 79L, 89L, 
120L, 128L, 90L, 134L, 107L, 169L, 120L, 103L, 48L, 58L, 132L, 
62L, 19L, 9L, 13L, 12L, 12L, 17L, 251L, 8L, 367L, 367L, 264L, 
5L, 170L, 113L, 234L, 134L, 143L, 189L, 224L, 255L, 296L, 448L, 
239L, 169L, 80L, 312L, 84L, 403L, 397L, 430L, 529L, 544L, 556L, 
565L, 549L, 555L, 4L, 11L, 0L, 18L, 18L, 19L, 19L, 18L, 18L, 
17L, 17L, 15L, 15L, 16L, 15L, 13L, 14L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 2L, 3L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 13L, 2L, 10L, 4L, 10L, 24L, 
33L, 33L, 63L, 42L), c3 = c(37L, 0L, 0L, 0L, 42L, 46L, 46L, 21L, 
26L, 6L, 2L, 7L, 11L, 4L, 0L, 4L, 1L, 0L, 0L, 2L, 29L, 29L, 0L, 
22L, 23L, 23L, 26L, 27L, 29L, 24L, 32L, 26L, 35L, 32L, 32L, 3L, 
3L, 10L, 1L, 5L, 1L, 6L, 1L, 0L, 5L, 11L, 6L, 81L, 15L, 14L, 
0L, 92L, 157L, 174L, 168L, 236L, 221L, 143L, 228L, 251L, 292L, 
273L, 281L, 33L, 39L, 260L, 57L, 53L, 24L, 22L, 26L, 37L, 37L, 
484L, 16L, 721L, 724L, 436L, 7L, 367L, 163L, 411L, 167L, 373L, 
275L, 599L, 637L, 773L, 866L, 615L, 223L, 63L, 531L, 59L, 878L, 
868L, 911L, 939L, 975L, 995L, 980L, 931L, 958L, 12L, 16L, 0L, 
12L, 13L, 12L, 11L, 9L, 12L, 11L, 11L, 10L, 1L, 0L, 0L, 0L, 1L, 
1L, 2L, 1L, 0L, 1L, 1L, 0L, 2L, 2L, 2L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 2L, 28L, 
5L, 28L, 3L, 12L, 39L, 40L, 50L, 90L, 80L), c4 = c(37L, 0L, 0L, 
0L, 42L, 46L, 46L, 21L, 26L, 6L, 2L, 7L, 11L, 4L, 0L, 4L, 1L, 
0L, 0L, 2L, 29L, 29L, 0L, 22L, 23L, 23L, 26L, 27L, 29L, 24L, 
32L, 26L, 35L, 32L, 32L, 3L, 3L, 10L, 1L, 5L, 1L, 6L, 1L, 0L, 
5L, 11L, 6L, 81L, 15L, 14L, 0L, 92L, 157L, 174L, 168L, 236L, 
221L, 143L, 228L, 251L, 292L, 273L, 281L, 33L, 39L, 260L, 57L, 
53L, 24L, 22L, 26L, 37L, 37L, 484L, 16L, 721L, 724L, 436L, 7L, 
367L, 163L, 411L, 167L, 373L, 275L, 599L, 637L, 773L, 866L, 615L, 
223L, 63L, 531L, 59L, 878L, 868L, 911L, 939L, 975L, 995L, 980L, 
931L, 958L, 12L, 16L, 0L, 12L, 13L, 12L, 11L, 9L, 12L, 11L, 11L, 
10L, 1L, 0L, 0L, 0L, 1L, 1L, 2L, 1L, 0L, 1L, 1L, 0L, 2L, 2L, 
2L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 
1L, 1L, 0L, 1L, 1L, 2L, 28L, 5L, 28L, 3L, 12L, 39L, 40L, 50L, 
90L, 80L), c5 = c(96L, 77L, 74L, 72L, 96L, 96L, 92L, 80L, 79L, 
79L, 76L, 76L, 66L, 55L, 64L, 78L, 110L, 100L, 165L, 171L, 38L, 
41L, 2L, 38L, 33L, 37L, 21L, 40L, 41L, 21L, 37L, 19L, 45L, 30L, 
22L, 22L, 28L, 34L, 30L, 31L, 25L, 40L, 34L, 33L, 34L, 46L, 41L, 
96L, 48L, 51L, 38L, 93L, 152L, 155L, 155L, 193L, 195L, 189L, 
222L, 213L, 284L, 248L, 230L, 56L, 70L, 208L, 82L, 85L, 67L, 
64L, 64L, 83L, 71L, 495L, 77L, 570L, 577L, 499L, 55L, 292L, 236L, 
352L, 244L, 296L, 351L, 391L, 440L, 483L, 653L, 417L, 194L, 57L, 
460L, 57L, 538L, 520L, 573L, 731L, 753L, 770L, 772L, 757L, 761L, 
35L, 73L, 66L, 70L, 70L, 71L, 70L, 74L, 79L, 82L, 83L, 85L, 69L, 
68L, 71L, 71L, 70L, 73L, 72L, 72L, 74L, 103L, 107L, 106L, 107L, 
109L, 106L, 106L, 105L, 106L, 105L, 108L, 104L, 105L, 106L, 106L, 
103L, 112L, 112L, 113L, 112L, 109L, 114L, 114L, 115L, 120L, 114L, 
97L, 125L, 103L, 124L, 107L, 116L, 145L, 139L, 138L, 177L, 139L
), c6 = c(96L, 77L, 74L, 72L, 96L, 96L, 92L, 80L, 79L, 79L, 76L, 
76L, 66L, 55L, 64L, 78L, 110L, 100L, 165L, 171L, 38L, 41L, 2L, 
38L, 33L, 37L, 21L, 40L, 41L, 21L, 37L, 19L, 45L, 30L, 22L, 22L, 
28L, 34L, 30L, 31L, 25L, 40L, 34L, 33L, 34L, 46L, 41L, 96L, 48L, 
51L, 38L, 93L, 152L, 155L, 155L, 193L, 195L, 189L, 222L, 213L, 
284L, 248L, 230L, 56L, 70L, 208L, 82L, 85L, 67L, 64L, 64L, 83L, 
71L, 495L, 77L, 570L, 577L, 499L, 55L, 292L, 236L, 352L, 244L, 
296L, 351L, 391L, 440L, 483L, 653L, 417L, 194L, 57L, 460L, 57L, 
538L, 520L, 573L, 731L, 753L, 770L, 772L, 757L, 761L, 35L, 73L, 
66L, 70L, 70L, 71L, 70L, 74L, 79L, 82L, 83L, 85L, 69L, 68L, 71L, 
71L, 70L, 73L, 72L, 72L, 74L, 103L, 107L, 106L, 107L, 109L, 106L, 
106L, 105L, 106L, 105L, 108L, 104L, 105L, 106L, 106L, 103L, 112L, 
112L, 113L, 112L, 109L, 114L, 114L, 115L, 120L, 114L, 97L, 125L, 
103L, 124L, 107L, 116L, 145L, 139L, 138L, 177L, 139L), c7 = c(28L, 
3L, 1L, 1L, 52L, 50L, 60L, 49L, 50L, 3L, 2L, 2L, 37L, 11L, 0L, 
1L, 2L, 2L, 0L, 1L, 28L, 30L, 1L, 17L, 23L, 28L, 11L, 30L, 32L, 
13L, 32L, 19L, 39L, 18L, 17L, 23L, 29L, 46L, 37L, 25L, 21L, 42L, 
32L, 29L, 30L, 41L, 44L, 141L, 72L, 64L, 25L, 93L, 219L, 234L, 
218L, 294L, 277L, 184L, 294L, 273L, 382L, 293L, 280L, 131L, 132L, 
386L, 157L, 99L, 77L, 75L, 68L, 66L, 88L, 615L, 55L, 746L, 740L, 
685L, 27L, 305L, 158L, 511L, 151L, 326L, 371L, 605L, 650L, 727L, 
886L, 623L, 314L, 170L, 734L, 162L, 937L, 908L, 987L, 964L, 997L, 
1002L, 1007L, 960L, 980L, 28L, 75L, 61L, 96L, 98L, 97L, 96L, 
93L, 101L, 99L, 100L, 98L, 91L, 90L, 90L, 89L, 87L, 76L, 75L, 
75L, 76L, 88L, 92L, 87L, 86L, 88L, 87L, 85L, 87L, 87L, 83L, 86L, 
87L, 86L, 86L, 89L, 83L, 83L, 84L, 84L, 86L, 83L, 86L, 88L, 87L, 
88L, 84L, 81L, 118L, 90L, 120L, 90L, 101L, 127L, 134L, 140L, 
172L, 160L), c8 = c(28L, 3L, 1L, 1L, 52L, 50L, 60L, 49L, 50L, 
3L, 2L, 2L, 37L, 11L, 0L, 1L, 2L, 2L, 0L, 1L, 28L, 30L, 1L, 17L, 
23L, 28L, 11L, 30L, 32L, 13L, 32L, 19L, 39L, 18L, 17L, 23L, 29L, 
46L, 37L, 25L, 21L, 42L, 32L, 29L, 30L, 41L, 44L, 141L, 72L, 
64L, 25L, 93L, 219L, 234L, 218L, 294L, 277L, 184L, 294L, 273L, 
382L, 293L, 280L, 131L, 132L, 386L, 157L, 99L, 77L, 75L, 68L, 
66L, 88L, 615L, 55L, 746L, 740L, 685L, 27L, 305L, 158L, 511L, 
151L, 326L, 371L, 605L, 650L, 727L, 886L, 623L, 314L, 170L, 734L, 
162L, 937L, 908L, 987L, 964L, 997L, 1002L, 1007L, 960L, 980L, 
28L, 75L, 61L, 96L, 98L, 97L, 96L, 93L, 101L, 99L, 100L, 98L, 
91L, 90L, 90L, 89L, 87L, 76L, 75L, 75L, 76L, 88L, 92L, 87L, 86L, 
88L, 87L, 85L, 87L, 87L, 83L, 86L, 87L, 86L, 86L, 89L, 83L, 83L, 
84L, 84L, 86L, 83L, 86L, 88L, 87L, 88L, 84L, 81L, 118L, 90L, 
120L, 90L, 101L, 127L, 134L, 140L, 172L, 160L), k1 = c(39L, 64L, 
68L, 69L, 38L, 38L, 41L, 51L, 54L, 84L, 83L, 84L, 57L, 50L, 43L, 
58L, 72L, 71L, 29L, 35L, 0L, 0L, 10L, 1L, 1L, 0L, 3L, 0L, 0L, 
1L, 0L, 3L, 0L, 0L, 0L, 14L, 14L, 9L, 15L, 18L, 24L, 20L, 20L, 
27L, 28L, 10L, 28L, 27L, 59L, 64L, 73L, 43L, 19L, 7L, 27L, 5L, 
23L, 30L, 29L, 65L, 10L, 46L, 27L, 160L, 168L, 95L, 175L, 255L, 
265L, 271L, 270L, 76L, 269L, 77L, 14L, 12L, 11L, 118L, 382L, 
204L, 220L, 181L, 290L, 290L, 114L, 209L, 89L, 159L, 7L, 144L, 
95L, 9L, 180L, 411L, 105L, 125L, 97L, 19L, 3L, 3L, 2L, 12L, 1L, 
540L, 1L, 32L, 14L, 14L, 13L, 13L, 15L, 14L, 12L, 11L, 12L, 11L, 
12L, 13L, 13L, 9L, 18L, 17L, 8L, 18L, 6L, 2L, 1L, 2L, 1L, 2L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 1L, 0L, 2L, 1L, 21L, 28L, 49L, 50L, 54L, 45L, 
44L), k2 = c(39L, 64L, 68L, 69L, 38L, 38L, 41L, 51L, 54L, 84L, 
83L, 84L, 57L, 50L, 43L, 58L, 72L, 71L, 29L, 35L, 0L, 0L, 10L, 
1L, 1L, 0L, 3L, 0L, 0L, 1L, 0L, 3L, 0L, 0L, 0L, 14L, 14L, 9L, 
15L, 18L, 24L, 20L, 20L, 27L, 28L, 10L, 28L, 27L, 59L, 64L, 73L, 
43L, 19L, 7L, 27L, 5L, 23L, 30L, 29L, 65L, 10L, 46L, 27L, 160L, 
168L, 95L, 175L, 255L, 265L, 271L, 270L, 76L, 269L, 77L, 14L, 
12L, 11L, 118L, 382L, 204L, 220L, 181L, 290L, 290L, 114L, 209L, 
89L, 159L, 7L, 144L, 95L, 9L, 180L, 411L, 105L, 125L, 97L, 19L, 
3L, 3L, 2L, 12L, 1L, 540L, 1L, 32L, 14L, 14L, 13L, 13L, 15L, 
14L, 12L, 11L, 12L, 11L, 12L, 13L, 13L, 9L, 18L, 17L, 8L, 18L, 
6L, 2L, 1L, 2L, 1L, 2L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 0L, 2L, 1L, 21L, 
28L, 49L, 50L, 54L, 45L, 44L), k3 = c(84L, 122L, 120L, 120L, 
92L, 88L, 90L, 107L, 98L, 114L, 120L, 117L, 91L, 64L, 59L, 100L, 
113L, 109L, 56L, 136L, 1L, 0L, 29L, 7L, 4L, 6L, 5L, 6L, 6L, 9L, 
7L, 11L, 7L, 10L, 9L, 44L, 46L, 38L, 51L, 60L, 79L, 75L, 80L, 
83L, 80L, 41L, 97L, 61L, 133L, 135L, 180L, 100L, 50L, 28L, 75L, 
18L, 79L, 94L, 100L, 117L, 47L, 74L, 68L, 393L, 390L, 191L, 416L, 
504L, 532L, 545L, 545L, 181L, 556L, 175L, 19L, 24L, 19L, 312L, 
766L, 389L, 416L, 418L, 639L, 475L, 239L, 293L, 70L, 135L, 37L, 
122L, 84L, 42L, 408L, 886L, 93L, 115L, 65L, 67L, 35L, 37L, 47L, 
50L, 54L, 942L, 9L, 43L, 29L, 29L, 29L, 29L, 28L, 27L, 25L, 25L, 
26L, 32L, 33L, 32L, 33L, 30L, 26L, 23L, 24L, 23L, 8L, 1L, 2L, 
2L, 2L, 2L, 2L, 4L, 4L, 4L, 4L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 
4L, 4L, 3L, 3L, 4L, 3L, 2L, 2L, 0L, 7L, 3L, 65L, 73L, 111L, 98L, 
133L, 107L, 64L), k4 = c(84L, 122L, 120L, 120L, 92L, 88L, 90L, 
107L, 98L, 114L, 120L, 117L, 91L, 64L, 59L, 100L, 113L, 109L, 
56L, 136L, 1L, 0L, 29L, 7L, 4L, 6L, 5L, 6L, 6L, 9L, 7L, 11L, 
7L, 10L, 9L, 44L, 46L, 38L, 51L, 60L, 79L, 75L, 80L, 83L, 80L, 
41L, 97L, 61L, 133L, 135L, 180L, 100L, 50L, 28L, 75L, 18L, 79L, 
94L, 100L, 117L, 47L, 74L, 68L, 393L, 390L, 191L, 416L, 504L, 
532L, 545L, 545L, 181L, 556L, 175L, 19L, 24L, 19L, 312L, 766L, 
389L, 416L, 418L, 639L, 475L, 239L, 293L, 70L, 135L, 37L, 122L, 
84L, 42L, 408L, 886L, 93L, 115L, 65L, 67L, 35L, 37L, 47L, 50L, 
54L, 942L, 9L, 43L, 29L, 29L, 29L, 29L, 28L, 27L, 25L, 25L, 26L, 
32L, 33L, 32L, 33L, 30L, 26L, 23L, 24L, 23L, 8L, 1L, 2L, 2L, 
2L, 2L, 2L, 4L, 4L, 4L, 4L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 
4L, 3L, 3L, 4L, 3L, 2L, 2L, 0L, 7L, 3L, 65L, 73L, 111L, 98L, 
133L, 107L, 64L), k5 = c(0L, 14L, 14L, 14L, 1L, 0L, 0L, 8L, 7L, 
5L, 5L, 5L, 0L, 3L, 0L, 8L, 2L, 3L, 18L, 15L, 0L, 2L, 38L, 3L, 
5L, 1L, 18L, 1L, 2L, 2L, 3L, 21L, 2L, 15L, 1L, 26L, 22L, 17L, 
27L, 33L, 41L, 39L, 42L, 45L, 51L, 14L, 50L, 31L, 82L, 84L, 108L, 
55L, 24L, 16L, 51L, 33L, 44L, 55L, 54L, 87L, 15L, 20L, 27L, 285L, 
297L, 151L, 293L, 343L, 363L, 374L, 376L, 57L, 382L, 24L, 25L, 
10L, 8L, 103L, 551L, 301L, 320L, 276L, 364L, 340L, 49L, 272L, 
171L, 195L, 24L, 180L, 161L, 11L, 254L, 663L, 188L, 229L, 158L, 
26L, 3L, 3L, 6L, 10L, 6L, 708L, 0L, 9L, 0L, 3L, 0L, 1L, 0L, 2L, 
0L, 0L, 1L, 9L, 9L, 9L, 10L, 10L, 6L, 6L, 1L, 6L, 2L, 0L, 5L, 
3L, 2L, 3L, 4L, 2L, 3L, 2L, 2L, 1L, 3L, 0L, 0L, 4L, 1L, 0L, 1L, 
5L, 2L, 0L, 1L, 2L, 0L, 2L, 5L, 1L, 3L, 3L, 43L, 50L, 78L, 75L, 
87L, 78L, 59L), k6 = c(0L, 14L, 14L, 14L, 1L, 0L, 0L, 8L, 7L, 
5L, 5L, 5L, 0L, 3L, 0L, 8L, 2L, 3L, 18L, 15L, 0L, 2L, 38L, 3L, 
5L, 1L, 18L, 1L, 2L, 2L, 3L, 21L, 2L, 15L, 1L, 26L, 22L, 17L, 
27L, 33L, 41L, 39L, 42L, 45L, 51L, 14L, 50L, 31L, 82L, 84L, 108L, 
55L, 24L, 16L, 51L, 33L, 44L, 55L, 54L, 87L, 15L, 20L, 27L, 285L, 
297L, 151L, 293L, 343L, 363L, 374L, 376L, 57L, 382L, 24L, 25L, 
10L, 8L, 103L, 551L, 301L, 320L, 276L, 364L, 340L, 49L, 272L, 
171L, 195L, 24L, 180L, 161L, 11L, 254L, 663L, 188L, 229L, 158L, 
26L, 3L, 3L, 6L, 10L, 6L, 708L, 0L, 9L, 0L, 3L, 0L, 1L, 0L, 2L, 
0L, 0L, 1L, 9L, 9L, 9L, 10L, 10L, 6L, 6L, 1L, 6L, 2L, 0L, 5L, 
3L, 2L, 3L, 4L, 2L, 3L, 2L, 2L, 1L, 3L, 0L, 0L, 4L, 1L, 0L, 1L, 
5L, 2L, 0L, 1L, 2L, 0L, 2L, 5L, 1L, 3L, 3L, 43L, 50L, 78L, 75L, 
87L, 78L, 59L), k7 = c(0L, 36L, 42L, 44L, 0L, 0L, 0L, 3L, 3L, 
49L, 50L, 51L, 0L, 0L, 0L, 0L, 0L, 0L, 31L, 158L, 0L, 1L, 28L, 
14L, 11L, 9L, 27L, 14L, 12L, 14L, 14L, 28L, 14L, 32L, 19L, 41L, 
37L, 26L, 39L, 57L, 85L, 75L, 82L, 87L, 87L, 37L, 91L, 54L, 124L, 
138L, 206L, 150L, 44L, 18L, 92L, 38L, 76L, 95L, 101L, 155L, 20L, 
90L, 48L, 375L, 344L, 135L, 379L, 519L, 537L, 549L, 563L, 67L, 
557L, 91L, 43L, 30L, 35L, 125L, 784L, 491L, 519L, 324L, 627L, 
503L, 215L, 296L, 68L, 203L, 42L, 173L, 58L, 43L, 222L, 812L, 
64L, 98L, 36L, 65L, 36L, 45L, 42L, 50L, 43L, 962L, 0L, 36L, 0L, 
0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 15L, 17L, 15L, 13L, 12L, 25L, 
27L, 8L, 26L, 7L, 2L, 5L, 5L, 4L, 5L, 5L, 5L, 5L, 6L, 5L, 4L, 
6L, 0L, 0L, 5L, 0L, 1L, 0L, 5L, 3L, 0L, 0L, 4L, 0L, 1L, 4L, 2L, 
9L, 3L, 59L, 77L, 123L, 107L, 144L, 119L, 79L), k8 = c(0L, 36L, 
42L, 44L, 0L, 0L, 0L, 3L, 3L, 49L, 50L, 51L, 0L, 0L, 0L, 0L, 
0L, 0L, 31L, 158L, 0L, 1L, 28L, 14L, 11L, 9L, 27L, 14L, 12L, 
14L, 14L, 28L, 14L, 32L, 19L, 41L, 37L, 26L, 39L, 57L, 85L, 75L, 
82L, 87L, 87L, 37L, 91L, 54L, 124L, 138L, 206L, 150L, 44L, 18L, 
92L, 38L, 76L, 95L, 101L, 155L, 20L, 90L, 48L, 375L, 344L, 135L, 
379L, 519L, 537L, 549L, 563L, 67L, 557L, 91L, 43L, 30L, 35L, 
125L, 784L, 491L, 519L, 324L, 627L, 503L, 215L, 296L, 68L, 203L, 
42L, 173L, 58L, 43L, 222L, 812L, 64L, 98L, 36L, 65L, 36L, 45L, 
42L, 50L, 43L, 962L, 0L, 36L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 
1L, 15L, 17L, 15L, 13L, 12L, 25L, 27L, 8L, 26L, 7L, 2L, 5L, 5L, 
4L, 5L, 5L, 5L, 5L, 6L, 5L, 4L, 6L, 0L, 0L, 5L, 0L, 1L, 0L, 5L, 
3L, 0L, 0L, 4L, 0L, 1L, 4L, 2L, 9L, 3L, 59L, 77L, 123L, 107L, 
144L, 119L, 79L), b1 = structure(c(7L, 3L, 3L, 3L, 7L, 7L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 7L, 1L, 1L, 7L, 
7L, 7L, 1L, 7L, 1L, 1L, 1L, 1L, 7L, 1L, 1L, 1L, 7L, 7L, 7L, 7L, 
5L, 5L, 7L, 7L, 5L, 5L, 7L, 7L, 3L, 5L, 5L, 5L, 3L, 7L, 7L, 7L, 
1L, 7L, 7L, 7L, 3L, 1L, 7L, 7L, 7L, 7L, 3L, 7L, 5L, 5L, 5L, 5L, 
7L, 5L, 7L, 7L, 1L, 1L, 3L, 5L, 3L, 7L, 3L, 3L, 3L, 7L, 3L, 7L, 
3L, 1L, 7L, 7L, 7L, 3L, 5L, 7L, 7L, 7L, 1L, 1L, 1L, 1L, 1L, 1L, 
5L, 1L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 5L, 5L, 7L, 5L, 5L, 6L, 6L, 2L, 6L, 2L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 6L, 6L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 2L, 
1L, 7L, 7L, 7L, 7L, 3L, 7L, 7L, 3L, 7L), .Label = c(""CP"", ""HF"", 
""HP"", ""KF"", ""KP"", ""NF"", ""NP""), class = ""factor""), b2 = structure(c(7L, 
3L, 3L, 3L, 7L, 7L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 7L, 1L, 1L, 7L, 7L, 7L, 1L, 7L, 1L, 1L, 1L, 1L, 7L, 1L, 
1L, 1L, 7L, 7L, 7L, 7L, 5L, 5L, 7L, 7L, 5L, 5L, 7L, 7L, 3L, 5L, 
5L, 5L, 3L, 7L, 7L, 7L, 1L, 7L, 7L, 7L, 3L, 1L, 7L, 7L, 7L, 7L, 
3L, 7L, 5L, 5L, 5L, 5L, 7L, 5L, 7L, 7L, 1L, 1L, 3L, 5L, 3L, 7L, 
3L, 3L, 3L, 7L, 3L, 7L, 3L, 1L, 7L, 7L, 7L, 3L, 5L, 7L, 7L, 7L, 
1L, 1L, 1L, 1L, 1L, 1L, 5L, 1L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 5L, 5L, 7L, 5L, 5L, 6L, 6L, 2L, 6L, 
2L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 6L, 6L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 2L, 1L, 7L, 7L, 7L, 7L, 3L, 7L, 7L, 3L, 7L
), .Label = c(""CP"", ""HF"", ""HP"", ""KF"", ""KP"", ""NF"", ""NP""), class = ""factor""), 
    b3 = structure(c(3L, 5L, 5L, 5L, 3L, 3L, 3L, 5L, 7L, 5L, 
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 5L, 7L, 7L, 
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 1L, 7L, 7L, 5L, 5L, 7L, 5L, 5L, 
    5L, 5L, 5L, 5L, 5L, 7L, 5L, 3L, 5L, 5L, 5L, 3L, 7L, 7L, 3L, 
    7L, 7L, 7L, 3L, 3L, 7L, 7L, 7L, 5L, 5L, 3L, 5L, 5L, 5L, 5L, 
    5L, 7L, 5L, 7L, 7L, 1L, 1L, 3L, 5L, 3L, 7L, 3L, 7L, 3L, 7L, 
    3L, 7L, 1L, 1L, 7L, 7L, 7L, 3L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 5L, 3L, 5L, 7L, 3L, 7L, 7L, 7L, 3L, 3L, 3L, 7L, 
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 2L, 2L, 2L, 
    6L, 4L, 4L, 4L, 4L, 6L, 4L, 6L, 6L, 6L, 6L, 6L, 6L, 4L, 4L, 
    6L, 6L, 4L, 6L, 2L, 7L, 1L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 3L, 
    7L), .Label = c(""CP"", ""HF"", ""HP"", ""KF"", ""KP"", ""NF"", ""NP""), class = ""factor""), 
    b4 = structure(c(3L, 5L, 5L, 5L, 3L, 3L, 3L, 5L, 7L, 5L, 
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 1L, 1L, 5L, 7L, 7L, 
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 1L, 7L, 7L, 5L, 5L, 7L, 5L, 5L, 
    5L, 5L, 5L, 5L, 5L, 7L, 5L, 3L, 5L, 5L, 5L, 3L, 7L, 7L, 3L, 
    7L, 7L, 7L, 3L, 3L, 7L, 7L, 7L, 5L, 5L, 3L, 5L, 5L, 5L, 5L, 
    5L, 7L, 5L, 7L, 7L, 1L, 1L, 3L, 5L, 3L, 7L, 3L, 7L, 3L, 7L, 
    3L, 7L, 1L, 1L, 7L, 7L, 7L, 3L, 5L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 5L, 3L, 5L, 7L, 3L, 7L, 7L, 7L, 3L, 3L, 3L, 7L, 
    5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 2L, 2L, 2L, 
    6L, 4L, 4L, 4L, 4L, 6L, 4L, 6L, 6L, 6L, 6L, 6L, 6L, 4L, 4L, 
    6L, 6L, 4L, 6L, 2L, 7L, 1L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 3L, 
    7L), .Label = c(""CP"", ""HF"", ""HP"", ""KF"", ""KP"", ""NF"", ""NP""), class = ""factor""), 
    b5 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 4L, 1L, 3L, 1L, 4L, 
    1L, 2L, 1L, 1L, 4L, 1L, 4L, 1L, 4L, 4L, 2L, 4L, 2L, 2L, 2L, 
    4L, 2L, 2L, 2L, 2L, 4L, 2L, 4L, 2L, 2L, 4L, 2L, 1L, 1L, 4L, 
    4L, 4L, 4L, 4L, 4L, 1L, 4L, 4L, 4L, 4L, 2L, 4L, 4L, 4L, 3L, 
    3L, 4L, 4L, 1L, 4L, 1L, 1L, 1L, 3L, 2L, 4L, 2L, 2L, 2L, 4L, 
    2L, 4L, 4L, 1L, 4L, 4L, 4L, 2L, 3L, 4L, 2L, 4L, 1L, 1L, 1L, 
    1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 4L, 1L, 4L, 2L, 2L, 4L, 4L, 2L, 
    4L), .Label = c(""CP"", ""HP"", ""KP"", ""NP""), class = ""factor""), 
    b6 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 4L, 1L, 3L, 1L, 4L, 
    1L, 2L, 1L, 1L, 4L, 1L, 4L, 1L, 4L, 4L, 2L, 4L, 2L, 2L, 2L, 
    4L, 2L, 2L, 2L, 2L, 4L, 2L, 4L, 2L, 2L, 4L, 2L, 1L, 1L, 4L, 
    4L, 4L, 4L, 4L, 4L, 1L, 4L, 4L, 4L, 4L, 2L, 4L, 4L, 4L, 3L, 
    3L, 4L, 4L, 1L, 4L, 1L, 1L, 1L, 3L, 2L, 4L, 2L, 2L, 2L, 4L, 
    2L, 4L, 4L, 1L, 4L, 4L, 4L, 2L, 3L, 4L, 2L, 4L, 1L, 1L, 1L, 
    1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 4L, 1L, 4L, 2L, 2L, 4L, 4L, 2L, 
    4L), .Label = c(""CP"", ""HP"", ""KP"", ""NP""), class = ""factor""), 
    b7 = structure(c(2L, 4L, 4L, 4L, 2L, 2L, 2L, 2L, 2L, 4L, 
    4L, 4L, 2L, 2L, 5L, 1L, 1L, 1L, 4L, 4L, 2L, 2L, 4L, 3L, 6L, 
    6L, 6L, 3L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 3L, 3L, 3L, 3L, 6L, 
    6L, 3L, 6L, 6L, 6L, 6L, 3L, 6L, 3L, 3L, 4L, 3L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 3L, 2L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 4L, 4L, 
    4L, 6L, 4L, 2L, 6L, 2L, 2L, 2L, 4L, 3L, 6L, 3L, 6L, 3L, 6L, 
    3L, 6L, 6L, 2L, 6L, 6L, 6L, 6L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 4L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 6L, 6L, 6L, 6L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L, 2L, 6L, 3L, 3L, 6L, 3L, 3L, 
    6L), .Label = c(""CF"", ""CP"", ""HP"", ""KP"", ""NF"", ""NP""), class = ""factor""), 
    b8 = structure(c(2L, 4L, 4L, 4L, 2L, 2L, 2L, 2L, 2L, 4L, 
    4L, 4L, 2L, 2L, 5L, 1L, 1L, 1L, 4L, 4L, 2L, 2L, 4L, 3L, 6L, 
    6L, 6L, 3L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 3L, 3L, 3L, 3L, 6L, 
    6L, 3L, 6L, 6L, 6L, 6L, 3L, 6L, 3L, 3L, 4L, 3L, 6L, 6L, 6L, 
    6L, 6L, 6L, 6L, 3L, 2L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 4L, 4L, 
    4L, 6L, 4L, 2L, 6L, 2L, 2L, 2L, 4L, 3L, 6L, 3L, 6L, 3L, 6L, 
    3L, 6L, 6L, 2L, 6L, 6L, 6L, 6L, 4L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 4L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 6L, 6L, 6L, 6L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 6L, 2L, 6L, 3L, 3L, 6L, 3L, 3L, 
    6L), .Label = c(""CF"", ""CP"", ""HP"", ""KP"", ""NF"", ""NP""), class = ""factor"")), .Names = c(""chr"", 
""pos_c"", ""c1"", ""c2"", ""c3"", ""c4"", ""c5"", ""c6"", ""c7"", ""c8"", ""k1"", 
""k2"", ""k3"", ""k4"", ""k5"", ""k6"", ""k7"", ""k8"", ""b1"", ""b2"", ""b3"", ""b4"", 
""b5"", ""b6"", ""b7"", ""b8""), class = ""data.frame"", row.names = c(NA, 
-161L))
</code></pre>
","<p>You can try:</p>

<pre><code>t(apply(df[,-1], 1, function(rg){
                occ_rg &lt;- table(rg)
                rg[grep(""HP"",rg)] &lt;- names(occ_rg)[which.max(occ_rg)]
                return(rg)}))
</code></pre>

<p><em>So, to have your new df:</em></p>

<pre><code>df &lt;- data.frame(chr=df[, 1], t(apply(df[,-1], 1, function(rg){
                                                  occ_rg &lt;- table(rg)
                                                  rg[grep(""HP"",rg)] &lt;- names(occ_rg)[which.max(occ_rg)]
                                                  return(rg)})), 
                 stringsAsFactors=F)
#  chr b1 b2 b3 b4 b5 b6 b7 b8
#1   1 CP CP CP CP CP CP CP CP
#2   1 KP KP KP KP CP CP KP KP
#3   1 CP CP CP CP KP KP CP CP
#4   1 CP CP CP CP KP KP CP CP
#5   1 KP KP CP CP CP CP CP CP
</code></pre>

<p><strong>EDIT</strong>  </p>

<p>If you have other columns and the columns you want to change are the only ones beginning with ""b"", you can do :</p>

<pre><code>df[, grepl(""^b"", colnames(df))] &lt;- t(apply(df[, grepl(""^b"", colnames(df))], 
                                           1, 
                                           function(rg){
                                                   occ_rg &lt;- table(rg)
                                                   rg[grep(""HP"",rg)] &lt;- names(occ_rg)[which.max(occ_rg)]
                                                   return(rg)}))
</code></pre>

<p><strong><em>Example:</em></strong></p>

<p>With this df:</p>

<pre><code>#  chr c1 b1 b2 b3 b4 b5 b6 b7 b8 c2
#1   1  1 HP HP CP CP CP CP CP CP 11
#2   1  2 HP HP KP KP CP CP KP KP 12
#3   1  3 CP CP CP CP KP KP HP HP 13
#4   1  4 CP CP HP HP KP KP CP CP 14
#5   1  5 KP KP CP CP HP HP CP CP 15
</code></pre>

<p>You get:</p>

<pre><code>#  chr c1 b1 b2 b3 b4 b5 b6 b7 b8 c2
#1   1  1 CP CP CP CP CP CP CP CP 11
#2   1  2 KP KP KP KP CP CP KP KP 12
#3   1  3 CP CP CP CP KP KP CP CP 13
#4   1  4 CP CP CP CP KP KP CP CP 14
#5   1  5 KP KP CP CP CP CP CP CP 15
</code></pre>

<p><strong>EDIT 2</strong></p>

<p>If you have other values than ""HP"", ""CP"" and ""KP"" and want to replace ""HP"" by either ""CP"" or ""KP"", depending on which occurs the most, you can do:</p>

<pre><code>df[, grepl(""^b"", colnames(df))] &lt;- t(apply(df[, grepl(""^b"", colnames(df))], 
                                           1, 
                                           function(rg){
                                                   occ_rg &lt;- table(rg)
                                                   occ_rg &lt;- occ_rg[grepl(""KP|CP"", names(occ_rg))]
                                                   rg[grep(""HP"",rg)] &lt;- names(occ_rg)[which.max(occ_rg)]
                                                   return(rg)}))
</code></pre>

<p><strong><em>Explanation (for edit2):</em></strong>  </p>

<pre><code>df[, grepl(""^b"", colnames(df))] &lt;- # only the columns beginning with b are considered (so the other ones will remain untouched)

      t( # the results of apply will be transposed
         apply(df[, grepl(""^b"", colnames(df))], # apply on df with only the columns beginning by b
               1, # by row
               function(rg){ # a function that takes a vector ""rg"" as input
                   occ_rg &lt;- table(rg) # computes the table
                   occ_rg &lt;- occ_rg[grepl(""KP|CP"", names(occ_rg))] # keep only the occurrences of either ""KP"" or ""CP""
                   rg[grep(""HP"",rg)] &lt;- names(occ_rg)[which.max(occ_rg)] # replace in the vector rg the ""HP"" elements by ""KP"" or ""CP"" depending on which occurs the most
                   return(rg) # finally returns the vector rg
               }))
</code></pre>
"
21155737,PCA Feature selection using R,2,3,3,"<p>I am a biologist. An output of my experiment contains large number of features(which are stored as numbers of columns and 563 rows). The columns are the features which are 8603 in number which are quite high.</p>

<p>So, when I tried to do PCA analysis in R and it gives ""out of memory"" errors.</p>

<p>I have tried also  doing princomp in pieces, but it does not seem to  work for our
approach.</p>

<p>I tried using the Script given in the link...</p>

<p><a href=""http://www.r-bloggers.com/introduction-to-feature-selection-for-bioinformaticians-using-r-correlation-matrix-filters-pca-backward-selection/"" rel=""nofollow"">http://www.r-bloggers.com/introduction-to-feature-selection-for-bioinformaticians-using-r-correlation-matrix-filters-pca-backward-selection/</a></p>

<p>But still it does not wok :(</p>

<p>I am trying to use the following code</p>

<pre><code>bumpus &lt;- read.table(""http://www.ndsu.nodak.edu/ndsu/doetkott/introsas/rawdata/bumpus.html"", 
                     skip=20, nrows=49, 
                     col.names=c(""id"",""total"",""alar"",""head"",""humerus"",""sternum""))

boxplot(bumpus, main=""Boxplot of Bumpus' data"") ## in this step it is showing the ERROR

# we first standardize the data:
bumpus.scaled &lt;- data.frame( apply(bumpus,2,scale) )
boxplot(bumpus.scaled, main=""Boxplot of standardized Bumpus' data"")

pca.res &lt;- prcomp(bumpus.scaled, retx=TRUE)
pca.res

# note:
# PC.1 is some kind of average of all the measurements 
#    =&gt; measure of size of the bird
# PC.2 has a negative weight for 'sternum' 
#    and positive weights for 'alar', 'head' and 'humerus'
#    =&gt; measure of shape of the bird

# first two principal components:
pca.res$x[,1:2]
plot(pca.res$x[,1:2], pch="""", main=""PC.1 and PC.2 for Bumpus' data (blue=survived, red=died)"")
text(pca.res$x[,1:2], labels=c(1:49), col=c(rep(""blue"",21),rep(""red"",28)))
abline(v=0, lty=2)
abline(h=0, lty=2)

# compare to segment plot:
windows()
palette(rainbow(12, s = 0.6, v = 0.75)) 
stars(bumpus, labels=c(1:49), nrow=6, key.loc=c(20,-1), 
      main=""Segment plot of Bumpus' data"", draw.segment=TRUE) 

# compare to biplot:
windows()
biplot(pca.res, scale=0)
# what do the arrows mean?
# consider the arrow for sternum:
abline(0, pca.res$rotation[5,2]/pca.res$rotation[5,1])
# consider the arrow for head:
abline(0, pca.res$rotation[3,2]/pca.res$rotation[3,1])
</code></pre>

<p>But second line</p>

<p>boxplot(bumpus, main=""Boxplot of Bumpus' data"") ## shows an error</p>

<p>The error is </p>

<pre><code>Error: cannot allocate vector of size 1.4 Mb

In addition: There were 27 warnings (use warnings() to see them)
</code></pre>

<p>Please help!</p>
","<p>In cases where the number of features is either huge or exceeds the number of
observations, it is well advised to calculate the principal components based on
the <em>transposed dataset</em>. This is especially true in your case because the default
implies calculation of a 8603 x 8603 covariance matrix which itself already
consumes about 500 MB of memory (oh well, this isn't too much, but hey...).</p>

<p>Assuming that the rows of your matrix <code>X</code> correspond to observations
and columns correspond to features, center your data and then perform PCA on the
transpose of the centered <code>X</code>. There won't be more eigenpairs than number of
observations anyway. Finally, multiply each resulting eigenvector by <code>X^T</code>. You do
not need to do the latter for the eigenvalues (see way below for a detailed explanation):</p>

<h2>What you want</h2>

<p>This code demonstrates the implementation of PCA on the transposed dataset and compares the results of <code>prcomp</code> and the ""transposed PCA"":</p>

<pre><code>pca.reduced &lt;- function(X, center=TRUE, retX=TRUE) {
  # Note that the data must first be centered on the *original* dimensions
  # because the centering of the 'transposed covariance' is meaningless for
  # the dataset. This is also why Sigma must be computed dependent on N
  # instead of simply using cov().
  if (center) {
    mu &lt;- colMeans(X)
    X &lt;- sweep(X, 2, mu, `-`)
  }
  # From now on we're looking at the transpose of X:
  Xt &lt;- t(X)
  aux &lt;- svd(Xt)
  V &lt;- Xt %*% aux$v
  # Normalize the columns of V.
  V &lt;- apply(V, 2, function(x) x / sqrt(sum(x^2)))
  # Done.
  list(X = if (retX) X %*% V else NULL,
       V = V,
       sd = aux$d / sqrt(nrow(X)-1),
       mean = if (center) mu else NULL)
}

# Example data (low-dimensional, but sufficient for this example):
X &lt;- cbind(rnorm(1000), rnorm(1000) * 5, rnorm(1000) * 3)

original   &lt;- prcomp(X, scale=FALSE)
transposed &lt;- pca.reduced(X)

# See what happens:    
&gt; print(original$sdev)
[1] 4.6468136 2.9240382 0.9681769
&gt; print(transposed$sd)
[1] 4.6468136 2.9240382 0.9681769
&gt; 
&gt; print(original$rotation)
               PC1           PC2          PC3
[1,] -0.0055505001  0.0067322416  0.999961934
[2,] -0.9999845292 -0.0004024287 -0.005547916
[3,]  0.0003650635 -0.9999772572  0.006734371
&gt; print(transposed$V)
              [,1]          [,2]         [,3]
[1,]  0.0055505001  0.0067322416 -0.999961934
[2,]  0.9999845292 -0.0004024287  0.005547916
[3,] -0.0003650635 -0.9999772572 -0.006734371
</code></pre>

<h2>Details</h2>

<p>To see why it is possible to work on the transposed matrix consider the
following:</p>

<p>The general form of the eigenvalue equation is</p>

<pre><code>          A x = λ x                               (1)
</code></pre>

<p>Without loss of generality, let <code>M</code> be a centered ""copy"" of your original
dataset <code>X</code>. Substitution of <code>M^T M</code> for <code>A</code> yields</p>

<pre><code>          M^T M x = λ x                           (2)
</code></pre>

<p>Multiplication of this equation by <code>M</code> yields</p>

<pre><code>          M M^T M x = λ M x                       (3)
</code></pre>

<p>Consequent substitution of <code>y = M x</code> yields</p>

<pre><code>          M M^T y = λ y                           (4)
</code></pre>

<p>One can already see that <code>y</code> corresponds to an eigenvector of the ""covariance""
matrix of the <em>transposed</em> dataset (note that <code>M M^T</code> is in fact no real
covariance matrix as the dataset <code>X</code> was centered along its columns and not its
rows. Also, scaling must be done by means of the number of samples (rows of <code>M</code>)
and not the number of features (columns of <code>M</code> resp. rows of <code>M^T</code>).</p>

<p>It can also be seen that the eigenvalues are the <em>same</em> for <code>M M^T</code> and <code>M^T M</code>.</p>

<p>Finally, one last multiplication by <code>M^T</code> results in</p>

<pre><code>          (M^T M) M^T y = λ M^T y                 (5)
</code></pre>

<p>where <code>M^T M</code> is the original covariance matrix.</p>

<p>From equation (5) it follows that <code>M^T y</code> is an eigenvector of <code>M^T M</code> with
eigenvalue <code>λ</code>.</p>
"
19248598,can't open sockets for parallel cluster,2,2,2,"<p>I am trying to use the <code>parallel</code> package, and found that <code>makeCluster</code> fails to complete. I've traced the hang to the following line in <code>newPSOCKnode</code> : </p>

<pre><code>con &lt;- socketConnection(""localhost"", port = port, server = TRUE, 
    blocking = TRUE, open = ""a+b"", timeout = timeout)
</code></pre>

<p>That command stalls (granted the default timeout is a large value).  My suspicion is this is due to some ""overzealous IT rules"" laid down on our work computers, but would welcome any suggestions as to how to trace (and fix) the source of the problem.  This is Windows7-64, ""Enterprise"", R 3.0.1 .  </p>

<p>More info:  inside debugging session, I set <code>timeout &lt; - 10</code>, but it still hangs -- as though <code>socketConnection</code> is getting trapped somewhere that it can't even check the timeout value.</p>

<p>Here's my dump at the same point as Richie Cotton's data:</p>

<pre><code>Browse[3]&gt; ls.str()
arg :  chr ""parallel:::.slaveRSOCK()""
cmd :  chr ""\""C:/Users/carl.witthoft/Documents/R/R-3.0.1/bin/x64/Rscript\"" -e \""parallel:::.slaveRSOCK()\"" MASTER=localhost PORT=11017 OUT=""| __truncated__
env :  chr ""MASTER=localhost PORT=11017 OUT=/dev/null TIMEOUT=2592000 METHODS=TRUE XDR=TRUE""
machine :  chr ""localhost""
manual :  logi FALSE
master :  chr ""localhost""
methods :  logi TRUE
options : &lt;environment: 0x000000000ccac6a0&gt; 
outfile :  chr ""/dev/null""
port :  int 11017
rank :  int 1
renice :  int NA
rscript :  chr ""\""C:/Users/carl.witthoft/Documents/R/R-3.0.1/bin/x64/Rscript\""""
timeout :  num 2592000
useXDR :  logi TRUE
</code></pre>

<p>So aside from a different port number, I think everything matches up.</p>

<p>Next trick:  I opened a shell and ran <code>netsh advfirewall firewall add rule name=""Open Port 11017"" dir=in action=allow protocol=TCP localport=11017</code>  and got an ""OK"" response.
I ran <code>netstat -a -n</code>  and found the following line:  </p>

<p><code>TCP    0.0.0.0:11017          0.0.0.0:0              LISTENING</code></p>

<p>But running <code>makePSOCKcluster</code>  still hangs at the same place. </p>

<p>NEXT:
I tried running <code>R</code> from the command line (via cygwin bash), and the error message I get is <code>Error in loadhistory(file) : no history mechanism available
Execution halted</code> , after which -C returns me to the R-prompt.</p>
","<p>What you're describing is the classic problem with PSOCK clusters: <em><code>makeCluster</code> hangs</em>.  It can hang for dozens of reasons because it has to create all of the processes, called ""worker"" processes, that will perform the actual work of the ""cluster"", and that involves starting new R sessions using the Rscript command that will execute the <code>.slaveRSOCK</code> function, which will create a socket connection back to the master and then execute the <code>slaveLoop</code> function where it will eventually execute the tasks sent to it by the master.  If anything goes wrong starting any of the worker processes (and trust me: a lot can go wrong), the master will hang while executing <code>socketConnection</code>, waiting for the worker to connect to it even though that worker may have died or never even been created successfully.</p>

<p>For many failure scenarios, using the <code>outfile</code> argument is great because it often reveals the error that causes the worker process to die and thus the master to hang.  But if that reveals nothing, I go to manual mode.  In manual mode, the master prints the command to start each worker instead of executing the command itself.  It's more work, but it gives you complete control, and you can even debug into the workers if you need to.</p>

<p>Here's an example:</p>

<pre><code>&gt; library(parallel)
&gt; cl &lt;- makePSOCKcluster(1, manual=TRUE, outfile='log.txt')
Manually start worker on localhost with
   '/usr/lib/R/bin/Rscript' -e 'parallel:::.slaveRSOCK()' MASTER=localhost
PORT=10187 OUT=log.txt TIMEOUT=2592000 METHODS=TRUE XDR=TRUE 
</code></pre>

<p>At this point, your R session is hung because it's executing <code>socketConnection</code>, just as you described.  It's now your job to open a new terminal window (command prompt, or whatever), and paste in that Rscript command.  As soon as you've executed it, <code>makePSOCKcluster</code> should return since we only requested one worker.  Of course, if something goes wrong, it won't return, but if you're lucky, you'll get an error message in your terminal window and you'll have an important clue that will hopefully lead to a solution to your problem.  If you're not so lucky, the Rscript command will also hang, and you'll have to dive in even deeper.</p>

<p>To debug the worker, you don't execute the displayed Rscript command because you need an interactive session.  Instead, you start an R session with a command such as:</p>

<pre><code>$ R --vanilla --args MASTER=localhost PORT=10187 OUT=log.txt TIMEOUT=2592000 METHODS=TRUE XDR=TRUE
</code></pre>

<p>In that R session, you can put a breakpoint on the <code>.slaveRSOCK</code> function and then execute it:</p>

<pre><code>&gt; debug(parallel:::.slaveRSOCK)
&gt; parallel:::.slaveRSOCK()
</code></pre>

<p>Now you can start stepping through the code, possibly setting breakpoints on the <code>slaveLoop</code> and <code>makeSOCKmaster</code> functions.  In your case, I assume that it will hang trying to create the socket connection, in which case the title of your question will be appropriate.</p>

<p>For more information on this kind of problem, see <a href=""https://stackoverflow.com/a/17976175/2109128"">my answer to a similar question.</a></p>

<p><strong>UPDATE</strong></p>

<p>Now that this particular problem has been resolved, I can add two tips for debugging <code>makePSOCKcluster</code> problems:</p>

<ul>
<li>Check to see if anything in your .Rprofile only works in interactive mode</li>
<li>On Windows, use the Rterm command rather than Rgui so that you're more likely to see error messages and output from using <code>outfile=''</code>.</li>
</ul>
"
16855871,How to successfully interrupt R from tcltk scripting widget?,2,2,2,"<p>I am trying to create a scripting widget for R using the <code>tcltk</code> package. But I don't know how to to create a STOP button to interrupt a script coming from the widget. Basically, I would like to have a button, a menu option, and/or a key binding that will interrupt the current script execution, but I can't figure out how to make it work.</p>

<p>One (non-ideal) strategy is to just use the RGui STOP button (or <code>&lt;ESC&gt;</code> or <code>&lt;Ctrl-c&gt;</code> on the console), but this seems cause the tk widget to hang permanently.</p>

<p>Here's a minimal example of the widget based on the tcl/tk examples (<a href=""http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/evalRcode.html"" rel=""nofollow"">http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/evalRcode.html</a>):</p>

<pre><code>require(tcltk)

tkscript &lt;- function() {
    tt &lt;- tktoplevel()
    txt &lt;- tktext(tt, height=10)
    tkpack(txt)
    run &lt;- function() {
        code &lt;- tclvalue(tkget(txt,""0.0"",""end""))
        e &lt;- try(parse(text=code))
        if (inherits(e, ""try-error"")) {
            tkmessageBox(message=""Syntax error"", icon=""error"")
            return()
        }
        print(eval(e))
    }
    tkbind(txt,""&lt;Control-r&gt;"",run)
}

tkscript()
</code></pre>

<p>In the scripting widget if you try executing <code>Sys.sleep(20)</code> and then interrupt from the console, the widget hangs. The same thing happens if one were to run, for example, an infinite loop, like <code>while(TRUE) 2+2</code>.</p>

<p>I think what I'm experiencing may be similar to the bug reported here: <a href=""https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=14730"" rel=""nofollow"">https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=14730</a></p>

<p>Also, I should mention that I'm running this on R 3.0.0 on Windows (x64), so maybe the problem is platform-specific.</p>

<p>Any thoughts on how to interrupt the running script without causing the widget to hang?</p>
","<p>It depends on what the script is doing; a script that is sitting waiting for the user to do something is easy to interrupt (since you can make it listen for your interruption message) but a script that is doing an intensive loop is rather more tricky. The possible solutions depend on the version of Tcl inside.</p>

<h2>Interpreter Cancellation — Requires 8.6</h2>

<p>If you are using Tcl 8.6, you can use interpreter cancellation to stop the script. All you have to do is arrange for:</p>

<pre><code>interp cancel -unwind
</code></pre>

<p>to be run, and the script will return control back to you. A reasonable way of doing this would be to use the extra Tcl package <em>TclX</em> (or <em>Expect</em>) to install a signal handler that will run the command when a signal is received:</p>

<pre><code>package require Tcl 8.6
package require TclX

# Our signal handler
proc doInterrupt {} {
    # Print a message so you can see what's happening
    puts ""It goes boom!""
    # Unwind the stack back to the R code
    interp cancel -unwind
}

# Install it...
signal trap sigint doInterrupt

# Now evaluate the code which might try to run forever
</code></pre>

<p>Adding signal handling in earlier versions is possible, but not quite as easy as you can't guarantee that things will return control to you so easily; the stack unwinding isn't there.</p>

<h2>Execution Time Limiting — Requires 8.5 (or 8.6)</h2>

<p>The other thing you could try is setting an execution time limit on a slave interpreter and running the user script in that slave. The time limit machinery will then guarantee a trap back to you every so often, giving you a chance to check for interruption and a way to do the stack unwinding. This is a <em>considerably</em> more complex method.</p>

<pre><code>proc nextSecond {} {
    clock add [clock seconds] 1 second
}
interp create child
proc checkInterrupt {} {
    if {[""decide if the R code wanted an interrupt""]} {
        # Do nothing
        return
    }
    # Reset the time limit to another second ahead
    interp limit child time -seconds [nextSecond]
}

interp limit child time -seconds [nextSecond] -command checkInterrupt
interp eval child ""the user script""
</code></pre>

<p>Think of this mechanism being a lot like how an operating system works, and yes, it can stop a tight loop.</p>

<h2>Use a Subprocess — Any version of Tcl</h2>

<p>The most portable mechanism is to run the script in a subprocess (with the <code>tclsh</code> program; the exact name varies by version, platform and distribution, but it's all variations on that) and just kill off that subprocess when it is no longer wanted with <a href=""http://stat.ethz.ch/R-manual/R-devel/library/tools/html/pskill.html""><code>pskill</code></a>. The downside of this is that you cannot (easily) carry any state over from one execution to another; subprocesses are pretty isolated from each other. The other methods described above can leave the state able to be accessed from another run: they do a real interrupt, whereas this destroys.</p>

<p>Also, I don't know exactly how to start the subprocess in such a way that you can communicate with it from R while it is still running; <code>system</code> and <code>system2</code> don't seem to quite give enough control, and hacking something with forking is non-portable. Needs an R expert here. Alternatively, use a Tcl script (running inside the R process) to do it with:</p>

<pre><code>set executable ""tclsh"";    # Adjust this line
set scriptfile ""file/where/you/put/the_user/script.tcl""

# Open a bi-directional pipe to talk to the subprocess
set pipeline [open |[list $executable $scriptfile] ""r+""]

# Get the subprocess's PID
set thePID [pid $pipeline]
</code></pre>

<p>That is actually reasonably portable to Windows (if not perfectly so) but intermediate states with forking are not.</p>
"
13571359,Join and sum not compatible matrices,1,1,3,"<p>My goal is to <em>""sum""</em> two <strong>not compatible matrices</strong> (matrices with different dimensions) using (and preserving) row and column names.</p>

<p>I've figured this approach: convert the matrices to <code>data.table</code> objects, join them and then sum columns vectors.</p>

<p>An example:</p>

<pre><code>&gt; M1
  1 3 4 5 7 8
1 0 0 1 0 0 0
3 0 0 0 0 0 0
4 1 0 0 0 0 0
5 0 0 0 0 0 0
7 0 0 0 0 1 0
8 0 0 0 0 0 0
&gt; M2
  1 3 4 5 8
1 0 0 1 0 0
3 0 0 0 0 0
4 1 0 0 0 0
5 0 0 0 0 0
8 0 0 0 0 0
&gt; M1 %ms% M2
  1 3 4 5 7 8
1 0 0 2 0 0 0
3 0 0 0 0 0 0
4 2 0 0 0 0 0
5 0 0 0 0 0 0
7 0 0 0 0 1 0
8 0 0 0 0 0 0
</code></pre>

<p>This is my code:</p>

<pre><code>M1 &lt;- matrix(c(0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0), byrow = TRUE, ncol = 6)
colnames(M1) &lt;- c(1,3,4,5,7,8)
M2 &lt;- matrix(c(0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0), byrow = TRUE, ncol = 5)
colnames(M2) &lt;- c(1,3,4,5,8)
# to data.table objects
DT1 &lt;- data.table(M1, keep.rownames = TRUE, key = ""rn"")
DT2 &lt;- data.table(M2, keep.rownames = TRUE, key = ""rn"")
# join and sum of common columns
if (nrow(DT1) &gt; nrow(DT2)) {
    A &lt;- DT2[DT1, roll = TRUE]
    A[, list(X1 = X1 + X1.1, X3 = X3 + X3.1, X4 = X4 + X4.1, X5 = X5 + X5.1, X7, X8 = X8 + X8.1), by = rn]
}
</code></pre>

<p>That outputs:</p>

<pre><code>   rn X1 X3 X4 X5 X7 X8
1:  1  0  0  2  0  0  0
2:  3  0  0  0  0  0  0
3:  4  2  0  0  0  0  0
4:  5  0  0  0  0  0  0
5:  7  0  0  0  0  1  0
6:  8  0  0  0  0  0  0
</code></pre>

<p>Then I can convert back this <code>data.table</code> to a <code>matrix</code> and fix row and column names.</p>

<p>The <strong>questions</strong> are:</p>

<ul>
<li><p>how to generalize this procedure?</p>

<p>I need a way to automatically create <code>list(X1 = X1 + X1.1, X3 = X3 + X3.1, X4 = X4 + X4.1, X5 = X5 + X5.1, X7, X8 = X8 + X8.1)</code> because i want <strong>to apply this function to matrices which dimensions (and row/columns names) are not known in advance</strong>.</p>

<p>In summary I need a <strong>merge</strong> procedure that behaves as described.</p></li>
<li><p>there are other strategies/implementations that achieve the same goal that are, at the same time, faster and generalized? (hoping that some <code>data.table</code> monster help me)</p></li>
<li><p>to what kind of <strong>join</strong> (inner, outer, etc. etc.) is assimilable this procedure?</p></li>
</ul>

<p>Thanks in advance.</p>

<p>p.s.: I'm using data.table version 1.8.2</p>

<hr>

<p><strong>EDIT - SOLUTIONS</strong></p>

<p>@Aaron solution. No external libraries, only base R. It works also on <strong>list of matrices</strong>.</p>

<pre><code>add_matrices_1 &lt;- function(...) {
  a &lt;- list(...)
  cols &lt;- sort(unique(unlist(lapply(a, colnames))))
  rows &lt;- sort(unique(unlist(lapply(a, rownames))))
  out &lt;- array(0, dim = c(length(rows), length(cols)), dimnames = list(rows,cols))
  for (m in a) out[rownames(m), colnames(m)] &lt;- out[rownames(m), colnames(m)] + m
  out
}
</code></pre>

<p>@MadScone solution. Use <code>reshape2</code> package. It works only on <strong>two matrices per call</strong>.</p>

<pre><code>add_matrices_2 &lt;- function(m1, m2) {
  m &lt;- acast(rbind(melt(M1), melt(M2)), Var1~Var2, fun.aggregate = sum)
  mn &lt;- unique(colnames(m1), colnames(m2))
  rownames(m) &lt;- mn
  colnames(m) &lt;- mn
  m
}
</code></pre>

<p>@Aaron solution. Use <code>Matrix</code> package. It work only on <strong>sparse matrices</strong>, also on list of them.</p>

<pre><code>add_matrices_3 &lt;- function(...) {
  a &lt;- list(...)
  cols &lt;- sort(unique(unlist(lapply(a, colnames))))
  rows &lt;- sort(unique(unlist(lapply(a, rownames))))
  nrows &lt;- length(rows)
  ncols &lt;- length(cols)
  newms &lt;- lapply(a, function(m) {
    s &lt;- summary(m)
    i &lt;- match(rownames(m), rows)[s$i]
    j &lt;- match(colnames(m), cols)[s$j]
    ilj &lt;- i &lt; j
    sparseMatrix(
      i         = ifelse(ilj, i, j),
      j         = ifelse(ilj, j, i),
      x         = s$x,
      dims      = c(nrows, ncols),
      dimnames  = list(rows, cols),
      symmetric = TRUE
    )
  })
  Reduce(`+`, newms)
}
</code></pre>

<hr>

<p><strong>BENCHMARK</strong> (100 runs with <code>microbenchmark</code> package)</p>

<pre><code>Unit: microseconds
   expr                min         lq    median         uq       max
1 add_matrices_1   196.009   257.5865   282.027   291.2735   549.397
2 add_matrices_2 13737.851 14697.9790 14864.778 16285.7650 25567.448
</code></pre>

<p>No need to comment the benchmark: @Aaron solution wins.</p>

<p><strong>Details</strong></p>

<p>For insights about performances (that depend of the size and the sparsity of the matrices) see @Aaron's edit (and the solution for sparse matrices: <code>add_matrices_3</code>).</p>
","<p>I'd just line up the names and go to town with base R.</p>

<p>Here's a simple function that takes an unspecified number of matrices and adds them up by their row/column names.</p>

<pre><code>add_matrices_1 &lt;- function(...) {
  a &lt;- list(...)
  cols &lt;- sort(unique(unlist(lapply(a, colnames))))
  rows &lt;- sort(unique(unlist(lapply(a, rownames))))
  out &lt;- array(0, dim=c(length(rows), length(cols)), dimnames=list(rows,cols))
  for(M in a) { out[rownames(M), colnames(M)] &lt;- out[rownames(M), colnames(M)] + M }
  out
}
</code></pre>

<p>It then works like this:</p>

<pre><code># giving them rownames and colnames
colnames(M1) &lt;- rownames(M1) &lt;- c(1,3,4,5,7,8)
colnames(M2) &lt;- rownames(M2) &lt;- c(1,3,4,5,8)

add_matrices_1(M1, M2)
#   1 3 4 5 7 8
# 1 0 0 2 0 0 0
# 3 0 0 0 0 0 0
# 4 2 0 0 0 0 0
# 5 0 0 0 0 0 0
# 7 0 0 0 0 1 0
# 8 0 0 0 0 0 0
</code></pre>

<p>For bigger matrices, however, it doesn't do as well.  Here's a function to make a matrix, choosing <code>n</code> columns out of <code>N</code> possibilities, and filling <code>k</code> spots with non-zero values.  (This assumes symmetrical matrices.) </p>

<pre><code>makeM &lt;- function(N, n, k) {
  s1 &lt;- sample(N, n)
  M1 &lt;- array(0, dim=c(n,n), dimnames=list(s1, s1))
  r1 &lt;- sample(n,k, replace=TRUE)
  c1 &lt;- sample(n,k, replace=TRUE)
  M1[cbind(c(r1,c1), c(c1,r1))] &lt;- sample(N,k)
  M1
}
</code></pre>

<p>Then here's another version that uses sparse matrices.</p>

<pre><code>add_matrices_3 &lt;- function(...) {
  a &lt;- list(...)
  cols &lt;- sort(unique(unlist(lapply(a, colnames))))
  rows &lt;- sort(unique(unlist(lapply(a, rownames))))
  nrows &lt;- length(rows)
  ncols &lt;- length(cols)
  newms &lt;- lapply(a, function(m) {
    s &lt;- summary(m)
    i &lt;- match(rownames(m), rows)[s$i]
    j &lt;- match(colnames(m), cols)[s$j]
    ilj &lt;- i&lt;j
    sparseMatrix(i=ifelse(ilj, i, j),
                 j=ifelse(ilj, j, i),
                 x=s$x,
                 dims=c(nrows, ncols),
                 dimnames=list(rows, cols), symmetric=TRUE)
  })
  Reduce(`+`, newms)
}
</code></pre>

<p>This version is definitely faster when the matrices are large and sparse.  (Note that I'm not timing the conversion to a sparse symmetric matrix, as hopefully if that's a suitable format, you'll use that format throughout your code.)</p>

<pre><code>set.seed(50)
M1 &lt;- makeM(10000, 5000, 50)
M2 &lt;- makeM(10000, 5000, 50)
mm2 &lt;- Matrix(M2)
mm1 &lt;- Matrix(M1)
system.time(add_matrices_1(M1, M2))
#   user  system elapsed 
#  2.987   0.841   4.133 
system.time(add_matrices_3(mm1, mm2))
#   user  system elapsed 
#  0.042   0.012   0.504 
</code></pre>

<p>But when the matrices are small, my first solution is still faster.</p>

<pre><code>set.seed(50)
M1 &lt;- makeM(100, 50, 20)
M2 &lt;- makeM(100, 50, 20)
mm2 &lt;- Matrix(M2)
mm1 &lt;- Matrix(M1)
microbenchmark(add_matrices_1(M1, M2), add_matrices_3(mm1, mm2))
# Unit: microseconds
#                       expr      min       lq   median        uq       max
# 1   add_matrices_1(M1, M2)  398.495  406.543  423.825  544.0905  43077.27
# 2 add_matrices_3(mm1, mm2) 5734.623 5937.473 6044.007 6286.6675 509584.24
</code></pre>

<p>Moral of the story: Size and sparsity matter.  </p>

<p>Also, getting it right is more important than saving a few microseconds.  It's almost always best to use simple functions and don't worry about speed unless you run into trouble.  So in small cases, I'd prefer MadScone's solution, as it's easy to code and simple to understand.  When that gets slow, I'd write a function like my first attempt.  When that gets slow, I'd write a function like my second attempt.</p>
"
37217770,Using apply using multiple sources of data?,1,3,3,"<p>I'm still in the beginning stages of R but I've gotten a few functions down and now I'm looking for my final ""project.""</p>

<p>I've created a function that takes each of my four sources of data (different populations) and creates histograms, performs kolmogorov-smirnov tests, and then graphs any significant results for a given row. What I want to do is turn it into an apply function. However, the issue is that my function takes four variables, and I don't know a way to make apply take four sources of data.</p>

<pre><code>hist_fx &lt;- function(w,x,y,z) {
  hist(w,prob=TRUE,col=""green"",xlim=c(-1,1),ylim=c(0,3))
    lines(density(w),col=""red"")
    abline(v=c(mean(w)),col=""red"")

  hist(x,prob=TRUE,col=""blue"",xlim=c(-1,1),ylim=c(0,3))
    lines(density(x),col=""red"")
    abline(v=c(mean(x)),col=""red"")


  hist(y,prob=TRUE,col=""yellow"",xlim=c(-1,1),ylim=c(0,3))
    lines(density(y),col=""red"")
    abline(v=c(mean(y)),col=""red"")


  hist(z,prob=TRUE,col=""purple"",xlim=c(-1,1),ylim=c(0,3))
    lines(density(z),col=""red"")
    abline(v=c(mean(z)),col=""red"")

  all &lt;- c(w,x,y,z)
    hist(all,prob=TRUE,xlim=c(-1,0.5),ylim=c(0,3))
    lines(density(w),col=""purple"")
    lines(density(x),col=""red"")
    lines(density(y),col=""blue"")
    lines(density(z),col=""green"")

  plot(ecdf(w),col=""green"")
  plot(ecdf(x),col=""blue"",add=TRUE)
  plot(ecdf(y),col=""red"",add=TRUE)
  plot(ecdf(z),col=""purple"",add=TRUE)

  t1 &lt;- ks.test(w,x)
    print(t1)
  t2 &lt;- ks.test(w,y)
    print(t2)
  t3 &lt;- ks.test(w,z)
    print(t3)

  if(t1$p.value &lt; 0.05) {
        plot(ecdf(w),col=""green"")
        plot(ecdf(x),col=""blue"",add=TRUE)
    }
  if(t2p.value &lt; 0.05) {
        plot(ecdf(w),col=""green"")
        plot(ecdf(y),col=""red"",add=TRUE)
    }
  if(t3$p.value &lt; 0.05) {
        plot(ecdf(w),col=""green"")
        plot(ecdf(z),col=""purple"",add=TRUE)
    }
}
</code></pre>

<p>I'm able to use this function with apply for one population at a time (i.e. turn hist_fx into a function of one variable). However, I can't find a way to make this work for all four populations at the same time. I've messed around with some for loops, though they haven't been successful as of yet.</p>

<p>One last thing that might be of use: my data is arranged such that independent variables are the rows and the dependent variables are columns. Consequently, I need to run these per row (hence my idea of a for loop). </p>

<p>EDIT:</p>

<p>Here's the dput for one of the populations:</p>

<blockquote>
  <p>dput(k2)
  structure(c(-0.15, 0.13, 0.23, -0.23, 0.06, -0.11, 0.107, 0.06, 
  -0.17, 0.12, 0.06, -0.25, -0.32, 0.13, 0.06, -0.2, -0.08, 0.06, 
  0.12, 0.02, 0.11, -0.11, -0.15, 0.097, 0.347, -0.307, 0.097, 
  -0.047, 0.09, 0.01, -0.217, 0.117, 0.03, -0.3, -0.33, 0.13, 0.19, 
  -0.24, -0.08, -0.01, 0.15, 0.61, 0.18, -0.15, -0.103, 0.135, 
  0.31, -0.25, 0.157, -0.105, -0.08, 0.01, -0.165, 0.17, 0.1, -0.23, 
  -0.28, 0.15, 0.13, -0.14, -0.06, 0.01, 0.07, -0.02, 0.11, -0.06, 
  -0.123, 0.13, 0.35, -0.27, 0.165, -0.065, 0.135, 0.13, -0.17, 
  0.135, 0.08, -0.21, -0.25, 0.2, 0.16, -0.18, NA, -0.04, 0.05, 
  -0.02, 0.13, -0.14, -0.13, 0.098, 0.27, -0.193, 0.062, -0.08, 
  0.057, 0.028, -0.199, 0.1, 0.04, -0.24, -0.32, 0.13, 0.13, -0.15, 
  -0.05, 0.01, 0.08, -0.04, 0.1, -0.1, -0.14, 0.154, 0.261, -0.194, 
  0.1, -0.129, 0.063, 0.142, -0.136, 0.136, 0.08, -0.23, -0.24, 
  0.12, 0.1, -0.16, -0.06, 0.04, 0.09, -0.01, 0.04, -0.08, -0.127, 
  0.133, 0.337, -0.06, 0.11, -0.107, 0.16, 0.167, -0.183, 0.103, 
  0.05, -0.2, -0.3, 0.22, -0.01, -0.17, -0.14, 0.02, 0.07, 0.01, 
  0.11, -0.11, -0.155, 0.221, 0.22, -0.172, 0.09, -0.15, 0.12, 
  0.03, -0.153, 0.146, 0.11, -0.2, -0.24, 0.16, 0.07, -0.19, -0.1, 
  0.03, 0.17, 0.02, 0.09, -0.16, -0.062, 0.19, 0.269, -0.265, 0.118, 
  -0.11, 0.126, 0.094, -0.186, 0.151, 0.08, -0.26, -0.31, 0.13, 
  0.09, -0.23, -0.12, 0.05, 0.13, 0.01, 0.11, -0.14, -0.095, 0.14, 
  0.24, -0.46, 0.09, -0.17, 0.08, 0.01, -0.24, 0.16, 0.04, -0.38, 
  -0.39, 0.11, 0.06, -0.31, -0.25, 0.03, 0.21, -0.14, 0, -0.22, 
  -0.07, 0.148, 0.311, -0.27, 0.11, -0.055, 0.16, 0.04, -0.197, 
  0.064, 0.09, -0.24, -0.34, 0.17, 0.07, -0.15, -0.18, 0.03, 0.13, 
  0.07, 0.13, -0.08, -0.136, 0.142, 0.27, -0.257, 0.1, -0.13, 0.103, 
  0.064, -0.197, 0.118, 0.06, -0.29, -0.35, 0.13, 0.1, -0.19, -0.13, 
  0.01, 0.1, -0.01, 0.13, -0.15), .Dim = c(22L, 12L))</p>
</blockquote>

<p>To further clarify, here's the format of the actual data frame:</p>

<p>c1   c2  c3  c4 </p>

<p>r2   x   x   x</p>

<p>r3   x   x   x</p>

<p>r4   x   x   x</p>

<p>Each column represents a star's values for the variable on the row. As such, I want to create a histogram for each row, for each dataset.</p>

<p>For the values of the function, I just used those variables for simplicity's sake. w = population 1, x = population 2, y = population 3, z = population 4.</p>

<p>As for an example:</p>

<pre><code> &gt; hist_fx(k2[1,],n2[1,],j2[1,],g2[1,])

    Two-sample Kolmogorov-Smirnov test

data:  w and x
D = 1, p-value = 1.229e-05
alternative hypothesis: two-sided


    Two-sample Kolmogorov-Smirnov test

data:  w and y
D = 1, p-value = 1.229e-05
alternative hypothesis: two-sided


    Two-sample Kolmogorov-Smirnov test

data:  w and z
D = 1, p-value = 1.229e-05
alternative hypothesis: two-sided
</code></pre>

<p>My problem is that currently, I can only run the function one row at a time. I'd like to be able to do it for all rows. I was thinking of using apply because I've used it in a very similar context except only for one source of data.</p>
","<p>Not quite sure of your needs but consider transposing, <code>t()</code> to run plots column-wise for row data. And consider using <code>mapply()</code>, the multivariate type of the apply family which runs an operation element-wise at the same time for equal-length objects. Even break apart the operations as running them together may only print/plot the last iteration to screen.</p>

<p>Transpose <em>(data used were slight variations of posted dput matrix)</em></p>

<pre><code>pop1 &lt;- data.frame(t(data))
pop2 &lt;- data.frame(t(data))
pop3 &lt;- data.frame(t(data))
pop4 &lt;- data.frame(t(data))
</code></pre>

<p>Histograms</p>

<pre><code>hist_fx &lt;- function(w,x,y,z) {

  whist &lt;- hist(w,prob=TRUE,col=""green"",xlim=c(-1,1),ylim=c(0,3))
  lines(density(w),col=""red"")
  abline(v=c(mean(w)),col=""red"")

  xhist &lt;- hist(x,prob=TRUE,col=""blue"",xlim=c(-1,1),ylim=c(0,3))
  lines(density(x),col=""red"")
  abline(v=c(mean(x)),col=""red"")      

  yhist &lt;- hist(y,prob=TRUE,col=""yellow"",xlim=c(-1,1),ylim=c(0,3))
  lines(density(y),col=""red"")
  abline(v=c(mean(y)),col=""red"")      

  zhist &lt;- hist(z,prob=TRUE,col=""purple"",xlim=c(-1,1),ylim=c(0,3))
  lines(density(z),col=""red"")
  abline(v=c(mean(z)),col=""red"")

}

# HISTOGRAM PLOTS FOR EACH DF COLUMN 
output &lt;- mapply(hist_fx, w=pop1, x=pop2, y=pop3, z=pop4)
</code></pre>

<p>Kolmogorov-Smirnov tests <em>(using slight variations of dput data)</em></p>

<pre><code>hist_fx &lt;- function(w,x,y,z) {
  t1 &lt;- ks.test(w,x)      
  t2 &lt;- ks.test(w,y)      
  t3 &lt;- ks.test(w,z)   

  if(t1$p.value &lt; 0.05) {
     plot(ecdf(w),col=""green"")
     plot(ecdf(x),col=""blue"",add=TRUE)
  }
  if(t2$p.value &lt; 0.05) {
     plot(ecdf(w),col=""green"")
     plot(ecdf(y),col=""red"",add=TRUE)
  }
  if(t3$p.value &lt; 0.05) {
     plot(ecdf(w),col=""green"")
     plot(ecdf(z),col=""purple"",add=TRUE)
  }

  return(c(t1, t2, t3))
}

output &lt;- mapply(hist_fx, w=pop1, x=pop2, y=pop3, z=pop4)

output    
#             X1                                  
# statistic   0.1666667                           
# p.value     0.9962552                           
# alternative ""two-sided""                         
# method      ""Two-sample Kolmogorov-Smirnov test""
# data.name   ""w and x""                           
# statistic   0.25                                
# p.value     0.8474885                           
# alternative ""two-sided""                         
# method      ""Two-sample Kolmogorov-Smirnov test""
# data.name   ""w and y""                           
# statistic   0.08333333                          
# p.value     1                                   
# alternative ""two-sided""                         
# method      ""Two-sample Kolmogorov-Smirnov test""
# data.name   ""w and z""                           
#             X2                                  
# statistic   0.25                                
# p.value     0.8474885                           
# alternative ""two-sided""                         
# method      ""Two-sample Kolmogorov-Smirnov test""
# data.name   ""w and x""                           
# statistic   0.08333333                          
# p.value     1                                   
# alternative ""two-sided""                         
# method      ""Two-sample Kolmogorov-Smirnov test""
# data.name   ""w and y""                           
# statistic   0.1666667                           
# p.value     0.9962552                           
# alternative ""two-sided""                         
# method      ""Two-sample Kolmogorov-Smirnov test""
# data.name   ""w and z""           
# ...           
</code></pre>
"
32539222,Group boxplot data while keeping their individual X axis labels in ggplot2 in R,1,3,1,"<p>I am plotting the Rank of Gene1 count (y-axis) against the Type of sample (x-axis). I wish to group the Type of samples according to their tissue of origin (Breast, Colorectum, Lung) and color-code whether they come from Cancer or Normal Tissue with Red and Green colour, respectively.</p>

<p>I produced the graph 1) BOXPLOTS WITH FACETS (please see below), which is close to my vision but shows some major issues. I have a few questions to improve on the graph:</p>

<p>[IMG]<a href=""http://i57.tinypic.com/10yfmmw.png[/IMG]"" rel=""nofollow"">http://i57.tinypic.com/10yfmmw.png[/IMG]</a></p>

<p><strong>1)</strong> Each facet ended up with 9 lanes (columns) from which many are not occupied by a box. How do I remove the lanes (columns) not occupied by the box in each facet?</p>

<p><strong>2)</strong> Can I plot this graph without using the facets while still retaining the grouping as shown on the figure?</p>

<p><strong>3)</strong> Is it possible to create the two layers of facet labels? I.e. I would like to place label ""Gene1"" above and across the existing facet labels. This would enable me to produce the same graph as shown below for the Gene2 so I could facet both graphs next to each other with ""master"" facet label on the top of each graph.</p>

<p>I hope this makes sense. Thank you everyone for your suggestions and ideas.</p>

<p>Please see the following code which will enable you to download my data and reproduce the graph:</p>

<p><strong>TEST FILE IMPORT</strong></p>

<pre><code>fileURL &lt;- ""https://dl.dropboxusercontent.com/u/4098921/testfile.csv""
test &lt;- read.csv(fileURL,header=T)
head(test)


&gt; head(test)
  Subset Tissue        Type id Gene1 Gene2
1 Normal Breast GTEx_Breast  1  5027 12597
2 Normal Breast GTEx_Breast  2  5287 12338
3 Normal Breast GTEx_Breast  3  2385 12543
4 Normal Breast GTEx_Breast  4  3174 12266
5 Normal Breast GTEx_Breast  5  6593 11350
6 Normal Breast GTEx_Breast  6  4648 10932
</code></pre>

<p><strong>1) BOXPLOT WITH FACETS</strong></p>

<pre><code>library(ggplot2)
ggplot(test,aes(x=Type, y=Gene1, fill=Subset))+
geom_boxplot(notch=T, notchwidth=0.5,outlier.shape=1,outlier.size=2, coef=1.5)+
theme(axis.text=element_text(color=""black""))+
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.4))+
theme(panel.grid.minor=element_blank())+
labs(size= ""Type"",x = """",y = ""Rank of Gene1 count"", title = ""1) BOXPLOT WITH FACETS"")+
scale_fill_manual(values=c(""red"",""lawngreen""),name=""Subset"",
labels=c(""Cancer (TCGA)"", ""Normal (GTEx)""))+
facet_grid(~Tissue)
</code></pre>
","<p>You can remove the non-used labels on the x-axis by adding the <code>scales = ""free_x""</code> to <code>facet_grid</code>. If you also add <code>space = ""free""</code> you will get boxplots of equal size. With:</p>

<pre><code>ggplot(test,aes(x=Type, y=Gene1, fill=Subset))+
  geom_boxplot(notch=T, notchwidth=0.5,outlier.shape=1,outlier.size=2, coef=1.5)+
  theme(axis.text=element_text(color=""black""))+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.4))+
  theme(panel.grid.minor=element_blank())+
  labs(size= ""Type"",x = """",y = ""Rank of Gene1 count"", title = ""1) BOXPLOT WITH FACETS"")+
  scale_fill_manual(values=c(""red"",""lawngreen""),name=""Subset"",
                    labels=c(""Cancer (TCGA)"", ""Normal (GTEx)""))+
  facet_grid(~Tissue, scales = ""free_x"", space = ""free"")
</code></pre>

<p>you will get the following plot:</p>

<p><a href=""https://i.stack.imgur.com/c1UOf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c1UOf.png"" alt=""enter image description here""></a></p>

<p>When you don't want to use facet but keep the grouping, you might want to create a new variable that takes the grouping into account. you can do that with <code>interaction</code>:</p>

<pre><code># create the new variable
test$newType &lt;- factor(interaction(test$Tissue,test$Type))
# set the correct order of the new variable
test$newType &lt;- factor(test$newType,
                       levels=levels(test$newType)[order(levels(test$newType))],
                       labels=levels(test$Type)[order(levels(test$newType))])
</code></pre>

<p>Then you can make a new plot with:</p>

<pre><code>ggplot(test,aes(x=newType, y=Gene1, fill=Subset))+
  geom_boxplot(notch=T, notchwidth=0.5,outlier.shape=1,outlier.size=2, coef=1.5)+
  theme(axis.text=element_text(color=""black""))+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.4))+
  theme(panel.grid.minor=element_blank())+
  labs(size= ""Type"",x = """",y = ""Rank of Gene1 count"", title = ""1) BOXPLOT WITH FACETS"")+
  scale_fill_manual(values=c(""red"",""lawngreen""),name=""Subset"",
                    labels=c(""Cancer (TCGA)"", ""Normal (GTEx)""))
</code></pre>

<p>which produces the following plot:</p>

<p><a href=""https://i.stack.imgur.com/SmPDL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SmPDL.png"" alt=""enter image description here""></a></p>

<p>If you want include both <code>Gene1</code> and <code>Gene2</code> in your plot, the best thing to do is first reshape your data to long format:</p>

<pre><code>library(tidyr)
test2 &lt;- test %&gt;% gather(gene,value,5:6)
</code></pre>

<p>The you can make a plot with:</p>

<pre><code>ggplot(test2,aes(x=Type, y=value, fill=Subset))+
  geom_boxplot(notch=T, notchwidth=0.5,outlier.shape=1,outlier.size=2, coef=1.5)+
  theme(axis.text=element_text(color=""black""))+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.4))+
  theme(panel.grid.minor=element_blank())+
  labs(size= ""Type"",x = """",y = ""Rank of Gene1 count"", title = ""1) BOXPLOT WITH FACETS"")+
  scale_fill_manual(values=c(""red"",""lawngreen""),name=""Subset"",
                    labels=c(""Cancer (TCGA)"", ""Normal (GTEx)""))+
  facet_grid(gene~Tissue, scales = ""free_x"", space = ""free"")
</code></pre>

<p>Above code will give you the following plot:</p>

<p><a href=""https://i.stack.imgur.com/4hGcB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4hGcB.png"" alt=""enter image description here""></a></p>

<p>An option to include the different Gene's without using an additional facet layer, you can use:</p>

<pre><code>ggplot(test2,aes(x=Type, y=value, fill=Subset, alpha=gene))+
  geom_boxplot(notch=T, notchwidth=0.5,outlier.shape=1,outlier.size=2, coef=1.5)+
  theme(axis.text=element_text(color=""black""))+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.4))+
  theme(panel.grid.minor=element_blank())+
  labs(size= ""Type"",x = """",y = ""Rank of Gene count"", title = ""BOXPLOT WITH FACETS"")+
  scale_fill_manual(values=c(""red"",""lawngreen""),name=""Subset"",
                    labels=c(""Cancer (TCGA)"", ""Normal (GTEx)""))+
  facet_grid(.~Tissue, scales = ""free_x"", space = ""free"") +
  theme_bw() +
  theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1))
</code></pre>

<p>the result:</p>

<p><a href=""https://i.stack.imgur.com/rv8fQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rv8fQ.png"" alt=""enter image description here""></a></p>
"
39596428,R time from nearest zero crossing,2,3,3,"<p>I would to add columns that are the distance in time to the closest zero crossing.  The distance in time can be negative if the point is after the zero crossings.</p>

<p>Here is a calculation and plotting of the zero crossings</p>

<pre><code># Data generation
t=seq(0,10,0.05)
h = sin(t)+3*cos(3*t)
dh = cos(t)-9*sin(3*t)
plot(t,h,type='b')

# Find indices of zero crossings

df &lt;- data.frame(h,dh)
zero_cross_down &lt;- df[-1,] &lt; 0 &amp; df[-nrow(df),] &gt;= 0
zero_cross_up &lt;- df[-1,] &gt;= 0 &amp; df[-nrow(df),] &lt; 0
indx_h_cross_down = which(zero_cross_down[,1])
indx_h_cross_up = which(zero_cross_up[,1])

# Find times of zero crossings and plot
hb&lt;-df[,""h""][indx_h_cross_down]
ha&lt;-df[,""h""][indx_h_cross_down+1]
tb&lt;-indx_h_cross_down*0.05
ta&lt;-(indx_h_cross_down+1)*0.05
tzcd_h &lt;- tb + (ta-tb)/(ha-hb)*(0-hb)
points(tb,0*tzcd_h,col=""red"",pch='v')

hb&lt;-df[,""h""][indx_h_cross_up]
ha&lt;-df[,""h""][indx_h_cross_up+1]
tb&lt;-indx_h_cross_up*0.05
ta&lt;-(indx_h_cross_up+1)*0.05
tzcu_h &lt;- tb + (ta-tb)/(ha-hb)*(0-hb)

points(tb,0*tzcu_h,col=""green"",pch='^')
</code></pre>

<p>But, now, I don't know how to calculate and a column to dh that is the time to the nearest crossing.</p>
","<p>If I interpret what the OP wants correctly, we can compute the time to the <em>next</em> nearest crossing (either up or down) for each element in <code>t</code> using:</p>

<pre><code>dt &lt;- outer(-t,c(tzcd_h,tzcu_h),""+"")
dt[dt &lt; 0] &lt;- Inf
dt_cross &lt;- do.call(pmin.int, as.data.frame(dt))
</code></pre>

<p>Notes:</p>

<ol>
<li>Use <code>outer</code> to compute the difference in time between each crossing and each <code>t</code>. The result of this is a matrix where each row corresponds to an element in <code>t</code>, each column corresponds to a crossing, and each element of this matrix is the time difference.</li>
<li>We are only interested in positive time differences (i.e., to the next crossing), so we set all negative values in the matrix to <code>Inf</code>.</li>
<li>Finally, we compute the minimum time difference for each row (i.e., each element in <code>t</code>) using a fast method for performing row-wise <code>min</code> of a matrix as detailed <a href=""https://stackoverflow.com/questions/6338517/equivalent-to-rowmeans-for-min/6338640#6338640"">in this SO answer</a>.</li>
<li>If the requirement is to compute the time difference to just the next up crossing (and not the next either up or down), then replace <code>c(tzcd_h,tzcu_h)</code> with <code>tzcu_h</code> in the <code>outer</code> computation. Similarly, if the requirement is to compute the time difference to just the next down crossing, then replace <code>c(tzcd_h,tzcu_h)</code> with <code>tzcd_h</code> in the <code>outer</code> computation. Obviously, we can also compute both of these separately.</li>
</ol>

<p>The resulting <code>dt_cross</code> is a vector of the same length as <code>t</code> that contains the minimum time to the next crossing. This vector can be added to the <code>df</code> using <code>df$dt_cross &lt;- dt_cross</code>.</p>

<pre><code>head(dt_cross)
##[1] 0.6354627 0.5854627 0.5354627 0.4854627 0.4354627 0.3854627
</code></pre>

<p>We can plot this <code>dt_cross</code> as blue dots overlaid on the plot generated by the OP's code to see what is happening:</p>

<pre><code>points(t,dt_cross,col=""blue"",pch='.')
</code></pre>

<p><a href=""https://i.stack.imgur.com/IPWpQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IPWpQ.png"" alt=""time to next crossing""></a></p>

<p>Note that the last 20 values of <code>dt_cross</code> are <code>Inf</code>. This is because the last crossing found is before these 20 values of <code>t</code>.</p>

<hr>

<h3>Update to new requirements</h3>

<p>By definition, a <em>distance</em> is a <em>metric</em> and is <em>non-negative</em>. In the following we distinguish <em>time distance</em> from <em>time difference</em>, which can be either positive, zero, or negative. If we are interested in computing the time distance to the nearest down crossing, then the computation is:</p>

<pre><code>dt_n &lt;- dt_p &lt;- outer(-t,tzcd_h,""+"")
dt_p[dt_p &lt; 0] &lt;- Inf
dt_cross_p &lt;- do.call(pmin.int, as.data.frame(dt_p))
dt_n[dt_n &gt; 0] &lt;- -Inf
dt_cross_n &lt;- do.call(pmax.int, as.data.frame(dt_n))
dt_cross &lt;- ifelse(dt_cross_p &lt; -dt_cross_n, dt_cross_p, dt_cross_n)
</code></pre>

<p>Notes:</p>

<ol>
<li>Use <code>outer</code> as before to compute the time difference matrix between each down crossing and each element in <code>t</code>. We set this result to both <code>dt_n</code> and <code>dt_p</code>.</li>
<li><code>dt_p</code> will be use to find the minimum positive time difference to a down crossing point for each element in <code>t</code>. We set all its negative values to <code>Inf</code> and compute its row-wise minimum as before. The result is a vector <code>dt_cross_p</code>, which can be interpreted as being the minimum time distance to the next down crossing for each element in <code>t</code>.</li>
<li>Conversely, <code>dt_n</code> will be use to find the maximum negative time difference to a down crossing point for each element in <code>t</code>. We set all its positive values to <code>-Inf</code> and call <code>pmax</code> likewise. The result is a vector <code>dt_cross_n</code>, which can be interpreted as being the minimum time distance (maximum negative time difference) to the previous down crossing for each element in <code>t</code>.</li>
<li>The time difference to the nearest down crossing for each element in <code>t</code> is then its corresponding element in <code>dt_cross_p</code> if <code>dt_cross_p &lt; -dt_cross_n</code> and <code>dt_cross_n</code>, otherwise. We make this computation in vectorized fashion using <code>ifelse</code>.</li>
</ol>

<p>We now have the plot:</p>

<p><a href=""https://i.stack.imgur.com/iwMgb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iwMgb.png"" alt=""delta time to nearest down crossing""></a></p>
"
28208628,Wraping of equispaced point set in a given direction,1,3,3,"<p>Let us consider a 2-D (latitude, longitude) point set. The points in the set are equispaced (approx.) with mutual distance 0.1 degree \times 0.1 degree . Each point in the set is the centre of a square grid of side length 0.1 degree (i.e., intersect point of two diagonals of the square). Each square is adjacent to the neighbouring squares.</p>

<p>Our <strong>goal</strong> is to get the coordinates of the <strong>outline polygon</strong> formed by the bounding sides of the square grids with  given direction (will be illustrated with a figure). This polygon has <strong>no hole</strong> inside. </p>

<p>Let us  consider a sample data set of size 10 (point set).</p>

<pre><code>lat_x &lt;- c(21.00749, 21.02675, 21.00396, 21.04602, 21.02317, 
           21.06524, 21.00008, 21.04247, 21.08454, 21.0192)
</code></pre>

<p>and </p>

<pre><code>lon_y &lt;- c(88.21993, 88.25369, 88.31292, 88.28740, 88.34669, 
           88.32118, 88.40608, 88.38045, 88.35494, 88.43984) 
</code></pre>

<p>Here is the rough plot of the above points followed by some illustration, <img src=""https://i.stack.imgur.com/zrhnZ.jpg"" alt=""enter image description here""></p>

<p>The black points are the (lat,lon) points in the above sample.</p>

<p>The blue square boxes are the square grid. </p>

<p>The given directions (\theta) of the squares are $theta$=50 degree.</p>

<p>Our goal is to get the ordered (clockwise or counter clockwise) co-ordinates of the outline polygon (in yellow colour). </p>

<p><strong>Note:</strong> This question is very similar to <a href=""https://stackoverflow.com/questions/27897918/wraping-of-two-dimensional-point-set"">this</a> with a nice answer given by @laune. There the goal is to get the outline polygon without direction (or 0 degree direction). But in the the present set up I need to include the direction (non-zero) while drawing the square grids and the resulted polygon. </p>

<p>I would gratefully appreciate any suggestion, java or R codes or helpful reference given by anyone solving the above problem.</p>
","<p>I would do it like this:</p>

<p><img src=""https://i.stack.imgur.com/ukjD4.png"" alt=""grid outline""></p>

<ol>
<li><p><strong>may be some 2D array point grouping to match the grid</strong></p>

<p>that should speed up all the following operations.</p></li>
<li><p><strong>compute average grid sizes (img1 from left)</strong></p>

<p>as two vectors</p></li>
<li><p><strong>create blue points (img2)</strong></p>

<p>as: <code>gray_point (+/-) 0.5*blue_vector</code></p></li>
<li><p><strong>create red points (img3)</strong></p>

<p>as: <code>blue_point (+/-) 0.5*red_vector</code></p></li>
<li><p><strong>create list of gray lines (img4)</strong></p>

<p>take all 2 original (gray) points that have distance close to average grid distance and add line for them</p></li>
<li><p><strong>create list of red lines (img4)</strong></p>

<p>take all 2 original (gray) points that have distance close to average grid distance and add line for them if it is not intersecting any line from gray lines</p></li>
<li><p><strong>reorder line points to match polygon winding ...</strong></p></li>
<li><p><strong>angle</strong></p></li>
</ol>

<p><br>compute angle of red vector (via atan2)
<br>compute angle of blue vector (via atan2)
<br>return the one with smaller absolute value</p>

<p><strong>[edit1] response to comments</strong></p>

<p><strong>grid size</strong></p>

<p>find few points that are closest to each other so pick any point and find all closest points to it. The possible distances should be near:</p>

<pre><code>sqrt(1.0)*d,sqrt(1+1)*d,sqrt(1+2)*d,sqrt(2+2)*d,...
</code></pre>

<p>where <code>d</code> is the grid size so compute <code>d</code> for few picked points. Remember the first smallest <code>d</code> found and throw away all that are not similar to smallest one. Make average of them and let call it <code>d</code></p>

<p><strong>grid vectors</strong></p>

<p>Take any point <code>A</code> and find closest point <code>B</code> to it with distance near <code>d</code>. For example <code>+/-10%</code> comparison: <code>|(|A-B|-d)|&lt;=0.1*d</code> Now the grid vector is <code>(B-A)</code>. Find few of them (picking different <code>A,B</code>) and group them by sign of <code>x,y</code> coordinates into 4 groups.</p>

<p>Then join negative direction groups together by negating one group vectors so you will have 2 list of vectors (red,blue direction) and make average vectors from them (red,blue vectors)</p>

<p><strong>shifting points</strong></p>

<p>You take any point <code>A</code> and add or substract half of red or blue <strong>vector</strong> to it (not its size!!!) for example:</p>

<pre><code>A.x+=0.5*red_vector.x;
A.y+=0.5*red_vector.y;
</code></pre>

<p><strong>line lists</strong></p>

<p>Make 2 nested <code>for</code>s per each 2 point combination <code>A,B</code> (original for gray lines,shifted red ones for red outline lines) and add condition for distance</p>

<pre><code>|(|A-B|-d)|&lt;=0.1*d
</code></pre>

<p>if it is <code>true</code> add line <code>(A,B)</code> to the list. Here pseudo C++ example:</p>

<pre><code>    int i,j,N=?;       // N is number of input points in pnt[]
    double x,y,d=?,dd=d*d,de=0.1*d; // d is the avg grid size
    double pnt[N][2]=?;    // your 2D points
    for (i=0;i&lt;N;i++)      // i - all points
     for (j=i+1;j&lt;N;j++)   // j - just the rest no need to test already tested combinations
       {
       x=pnt[i][0]-pnt[j][0];
       y=pnt[i][1]-pnt[j][1];
       if (fabs((x*x)+(y*y)-dd)&lt;=de) ... // add line pnt[i],pnt[j] to the list...
       }
</code></pre>
"
40729027,Linking Dynamic and Static Libraries Rcpp,2,2,2,"<p>I am in the process of linking using a static library and dynamic library but, I run into this error:</p>

<pre><code>/usr/bin/ld: ../src/SeqLib/bin//libseqlib.a(libseqlib_a-FermiAssembler.o): relocation R_X86_64_32S against `_ZNSs4_Rep20_S_empty_rep_storageE' can not be used when making a shared object; recompile with -fPIC
</code></pre>

<p>Here is how my Makevars file looks like:</p>

<pre><code>( cd SeqLib; ./configure --enable-shared; make)
PKG_CPPFLAGS= -I../src/SeqLib/ -I../src/SeqLib/fermi-lite/ -I../src/SeqLib/htslib/cram/ -I../src/SeqLib/htslib/htslib/ -I../src/SeqLib/htslib/ -I../src/SeqLib/bwa/
PKG_LIBS=-fPIC -enable-shared-lib -L../src/SeqLib/bin/ -lbwa -lfml -lhts -lseqlib
</code></pre>

<p>I don't understand why I am getting this error when I included the <code>-fPIC</code> option in the <code>PKG_LIBS</code> variable. </p>
","<p><code>-fPIC</code> is a <em>compile only flag</em>.  If you include it when linking, the compiler will complaint about that.  If you are using a <code>Makefile</code> to compile your project (you have not included how do you use the variables cited in your question) the best approach is to define <code>-fPIC</code> in the <code>PKG_CFLAGS</code> in case your project contains only shared object targets.  I don't know the case of a Makevars, but probably the followind discussion will give you some hints on how to solve your problem.</p>

<p>If you are building programs and shared libraries, the best approach is to define a new <code>.SUFFIXES</code> dependency (let's call it <code>.pic_o</code>) and define a variable called <code>PKG_SHAREDCFLAGS=-fPIC</code> and then include a rule to compile those dependencies as this snippet shows:</p>

<pre><code>PKG_SHAREDCPPFLAGS = -fPIC
.SUFFIXES: .pic_o
.c.pic_o:
    $(CPP) $(PKG_CPPFLAGS) $(PKG_SHAREDCPPFLAGS) -c $@ -o $&lt;

libshared.so_objs = a.pic_o b.pic_o c.pic_o
libshared.so: $(libshared.so_objs)
    $(CPP) $(PKG_LDFLAGS) $(LDFLAGS) -o $@ $(libshared.so_objs) $(libshared.so_libs)
</code></pre>

<p><code>.pic_o</code> are <code>.o</code> files compiled with the <code>PIC</code> flag (aka <strong>P</strong>osition <strong>I</strong>ndependent <strong>C</strong>ode) which is required for object files that are going to be part of a shared object.</p>

<p>It's not dangerous to compile all your code as <code>-fPIC</code>, as the linker has no problem to link PIC files statically into your code (position independent files only introduce a little overhead in your program, as the compiler reserves an offset register to control where the library has been finally loaded, and this gives you one register less to operate with)  The introduction of a new <code>.pic_o</code> suffix is to allow to produce static and dynamic versions of your library, so the static one only links objects compiled without <code>-fPIC</code> and the dynamic one uses the PIC ones.  Let's suppose you have a library, composed of <code>a.c</code>, <code>b.c</code> <code>c.c</code> files, and you want to produce a dynamic and a static library from them.  The following approach will success:</p>

<pre><code># All the targets.
targets=libA.a libA.so.3.0
TOCLEAN += $(targets)

# object files of the libA.a static library.
libA.a_objs = a.o b.o c.o
TOCLEAN += $(libA.a_objs)

# object files of the libA.so.3.0 dynamic library.
libA.so.3.0_objs = a.pic_o b.pic_o c.pic_o
TOCLEAN += $(libA.so.3.0_objs)
libA.so.3.0_libs = -lc

all: $(targets)
clean:
    rm -f $(TOCLEAN)

.PHONY: all
.SUFFIXES: .pic_o

libA.a: $(libA.a_objs)
    $(AR) -r $@ $?
    $(RANLIB) $@

.c.pic_o:
    $(CC) $(CFLAGS) -fPIC -c $&lt; -o $@

libA.so.3.0: $(libA.so.3.0_objs)
    $(LD) -shared $(libA.so.3.0_objs) $(libA.so.3.0_libs) -o $@ 
</code></pre>

<p>And execution:</p>

<pre><code>lcu@FreeBSD:~/pru_19091$ make clean all
rm -f libA.a libA.so.3.0 a.o b.o c.o a.pic_o b.pic_o c.pic_o
cc -O2 -pipe -c a.c -o a.o
cc -O2 -pipe -c b.c -o b.o
cc -O2 -pipe -c c.c -o c.o
ar -r libA.a a.o b.o c.o
ar: warning: creating libA.a
ranlib libA.a
cc -O2 -pipe -fPIC -c a.c -o a.pic_o
cc -O2 -pipe -fPIC -c b.c -o b.pic_o
cc -O2 -pipe -fPIC -c c.c -o c.pic_o
ld -shared a.pic_o b.pic_o c.pic_o -lc -o libA.so.3.0
</code></pre>

<p>Suppose I touch file <code>b.c</code>:</p>

<pre><code>lcu@FreeBSD:~/pru_19091$ touch b.c
lcu@FreeBSD:~/pru_19091$ make 
cc -O2 -pipe -c b.c -o b.o
ar -r libA.a b.o
ranlib libA.a
cc -O2 -pipe -fPIC -c b.c -o b.pic_o
ld -shared a.pic_o b.pic_o c.pic_o -lc -o libA.so.3.0
</code></pre>
"
31059973,"downloading the datasciencetoolkit.org server to use (R, Vagrant, VirtualBox)",2,2,2,"<p>I am running R on a Windows computer. I find myself making many JSON requests to datasciencetoolkit.org and I recently discovered that datasciencetoolkit.org can be downloaded to a computer as  a ""virtual server."" I imagine this could cut down on my calculation time significantly, not to mention that it won't burden someone else's servers with my queries. </p>

<p>The trouble is,  I know nothing about creating my own virtual machine. Here is the guide for beginning. I am planning on using Vagrant since I don't believe Amazon is free (please correct me if I'm wrong.)  I am going to keep it to 3 questions.</p>

<p>1) I already downloaded Vagrant and ran the step that says:</p>

<pre><code>vagrant box add dstk http://static.datasciencetoolkit.org/dstk_0.50.box
</code></pre>

<p>...from within my own computer, not a virtual machine. I installed it to my own computer I think. Did I make a mistake? Do I need to do that over again?</p>

<p>2) How do I set up Vagrant on VirtualBox? I don't even know what that should look like.</p>

<p>3) Do I need to install R on the Virtual Machine as well? I am assuming that once all is done I will just need to run my requests in the virtual machine by typing in the <a href=""http://localhost:8080"" rel=""nofollow"">http://localhost:8080</a> address, but I will cross that bridge when I get to it. Right now I just want to set up my virtual machine with the datasciencetoolkit.</p>

<p>Thank you!!</p>
","<h3>Installing Vagrant for Windows</h3>

<p>Vagrant is a tool to help you use VirtualBox more easily and consistently.
VirtualBox is a free program which hosts and manages virtual machines. Vagrant
looks for a file called <code>Vagrantfile</code>, which is a construction plan that takes a
basic or pre-configured system image, configures it according to your plan, and
then hosts it via VirtualBox.</p>

<p>I assume you have some ""power user"" comfort with Windows. If you find it
difficult to model what's going on, it may be possible that this toolchain could
bring on more pain than doing things manually.</p>

<ol>
<li><p>Download a ssh client, such as Putty. Just in case.</p></li>
<li><p>Before you do anything, make sure you have both the latest 'VirtualBox' AND
'VirtualBox Extension Pack' installed.</p></li>
<li><p>Download and install Vagrant. While installing, make sure to choose the
option to add Vagrant to your Windows PATH variable.</p></li>
<li><p>Create an empty project directory, such as at <code>C:/Projects/R/</code>, and navigate
to it.</p></li>
<li><p>Right-click inside the folder to bring up the standard system dialogue, and
select <code>Open command window here</code>. A command line should pop up.</p></li>
<li><p>Type <code>vagrant init http://static.datasciencetoolkit.org/dstk_0.50.box</code>. You
should see this success message:</p>

<pre><code> A 'Vagrantfile' has been placed in this directory. You are now ready to
 'vagrant up' your first virtual environment! Please read... [and so on].
</code></pre>

<p>Confirm that a <code>Vagrantfile</code> has indeed been created in your project
directory, which I assume to be at <code>C:/Projects/R/</code>.</p>

<p>If you receive a fail message at this step about not recognizing the command
called vagrant, then something may be wrong with your PATH variable.</p></li>
<li><p>Edit that <code>Vagrantfile</code> with a basic text editor and completely replace with this:</p>

<pre><code>Vagrant.configure(2) do |config|

    config.vm.box = ""http://static.datasciencetoolkit.org/dstk_0.50.box""
    config.vm.network ""forwarded_port"", guest:80, host:8080
    # config.ssh.username = 'vagrant'
    # config.ssh.password = 'nova'
    config.ssh.insert_key = true   # important: touch if you understand.

    config.vm.provider ""virtualbox"" do |v|
        v.gui    = false            # optionally brings up VirtualBox visual interface
        v.name   = ""vbox_vagrant""   # name that VirtualBox will use for your virtual machine
        v.memory = 2048             # megabytes ram
        v.cpus   = 2                # cores
    end

end
</code></pre></li>
<li><p>Type <code>vagrant up</code>, which will start downloading an image. Your image could
take a few hours. This command will only be slow the first time. After the
initial download, your virtual machine will then launch via VirtualBox, but
the VirtualBox visual interface won't launch because we set the option above.</p></li>
<li><p>Time to check if it works. In the command console, which should still be
pointed to the correct project directory, type in <code>vagrant ssh</code>. You should
now have command-line access to your virtual machine. Success! If you're
saying there's a server in there, then try going to your browser, visit
and the URL <code>localhost:8080</code>. I'm speculating about your situation so it may
not work.</p></li>
<li><p>To stop the virtual machine, do <code>vagrant halt</code>. To delete it, type <code>vagrant
destroy</code>. To check the status, type <code>vagrant status</code>. People create and
destroy virtual machines all the time, because once you download the 'box',
these steps become very fast.</p></li>
<li><p>I haven't checked if the image you downloaded already as R installed, but I
presume so. If not, then you need to do it on the virtual machine.</p></li>
</ol>
"
36433142,Create list of (NxN) martices with all combination of 0s and 1s R language,1,1,3,"<p>I'm totally beginner in R language and it would be nice to get some hints from you guys. </p>

<p>I need to create list of all possible combination of matrices which elements will be 0s and 1s. These matrices need to have the same number of rows and columns. </p>

<p>For example in 2x2 matrices case there will be 4^2 = 16 possibilities. </p>
","<p>Your question inspired me to try to write a generic function that can generate a list of all matrices of dimension <code>nr</code> by <code>nc</code> that can be formed with the elements of a given vector <code>x</code>. Here's the result:</p>

<pre><code>allmatrices &lt;- function(x,nr,nc,...) {
    nx &lt;- length(x);
    divs &lt;- nx^seq(0L,nr*nc-1L);
    lapply(seq(0L,nx^(nr*nc)-1L),function(i) matrix(x[i%/%divs%%nx+1L],nr,...));
}; ## end allmatrices()
</code></pre>

<hr>

<p>Here's how you can use it to generate your required binary 2x2 matrix:</p>

<pre><code>allmatrices(0:1,2L,2L);
## [[1]]
##      [,1] [,2]
## [1,]    0    0
## [2,]    0    0
## 
## [[2]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    0
## 
## [[3]]
##      [,1] [,2]
## [1,]    0    0
## [2,]    1    0
## 
## [[4]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    1    0
## 
## [[5]]
##      [,1] [,2]
## [1,]    0    1
## [2,]    0    0
## 
## [[6]]
##      [,1] [,2]
## [1,]    1    1
## [2,]    0    0
## 
## [[7]]
##      [,1] [,2]
## [1,]    0    1
## [2,]    1    0
## 
## [[8]]
##      [,1] [,2]
## [1,]    1    1
## [2,]    1    0
## 
## [[9]]
##      [,1] [,2]
## [1,]    0    0
## [2,]    0    1
## 
## [[10]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## [[11]]
##      [,1] [,2]
## [1,]    0    0
## [2,]    1    1
## 
## [[12]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    1    1
## 
## [[13]]
##      [,1] [,2]
## [1,]    0    1
## [2,]    0    1
## 
## [[14]]
##      [,1] [,2]
## [1,]    1    1
## [2,]    0    1
## 
## [[15]]
##      [,1] [,2]
## [1,]    0    1
## [2,]    1    1
## 
## [[16]]
##      [,1] [,2]
## [1,]    1    1
## [2,]    1    1
</code></pre>

<hr>

<p>Note that this results in identical output to the clever solution given by @alexis_laz in his comment:</p>

<pre><code>identical(allmatrices(0:1,2L,2L),{ n &lt;- 2; lapply(0:(((n*n)^2)-1),function(i) matrix(as.integer(head(intToBits(i),n*n)),n,n)); });
## [1] TRUE
</code></pre>

<hr>

<p>And here's another example generating a list of 2x3 matrices formed with the first three letters:</p>

<pre><code>allmatrices(letters[1:3],2L,3L);
## [[1]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""a""  ""a""
## [2,] ""a""  ""a""  ""a""
##
## [[2]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""a""  ""a""
## [2,] ""a""  ""a""  ""a""
##
## [[3]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""a""  ""a""
## [2,] ""a""  ""a""  ""a""
##
## [[4]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""a""  ""a""
## [2,] ""b""  ""a""  ""a""
##
## [[5]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""a""  ""a""
## [2,] ""b""  ""a""  ""a""
##
## [[6]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""a""  ""a""
## [2,] ""b""  ""a""  ""a""
##
## [[7]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""a""  ""a""
## [2,] ""c""  ""a""  ""a""
##
## [[8]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""a""  ""a""
## [2,] ""c""  ""a""  ""a""
##
## [[9]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""a""  ""a""
## [2,] ""c""  ""a""  ""a""
##
## [[10]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""b""  ""a""
## [2,] ""a""  ""a""  ""a""
##
## [[11]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""b""  ""a""
## [2,] ""a""  ""a""  ""a""
##
## ... snip ...
##
## [[719]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""b""  ""c""
## [2,] ""c""  ""c""  ""c""
##
## [[720]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""b""  ""c""
## [2,] ""c""  ""c""  ""c""
##
## [[721]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""c""  ""c""
## [2,] ""a""  ""c""  ""c""
##
## [[722]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""c""  ""c""
## [2,] ""a""  ""c""  ""c""
##
## [[723]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""c""  ""c""
## [2,] ""a""  ""c""  ""c""
##
## [[724]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""c""  ""c""
## [2,] ""b""  ""c""  ""c""
##
## [[725]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""c""  ""c""
## [2,] ""b""  ""c""  ""c""
##
## [[726]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""c""  ""c""
## [2,] ""b""  ""c""  ""c""
##
## [[727]]
##      [,1] [,2] [,3]
## [1,] ""a""  ""c""  ""c""
## [2,] ""c""  ""c""  ""c""
##
## [[728]]
##      [,1] [,2] [,3]
## [1,] ""b""  ""c""  ""c""
## [2,] ""c""  ""c""  ""c""
##
## [[729]]
##      [,1] [,2] [,3]
## [1,] ""c""  ""c""  ""c""
## [2,] ""c""  ""c""  ""c""
</code></pre>

<p>As you can see, this algorithm balloons out of control real fast. Be careful!</p>

<hr>

<p>In addition to parameterizing the cell values and the dimensions, I also pass variadic arguments to the <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/matrix.html"" rel=""nofollow""><code>matrix()</code></a> calls, providing a little more flexibility. So, for example, you can fill the matrices <code>byrow</code> rather than by column, and pass dimension names if you want:</p>

<pre><code>allmatrices(0:1,2L,2L,byrow=T,dimnames=list(letters[3:4],letters[1:2]));
## [[1]]
##   a b
## c 0 0
## d 0 0
##
## [[2]]
##   a b
## c 1 0
## d 0 0
##
## [[3]]
##   a b
## c 0 1
## d 0 0
##
## [[4]]
##   a b
## c 1 1
## d 0 0
##
## [[5]]
##   a b
## c 0 0
## d 1 0
##
## [[6]]
##   a b
## c 1 0
## d 1 0
##
## [[7]]
##   a b
## c 0 1
## d 1 0
##
## [[8]]
##   a b
## c 1 1
## d 1 0
##
## [[9]]
##   a b
## c 0 0
## d 0 1
##
## [[10]]
##   a b
## c 1 0
## d 0 1
##
## [[11]]
##   a b
## c 0 1
## d 0 1
##
## [[12]]
##   a b
## c 1 1
## d 0 1
##
## [[13]]
##   a b
## c 0 0
## d 1 1
##
## [[14]]
##   a b
## c 1 0
## d 1 1
##
## [[15]]
##   a b
## c 0 1
## d 1 1
##
## [[16]]
##   a b
## c 1 1
## d 1 1
</code></pre>

<hr>

<p>Performance testing:</p>

<pre><code>library(microbenchmark);
bgoldst &lt;- function() allmatrices(0:1,2L,2L);
alexis &lt;- function() { n &lt;- 2; lapply(0:(((n*n)^2)-1),function(i) matrix(as.integer(head(intToBits(i),n*n)),n,n)); };
identical(bgoldst(),alexis());
## [1] TRUE
microbenchmark(bgoldst(),alexis(),times=1000L);
## Unit: microseconds
##       expr     min      lq      mean  median       uq      max neval
##  bgoldst()  62.865  70.136  80.15788  73.130  77.4060 1029.362  1000
##   alexis() 187.741 205.702 229.48292 217.677 226.8705 1261.150  1000
</code></pre>

<p><strike>It's likely the <code>intToBits()</code>/<code>as.integer()</code> conversion that's costing @alexis_laz's solution some speed.</strike></p>

<p><strong>Edit:</strong> Well-played, @alexis_laz! You win this round:</p>

<pre><code>alexis &lt;- function() { n &lt;- 2; lapply(0:(((n*n)^2)-1),function(i) matrix(as.integer(intToBits(i)[seq_len(n*n)]),n,n)); };
identical(bgoldst(),alexis());
## [1] TRUE
microbenchmark(bgoldst(),alexis(),times=1000L);
## Unit: microseconds
##       expr    min     lq     mean median      uq      max neval
##  bgoldst() 64.148 70.136 77.98883 72.702 77.4055 1093.083  1000
##   alexis() 48.325 54.313 63.70390 56.878 61.1550 3271.121  1000
</code></pre>
"
11105131,Cannot install R-forge package using install.packages,2,2,2,"<p><a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/10265"">This</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/10022"">question</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/9715"">is</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/9359"">asked</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/9169/focus=9170"">over</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/9135"">and</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/8645"">over</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/7965"">and</a>, <a href=""http://thread.gmane.org/gmane.comp.lang.r.finance/7173"">over</a>, 
on the <a href=""https://stat.ethz.ch/mailman/listinfo/r-sig-finance"">R-sig-finance</a> mailing list, but I do not think it has been asked on stackoverflow.</p>

<p>It goes like this:</p>

<p>Where can I obtain the latest version of package XYZ that is hosted on R-forge?  I tried to install it with <code>install.packages</code>, but this is what happened:</p>

<pre><code>&gt; install.packages(""XYZ"",repos=""http://r-forge.r-project.org"")
Warning message: package ‘XYZ’ is not available (for R version 2.15.0)
</code></pre>

<p>Looking on the R-forge website for XYZ, I see that the package failed to build.
Therefore, there is no link to download the source.  Is there any other way 
to get the source code?  Once I get the source code, how can I turn that into a 
package that I can load with <code>library(""XYZ"")</code>?</p>
","<p>R-Forge may fail to build a package for a few different reasons.  It could be that 
the documentation has not been updated to reflect recent changes in the code.  Or,
it could be that some of the dependencies were not available at build time.</p>

<p>You can checkout the source code using svn.  First, search for the project on the
R-Forge website and go to the project home page -- for example <a href=""http://r-forge.r-project.org/projects/returnanalytics/"" rel=""nofollow noreferrer"">http://r-forge.r-project.org/projects/returnanalytics/</a>
Click the SCM link to get to a page like this  <a href=""http://r-forge.r-project.org/scm/?group_id=579"" rel=""nofollow noreferrer"">http://r-forge.r-project.org/scm/?group_id=579</a> </p>

<p>This page will tell you the command to use to checkout the project.  In this case you get</p>

<blockquote>
  <p>This project's SVN repository can be checked out through anonymous access with the following command(s).</p>
  
  <p>svn checkout svn://svn.r-forge.r-project.org/svnroot/returnanalytics/</p>
</blockquote>

<p>If you are on Windows, you probably want to download and install <a href=""http://tortoisesvn.net/"" rel=""nofollow noreferrer"">TortoiseSVN</a></p>

<p>Once you have installed TortoiseSVN, you can right click in a Windows Explorer window and select
""SVN checkout"".  In the ""URL of repository:"" field, enter everything except the 
""svn checkout "" part of the command that you found on R-Forge.  In this case, you'd
enter ""svn://svn.r-forge.r-project.org/svnroot/returnanalytics/"".</p>

<p>When you click OK, the project will be downloaded into the current directory.</p>

<p>If you are on a UNIX-alike system (or if you installed the command line client tools 
when you installed TortoiseSVN for Windows, which is not the default), you can 
type the command that R-forge gave you in your terminal (System terminal, not the R terminal)</p>

<pre><code>svn checkout svn://svn.r-forge.r-project.org/svnroot/returnanalytics/
</code></pre>

<p>That will create a new directory under the current working directory that 
contains all of the files in the package.  In the top level of that directory
will be a subdirectory called ""pkg"".  This particular project (returnanalytics)
contains more than one package. </p>

<pre><code>ls returnanalytics/pkg
#FactorAnalytics  MPO  PApages  PerformanceAnalytics  PortfolioAnalytics
</code></pre>

<p>But some R-forge projects only have a single package. e.g. </p>

<pre><code>svn checkout svn://svn.r-forge.r-project.org/svnroot/random/
#Checked out revision 14.
ls random/pkg
#DESCRIPTION  inst  man  NAMESPACE  R
</code></pre>

<p>Now that you have a local copy all of the code, if you would like to be able to 
install the package, you have to build it first.  </p>

<p>A WORD OF CAUTION:  Since R-Forge failed to build the package, there is a good chance
that there are problems with the package.  Therefore, if you just build it, you may find
that some things do not work as expected.  In particular, it is likely that there
is missing or incomplete documentation. </p>

<p>If you are on a UNIX-alike system, the package can be built and installed relatively easily.  For a multi-package project like returnanalytics, if you want to install, e.g. the
PortfolioAnalytics package, you can do it like this</p>

<pre><code>R --vanilla CMD INSTALL --build returnanalytics/pkg/PortfolioAnalytics 
</code></pre>

<p>""PortfolioAnalytics"" is the name of the directory that contains the package that
you want to build/install.  For a single-package project, you can build and install like
this</p>

<pre><code>R --vanilla CMD INSTALL --build random/pkg
</code></pre>

<p>If you would like to build/install a package on Windows, see <a href=""https://stackoverflow.com/questions/4739837/how-do-i-install-an-r-package-from-the-source-tarball-on-windows"">this question</a> and follow the <a href=""http://cran.r-project.org/doc/manuals/R-admin.html#Windows-packages"" rel=""nofollow noreferrer"">two</a> <a href=""http://cran.r-project.org/doc/manuals/R-admin.html#The-Windows-toolset"" rel=""nofollow noreferrer"">links</a> that @JoshuaUlrich provided</p>

<p>More information can be found in <a href=""http://cran.r-project.org/doc/manuals/R-admin.html#Add_002don-packages"" rel=""nofollow noreferrer"">R Installation and Administration</a>, <a href=""http://download.r-forge.r-project.org/R-Forge_Manual.pdf"" rel=""nofollow noreferrer"">the R-Forge User Manual</a>, and <a href=""http://svnbook.red-bean.com/"" rel=""nofollow noreferrer"">the SVN manual</a>.</p>
"
37095844,parsing xml file and its elements in R,1,1,4,"<p>I am trying to parse a XML file that has the following structure</p>

<pre><code> &lt;xs:schema attributeFormDefault=""qualified"" elementFormDefault=""qualified"" targetNamespace=""http://12345hc.com/xsd""&gt;
            &lt;xs:complexType name=""Context""&gt;
                &lt;xs:sequence&gt;
                    &lt;xs:element minOccurs=""0"" name=""aNum"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""aId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""bURI"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""facility"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""fSessionId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""ID"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""pwrd"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""profileID"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""sToken"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""sId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""tId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""uNum"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""webURI"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""xZRT"" nillable=""true"" type=""xs:string""/&gt;
                &lt;/xs:sequence&gt;
            &lt;/xs:complexType&gt;
        &lt;/xs:schema&gt;
</code></pre>

<p>I trying to select the node having <code>name=""Context""</code> tag, </p>

<p>within this tag select the element labeled as ID,<br>
<code>&lt;xs:element minOccurs=""0"" name=""ID"" nillable=""true"" type=""xs:string""/&gt;</code> </p>

<p>and add value ""111111"" to this element <code>ID</code>. </p>

<p>Any pointers/answers on accomplishing this will be very helpful. Thanks in advance.</p>
","<p>You could do</p>

<pre><code>txt &lt;- '&lt;xs:schema attributeFormDefault=""qualified"" xmlns:xs=""http://www.w3.org/2001/XMLSchema"" elementFormDefault=""qualified"" targetNamespace=""http://12345hc.com/xsd""&gt;
            &lt;xs:complexType name=""Context""&gt;
                &lt;xs:sequence&gt;
                    &lt;xs:element minOccurs=""0"" name=""aNum"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""aId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""bURI"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""facility"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""fSessionId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""ID"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""pwrd"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""profileID"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""sToken"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""sId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""tId"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""uNum"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""webURI"" nillable=""true"" type=""xs:string""/&gt;
                    &lt;xs:element minOccurs=""0"" name=""xZRT"" nillable=""true"" type=""xs:string""/&gt;
                &lt;/xs:sequence&gt;
            &lt;/xs:complexType&gt;
        &lt;/xs:schema&gt;'
library(XML)
xml &lt;- xmlParse(txt, asText=TRUE)
ns &lt;- getNodeSet(xml, '//*[@name=""Context""]/xs:sequence/xs:element')
id &lt;- which(sapply(ns, xmlGetAttr, ""name"") == ""ID"")
xmlValue(ns[[id]]) &lt;- ""11111""
xml
# &lt;?xml version=""1.0""?&gt;
# &lt;xs:schema xmlns:xs=""http://www.w3.org/2001/XMLSchema"" attributeFormDefault=""qualified"" elementFormDefault=""qualified"" targetNamespace=""http://12345hc.com/xsd""&gt;
#   &lt;xs:complexType name=""Context""&gt;
#     &lt;xs:sequence&gt;
#       &lt;xs:element minOccurs=""0"" name=""aNum"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""aId"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""bURI"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""facility"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""fSessionId"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""ID"" nillable=""true"" type=""xs:string""&gt;11111&lt;/xs:element&gt;
#       &lt;xs:element minOccurs=""0"" name=""pwrd"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""profileID"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""sToken"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""sId"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""tId"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""uNum"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""webURI"" nillable=""true"" type=""xs:string""/&gt;
#       &lt;xs:element minOccurs=""0"" name=""xZRT"" nillable=""true"" type=""xs:string""/&gt;
#     &lt;/xs:sequence&gt;
#   &lt;/xs:complexType&gt;
# &lt;/xs:schema&gt;
</code></pre>
"
31311688,Merging two irregular zoo time-series is messing up the structure,1,1,1,"<p>I am working with trade dataset with thousands of rows. Every record has a unique key based on a symbol and date. Trade records for a given symbol are irregular, hence using zoo will be natural choice. I need to use lag and merge to create a new dataset. However, I don't know how to setup multi-column index in zoo in order to use lag function. Below is a sample dataset and intended output.</p>

<pre><code>df = data.frame(
    dt = as.Date(c(""2015-01-01"", ""2015-01-05"", ""2015-01-06"",
                   ""2015-01-01"", ""2015-01-02"")),
    id = c(""i1"", ""i1"", ""i1"", ""i2"", ""i2""),
    v1 = c(110, 115, 119, 212, 213),
    v2 = c(100, 170, 180, 202, 210),
    v3 = c(11, 13, 16, 22, 24)
)
df$id = as.character(df$id)
</code></pre>

<p>And the output should be</p>

<pre><code>2015-01-01, i1, 110, 100, 11, 2015-01-05, i1, 115, 170, 13 
2015-01-05, i1, 115, 170, 13, 2015-01-06, i1, 119, 180, 16 
2015-01-06, i1, 119, 180, 16, NA, NA, NA, NA, NA
2015-01-01, i2, 212, 202, 22, 2015-01-02, i2, 213, 210, 24 
2015-01-02, i2, 213, 210, 24, NA, NA, NA, NA, NA
</code></pre>

<p>Point to note that I need to merge complete rows regardless of number of columns. Following is one possible way to solve the ""grouped"" lag operation based on zoo which will merge complete rows.</p>

<pre><code>doProcessing = function(df){
  icolnames = colnames(df)
  tt = zoo(df, df$dt)
  tt1 = merge(tt, lag(tt, 1))
  colnames(tt1) = c(icolnames, paste0(""lag_"", icolnames))
  data.frame(tt1, stringsAsFactors=F)
}
fin_df = do.call(rbind, with(df, by(df, list(id), doProcessing, simplify=F)))
</code></pre>

<p>This final output frame has every field as factor, which is different than the original data frame.</p>

<pre><code>&gt; str(df)
'data.frame':   5 obs. of  5 variables:
 $ dt: Date, format: ""2015-01-05"" ""2015-01-01"" ...
 $ id: chr  ""i1"" ""i1"" ""i1"" ""i2"" ...
 $ v1: num  115 110 119 212 213
 $ v2: num  170 100 180 202 210
 $ v3: num  13 11 16 22 24
</code></pre>

<p>resultant data frame looks like</p>

<pre><code>&gt; str(fin_df)
'data.frame':   5 obs. of  10 variables:
 $ dt    : Factor w/ 4 levels ""2015-01-01"",""2015-01-05"",..: 1 2 3 1 4
 $ id    : Factor w/ 2 levels ""i1"",""i2"": 1 1 1 2 2
 $ v1    : Factor w/ 5 levels ""110"",""115"",""119"",..: 1 2 3 4 5
 $ v2    : Factor w/ 5 levels ""100"",""170"",""180"",..: 1 2 3 4 5
 $ v3    : Factor w/ 5 levels ""11"",""13"",""16"",..: 1 2 3 4 5
 $ lag_dt: Factor w/ 3 levels ""2015-01-05"",""2015-01-06"",..: 1 2 NA 3 NA
 $ lag_id: Factor w/ 2 levels ""i1"",""i2"": 1 1 NA 2 NA
 $ lag_v1: Factor w/ 3 levels ""115"",""119"",""213"": 1 2 NA 3 NA
 $ lag_v2: Factor w/ 3 levels ""170"",""180"",""210"": 1 2 NA 3 NA
 $ lag_v3: Factor w/ 3 levels ""13"",""16"",""24"": 1 2 NA 3 NA
</code></pre>

<p>What am I doing wrong and How do I get the correct structure as per original data frame?</p>

<p>I asked this question as per this link <a href=""https://stackoverflow.com/questions/30845134/multipart-index-in-zoo-timeseries"">multipart index in zoo timeseries</a>
However I messed up that thread badly, hence didn't receive any response. Need to fix this in a correct way as manual fixing is not elegant and not the ""R"" way of doing things.</p>
","<p>You have an overlap in indexes for the groups. To avoid a lot of missings, a solution is to use a list containing every id as it's own time-series (<code>zoo</code> objects):</p>

<pre><code>&gt;  myTsList &lt;- tapply(1:nrow(df), df$id, function(x) { zoo::zoo(df[x, ], df$dt[x]) } )
&gt;  myTsList 
$i1
           dt         id v1  v2  v3
2015-01-01 2015-01-01 i1 110 100 11
2015-01-05 2015-01-05 i1 115 170 13
2015-01-06 2015-01-06 i1 119 180 16

$i2
           dt         id v1  v2  v3
2015-01-01 2015-01-01 i2 212 202 22
2015-01-02 2015-01-02 i2 213 210 24
</code></pre>

<p>Then you can easily do the <code>grouped lag</code> as you are talking about:</p>

<pre><code>&gt;  res &lt;- lapply(myTsList, function(x) merge(x, lag(x), suffixes=c("""",""lag"")) )
&gt;  res
$i1
           dt.        id. v1. v2. v3. dt.lag     id.lag v1.lag v2.lag v3.lag
2015-01-01 2015-01-01 i1  110 100 11  2015-01-05 i1     115    170    13    
2015-01-05 2015-01-05 i1  115 170 13  2015-01-06 i1     119    180    16    
2015-01-06 2015-01-06 i1  119 180 16  &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  

$i2
           dt.        id. v1. v2. v3. dt.lag     id.lag v1.lag v2.lag v3.lag
2015-01-01 2015-01-01 i2  212 202 22  2015-01-02 i2     213    210    24    
2015-01-02 2015-01-02 i2  213 210 24  &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt; 
</code></pre>

<p>of course you can then <code>bind</code> the groups if you want to have a <code>data.frame</code> structure, but we need to convert them first because of the overlap in indexes: </p>

<pre><code>&gt; Reduce(rbind, lapply(res, as.data.frame))
                   dt. id. v1. v2. v3.     dt.lag id.lag v1.lag v2.lag v3.lag
2015-01-01  2015-01-01  i1 110 100  11 2015-01-05     i1    115    170     13
2015-01-05  2015-01-05  i1 115 170  13 2015-01-06     i1    119    180     16
2015-01-06  2015-01-06  i1 119 180  16       &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
2015-01-011 2015-01-01  i2 212 202  22 2015-01-02     i2    213    210     24
2015-01-02  2015-01-02  i2 213 210  24       &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
</code></pre>

<p><strong>EDIT:</strong> If you don't need the time-series at all, but only the final output as a <code>data.frame</code>, then inspired by my suggestion you could do something along:</p>

<pre><code>df$ind &lt;- 1:nrow(df)
myTsList &lt;- tapply(1:nrow(df), df$id, function(x) zoo::zoo(df[x, ""ind""], df$dt[x])  )
res &lt;- lapply(myTsList, function(x) merge(x, lag(x)) )
newDf&lt;- Reduce(rbind, lapply(res, as.data.frame))
df$ind &lt;- NULL
as.data.frame(cbind(df[newDf[,1],],df[newDf[,2],]))

          dt id  v1  v2 v3         dt   id  v1  v2 v3
1 2015-01-01 i1 110 100 11 2015-01-05   i1 115 170 13
2 2015-01-05 i1 115 170 13 2015-01-06   i1 119 180 16
3 2015-01-06 i1 119 180 16       &lt;NA&gt; &lt;NA&gt;  NA  NA NA
4 2015-01-01 i2 212 202 22 2015-01-02   i2 213 210 24
5 2015-01-02 i2 213 210 24       &lt;NA&gt; &lt;NA&gt;  NA  NA NA
</code></pre>

<p>this will also keep the correct classes etc. from the original <code>data.frame</code>.</p>

<p><strong>EDIT*</strong> A simpler <code>dplyr</code> solution:</p>

<pre><code>library(dplyr)
merge( 
    df,
    df %&gt;% group_by(id) %&gt;% mutate(lag=lag(dt)), 
    by.x=c(""id"",""dt""), by.y=c(""id"",""lag""), all.x=TRUE
)

  id         dt v1.x v2.x v3.x         dt v1.y v2.y v3.y
1 i1 2015-01-01  110  100   11 2015-01-05  115  170   13
2 i1 2015-01-05  115  170   13 2015-01-06  119  180   16
3 i1 2015-01-06  119  180   16       &lt;NA&gt;   NA   NA   NA
4 i2 2015-01-01  212  202   22 2015-01-02  213  210   24
5 i2 2015-01-02  213  210   24       &lt;NA&gt;   NA   NA   NA
</code></pre>
"
46262194,Pairwise distance calculation nested data frame,1,1,1,"<p>I am looking for a way to calculate the separation distance between points in a pairwise fashion and store the results for each individual point in an accompanying nested data frame.</p>

<p>For example, I have this data frame (from the maps package) that contains information about us cities including their physical locations. I have discarded the rest of the information and nested the coordinates in a nested data frame. I intend to use <code>distHaversine()</code> from the <code>geosphere</code> package to calculate these distances.</p>

<pre><code>library(tidyverse)

df &lt;- maps::us.cities %&gt;% 
  slice(1:20) %&gt;% 
  group_by(name) %&gt;% 
  nest(long, lat, .key = coords)

                   name            coords
                  &lt;chr&gt;           &lt;list&gt;
 1           Abilene TX &lt;tibble [1 x 2]&gt;
 2             Akron OH &lt;tibble [1 x 2]&gt;
 3           Alameda CA &lt;tibble [1 x 2]&gt;
 4            Albany GA &lt;tibble [1 x 2]&gt;
 5            Albany NY &lt;tibble [1 x 2]&gt;
 ...(With 15 more rows)
</code></pre>

<p>I have looked into using the map family of functions coupled with mutate, but I am having a difficult time. The desired results are in the form as follows:</p>

<pre><code>                   name            coords            sep_dist
                  &lt;chr&gt;           &lt;list&gt;            &lt;list&gt;
 1           Abilene TX &lt;tibble [1 x 2]&gt; &lt;tibble [19 x 2]&gt;
 2             Akron OH &lt;tibble [1 x 2]&gt; &lt;tibble [19 x 2]&gt;
 3           Alameda CA &lt;tibble [1 x 2]&gt; &lt;tibble [19 x 2]&gt;
 4            Albany GA &lt;tibble [1 x 2]&gt; &lt;tibble [19 x 2]&gt;
 5            Albany NY &lt;tibble [1 x 2]&gt; &lt;tibble [19 x 2]&gt;
 ...(With 15 more rows)
</code></pre>

<p>With the sep_dist tibbles looking something like this:</p>

<pre><code>               location  distance
                  &lt;chr&gt;     &lt;dbl&gt; 
 1             Akron OH      1003
 2           Alameda CA       428
 3            Albany GA      3218
 4            Albany NY      3627
 5            Albany OR        97
 ...(With 14 more rows)                       -distances completely made up
</code></pre>

<p>Where location is the point that is being compared to name (in this case Abilene).</p>
","<p>We can expand a ""grid"" with all the combination of location name and coordinates, but remove the combination with the same location name. After that, use <code>map2_dbl</code> to apply the <code>distHaversine</code> function.</p>

<pre><code>library(tidyverse)
library(geosphere)

df2 &lt;- df %&gt;%
  # Create the grid
  mutate(name1 = name) %&gt;%
  select(starts_with(""name"")) %&gt;%
  complete(name, name1) %&gt;%
  filter(name != name1) %&gt;%
  left_join(df, by = ""name"") %&gt;%
  left_join(df, by = c(""name1"" = ""name"")) %&gt;%
  # Grid completed. Calcualte the distance by distHaversine
  mutate(distance = map2_dbl(coords.x, coords.y, distHaversine))

df2
# A tibble: 380 x 5
         name          name1         coords.x         coords.y  distance
        &lt;chr&gt;          &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;     &lt;dbl&gt;
 1 Abilene TX       Akron OH &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 1881904.4
 2 Abilene TX     Alameda CA &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 2128576.9
 3 Abilene TX      Albany GA &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 1470577.2
 4 Abilene TX      Albany NY &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 2542025.1
 5 Abilene TX      Albany OR &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 2429367.3
 6 Abilene TX Albuquerque NM &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt;  702287.5
 7 Abilene TX  Alexandria LA &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt;  700093.2
 8 Abilene TX  Alexandria VA &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 2161594.6
 9 Abilene TX    Alhambra CA &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 1718967.5
10 Abilene TX Aliso Viejo CA &lt;tibble [1 x 2]&gt; &lt;tibble [1 x 2]&gt; 1681868.8
# ... with 370 more rows
</code></pre>

<p>To create the final output, we can <code>group_by</code> based on name and <code>nest</code> all the other desired columns.</p>

<pre><code>df3 &lt;- df2 %&gt;%
  select(-starts_with(""coord"")) %&gt;%
  group_by(name) %&gt;%
  nest()

df3
# A tibble: 20 x 2
                   name              data
                  &lt;chr&gt;            &lt;list&gt;
 1           Abilene TX &lt;tibble [19 x 2]&gt;
 2             Akron OH &lt;tibble [19 x 2]&gt;
 3           Alameda CA &lt;tibble [19 x 2]&gt;
 4            Albany GA &lt;tibble [19 x 2]&gt;
 5            Albany NY &lt;tibble [19 x 2]&gt;
 6            Albany OR &lt;tibble [19 x 2]&gt;
 7       Albuquerque NM &lt;tibble [19 x 2]&gt;
 8        Alexandria LA &lt;tibble [19 x 2]&gt;
 9        Alexandria VA &lt;tibble [19 x 2]&gt;
10          Alhambra CA &lt;tibble [19 x 2]&gt;
11       Aliso Viejo CA &lt;tibble [19 x 2]&gt;
12             Allen TX &lt;tibble [19 x 2]&gt;
13         Allentown PA &lt;tibble [19 x 2]&gt;
14             Aloha OR &lt;tibble [19 x 2]&gt;
15          Altadena CA &lt;tibble [19 x 2]&gt;
16 Altamonte Springs FL &lt;tibble [19 x 2]&gt;
17           Altoona PA &lt;tibble [19 x 2]&gt;
18          Amarillo TX &lt;tibble [19 x 2]&gt;
19              Ames IA &lt;tibble [19 x 2]&gt;
20           Anaheim CA &lt;tibble [19 x 2]&gt;
</code></pre>

<p>And each data frame in the <code>data</code> now looks like this.</p>

<pre><code>df3$data[[1]]
# A tibble: 19 x 2
                  name1  distance
                  &lt;chr&gt;     &lt;dbl&gt;
 1             Akron OH 1881904.4
 2           Alameda CA 2128576.9
 3            Albany GA 1470577.2
 4            Albany NY 2542025.1
 5            Albany OR 2429367.3
 6       Albuquerque NM  702287.5
 7        Alexandria LA  700093.2
 8        Alexandria VA 2161594.6
 9          Alhambra CA 1718967.5
10       Aliso Viejo CA 1681868.8
11             Allen TX  296560.4
12         Allentown PA 2342363.5
13             Aloha OR 2457938.8
14          Altadena CA 1719207.6
15 Altamonte Springs FL 1805480.9
16           Altoona PA 2102993.0
17          Amarillo TX  361520.0
18              Ames IA 1194234.7
19           Anaheim CA 1694698.9
</code></pre>
"
37046256,Speed up simple R code (vectorize?),2,3,3,"<p>I have two positive integer vectors specifying start and end ""positions"" of ranges</p>

<pre><code>starts &lt;- sample(10^6,replace = T)
ends &lt;- starts+sample(100:1000,length(starts),replace=T)
</code></pre>

<p>So these specify 1000000 ranges that are 100 to 1000 units long. 
Now I want to know how many times a position (positive integer) is ""covered"" by a range. For this I do:</p>

<pre><code>coverage &lt;- integer(max(ends))
for(i in seq(length(starts))) {
      coverage[starts[i]:ends[i]] &lt;- coverage[starts[i]:ends[i]] + 1 
}
</code></pre>

<p>But because of the for loop, it's relatively slow. For billions of ranges, it can take a very long time. 
I cannot find a way to vectorize this code. I could split the work and use multiple CPUs, but the speed gain would be marginal. apply, lapply and other meta-functions do not improve speed (as expected). For instance</p>

<pre><code>coverage &lt;- tabulate(unlist(Map(':', starts,ends)))
</code></pre>

<p>is also slow because of the ""Map"" part. I fear it also takes more memory.</p>

<p>Any ideas? </p>
","<p>You could keep a count of ranges that start and end at any specific index and then apply a cumulative sum over the difference of these.</p>

<ol>
<li>Aggregate the number of ranges that start at each index</li>
<li>Aggregate the number of ranges that end at one position before each index (if <code>ends</code> are inclusive)</li>
<li>Calculate the net change: <code>count of starts - count of ends</code></li>
<li>Loop over indexes and sum up the net changes cumulatively. This will give the number ranges that started earlier than this index and not ended yet at this index.</li>
</ol>

<p>The ""covered"" number is equal to this cumulative sum at each index.</p>

<p>I tried this approach using sparse vectors to cut down on memory usage. Although it may be faster with normal vectors, not sure.
With <code>sparseVector</code> it was 5.7x faster than the loop approach for the given example.</p>

<pre><code>library(Matrix)

set.seed(123)

starts &lt;- sample(10^6,replace = T)
ends &lt;- starts+sample(100:1000,length(starts),replace=T)

v.cov &lt;- NULL
fun1 &lt;- function() {
  coverage &lt;- integer(max(ends))
  for(i in seq(length(starts))) {
    coverage[starts[i]:ends[i]] &lt;- coverage[starts[i]:ends[i]] + 1 
  }
  v.cov &lt;&lt;- coverage
}
# Testing ""for loop"" approach
system.time(fun1())
# user  system elapsed 
# 21.84    0.00   21.83 

v.sum &lt;- NULL
fun2 &lt;- function() {      
  # 1. Aggregate the number of ranges that start at each index
  t.starts &lt;- table(starts)
  i.starts &lt;- strtoi(names(t.starts))
  x.starts &lt;- as.vector(t.starts)
  sv.starts &lt;- sparseVector(x=x.starts, i=i.starts, length=max(ends)+1)  # to match length of sv.ends below
  # 2. Aggregate the number of ranges that end at one position before each index
  t.ends &lt;- table(ends)
  i.ends &lt;- strtoi(names(t.ends))+1  # because ""ends"" are inclusive 
  x.ends &lt;- as.vector(t.ends)
  sv.ends &lt;- sparseVector(x=x.ends, i=i.ends, length=max(ends)+1)

  sv.diff &lt;- sv.starts - sv.ends
  v.sum &lt;&lt;- cumsum(sv.diff)[1:max(ends)]  # drop last element
}
# Testing ""cumulative sum"" approach
system.time(fun2())
# user  system elapsed 
# 3.828   0.000   3.823

identical(v.cov, v.sum)
# TRUE
</code></pre>

<p>Also, there is probably a better way to extract x's and i's for <code>sparseVector</code> constructor than using <code>table</code> and <code>strtoi(names(x))</code>that may boost speed further.</p>

<p><strong>EDIT</strong></p>

<p>Avoid <code>strtoi</code> using a 1-column <code>sparseMatrix</code> instead</p>

<pre><code>v.sum.mat &lt;- NULL
fun3 &lt;- function() {
  v.ones &lt;- rep(1, length(starts))
  m.starts &lt;- sparseMatrix(i=starts, j=v.ones, x=v.ones, dims=c(max(ends)+1,1))
  m.ends &lt;- sparseMatrix(i=ends+1, j=v.ones, x=v.ones, dims=c(max(ends)+1,1))
  m.diff &lt;- m.starts - m.ends
  v.sum.mat &lt;&lt;- cumsum(m.diff[,1])[1:max(ends)]
}
# Testing ""cumulative sum"" approach using matrix
system.time(fun3())
#   user  system elapsed 
#  0.456   0.028   0.486 

identical(v.cov, v.sum.mat)
# TRUE
</code></pre>

<p><strong>EDIT 2 - super fast, super short</strong></p>

<p>Based on comment by @alexis_laz, thank you!</p>

<pre><code>fun4 &lt;- function() {
  cumsum(tabulate(starts, max(ends) + 1L) - tabulate(ends + 1L, max(ends) + 1L))[1:max(ends)]
}
system.time(v.sum.tab &lt;- fun4())
# user  system elapsed 
# 0.040   0.000   0.041 

identical(as.integer(v.cov), v.sum.tab)
# TRUE
</code></pre>
"
6849410,What are the key components and functions for standard model objects in R?,2,2,2,"<p>I have implemented a new statistical model in R and it works in my sandbox, but I would like to make it more standard.  A good comparison is <code>lm()</code>, where I can take a model object and:</p>

<ul>
<li>apply the <code>summary()</code> function</li>
<li>extract the coefficients of the model</li>
<li>extract residuals from the fitted (training) data</li>
<li>update the model</li>
<li>apply the <code>predict()</code> function</li>
<li>apply <code>plot()</code> to pre-selected descriptive plots</li>
<li>engage in many other kinds of joy</li>
</ul>

<p>I've looked through the R manuals, searched online, and thumbed through several books, and, unless I'm overlooking something, I can't find a good tutorial on what should go into a new model package.</p>

<p>Although I'm most interested in thorough references or guides, I'll keep this post focused on a question with two components:</p>

<ol>
<li>What are the key components that are usually expected to be in a model object?</li>
<li>What are typical functions that are usually implemented in a modeling package?</li>
</ol>

<p>Answers could be from the R Core (or package developers) perspective or from the perspective of users, e.g. users expect to be able to use functions like summary, predict, residuals, coefficients, and often expect to pass a formula when fitting a model.</p>
","<p>Put into the object what you think is useful and necessary. I think a more important Question is how do you include this information, as well as how one accesses it.</p>

<p>At a minimum, provide a <code>print()</code> method so the entire object doesn't get dumped to the screen when you print the object. If you provide a <code>summary()</code> method, the convention is to have that object return an object of class <code>summary.foo</code> (where <code>foo</code> is your class) and then provide a <code>print.summary.foo()</code> method --- you don't want your <code>summary()</code> method doing any printing in and of itself.</p>

<p>If you have coefficients, fitted values and residuals and these are simple, then you can store them in your returned object as <code>$coefficients</code>, <code>$fitted.values</code> and <code>$residuals</code> respectively. Then the default methods for <code>coef()</code>, <code>fitted()</code> and <code>resid()</code> will work without you needing to add your own bespoke methods. If these are not simple, then provide your own methods for <code>coef()</code>, <code>fitted.values()</code> and <code>residuals()</code> for your class. By not simple, I mean, for example, if there are several types of residual and you need to process the stored residuals to get the requested type --- then you need your own method that takes a <code>type</code> argument or similar to select from the available types of residual. See <code>?residuals.glm</code> for an example.</p>

<p>If predictions are something that can be usefully provided, then a <code>predict()</code> method could be provided. Look at the <code>predict.lm()</code> method for example to see what arguments should be taken. Likewise, an <code>update()</code> can be provided if it makes sense to update the model by adding/removing terms or altering model parameters.</p>

<p><code>plot.lm()</code> gives an example of a method that provides several diagnostics plots of the fitted model. You could model your method on that function to select from a set of predefined diagnostics plots.</p>

<p>If your model has a likelihood, then providing a <code>logLik()</code> method to compute or extract it from the fitted model object would be standard, <code>deviance()</code> is another similar function if such a thing is pertinent. For confidence intervals on parameters, <code>confint()</code> is the standard method.</p>

<p>If you have a formula interface, then <code>formula()</code> methods can extract it. If you store it in a place that the default method searches for, then your life will be made easier. A simple way to store this is to store the matched call (<code>match.call()</code>) in the <code>$call</code> component. Methods to extract the model frame (<code>model.frame()</code>) and model matrix (<code>model.matrix()</code>) that are the data and the expanded (factors converted to variables using contrasts, plus any transformations or functions of the model frame data) model matrix are standard extractor functions. Look at examples from standard R modelling functions for ideas on how to store/extract this information.</p>

<p>If you do use a formula interface, try to follow the standard, non-standard evaluation method used in most R model objects that have a formula interface/method. You can find details of that on the <a href=""http://developer.r-project.org/"">R Developer</a> page, in particular the <a href=""http://developer.r-project.org/nonstandard-eval.pdf"">document</a> by Thomas Lumley. This gives plenty of advice on making your function work like one expects an R modelling function to work.</p>

<p>If you follow this paradigm, then extractors like <code>na.action()</code> should just work if you follow the standard (non-standard) rules.</p>
"
6699596,How to detemine the coding of accents?,2,1,4,"<p>This question is related to this previous <a href=""https://stackoverflow.com/questions/6697757/replace-accents-in-string-vector-with-latex-code"">one</a> on how to replace accented strings like <code>México</code> with equivalent <code>Latex</code> code <code>M\'{e}xico</code>.</p>

<p>My problem here is slightly different.  I am using a third party database with string variables with Spanish accents like above.  However, the encoding appears odd since this is the behavior I get:</p>

<pre><code>&gt; grep(""México"",temp$dest_nom_ent)
integer(0)
&gt; grep(""Mexico"",temp$dest_nom_ent)
integer(0)
&gt; grep(""xico"",temp$dest_nom_ent)
[1] 18 19 20
&gt; temp$dest_nom_ent[grep(""xico"",temp$dest_nom_ent)]
[2] ""México"" ""México"" ""México""
</code></pre>

<p>where <code>temp$dest_nom_ent</code> is a variable with state names of México.</p>

<p>My question, then, is how to convert the string variable from the third party database into an encoding that standard <code>R</code> functions will recognize.  Please note:</p>

<pre><code>&gt; Encoding(temp$dest_nom_ent)
 [1] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
 [8] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
[15] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
[22] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
[29] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
[36] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
[43] ""unknown"" ""unknown""
</code></pre>

<p>For further info I am using Windows 7 64.  Also note:</p>

<pre><code>&gt; charToRaw(temp$dest_nom_ent[18])
[1] 4d e9 78 69 63 6f
</code></pre>

<p>Which from this <a href=""http://www.science.co.il/language/Character-Code.asp?s=1252"" rel=""nofollow noreferrer"">source</a> coincides with Windows Spanish (Traditional Sort) locale.</p>

<pre><code>M=4d
é=e9
x=78
i=69
c=63
o=6f
</code></pre>

<p>And also note:</p>

<pre><code>&gt; charToRaw(""México"")
[1] 4d c3 a9 78 69 63 6f
&gt; Encoding(""México"")
[1] ""latin1""
</code></pre>

<p>I have tried the following unsuccessfully (e.g. meaning <code>grep(""é"",temp$dest_nom_ent)</code> returns null vector):</p>

<pre><code>Encoding(temp$dest_nom_ent)&lt;-""latin1""
temp$dest_nom_ent &lt;- iconv(temp$dest_nom_ent,"""",""latin1"")
temp$dest_nom_ent  &lt;- enc2utf8(temp$dest_nom_ent)
...
</code></pre>

<p>I checked supported character sets using <code>iconvlist()</code> and <code>""WINDOWS-1252""</code> is supported.  The following, however, did not work:</p>

<pre><code>&gt; temp1 &lt;- temp$dest_nom_ent[grep(""xico"",temp$dest_nom_ent)]
&gt; temp1
[1] ""México"" ""México"" ""México""
&gt; Encoding(temp1)&lt;-""WINDOWS-1252""
&gt; temp1 &lt;- iconv(temp1,""WINDOWS-1252"",""latin1"")
&gt; temp1
[1] ""México"" ""México"" ""México""
&gt; Encoding(temp1)
[1] ""latin1"" ""latin1"" ""latin1""
&gt; charToRaw(temp1[1])
[1] 4d e9 78 69 63 6f
&gt; grep(""é"",temp1)
integer(0)
</code></pre>

<p>which compares to:</p>

<pre><code>&gt; temp2 &lt;- c(""México"",""México"",""México"")
&gt; temp2
[1] ""México"" ""México"" ""México""
&gt; Encoding(temp2)
[1] ""latin1"" ""latin1"" ""latin1""
&gt; charToRaw(temp2[1])
[1] 4d c3 a9 78 69 63 6f
&gt; grep(""é"",temp2)
[1] 1 2 3)
</code></pre>

<p>Tried to find out the encoding by brute force like:</p>

<pre><code>try(for(i in 1:length(iconvlist())){
    temp1 &lt;- temp$dest_nom_ent[grep(""xico"",temp$dest_nom_ent)]
    Encoding(temp1)&lt;-iconvlist()[i]
    temp1 &lt;- iconv(temp1,iconvlist()[i],""latin1"")
    print(grep(""é"",temp1))
    print(i)
        },silent=FALSE)
</code></pre>

<p>I am not familiar with <code>try</code> function but it still scapes at error instead of ignoring it so cannot check whole list:</p>

<pre><code>...
[1] 17
integer(0)
[1] 18
integer(0)
[1] 19
integer(0)
[1] 20
Error in iconv(temp1, iconvlist()[i], ""latin1"") : 
  unsupported conversion from 'CP-GR' to 'latin1' in codepage 1252
</code></pre>

<p>Finally:</p>

<pre><code>&gt; Sys.getlocale()
[1] ""LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252""
&gt; d&lt;-c(""México"",""México"")
&gt; for(i in 1:7){d1 &lt;- str_sub(d[1],i,i); print(d1)}
[1] ""M""
[1] ""Ã""
[1] ""©
[1] ""x""
[1] ""i""
[1] ""c""
[1] ""o""
&gt; print(grep(""é"",d))
[1] 1 2
</code></pre>

<p>So it seems I will have to change the computer's locale as suggested <a href=""https://stackoverflow.com/questions/5205159/how-can-i-find-out-the-internal-code-representation-of-a-windows-1252-character"">here</a>.  Also see <a href=""https://stackoverflow.com/questions/5881731/strange-characters-interaction-of-r-and-windows-locale"">here</a></p>

<p>PS: In case you wonder how with an English_United States.1252 locale I managed to type <code>d&lt;-c(""México"",""México"")</code> the way is by setting up a secondary Spanish keyboard (traditional sort) using <code>Control Panel &gt; Clock, Language and Region &gt; Region and Language &gt; Keyboards and Languages &gt; Change Keyboards</code> and under <code>installed services</code> click add and navigate to Spanish traditional sort. Then under <code>advanced key settings</code> you can create a short-cut to switch keyboards. In my case <code>Shit+Alt</code>.  So if I want to type <code>ñ</code> in English default locale, I do <code>Shift+Alt</code> followed by <code>;</code> and then <code>Shift+Alt</code> to go back to English keyboard.</p>
","<p>Well, I could not determine the coding of accents but the following accomplishes what I wanted.  The trick was to convert to UTF-8, set the <code>sub()</code> option <code>useBytes=TRUE</code> and Joran's <a href=""https://stackoverflow.com/questions/6697757/replace-accents-in-string-vector-with-latex-code"">suggestion</a> to use <code>sanitize.text.function=function(x){x}</code> for <code>xtable()</code>.  Here is the sample code.  Easy to loop over all accented vowels:</p>

<pre><code>&gt; temp1 &lt;- unique(temp$dest_nom_ent)
&gt; temp1
 [1] ""Aguascalientes""                  ""Baja California""                
 [3] ""Baja California Sur""             ""Campeche""                       
 [5] ""Coahuila de Zaragoza""            ""Colima""                         
 [7] ""Chiapas""                         ""Guanajuato""                     
 [9] ""Guerrero""                        ""Hidalgo""                        
[11] ""Jalisco""                         ""México""                         
[13] ""Michoacán de Ocampo""             ""Morelos""                        
[15] ""Nayarit""                         ""Oaxaca""                         
[17] ""Puebla""                          ""Querétaro""                      
[19] ""Quintana Roo""                    ""San Luis Potosí""                
[21] ""Sinaloa""                         ""Tabasco""                        
[23] ""Tlaxcala""                        ""Veracruz de Ignacio de la Llave""
[25] ""Zacatecas""                      
&gt; temp1 &lt;- iconv(unique(temp1),"""",""UTF-8"")
&gt; temp1
 [1] ""Aguascalientes""                  ""Baja California""                
 [3] ""Baja California Sur""             ""Campeche""                       
 [5] ""Coahuila de Zaragoza""            ""Colima""                         
 [7] ""Chiapas""                         ""Guanajuato""                     
 [9] ""Guerrero""                        ""Hidalgo""                        
[11] ""Jalisco""                         ""México""                         
[13] ""Michoacán de Ocampo""             ""Morelos""                        
[15] ""Nayarit""                         ""Oaxaca""                         
[17] ""Puebla""                          ""Querétaro""                      
[19] ""Quintana Roo""                    ""San Luis Potosí""                
[21] ""Sinaloa""                         ""Tabasco""                        
[23] ""Tlaxcala""                        ""Veracruz de Ignacio de la Llave""
[25] ""Zacatecas""                      
&gt; Encoding(temp1)
 [1] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""unknown""
 [8] ""unknown"" ""unknown"" ""unknown"" ""unknown"" ""UTF-8""   ""UTF-8""   ""unknown""
[15] ""unknown"" ""unknown"" ""unknown"" ""UTF-8""   ""unknown"" ""UTF-8""   ""unknown""
[22] ""unknown"" ""unknown"" ""unknown"" ""unknown""
&gt; temp2 &lt;- sub(""é"", ""\\\\'{e}"", temp1, useBytes = TRUE)
&gt; temp2 &lt;- data.frame(temp2)
&gt; print(xtable(temp2),sanitize.text.function=function(x){x})
% latex table generated in R 2.13.1 by xtable 1.5-6 package
% Fri Jul 15 13:52:44 2011
\begin{table}[ht]
\begin{center}
\begin{tabular}{rl}
  \hline
 &amp; temp2 \\ 
  \hline
1 &amp; Aguascalientes \\ 
  2 &amp; Baja California \\ 
  3 &amp; Baja California Sur \\ 
  4 &amp; Campeche \\ 
  5 &amp; Coahuila de Zaragoza \\ 
  6 &amp; Colima \\ 
  7 &amp; Chiapas \\ 
  8 &amp; Guanajuato \\ 
  9 &amp; Guerrero \\ 
  10 &amp; Hidalgo \\ 
  11 &amp; Jalisco \\ 
  12 &amp; M\'{e}xico \\ 
  13 &amp; Michoacán de Ocampo \\ 
  14 &amp; Morelos \\ 
  15 &amp; Nayarit \\ 
  16 &amp; Oaxaca \\ 
  17 &amp; Puebla \\ 
  18 &amp; Quer\'{e}taro \\ 
  19 &amp; Quintana Roo \\ 
  20 &amp; San Luis Potosí \\ 
  21 &amp; Sinaloa \\ 
  22 &amp; Tabasco \\ 
  23 &amp; Tlaxcala \\ 
  24 &amp; Veracruz de Ignacio de la Llave \\ 
  25 &amp; Zacatecas \\ 
   \hline
\end{tabular}
\end{center}
\end{table}
</code></pre>

<p>As actually implemented in a loop:</p>

<pre><code>temp$dest_nom_ent &lt;- iconv(
        temp$dest_nom_ent,"""",""UTF-8"")
temp$dest_nom_mun &lt;- iconv(
        temp$dest_nom_mun,"""",""UTF-8"")
accents &lt;-c(""á"",""é"",""í"",""ó"",""ú"")
latex &lt;-c(""\\\\'{a}"",""\\\\'{e}"",""\\\\'{i}"",""\\\\'{o}"",""\\\\'{u}"")
for(i in 1:5){
    temp$dest_nom_ent&lt;-sub(accents[i], latex[i], 
            temp$dest_nom_ent, useBytes = TRUE)
    temp$dest_nom_mun&lt;-sub(accents[i], latex[i], 
            temp$dest_nom_ent, useBytes = TRUE)
}
capture.output(
        print(xtable(temp),sanitize.text.function=function(x){x}),
        file = ""../paper/rTables.tex"", append = FALSE)
</code></pre>

<p>Still, the answer is incomplete in that I cannot explain what exactly was going on.  Found it through trial and error.</p>
"
30383545,"Efficiently find empirical density() for many arbitrary sample values (like dnorm(), but for empirical distribution)",1,3,3,"<p>Say you've defined an empirical density (<code>sample.density</code>) for a sample of <code>x.sample</code> as in the following:</p>

<pre><code>set.seed(1)
x.sample &lt;- rnorm(100)
sample.density &lt;- density(x.sample)
</code></pre>

<p>Now say that we have a gradient, <code>G</code>, for which we would like to know the expected densities </p>

<pre><code>G &lt;- seq(-2,2, length.out=20)
</code></pre>

<p>Based on the empirical distribution <code>sample.density</code>, what is the density of each value in <code>G</code>?</p>

<p>If I use a <code>for()</code> loop, I can get the answers like this:</p>

<pre><code>G.dens &lt;- c()
for(i in 1:length(G)){
    t.G &lt;- G[i]
    G.dens[i] &lt;- sample.density$y[which.min(abs(sample.density$x-t.G))]
}
</code></pre>

<p>The overarching idea is to do something like <code>dnorm()</code>, but instead of assuming that <code>x</code> is normally distributed with specified mean and s.d., I'd like to use a distribution determined empirically from an arbitrary sample (that isn't necessarily normal or any other well-described distribution that would be in the stats package).</p>
","<p>I think the comment by @MrFlick pointed me in the right direction. In addition to the suggested <code>approxfun</code> approach and my example <code>for</code> loop approach, I also realized I could use <code>mapply</code>. Note that my use of <code>approxfun</code> won't exactly match the result by the 2 other approaches which use <code>which.min</code>, but I'm not concerned with that difference in output too much, although others might be.</p>

<pre><code>First, reproducing the sample data from the question:
set.seed(1)
x.sample &lt;- rnorm(100)
sample.density &lt;- density(x.sample)
G &lt;- seq(-2,2, length.out=20)
</code></pre>

<p>Now, create a function for the loop version</p>

<h1>loop()</h1>

<pre><code>loop &lt;- function(x, ref){
    if(class(ref)!=""density""){
        ref &lt;- density(ref)
    }

    ref.y &lt;- ref$y
    ref.x &lt;- ref$x

    G.dens &lt;- c()
    for(i in 1:length(x)){
        t.G &lt;- x[i]
        G.dens[i] &lt;- ref.y[which.min(abs(ref.x-t.G))]
    }

    G.dens
}
</code></pre>

<p>Next, I'll use the approach I came up with that uses <code>mapply</code></p>

<h1>dsample()</h1>

<pre><code>dsample &lt;- function(x, ref){

    if(class(ref)!=""density""){
        ref &lt;- density(ref)
    }

    XisY &lt;- function(x,y){ # which of several X values most closely matches a single Y value?
        which.min(abs(y-x))
    }

    ref.y &lt;- ref$y
    ref.x &lt;- ref$x

    # ds &lt;- approxfun(ref)

    # ds(x)

    ref.y[mapply(XisY, x, MoreArgs=list(y=ref.x))]
}
</code></pre>

<p>Finally, the approach harnessing <code>approxfun</code> as suggested by @MrFlick:</p>

<h1>af()</h1>

<pre><code>af &lt;- function(x, ref){
    if(class(ref)!=""density""){
        ref &lt;- density(ref)
    }

    # XisY &lt;- function(x,y){ # which of several X values most closely matches a single Y value?
    #   which.min(abs(y-x))
    # }

    ref.y &lt;- ref$y
    ref.x &lt;- ref$x

    ds &lt;- approxfun(ref)

    ds(x)

    # ref.y[mapply(XisY, x, MoreArgs=list(y=ref.x))]
}
</code></pre>

<h1>Now to compare their speed:</h1>

<pre><code>microbenchmark(
    loop(G, sample.density),
    dsample(G, sample.density),
    af(G, sample.density)
)
# Unit: microseconds
#                        expr     min       lq     mean  median       uq      max neval
#     loop(G, sample.density) 221.801 286.6675 360.3698 348.065 409.9785  942.071   100
#  dsample(G, sample.density) 252.641 290.7900 359.0432 368.388 417.1510  520.960   100
#       af(G, sample.density) 201.331 227.8740 276.0425 253.339 273.6225 2545.081   100
</code></pre>

<h1>Now compare speed as the size of G increases:</h1>

<pre><code>speed.loop &lt;- c()
speed.dsample &lt;- c()
speed.af &lt;- c()
lengths &lt;- seq(20, 5E3, by=200)
for(i in 1:length(lengths)){
    G &lt;- seq(-2,2, length.out=lengths[i])
    bm &lt;- microbenchmark(
        loop(G, sample.density),
        dsample(G, sample.density),
        af(G, sample.density), times=10
    )

    means &lt;- aggregate(bm$time, by=list(bm$expr), FUN=mean)[,""x""]/1E6 # in milliseconds
    speed.loop[i] &lt;- means[1]
    speed.dsample[i] &lt;- means[2]
    speed.af[i] &lt;- means[3]


}

speed.ylim &lt;- range(c(speed.loop, speed.dsample, speed.af))
plot(lengths, (speed.loop), ylim=(speed.ylim), type=""l"", ylab=""Time (milliseconds)"", xlab=""# Elements in G"")
lines(lengths, (speed.dsample), col=""red"")
lines(lengths, (speed.af), col=""blue"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/V64Dt.png"" alt=""enter image description here""></p>
"
14547364,Generate distribution given percentile ranks,1,3,3,"<p>I'd like to generate a distribution in R given the following <a href=""http://en.wikipedia.org/wiki/Percentile_rank"" rel=""nofollow""><strong>score and percentile ranks</strong></a>.</p>

<pre><code>x &lt;- 1:10
PercRank &lt;- c(1, 7, 12, 23, 41, 62, 73, 80, 92, 99)
</code></pre>

<p><code>PercRank = 1</code> for example tells that 1% of the data has a <code>value/score &lt;= 1</code> (the first value of x). Similarly, <code>PercRank = 7</code> tells that 7% of the data has a <code>value/score &lt;= 2</code> etc.. </p>

<p>I am not aware of how one could find the underlying distribution. I'd be glad if I could get some guidance on how to go about obtaining the <code>pdf</code> of the underlying distribution from just this much information.</p>
","<p>From <a href=""http://en.wikipedia.org/wiki/Percentile_rank"" rel=""nofollow noreferrer"">Wikipedia</a>: </p>

<blockquote>
  <p>The percentile rank of a score is the percentage of scores in its frequency distribution that are the same or lower than it.</p>
</blockquote>

<p>In order to illustrate this, let's create a distribution, say, a <code>normal distribution</code>, with <code>mean=2</code> and <code>sd=2</code>, so that we can test (our code) later.</p>

<pre><code># 1000 samples from normal(2,2)
x1 &lt;- rnorm(1000, mean=2, sd=2)
</code></pre>

<p>Now, let's take the same <code>percentile rank</code> you've mentioned in your post. Let's divide it by 100 so that they represent cumulative probabilities.</p>

<pre><code>cum.p &lt;- c(1, 7, 12, 23, 41, 62, 73, 80, 92, 99)/100
</code></pre>

<p>And what are the values (<code>scores</code>) corresponding to these percentiles?</p>

<pre><code># generating values similar to your x.
x &lt;- c(t(quantile(x1, cum.p)))
&gt; x
 [1] -2.1870396 -1.4707273 -1.1535935 -0.8265444 -0.2888791  
         0.2781699  0.5893503  0.8396868  1.4222489  2.1519328
</code></pre>

<p>This means that 1% of the data is lesser than -2.18. 7% of the data is lesser than -1.47 etc... Now, we have the <code>x</code> and <code>cum.p</code> (equivalent to your <code>PercRank</code>). Let's forget <code>x1</code> and the fact that this should be a normal distribution. To find out what distribution it could be, let's get actual probabilities from the cumulative probabilities by using <code>diff</code> that takes the difference between nth and (n-1)th element. </p>

<pre><code>prob &lt;- c( cum.p[1], diff(cum.p), .01)
&gt; prob
# [1] 0.01 0.06 0.05 0.11 0.18 0.21 0.11 0.07 0.12 0.07 0.01
</code></pre>

<p>Now, all we have to do is is to generate samples of size, say, 100 (could be any number), for each interval of x <code>(x[1]:x[2], x[2]:x[3] ...)</code> and then finally sample from this huge data as many number of points as you need (say, 10000), with probabilities mentioned above.</p>

<p>This can be done by:</p>

<pre><code>freq &lt;- 10000 # final output size that we want

# Extreme values beyond x (to sample)
init &lt;- -(abs(min(x)) + 5) 
fin  &lt;- abs(max(x)) + 5

ival &lt;- c(init, x, fin) # generate the sequence to take pairs from
len &lt;- 100 # sequence of each pair

s &lt;- sapply(2:length(ival), function(i) {
    seq(ival[i-1], ival[i], length.out=len)
})
# sample from s, total of 10000 values with probabilities calculated above
out &lt;- sample(s, freq, prob=rep(prob, each=len), replace = T)
</code></pre>

<p>Now, we have 10000 samples from the distribution. Let's look at how it is. It should resemble a normal distribution with mean = 2 and sd = 2.</p>

<pre><code>&gt; hist(out)
</code></pre>

<p><img src=""https://i.stack.imgur.com/oIhvO.png"" alt=""normal_dist""></p>

<pre><code>&gt; c(mean(out), sd(out))
# [1] 1.954834 2.170683
</code></pre>

<p>It is a normal distribution (from the histogram) with <code>mean = 1.95</code> and <code>sd = 2.17 (~ 2)</code>. </p>

<p>Note: Some things what I've explained may have been roundabout and/or the code ""may/may not"" work with some other distributions. The point of this post was just to explain the concept with a simple example.</p>

<p><strong>Edit:</strong> In an attempt to clarify <code>@Dwin's</code> point, I tried the same code with <code>x = 1:10</code> corresponding to OP's question, with the same code by replacing the value of x.</p>

<pre><code>cum.p &lt;- c(1, 7, 12, 23, 41, 62, 73, 80, 92, 99)/100
prob &lt;- c( cum.p[1], diff(cum.p), .01)
x &lt;- 1:10

freq &lt;- 10000 # final output size that we want

# Extreme values beyond x (to sample)
init &lt;- -(abs(min(x)) + 1) 
fin  &lt;- abs(max(x)) + 1

ival &lt;- c(init, x, fin) # generate the sequence to take pairs from
len &lt;- 100 # sequence of each pair

s &lt;- sapply(2:length(ival), function(i) {
    seq(ival[i-1], ival[i], length.out=len)
})
# sample from s, total of 10000 values with probabilities calculated above
out &lt;- sample(s, freq, prob=rep(prob, each=len), replace = T)

&gt; quantile(out, cum.p) # ~ =&gt; x = 1:10
# 1%     7%    12%    23%    41%    62%    73%    80%    92%    99% 
# 0.878  1.989  2.989  4.020  5.010  6.030  7.030  8.020  9.050 10.010 

&gt; hist(out)
</code></pre>

<p><img src=""https://i.stack.imgur.com/3JJSG.png"" alt=""hist_OPs_data""></p>
"
31306271,Replace column in data frames based on second data frame,1,1,1,"<p>I have two data frames. Want to match the contents of df1$v1 and df2$v2 where they match, replace corresponding  df2$v2 content with df1v2 content.</p>

<pre><code>df1
v1 v2
1   a1
2   a2
3   a3

df2
v1  v2  v3 v4
c1   1  c3  c4
d1  2   d3  d4
e1  3   e3  e4   
</code></pre>

<p>Looking for this final output. </p>

<pre><code>df2
v1 v2 v3 v4
c1 a1 c3 c4
d1 a2 d3 d4
e1 a3 e3 e4
</code></pre>
","<p>The 'merge'-solution fails in some cases, e.g. if df1$""v1"" and df2$""v2"" do not match everywhere:</p>

<pre><code>df1 &lt;- data.frame( v1 = c(1,2,3),
                   v2 = c(""a1"",""a2"",""a3"") )

df2 &lt;- data.frame( v1 = c(""c1"",""d1"",""e1""),
                   v2 = c(1,5,3),
                   v3 = c(""c3"",""d3"",""e3""),
                   v4 = c(""c4"",""d4"",""e4"") )

out &lt;- merge(df2, df1, by.x='v2', by.y='v1', all.x=T)
out &lt;- out[,-1]

&gt; out
  v1 v3 v4   v2
1 c1 c3 c4   a1
2 e1 e3 e4   a3
3 d1 d3 d4 &lt;NA&gt;
</code></pre>

<p>Another example, where df1$""v1"" and df2$""v2"" do match everywhere:</p>

<pre><code>df1 &lt;- data.frame( v1 = c(1,2,1),
                   v2 = c(""a1"",""a2"",""a3"") )

df2 &lt;- data.frame( v1 = c(""c1"",""d1"",""e1""),
                   v2 = c(1,2,1),
                   v3 = c(""c3"",""d3"",""e3""),
                   v4 = c(""c4"",""d4"",""e4"") )

out &lt;- merge(df2, df1, by.x='v2', by.y='v1', all.x=T)
out &lt;- out[,-1]

&gt; out
  v1 v3 v4 v2
1 c1 c3 c4 a1
2 c1 c3 c4 a3
3 e1 e3 e4 a1
4 e1 e3 e4 a3
5 d1 d3 d4 a2
</code></pre>

<p>The following solution is not very elegant, but it works in these examples:</p>

<pre><code>f &lt;- function( dF1, match1, data1,
               dF2, match2, data2  )
{
  if ( is.factor(dF1[,data1]) )
  {
    dF2[,data2] &lt;- as.factor(dF2[,data2])
    levels(dF2[,data2]) &lt;- c(levels(dF2[,data2]),levels(dF1[,data1])) 
  }     
  n &lt;- which(dF1[,match1] == dF2[,match2])         
  dF2[n,data2] &lt;- dF1[n,data1]    
  return( dF2 )
}

out &lt;-f1( df1, ""v1"", ""v2"", df2, ""v2"", ""v2"" )
</code></pre>

<p>Example 1:</p>

<pre><code>&gt; out
  v1 v2 v3 v4
1 c1 a1 c3 c4
2 d1  5 d3 d4
3 e1 a3 e3 e4
</code></pre>

<p>Example 2:</p>

<pre><code>&gt; out
  v1 v2 v3 v4
1 c1 a1 c3 c4
2 d1 a2 d3 d4
3 e1 a3 e3 e4
</code></pre>

<p>If the rows where df1$v1 and df2$v2 do not match are not wanted in the output, they can be removed by the following modification:</p>

<pre><code>f &lt;- function( dF1, match1, data1,
               dF2, match2, data2  )
{
  if ( is.factor(dF1[,data1]) )
  {
    dF2[,data2] &lt;- as.factor(dF2[,data2])
    levels(dF2[,data2]) &lt;- c(levels(dF2[,data2]),levels(dF1[,data1])) 
  }     
  n &lt;- which(dF1[,match1] == dF2[,match2])         
  dF2[n,data2] &lt;- dF1[n,data1]    
  return( dF2[n,] )
}

out &lt;-f1( df1, ""v1"", ""v2"", df2, ""v2"", ""v2"" )
</code></pre>

<p>Example 1:</p>

<pre><code>&gt; out
  v1 v2 v3 v4
1 c1 a1 c3 c4
3 e1 a3 e3 e4
</code></pre>

<p>In the 'merge'-solution this can be achieved by 'all.x=F', but Example 2 still does not work.</p>
"
32564720,Dodge nested data points in ggplot,1,3,1,"<p>I am plotting 12 data points that are nested across two grouping levels called <code>type</code> and <code>treatment</code>. The group <code>type</code> includes the options <code>many</code> and <code>few</code>, while the group <code>treatment</code> includes the options <code>low</code> and <code>high</code>. As such, each of the four combinations occurs three times.</p>

<p>The code for the figure is as follows (requires package ggplot2):</p>

<pre><code>p20 &lt;-
ggplot(data = test2, lty.est = 1) +
geom_point(data = subset(test2, treatment == 'low'), aes(class, value, shape = type,
                                                           group = type, colour=treatment),
         stat = 'identity',position = position_dodge(width=0.5)) +
geom_point(data = subset(test2, treatment == 'high'), aes(class, value, shape = type, 
                                                          group = type, colour=treatment),
         stat = 'identity', position = position_dodge(width=0.9)) +
scale_colour_manual(values=c(""blue"",""red"")) +
scale_shape_manual(values=c(16,17)) +
geom_errorbar(data = subset(test2, treatment == 'low'), aes(class, ymin=value-se, width = 0.2,
                                                              ymax=value+se, group = type, 
                                                              colour = treatment), stat = 'identity', 
            position=position_dodge(width=0.5)) +
 geom_errorbar(data = subset(test2, treatment == 'high'), aes(class,     ymin=value-se, width = 0.2,
                                                             ymax=value+se, group = type, 
                                                             colour = treatment), stat = 'identity', 
            position=position_dodge(width=0.9)) +
xlab(""Class"") +
ylab(""Value"") +
scale_y_continuous(expand=c(0.0,0.0),
                 limits=c(8.25, 10.25),
                 breaks=c(8.5,9,9.5,10),
                 labels=c(""8.5"",""9"",""9.5"",""10"")) +
scale_x_discrete(limits=c(""one"", ""five"", ""ten""),
               labels=c(""One"", ""Five"", ""Ten"")) +
theme_bw() +
#  theme(legend.position=c(0.8,0.4)) +
theme(legend.title=element_blank()) +
theme(axis.title.x = element_text(vjust=0.1,face=""bold"", size=16),
    axis.text.x = element_text(vjust=0.1, size=14, angle=0)) +
theme(axis.title.y = element_text(angle=90, vjust=0.70, face=""bold"", size=18),
    axis.text.y = element_text(size=14)) +
theme(panel.grid.minor=element_blank(), panel.grid.major=element_blank()) +
theme(panel.border = element_rect(size=2, colour = ""black"", fill=NA, linetype=1)) +
theme(plot.margin = unit(c(0.3,0.4,0.28,0.0),""lines"")) 
</code></pre>

<p><a href=""https://i.stack.imgur.com/pDZBJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pDZBJ.png"" alt=""enter image description here""></a></p>

<p>What I would like to have changed in the current figure is the relative position of points. For example, the arrangement of the plots for each ‘Class’ should be (from left to right): few high, few low, many high and many low.</p>

<p>Any advice on how to adjust the code accordingly would be greatly appreciated.</p>

<p>Please find the data below:</p>

<pre><code>&gt; dput(test2)
structure(list(type = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L), .Label = c(""few"", ""many""), class = ""factor""), 
class = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 2L, 2L, 1L, 1L, 
3L, 3L), .Label = c(""five"", ""one"", ""ten""), class = ""factor""), 
treatment = structure(c(2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 
1L, 2L, 1L), .Label = c(""high"", ""low""), class = ""factor""), 
value = c(8.64, 8.78, 9.64, 9.87, 9.93, 9.99, 8.79, 8.93, 
9.69, 9.91, 9.98, 9.98), se = c(0.14, 0.13, 0.09, 0.05, 0.03, 
0.01, 0.13, 0.11, 0.08, 0.05, 0.02, 0.02)), .Names = c(""type"", 
""class"", ""treatment"", ""value"", ""se""), class = ""data.frame"", row.names =  c(NA, 
-12L))
</code></pre>
","<p>You can achieve that by using the <code>interaction</code> function. With:</p>

<pre><code>ggplot(data = test2) +
  geom_point(aes(class, value, shape = type, group = interaction(treatment,type), colour=treatment),
             stat = 'identity', position = position_dodge(width=0.5), size = 2.5) +
  geom_errorbar(aes(class, ymin=value-se, ymax=value+se, group = interaction(treatment,type), colour = treatment),
                stat = 'identity', position=position_dodge(width=0.5), width = 0.2) +
  labs(x=""Class"", y=""Value"") +
  scale_colour_manual(values=c(""blue"",""red"")) +
  scale_shape_manual(values=c(16,17)) +
  scale_x_discrete(limits=c(""one"", ""five"", ""ten""), labels=c(""One"", ""Five"", ""Ten"")) +
  scale_y_continuous(expand=c(0.0,0.0), limits=c(8.25, 10.25), breaks=c(8.5,9,9.5,10), labels=c(""8.5"",""9"",""9.5"",""10"")) +
  theme_bw() +
  theme(legend.title=element_blank(),
        axis.title.x = element_text(vjust=0.1,face=""bold"", size=16),
        axis.text.x = element_text(vjust=0.1, size=14, angle=0),
        axis.title.y = element_text(angle=90, vjust=0.70, face=""bold"", size=18),
        axis.text.y = element_text(size=14),
        panel.grid.minor=element_blank(), 
        panel.grid.major=element_blank(),
        panel.border = element_rect(size=2, colour = ""black"", fill=NA, linetype=1),
        plot.margin = unit(c(0.3,0.4,0.28,0.0),""lines""))
</code></pre>

<p>you get:</p>

<p><a href=""https://i.stack.imgur.com/L7Z1y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L7Z1y.png"" alt=""enter image description here""></a></p>

<hr>

<p>Another option is to create a interaction variable beforehand:</p>

<pre><code>test2$treattype &lt;- factor(interaction(test2$treatment,test2$type),
                          labels = c(""few high"",""few low"",""many high"",""many low""))
</code></pre>

<p>With:</p>

<pre><code>ggplot(data = test2) +
  geom_point(aes(class, value, shape = treattype, colour = treattype),
             stat = 'identity', position = position_dodge(width=0.5), size = 3) +
  geom_errorbar(aes(class, ymin=value-se, ymax=value+se, colour = treattype),
                stat = 'identity', position=position_dodge(width=0.5), width = 0.2) +
  labs(x=""Class"", y=""Value"") +
  scale_x_discrete(limits=c(""one"", ""five"", ""ten""), labels=c(""One"", ""Five"", ""Ten"")) +
  scale_y_continuous(expand=c(0.0,0.0), limits=c(8.25, 10.25), breaks=c(8.5,9,9.5,10), labels=c(""8.5"",""9"",""9.5"",""10"")) +
  scale_colour_manual(values=c(""blue"",""red"",""blue"",""red"")) +
  scale_shape_manual(values=c(16,17,16,17)) +
  theme_bw() +
  theme(legend.title=element_blank(),
        axis.title.x = element_text(vjust=0.1,face=""bold"", size=16),
        axis.text.x = element_text(vjust=0.1, size=14, angle=0),
        axis.title.y = element_text(angle=90, vjust=0.70, face=""bold"", size=18),
        axis.text.y = element_text(size=14),
        panel.grid.minor=element_blank(), 
        panel.grid.major=element_blank(),
        panel.border = element_rect(size=2, colour = ""black"", fill=NA, linetype=1),
        plot.margin = unit(c(0.3,0.4,0.28,0.0),""lines""))
</code></pre>

<p>you get a plot in which you have just one legend:</p>

<p><a href=""https://i.stack.imgur.com/HVn4N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HVn4N.png"" alt=""enter image description here""></a></p>

<hr>

<p>The following option is an extension of the solution offered by @jlhoward and also deals with the fact that the errorbars for class ""ten"" are hardly readable and integrates the interaction variable from the second option:</p>

<pre><code>test2$class &lt;- with(test2, factor(class, levels=unique(class)))

ggplot(test2, aes(x=type, y=value))+
  geom_point(aes(shape=treattype, color=treattype), position=position_dodge(width=0.5), size=3)+
  geom_errorbar(aes(ymin=value-se, ymax=value+se, shape=treattype, color=treattype),
                width=0.2, position=position_dodge(width=0.5))+
  scale_colour_manual(values=c(""blue"",""red"",""darkgreen"",""brown"")) +
  scale_shape_manual(values=c(16,17,16,17)) +
  facet_wrap(~class, scales=""free_y"")+
  theme_bw() +
  theme(legend.key = element_rect(colour=NA)) +
  guides(colour = guide_legend(title = ""treatment x type"",
                               override.aes = list(colour = c(""blue"",""red"",""darkgreen"",""brown""),
                                                   shape = c(16,17,16,17))),
         shape = FALSE)
</code></pre>

<p>this results in the following plot:</p>

<p><a href=""https://i.stack.imgur.com/6Yj6R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Yj6R.png"" alt=""enter image description here""></a></p>
"
29287038,Adding geom_point to ggmap (ggplot2),1,3,1,"<p>Assume the following minimal data set.  </p>

<pre><code>ddf &lt;- structure(list(Country = c(""Afghanistan"", ""Albania"", ""Algeria"", ""Angola"", ""Argentina"", ""Armenia""), 
x1 = c(16L, 7L, 2L, 1L, 11L, 4L), x2 = c(1150.36, 9506.12, 7534.06, 6247.28, 18749.34, 6190.75)), 
.Names = c(""Country"", ""x1"", ""x2""), row.names = c(1L, 3L, 4L, 5L, 6L, 7L), class = ""data.frame"", 
na.action = structure(2L, .Names = ""2"", class = ""omit""))
</code></pre>

<p>I borrowed code from <a href=""https://stackoverflow.com/questions/24136868/plot-map-with-values-for-countries-as-color-in-r"">this post</a> to generate the initial map.  I am assigning color to the countries based on the x1 variable as follows: </p>

<pre><code>library(RColorBrewer)
library(maptools)
library(ggplot2)
data(wrld_simpl)
wrld_simpl@data$id &lt;- wrld_simpl@data$NAME
wrld &lt;- fortify(wrld_simpl, region=""id"")
wrld &lt;- subset(wrld, id != ""Antarctica"") # we don't rly need Antarctica
gg &lt;- ggplot()
gg &lt;- gg + geom_map(data=wrld, map=wrld, aes(map_id=id, x=long, y=lat), fill=""white"", color=""#7f7f7f"", size=0.25)
gg &lt;- gg + geom_map(data=ddf, map=wrld, aes(map_id=Country, fill=x1),  color=""white"", size=0.25) 
</code></pre>

<p>I want to add <code>geom_point</code> to each one of these countries, and set the size of the geom equal to my <code>x2</code> variable.  I'm not quite sure how this is done.  My thinking has been guided by <a href=""https://stackoverflow.com/questions/15069963/getting-a-map-with-points-using-ggmap-and-ggplot2"">this post</a>, but so far no luck.  Any assistance would be greatly appreciated!</p>
","<p>You need a set of coordinates - the centres or centroids for each country. <code>wrld</code> contains long and lat for the boundaries (polygons) for each country. A simple method for calculating centroids is to take the mean of the range of the long and lat values for each country. </p>

<pre><code>ddf &lt;- structure(list(Country = c(""Afghanistan"", ""Albania"", ""Algeria"", ""Angola"", ""Argentina"", ""Armenia""), 
x1 = c(16L, 7L, 2L, 1L, 11L, 4L), x2 = c(1150.36, 9506.12, 7534.06, 6247.28, 18749.34, 6190.75)), 
.Names = c(""Country"", ""x1"", ""x2""), row.names = c(1L, 3L, 4L, 5L, 6L, 7L), class = ""data.frame"", 
na.action = structure(2L, .Names = ""2"", class = ""omit""))

library(RColorBrewer)
library(maptools)
library(ggplot2)

data(wrld_simpl)
wrld_simpl@data$id &lt;- wrld_simpl@data$NAME
wrld &lt;- fortify(wrld_simpl, region=""id"")
wrld &lt;- subset(wrld, id != ""Antarctica"") # we don't rly need Antarctica

# Select the countries
SelectedCountries = subset(wrld, id %in% c(""Afghanistan"", ""Albania"", ""Algeria"", ""Angola"", ""Argentina"", ""Armenia""))

# Get their centres
CountryCenters &lt;- aggregate(cbind(long, lat) ~ id, data = SelectedCountries, 
                    FUN = function(x) mean(range(x)))

# Merge with the DDf data frame
ddf = merge(ddf, CountryCenters, by.x = ""Country"", by.y = ""id"")

gg &lt;- ggplot() +
   geom_map(data=wrld, map=wrld, aes(map_id=id, x=long, y=lat), fill=""white"", color=""#7f7f7f"", size=0.25) +
   geom_map(data=ddf, map=wrld, aes(map_id=Country, fill = x1), size=0.25) +
   geom_point(data=ddf, aes(x=long, y=lat, size = x2), colour = ""red"")     # plot the points
</code></pre>

<p>For better centring, you can use the <code>Polygon</code> function from the <code>sp</code> package, as demonstrated here: <a href=""https://stackoverflow.com/questions/9441778/improve-centering-county-names-ggplot-maps"">stackoverflow.com/.../improve-centering-county-names-ggplot-maps</a>. It gets Argentina wrong, but I think it is something to do with islands. In <code>wrld</code>, I think <code>piece==1</code> selects the mainland, but it might not work for all countries.</p>

<pre><code>ddf &lt;- structure(list(Country = c(""Afghanistan"", ""Albania"", ""Algeria"", ""Angola"", ""Argentina"", ""Armenia""), 
x1 = c(16L, 7L, 2L, 1L, 11L, 4L), x2 = c(1150.36, 9506.12, 7534.06, 6247.28, 18749.34, 6190.75)), 
.Names = c(""Country"", ""x1"", ""x2""), row.names = c(1L, 3L, 4L, 5L, 6L, 7L), class = ""data.frame"", 
na.action = structure(2L, .Names = ""2"", class = ""omit""))

library(RColorBrewer)
library(maptools)
library(ggplot2)

data(wrld_simpl)
wrld_simpl@data$id &lt;- wrld_simpl@data$NAME
wrld &lt;- fortify(wrld_simpl, region=""id"")
wrld &lt;- subset(wrld, id != ""Antarctica"") # we don't rly need Antarctica

# Select the countries
SelectedCountries = subset(wrld, id %in% c(""Afghanistan"", ""Albania"", ""Algeria"", ""Angola"", ""Argentina"", ""Armenia""))


# Function to calculate centroids
GetCentroidPoint &lt;- function(SelectedCountries) {Polygon(SelectedCountries[c('long', 'lat')])@labpt}

# Apply the function to the selected countries
centroids = by(subset(SelectedCountries, piece == 1), factor(subset(SelectedCountries, piece == 1)$id), GetCentroidPoint) 

# Convert list to data frame
centroids &lt;- do.call(""rbind.data.frame"", centroids)  
names(centroids) &lt;- c('long', 'lat') 
centroids$Country = row.names(centroids)

# Merge with DDF
ddf = merge(ddf, centroids, by = 'Country')


gg &lt;- ggplot() +
   geom_map(data=wrld, map=wrld, aes(map_id=id, x=long, y=lat), fill=""white"", color=""#7f7f7f"", size=0.25) +
   geom_map(data=ddf, map=wrld, aes(map_id=Country, fill = x1), size=0.25) +
   geom_point(data=ddf, aes(x=long, y=lat, size = x2), colour = ""red"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/WLbYh.png"" alt=""enter image description here""></p>
"
45603412,Add dependencies for my Rcpp package,2,2,2,"<p>Rcpp beginner's question:</p>

<p>I want to improve my execution efficiency in R. So I write some code in cpp and use Rcpp to help me compile them. </p>

<p>Question is that I use some other R packages in my .cpp files and I want those packages to be installed and imported automatically when a user installs my package.</p>

<p>e.g.  If I use the R package 'gtools' in my files, I don't want the error:</p>

<pre><code>* installing to library 'C:/Program Files/R/R-3.4.1/library'
* installing *source* package 'pkgname' ...
make: Nothing to be done for `all`.
** libs
installing to C:/Program Files/R/R-3.4.1/library/pkgname/libs/i386
** R
** preparing package for lazy loading
Error in library(gtools) : there is no package called 'gtools'
Error : unable to load R code in package 'pkgname'
ERROR: lazy loading failed for package 'pkgname'
* removing 'C:/Program Files/R/R-3.4.1/library/pkgname'

Exited with status 1.
</code></pre>

<p>I tried to add depended package name to the <strong>DESCRIPTION</strong> file. i.e.</p>

<pre><code>Imports: Rcpp (&gt;= 0.12.12),gtools
LinkingTo: Rcpp, gtools
</code></pre>

<p>But it gives me following error:</p>

<pre><code>ERROR: dependency 'gtools' is not available for package 'pkgname'
</code></pre>

<p>I don't find any similar questions and please tell me if there are.</p>
","<p>First, you should probably make sure <code>gtools</code> is <em>installed</em> on your system. I say this because of the following error:</p>

<blockquote>
  <p>Error in library(gtools) : there is no package called 'gtools'</p>
</blockquote>

<p>With this being said, the main issue you are running into is uncertainty between the <code>LinkingTo:</code> and <code>Imports:</code> fields in the <code>DESCRIPTION</code> file. This is covered in <a href=""https://cran.r-project.org/doc/manuals/R-exts.html#Package-Dependencies"" rel=""nofollow noreferrer"">Section 1.1.3: Package Dependencies</a> of <a href=""https://cran.r-project.org/doc/manuals/R-exts.html"" rel=""nofollow noreferrer"">Writing R Extensions</a>.</p>

<p>Specifically, we have:</p>

<blockquote>
  <p>The <code>‘Imports’</code> field lists packages whose namespaces are imported from (as specified in the <code>NAMESPACE</code> file) but which do not need to be attached. Namespaces accessed by the <code>‘::’</code> and <code>‘:::’</code> operators must be listed here, or in <code>‘Suggests’</code> or <code>‘Enhances’</code> (see below). Ideally this field will include all the standard packages that are used, and it is important to include S4-using packages (as their class definitions can change and the <code>DESCRIPTION</code> file is used to decide which packages to re-install when this happens). Packages declared in the <code>‘Depends’</code> field should not also be in the <code>‘Imports’</code> field. Version requirements can be specified and are checked when the namespace is loaded (since R >= 3.0.0).</p>
</blockquote>

<p>And the <code>LinkingTo</code> field:</p>

<blockquote>
  <p>A package that wishes to make use of header files in other packages needs to 
  declare them as a comma-separated list in the field <code>‘LinkingTo’</code> in the 
  <code>DESCRIPTION</code> file. For example</p>

<pre><code>LinkingTo: link1, link2
</code></pre>
  
  <p>The <code>‘LinkingTo’</code> field can have a version requirement which is checked at installation.</p>
  
  <p>Specifying a package in ‘LinkingTo’ suffices if these are C++ headers containing source code or static linking is done at installation: the packages do not need to be (and usually should not be) listed in the ‘Depends’ or <code>‘Imports’</code> fields. This includes CRAN package <a href=""https://cran.r-project.org/package=BH"" rel=""nofollow noreferrer""><code>BH</code></a> and almost all users of <a href=""https://cran.r-project.org/package=RcppArmadillo"" rel=""nofollow noreferrer""><code>RcppArmadillo</code></a> and <a href=""https://cran.r-project.org/package=RcppEigen"" rel=""nofollow noreferrer""><code>RcppEigen</code></a>.</p>
  
  <p>For another use of <code>‘LinkingTo’</code> see <a href=""https://cran.r-project.org/doc/manuals/R-exts.html#Linking-to-native-routines-in-other-packages"" rel=""nofollow noreferrer"">Linking to native routines in other packages</a>.</p>
</blockquote>

<p>So, the <strong><code>Imports:</code></strong> is meant to specify packages that contain <em>R</em> functions that you wish to import. In particular, the function from a given package or the entire package itself must be specified in the <code>NAMESPACE</code> file. For packages that use <em>Rcpp</em>, you can typically expect <em>R</em> functions to be available if the author has exported the routine from C++. </p>

<p>Now, regarding the <strong><code>LinkingTo:</code></strong>, this is a bit more specific. If an author wishes to make available a <em>C++</em> API via header files they must explicitly declare the statements as is given in <em><a href=""https://cran.r-project.org/doc/manuals/R-exts.html#Linking-to-native-routines-in-other-packages"" rel=""nofollow noreferrer"">native methods</a></em> of <a href=""https://cran.r-project.org/doc/manuals/R-exts.html"" rel=""nofollow noreferrer"">Writing R Extensions</a>. Generally, packages that proceed in this manner are ""header-only"". These packages place the header definitions under <code>inst/include</code>, e.g.</p>

<pre><code>|- pkgname
   |- inst/
      |- include/
         |- pkgname.h
   |- R/
   |- man/
   |- DESCRIPTION
   |- NAMESPACE
</code></pre>

<p>However, another trend is to allow for ""non-header"" packages. This leads to a bit more complicated of topic as you have to understand shared objects and dynamic libraries. CRAN presents an overview of <em>how to ""Link"" packages</em> in <a href=""https://cran.r-project.org/doc/manuals/R-exts.html#Linking-to-other-packages"" rel=""nofollow noreferrer"">Section 5.8: Linking to other packages</a> of <a href=""https://cran.r-project.org/doc/manuals/R-exts.html"" rel=""nofollow noreferrer"">Writing R Extensions</a></p>

<p>If the author does <em>not</em> make available a C++ API, then there are <em>four</em> options:</p>

<ol>
<li>Ask the <em>author</em> nicely to support calling the <em>C++</em> API or submit a patch that enables access to the <em>C++</em> API.</li>
<li><a href=""http://gallery.rcpp.org/articles/r-function-from-c++/"" rel=""nofollow noreferrer"">Call an <em>R</em> function from <em>C++</em></a>. (This negates any performance gain from writing your code in <em>C++</em> though.)</li>
<li>Copy the implementation from the author's package while respecting their intellectual property.</li>
<li>Implement the desired functionality from scratch to avoid licensing issues.</li>
</ol>

<p>Unfortunately, this is the case for <code>gtools</code>. As the author(s) <a href=""https://github.com/cran/gtools/tree/a62d2b0a1f007063780bae7617baa74b1d9db79c/inst"" rel=""nofollow noreferrer"">do not provide a means to ""link"" to the <em>C++</em> version of package's code</a>.</p>
"
29288249,merge strings among rows by id,2,2,4,"<p>I wish to merge strings among rows by an id variable.  I know how to do that with the <code>R</code> code below.  However, my code seems vastly overly complex.</p>

<p>In the present case each string has two elements that are not dots.  Each pair of consecutive rows within an id have one element in common.  So, only one of those elements remains after the two rows are merged.</p>

<p>The desired result is shown and the <code>R</code> code below returns the desired result.  Thank you for any suggestions.  Sorry my <code>R</code> code is so long and convoluted, but it does work and my goal is to obtain more efficient code in base <code>R</code>.</p>

<pre><code>my.data &lt;- read.table(text = '
     id         my.string
      2    11..................
      2    .1...2..............
      2    .....2...3..........
      5    ....................
      6    ......2.....2.......
      6    ............2...4...
      7    .1...2..............
      7    .....2....3.........
      7    ..........3..3......
      7    .............34.....
      8    ....1.....1.........
      8    ..........12........
      8    ...........2....3...
      9    ..................44
     10    .2.......2..........
     11    ...2...2............
     11    .......2.....2......
     11    .............2...2..
', header = TRUE, na.strings = 'NA', stringsAsFactors = FALSE)
my.data

desired.result &lt;- read.table(text = '
     id         my.string
      2    11...2...3..........
      5    ....................
      6    ......2.....2...4...
      7    .1...2....3..34.....
      8    ....1.....12....3...
      9    ..................44
     10    .2.......2..........
     11    ...2...2.....2...2..
', header = TRUE, na.strings = 'NA', stringsAsFactors = FALSE)

# obtain position of first and last non-dot
# from: http://stackoverflow.com/questions/29229333/position-of-first-and-last-non-dot-in-a-string-with-regex

first.last.dot &lt;- data.frame(my.data, do.call(rbind, gregexpr(""^\\.*\\K[^.]|[^.](?=\\.*$)"", my.data[,2], perl=TRUE)))

# obtain non-dot elements
first.last.dot$first.element &lt;- as.numeric(substr(first.last.dot$my.string, first.last.dot$X1, first.last.dot$X1))
first.last.dot$last.element  &lt;- as.numeric(substr(first.last.dot$my.string, first.last.dot$X2, first.last.dot$X2))

# obtain some book-keeping variables
first.last.dot$number.within.group &lt;- sequence(rle(first.last.dot$id)$lengths)
most.records.per.id                &lt;- max(first.last.dot$number.within.group)
n.ids                              &lt;- length(unique(first.last.dot$id))

# create matrices for recording data
positions.per.id &lt;- matrix(NA, nrow = (n.ids), ncol=(most.records.per.id+1))
values.per.id    &lt;- matrix(NA, nrow = (n.ids), ncol=(most.records.per.id+1))

# use nested for-loops to fill matrices with data
positions.per.id[1,1] = first.last.dot$X1[1]
   values.per.id[1,1] = first.last.dot$first.element[1]

positions.per.id[1,2] = first.last.dot$X2[1]
   values.per.id[1,2] = first.last.dot$last.element[1]

j = 1

for(i in 2:nrow(first.last.dot)) {

     if(first.last.dot$id[i] != first.last.dot$id[i-1]) j = j + 1

      positions.per.id[j, (first.last.dot$number.within.group[i]+0)] = first.last.dot$X1[i]
      positions.per.id[j, (first.last.dot$number.within.group[i]+1)] = first.last.dot$X2[i]

      values.per.id[j, (first.last.dot$number.within.group[i]+0)] = first.last.dot$first.element[i]
      values.per.id[j, (first.last.dot$number.within.group[i]+1)] = first.last.dot$last.element[i]
}

# convert matrix data into new strings using nested for-loops
new.strings &lt;- matrix(0, nrow = nrow(positions.per.id), ncol = nchar(my.data$my.string[1]))

for(i in 1:nrow(positions.per.id)) {
     for(j in 1:ncol(positions.per.id)) {

          new.strings[i,positions.per.id[i,j]] &lt;- values.per.id[i,j]
     }
}

# format new strings
new.strings[is.na(new.strings)] &lt;- 0
new.strings[new.strings==0]     &lt;- '.'

new.strings2 &lt;- data.frame(id = unique(first.last.dot$id), my.string = (do.call(paste0, as.data.frame(new.strings))), stringsAsFactors = FALSE)
new.strings2

all.equal(desired.result, new.strings2)
# [1] TRUE
</code></pre>
","<p>Dude, this was tough. Please don't make me explain what I did.</p>

<pre><code>data.frame(id=unique(my.data$id), my.string=sapply(lapply(unique(my.data$id), function(id) gsub('^$','.',substr(gsub('\\.','',do.call(paste0,strsplit(my.data[my.data$id==id,'my.string'],''))),1,1)) ), function(x) paste0(x,collapse='') ), stringsAsFactors=F );
</code></pre>

<hr>

<p>Ok, I'll explain it:</p>

<p>It begins with this <code>lapply()</code> call:</p>

<pre><code>lapply(unique(my.data$id), function(id) ... )
</code></pre>

<p>As you can see, the above basically iterates over the unique ids in the data.frame, processing each one in turn. Here's the contents of the function:</p>

<pre><code>gsub('^$','.',substr(gsub('\\.','',do.call(paste0,strsplit(my.data[my.data$id==id,'my.string'],''))),1,1))
</code></pre>

<p>Let's take that in pieces, starting with the innermost subexpression:</p>

<pre><code>strsplit(my.data[my.data$id==id,'my.string'],'')
</code></pre>

<p>The above indexes all <code>my.string</code> cells for the current <code>id</code> value, and splits each string using <code>strsplit()</code>. This produces a <code>list</code> of character vectors, with each list component containing a vector of character strings, where the whole vector corresponds to the input string which was split. The use of the empty string as the delimiter causes each individual character in each input string to become an element in the output vector in the list component corresponding to said input string.</p>

<p>Here's an example of what the above expression generates (for id==2):</p>

<pre><code>[[1]]
 [1] ""1"" ""1"" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" "".""

[[2]]
 [1] ""."" ""1"" ""."" ""."" ""."" ""2"" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" "".""

[[3]]
 [1] ""."" ""."" ""."" ""."" ""."" ""2"" ""."" ""."" ""."" ""3"" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" "".""
</code></pre>

<p>The above <code>strsplit()</code> call is wrapped in the following (with the <code>...</code> representing the previous expression):</p>

<pre><code>do.call(paste0,...)
</code></pre>

<p>That calls <code>paste0()</code> once, passing the output vectors that were generated by <code>strsplit()</code> as arguments. This does a kind of element-wise pasting of all vectors, so you end up with a single vector like this, for each unique id:</p>

<pre><code> [1] ""1.."" ""11."" ""..."" ""..."" ""..."" "".22"" ""..."" ""..."" ""..."" ""..3"" ""..."" ""..."" ""..."" ""..."" ""..."" ""..."" ""..."" ""..."" ""..."" ""...""
</code></pre>

<p>The above <code>paste0()</code> call is wrapped in the following:</p>

<pre><code>gsub('\\.','',...)
</code></pre>

<p>That strips all literal dots from all elements, resulting in something like this, for each unique id:</p>

<pre><code> [1] ""1""  ""11"" """"   """"   """"   ""22"" """"   """"   """"   ""3""  """"   """"   """"   """"   """"   """"   """"   """"   """"   """"
</code></pre>

<p>The above <code>gsub()</code> call is wrapped in the following:</p>

<pre><code>substr(...,1,1)
</code></pre>

<p>That extracts just the first character of each element, which, if it exists, is the desired character in that position. Empty elements are acceptable, as that just means the id had no non-dot characters in any of its input strings at that position.</p>

<p>The above <code>substr()</code> call is wrapped in the following:</p>

<pre><code>gsub('^$','.',...)
</code></pre>

<p>That simply replaces empty elements with a literal dot, which is obviously necessary before we put the string back together. So we have, for id==2:</p>

<pre><code> [1] ""1"" ""1"" ""."" ""."" ""."" ""2"" ""."" ""."" ""."" ""3"" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" ""."" "".""
</code></pre>

<p>That completes the function that was given to the <code>lapply()</code> call. Thus, coming out of that call will be a <code>list</code> of character vectors representing the desired output strings. All that remains is collapsing the elements of those vectors back into a single string, which is why we then need this:</p>

<pre><code>sapply(..., function(x) paste0(x,collapse='') )
</code></pre>

<p>Using <code>sapply()</code> (simplify-apply) is appropriate because it automatically combines all desired output strings into a single character vector, rather than leaving them as a list:</p>

<pre><code>[1] ""11...2...3.........."" ""...................."" ""......2.....2...4..."" "".1...2....3..34....."" ""....1.....12....3..."" ""..................44"" "".2.......2.........."" ""...2...2.....2...2..""
</code></pre>

<p>Thus, all that remains is producing the full output data.frame, similar to the input data.frame:</p>

<pre><code>data.frame(id=unique(my.data$id), my.string=..., stringsAsFactors=F )
</code></pre>

<p>Resulting in:</p>

<pre><code>  id            my.string
1  2 11...2...3..........
2  5 ....................
3  6 ......2.....2...4...
4  7 .1...2....3..34.....
5  8 ....1.....12....3...
6  9 ..................44
7 10 .2.......2..........
8 11 ...2...2.....2...2..
</code></pre>

<p>And we're done!</p>
"
35665935,Store a double within a data frame in R,2,3,3,"<p>I have a dataframe as follows:</p>

<pre><code>fitnorm &lt;- data.frame(dataset=0,mean=0,sd=0,normopl=0)
</code></pre>

<p>Which goes through the following iterative process</p>

<pre><code>normdat &lt;- rnorm(25, mean = 30, sd = sqrt(9))
fitnorm[i,1] &lt;- normdat
fitnorm[i,2] &lt;- mean(normdat)
fitnorm[i,3] &lt;- sd(normdat)
fitnorm[i,4] &lt;- qnorm(1-(400/1000), mean=fitnorm[i,2], sd=fitnorm[i,3])
</code></pre>

<p>However, I am getting this error.</p>

<pre><code>Error in `[&lt;-.data.frame`(`*tmp*`, 1, 1, value = c(27.431413650154, 
30.3657212588031,  :  replacement has 25 rows, data has 1
</code></pre>

<p>I understand that this is becuase I am trying to place 'normdat' which is type double and size 25 into a single element of a dataframe. Shouldn't a dataframe be able to hold an object of type double? What am I doing wrong?</p>
","<blockquote>
  <p>Shouldn't a dataframe be able to hold an object of type double? What am I doing wrong?</p>
</blockquote>

<p>It can. But this is not what you're doing</p>

<p>In the line</p>

<pre><code>## I've assumed i &lt;- 1
fitnorm[i,1] &lt;- normdat
</code></pre>

<p>you are trying to assign a numerical vector (<code>normdat</code>) to one cell of the data frame (i.e. row <code>i</code>, column 1)</p>

<p>Are you instead trying to do</p>

<pre><code>fitnorm[i,1] &lt;- normdat[i]
</code></pre>

<p><strong>Update</strong></p>

<p>As per your comment, you can't store a vector in a single item of a data.frame, you need to use a list:</p>

<pre><code>lst &lt;- list(dataset = normdat,
            mean = mean(normdat),
            sd = sd(normdat),
            normopl = qnorm(1-(400/1000), mean=fitnorm[i,2], sd=fitnorm[i,3]))

## Which gives
lst
$dataset
 [1] 33.43470 28.66693 29.41060 32.95761 32.66531 29.86056 31.61961 29.32424 28.07063 31.80155
[11] 32.88489 31.90562 31.81625 24.62625 31.19141 27.41913 31.43993 29.60108 29.73310 23.77482
[21] 28.50347 27.22960 24.65698 27.13001 35.85981

$mean
[1] 29.82336

$sd
[1] 2.981638

$normopl
[1] 30.57875
</code></pre>

<p>If you <strong>did</strong> want to use a <code>data.frame</code> you'd have to make every column the same length</p>

<pre><code>fitnorm &lt;- data.frame(dataset = normdat,
                  mean = mean(normdat),
                  sd = sd(normdat),
                  normopl = qnorm(1-(400/1000), mean=fitnorm[i,2], sd=fitnorm[i,3]))

head(fitnorm)
#   dataset     mean       sd  normopl
#1 33.43470 29.82336 2.981638 30.57875
#2 28.66693 29.82336 2.981638 30.57875
#3 29.41060 29.82336 2.981638 30.57875
#4 32.95761 29.82336 2.981638 30.57875
#5 32.66531 29.82336 2.981638 30.57875
#6 29.86056 29.82336 2.981638 30.57875
</code></pre>

<p><strong>Edit by OP</strong></p>

<p>The above code worked. However, since the list had to be iterative, I made a slight alteration.</p>

<pre><code>fitnorm &lt;- list(dataset=list(),mean=list(),sd=list(),normopl=list())

for (i in 1:5000){
    normdat &lt;- rnorm(25, mean = 30, sd = sqrt(9))
    fitnorm$dataset[[i]] &lt;- normdat
    fitnorm$mean[[i]]&lt;- mean(normdat)
    fitnorm$sd[[i]] &lt;- sd(normdat)
    fitnorm$normopl[[i]] &lt;- qnorm(1-(400/1000), mean=fitnorm$mean[[i]], sd=fitnorm$sd[[i]])
    }

fitnorm$dataset[1]
[[1]]
 [1] 33.43470 28.66693 29.41060 32.95761 32.66531 29.86056 31.61961 29.32424 28.07063 31.80155
[11] 32.88489 31.90562 31.81625 24.62625 31.19141 27.41913 31.43993 29.60108 29.73310 23.77482
[21] 28.50347 27.22960 24.65698 27.13001 35.85981

fitnorm$mean[1]
[[1]]
[1] 29.82336

fitnorm$sd[1]
[[1]]
[1] 2.981638

fitnorm$normopl[1]
[[1]]
[1] 30.57875
</code></pre>

<p><strong>Update - Symbolix</strong></p>

<p>There's a 'rule of thumb' in <code>R</code> that I try to stick to that says to use an <code>lapply</code> instead of a <code>for</code> as it is <strong>generally</strong> more efficient (it works in <code>C</code>) - There's a lot of discussion about this on SO already.</p>

<p>Therefore, I would replace your <code>for</code> loop with</p>

<pre><code>lst &lt;- lapply(1:5000, function(x){

    normdat &lt;- rnorm(25, mean = 30, sd = sqrt(9))

    list(fitnorm = list(dataset = normdat,
                        mean = mean(normdat),
                        sd = sd(normdat),
                        normopl = qnorm(1-(400/1000), mean = mean(normdat), sd = sd(normdat))
    ))
  }) 
</code></pre>

<p>And a quick bit of benchmarking: </p>

<pre><code>Unit: milliseconds
           expr      min       lq     mean   median       uq      max neval
   fun_lapply() 220.2830 236.1661 252.7315 249.1904 267.1123 337.0799   100
 fun_for_loop() 373.5972 399.8972 427.1629 421.7407 442.4626 593.7227   100
</code></pre>

<p>Ultimately the gains in this example are marginal, but worth keeping in mind. </p>

<p><strong>Update - Symbolix 2</strong></p>

<p>You can also create a single <code>data.frame</code> if you prefer working with them:</p>

<p>Here I'm using the <code>data.table</code> package for the speed it offers</p>

<pre><code>library(data.table)
lst &lt;- lapply(1:5000, function(x){

  normdat &lt;- rnorm(25, mean = 30, sd = sqrt(9))
  data.table(id = x,
             dataset = normdat,
             mean = mean(normdat),
             sd = sd(normdat),
             normopl = qnorm(1-(400/1000), mean=mean(normdat), sd=sd(normdat)))
})

##lst is now a list of data.tables, so we can 'rbind' them together
dt &lt;- rbindlist(lst)

## now we have one data.table, and the 'id' column indicates 
## which dataset each row belongs too
dt
# id  dataset     mean       sd  normopl
# 1:    1 24.09486 29.46829 3.261638 30.29462
# 2:    1 26.30732 29.46829 3.261638 30.29462
# 3:    1 31.42603 29.46829 3.261638 30.29462
# 4:    1 29.69081 29.46829 3.261638 30.29462
# 5:    1 30.01235 29.46829 3.261638 30.29462
# ---                                         
# 124996: 5000 28.13584 30.39716 2.591752 31.05377
# 124997: 5000 27.44665 30.39716 2.591752 31.05377
# 124998: 5000 29.79728 30.39716 2.591752 31.05377
# 124999: 5000 28.73398 30.39716 2.591752 31.05377
# 125000: 5000 27.83779 30.39716 2.591752 31.05377
</code></pre>
"
14621094,How to replace the text inside an XML element in R?,2,2,4,"<p>I have one input xml file. </p>

<p>cat sample.xml</p>

<pre><code>&lt;Text&gt;
    &amp;lt;p&amp;gt;ABC &amp;lt;/p&amp;gt;
&lt;/Text&gt;
</code></pre>

<p>R script   </p>

<pre><code>library(XML)
doc = xmlTreeParse(""sample.xml"", useInternal = TRUE)
top&lt;-xmlRoot(doc)

sub(""&amp;lt;"",""&lt;"",top[[1]])
</code></pre>

<p>How can i fix above pblm?</p>

<p><strong>Error Message</strong>: Error in as.vector(x, ""character"") : 
  cannot coerce type 'externalptr' to vector of type 'character'</p>

<p>Edit: Aim is to use readHTMLTable() function for particular node in xml which has html table but it has xml markup( <code>&amp;gt;</code> and <code>&amp;lt;</code>) for > and &lt; which need to be repalced first as readHTMLTable function cannot handle xml markup. </p>
","<p>If your question is to know how to replace a string in the content of an XML node, then you can check the following code, using the <code>sample.xml</code> file you provided :</p>

<pre><code>## Parse the XML file
doc &lt;- xmlTreeParse(""sample.xml"", useInternal = TRUE)
## Select the nodes we want to update
nodes &lt;- getNodeSet(doc, ""//Text"")
## For each node, apply gsub on the content of the node
lapply(nodes, function(n) {
  xmlValue(n) &lt;- gsub(""ABC"",""foobar"",xmlValue(n))
})
</code></pre>

<p>Which will give you :</p>

<pre><code>R&gt; doc
&lt;?xml version=""1.0""?&gt;
&lt;Text&gt;
    &amp;lt;p&amp;gt;foobar &amp;lt;/p&amp;gt;
&lt;/Text&gt;
</code></pre>

<p>Here you can see that ""ABC"" as been replaced by ""foobar"".</p>

<p><strong>But,</strong> if you try this code with the substitution you want to achieve (replace ""&amp;lt;"" wit ""&lt;""), it apparently won't work :</p>

<pre><code>doc &lt;- xmlTreeParse(""sample.xml"", useInternal = TRUE)
nodes &lt;- getNodeSet(doc, ""//Text"")
lapply(nodes, function(n) {
  xmlValue(n) &lt;- gsub(""&amp;lt;"",""&lt;"",xmlValue(n))
})
</code></pre>

<p>will give you :</p>

<pre><code>R&gt; doc
&lt;?xml version=""1.0""?&gt;
&lt;Text&gt;
    &amp;lt;p&amp;gt;ABC &amp;lt;/p&amp;gt;
&lt;/Text&gt;
</code></pre>

<p>Why ? If you are working with XML files, you should know that some characters, mainly &lt;, >, &amp; and "" are reserved as they are part of the base XML syntax. As such, they cannot appear in the content of the nodes, otherwise parsing would fail. So they are replaced by <em>entities</em>, which are a sort of coding of these characters. For example, ""&lt;"" is coded as ""&amp;lt;"", ""&amp;"" is coded as ""&amp;amp;"", etc.</p>

<p>So here, the content of your  node contains a ""&lt;"" character, which has been automatically converted to his entity ""&amp;lt;"". What you try to do with your code is to replace ""&amp;lt;"" back with ""&lt;"", which R will gladly do for you, but as it is a text content of a node, the XML package will immediatly convert it back to ""&amp;lt;"".</p>

<p>So, if what you want to achieve is to convert your string ""&amp;lt;p&amp;gt;ABC &amp;lt;/p&amp;gt;"" to a new XML node ""&lt;p&gt;ABC &lt;/p&gt;"", you can't do it that way. A solution would be to parse your text string, detect the name and of the node (here, ""p"") from it, create a new node with <code>xmlNode()</code>, give it the text content ""ABC"" and replace the string with the node you just created.</p>

<p>Another quick and dirty way to do it would be first to replace all the entities in your file without parsing the XML. Something like this :</p>

<pre><code>txt &lt;- readLines(file(""sample.xml""))
txt &lt;- gsub(""&amp;lt;"", ""&lt;"", txt)
txt &lt;- gsub(""&amp;gt;"", ""&gt;"", txt)
writeLines(txt, file(""sample2.xml""))
doc2 &lt;- xmlTreeParse(""sample2.xml"", useInternal = TRUE)
</code></pre>

<p>Which gives :</p>

<pre><code>R&gt; doc2
&lt;?xml version=""1.0""?&gt;
&lt;Text&gt;
  &lt;p&gt;ABC &lt;/p&gt;
&lt;/Text&gt;
</code></pre>

<p>But <strong>this is dangerous</strong>, because if there is a ""real"" ""&amp;lt;"" entity in you file, parsing will fail. </p>
"
28695300,Regular expression in r. Grouping & Capturing,2,2,4,"<p>I'm trying to use regexp in R cran, using the library <code>stringr</code>. I was studing <code>str_match</code> and <code>str_replace</code> functions. I don't understand why they give different results when I use parentheses for Grouping :</p>

<pre><code>library(stringr)
s&lt;-""(.+?)( PIAZZALE | SS)(.+?)([0-9]{5})""

a&lt;-str_match(""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   47838"",perl(s))
b&lt;-str_replace(""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   47838"",perl(s), ""\\2"")

a[3]
#[1] "" PIAZZALE ""
b
#[1] "" SS""
</code></pre>
","<p>Try using just the expression <code>s</code> instead of <code>perl(s)</code>:</p>

<pre><code>library(stringr)
s&lt;-""(.+?)( PIAZZALE | SS)(.+?)([0-9]{5})""

a&lt;-str_match(""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   47838"",s)
b&lt;-str_replace(""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   47838"",s, ""\\2"")

a[3]
#[1] "" PIAZZALE ""
b
#[1] "" PIAZZALE ""
</code></pre>

<p>I've had a look in the documentation for this library: 
<a href=""http://cran.r-project.org/web/packages/stringr/stringr.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/stringr/stringr.pdf</a> </p>

<p>It suggests that while the <code>str_replace</code> method can accept POSIX patterns by default and also perl patterns if supplied, the <code>str_match</code> can only accept POSIX style patterns and will treat the pattern as such if supplied with a perl pattern. The reason they were supplying different values is that they were using different expression engines. <code>str_detect</code> can use perl expressions and returns either <code>TRUEE</code> or <code>FALSE</code>. could you potentially use the <code>str_detect</code> method instead of the match method?</p>

<hr>

<h2>The difference between POSIX and perl that causes this:</h2>

<p>The POSIX engine does not recognise lazy (non-greedy) quantifiers. </p>

<p>Your expression </p>

<pre><code>(.+?)( PIAZZALE | SS)(.+?)([0-9]{5}) 
</code></pre>

<p>would be seen as the perl equivalent of </p>

<pre><code>(.+)( PIAZZALE | SS)(.+)([0-9]{5})
</code></pre>

<p>Where the first quantified class <code>.+</code> would match as much as it can (the full string) before backtracking and evaluating the rest of the expression. It is successful when the first quantified class <code>.+</code> comes all the way back from the end of the string and consumes the characters <code>MONT SS DPR</code> leaving only  <code>SS</code> for the second capture group <code>a[3]</code></p>

<h2>Simplified Explanation of Engine Inner Workings</h2>

<p>Here is a simplified explanation of how the different engines are processing your string. All of your quantifiers/alternation are directly wrapped in capture groups so the numbered quantifiers in the following examples are also your capture groups:</p>

<p><strong>Perl:</strong></p>

<pre><code>Quantifier 1: ""M""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MO""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MON""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT""
Quantifier 2: "" SS""
Quantifier 3: "" ""
Quantifier 4: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT""
Quantifier 2: "" SS""
Quantifier 3: "" D""
Quantifier 4: FAILED - MUST BACKTRACK

...

Quantifier 1: ""MONT""
Quantifier 2: "" SS""
Quantifier 3: "" DPR   PIAZZALE CADORNA, 1A RICCIONE   ""
Quantifier 4: ""47838""

SUCCESS
</code></pre>

<p><strong>POSIX:</strong></p>

<pre><code>Quantifier 1: ""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   47838""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   4783""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   478""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT SS DPR   PIAZZALE CADORNA, 1A RICCIONE   47""
Quantifier 2: FAILED - MUST BACKTRACK

...

Quantifier 1: ""MONT SS DPR   P""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT SS DPR   ""
Quantifier 2: FAILED - MUST BACKTRACK

Quantifier 1: ""MONT SS DPR  ""
Quantifier 2: "" PIZZALE ""
Quantifier 3: ""CADORNA, 1A RICCIONE   47838""
Quantifier 4: FAILED - MUST BACKTRACK

...

Quantifier 1: ""MONT SS DPR  ""
Quantifier 2: "" PIZZALE ""
Quantifier 3: ""CADORNA, 1A RICCIONE   ""
Quantifier 4: ""47838""

SUCCESS
</code></pre>
"
41217250,"how to debug errors like: ""dim(x) must have a positive length"" with caret",2,2,2,"<p>I'm running a predict over a fit similar to what is found in the caret guide:</p>

<p><a href=""http://topepo.github.io/caret/measuring-performance.html"" rel=""nofollow noreferrer"">Caret Measuring Performance</a></p>

<pre><code>predictions &lt;-  predict(caretfit, testing, type = ""prob"")
</code></pre>

<p>But I get the error:</p>

<pre><code>Error in apply(x, 1, paste, collapse = "","") : 
dim(X) must have a positive length
</code></pre>

<p>I would like to know 1) the general way to diagnose these errors that are the result of bad inputs into functions like this or 2) why my code is failing.</p>

<p>1)
So looking at the error It's something to do with 'X'.  Which argument is x? Obviously the first one in 'apply', but which argument in predict is eventually passed to apply?  Looking at traceback():</p>

<pre><code>10: stop(""dim(X) must have a positive length"")
9: apply(x, 1, paste, collapse = "","")
8: paste(apply(x, 1, paste, collapse = "",""), collapse = ""\n"")
7: makeDataFile(x = newdata, y = NULL)
6: predict.C5.0(modelFit, newdata, type = ""prob"")
5: predict(modelFit, newdata, type = ""prob"") at C5.0.R#59
4: method$prob(modelFit = modelFit, newdata = newdata, submodels = param)
3: probFunction(method = object$modelInfo, modelFit = object$finalModel, 
   newdata = newdata, preProc = object$preProcess)
2: predict.train(caretfit, testing, type = ""prob"")
1: predict(caretfit, testing, type = ""prob"")
</code></pre>

<p>Now, this problem would be easy to solve if I could follow the code through and understand the problem as opposed to these general errors.  I can trace the code using this traceback to the code at C5.0.R#59.  (It looks like there's no way to get line numbers on every  trace?)  I can follow this code as far as this line 59 and then (I think) the predict function on line 44:</p>

<p><a href=""https://github.com/topepo/caret/blob/36f8216a1f18e17021adfd17ebdffa8e9f058949/models/files/C5.0.R"" rel=""nofollow noreferrer"">Github Caret C5.0 source</a></p>

<p>But after this I'm not sure where the logic flows.  I don't see 'makeDataFile' anywhere in the caret source or, if it's in another package, how it got there.  I've also tried Rstudio debugging, debug() and browser(). None provide the stacktrace I would expect from other languages.  Any suggestion on how to follow the code when you don't know what an error msg means?</p>

<p>2)  As for my particular inputs, 'caretfit' is simply the result of a caret fit and the testing data is 3million rows by 59 columns:</p>

<pre><code>fitcontrol &lt;- trainControl(method = ""repeatedcv"",
                       number = 10,
                       repeats = 1,
                       classProbs = TRUE,
                       summaryFunction = custom.summary,
                       allowParallel = TRUE)


fml &lt;- as.formula(paste(""OUTVAR ~"",paste(colnames(training[,1:(ncol(training)-2)]),collapse=""+"")))
caretfit &lt;- train(fml,
             data = training[1:200000,],
             method = ""C5.0"",
             trControl = fitcontrol,
             verbose = FALSE,
             na.action = na.pass)
</code></pre>
","<p><strong>1 Debuging Procedure</strong> </p>

<p>You can pinpoint the problem using a couple of functions.</p>

<p>Although there still doesn't seem to be anyway to get a full stacktrace with line numbers in code (Boo!), you can use the functions you do get from the traceback and use the function getAnywhere() to search for the function you are looking for. So for example, you can do:</p>

<pre><code>getAnywhere(makeDataFile)
</code></pre>

<p>to see the location and source. (Which also works great in windows when the libraries are often bundled up in binaries.)  Then you have to use source or github to find the specific line numbers or to trace through the logic of the code.</p>

<p>In my particular problem if I run:</p>

<pre><code>newdata &lt;- testing
caseString &lt;- C50:::makeDataFile(x = newdata, y = NULL)
</code></pre>

<p>(Note the three "":"".) I can see that this step completes at this level, So it appears as if something is happening to my training dataset along the way.</p>

<p>So using gitAnywhere() and github over and over through my traceback I can find the line number manually (Boo!)</p>

<ol>
<li>in caret/R/predict.train.R, predict.train (defined on line 108)
calls probFunction on line 153 </li>
<li>in caret/R/probFunction, probFunction
(defined on line 3) calls method$prob function which is a stored
function in the fit object caretfit$modelInfo$prob which can be
inspected by entering this into the console.  This is the same
function found in caret/models/files/C5.0.R on line 58 which calls
'predict' on line 59 </li>
<li>something in caret knows to use
C50/R/predict.C5.0.R which you can see by searching with
getAnywhere() </li>
<li>this function runs makeDataFile on line 25 (part of
the C50 package) </li>
<li>which calls paste, which calls apply, which dies
with stop</li>
</ol>

<p><strong>2 Particular Problem with caret's predict</strong></p>

<p>As for my problem, I kept inspecting the code, and adding inputs at different levels and it would complete successfully.  What happens is that some modification happens to my dataset in predict.train.R which causes it to fail.  Well it turns out that I wasn't including my 'na.action' argument, which for my tree-based data, used 'na.pass'.  If I include this argument:</p>

<pre><code>prediction &lt;- predict(caretfit, testing, type = ""prob"", na.action = na.pass)
</code></pre>

<p>it works as expected.  line 126 of predict.train  makes use of this argument to decide whether to include non-complete cases in the prediction.  My data has no complete cases and so it failed complaining of needing a matrix of some positive length.</p>

<p>Now how one would be able to know the answer to this apply error is due to a missing na.action argument is not obvious at all, hence the need for a good debugging procedure.  If anyone knows of other ways to debug (keeping in mind that in windows, stepping through library source in Rstudio doesnt work very well), please answer or comment.</p>
"
35118693,How to swap column values to be column name in R,1,1,1,"<p>I have created a minimal example to demonstrate my problem. </p>

<p>I have a data frame and I want to swap the columns to be based on the values within the columns. In other words I want to convert something like this:</p>

<pre><code>structure(list(index = c(""a"", ""b"", ""c""), A = c(""zz"", ""yy"", ""xx""), B = c(""yy"", NA, ""vv""), C = c(""xx"", ""ww"", ""vv"")), class = c(""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -3L), .Names = c(""index"", ""A"", ""B"", ""C""))
</code></pre>

<p>into a data frame that is something like this:</p>

<pre><code>structure(list(index = c(""a"", ""b"", ""c"", ""c""), vv = c(NA, NA, ""B"", ""C""), ww = c(NA, ""C"", NA, NA), xx = c(""C"", NA, ""A"", NA),     yy = c(""B"", ""A"", NA, NA), zz = c(""A"", NA, NA, NA)), class = c(""tbl_df"", ""tbl"", ""data.frame""), row.names = c(NA, -4L), .Names = c(""index"", ""vv"", ""ww"", ""xx"", ""yy"", ""zz""))
</code></pre>

<p>UPDATE</p>

<p>Although at least one solution worked for my minimal example, it appears not to work for my main application. Posted below is a snippet of my actual df.</p>

<pre><code>structure(list(index = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L), .Label = c(""16fA"", ""16fB"", ""16gA"", ""16gB"", ""16gC"", ""16gD"", ""16gE"", ""16gF"", ""16gG"", ""16gP""), class = ""factor""), AA = structure(c(1L, NA, NA, NA, NA, NA, NA, 1L, NA, NA, NA, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, NA, NA, NA, NA), .Label = ""GEC (1)"", class = ""factor""),     BB = structure(c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,     NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1L, NA, NA,     NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1L, NA, 
NA, NA, 1L, NA, NA, NA, NA, NA, NA, NA), .Label = ""BER (3)"", class =""factor""), 
CC = structure(c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,     NA, NA, NA, NA, 1L, NA, NA, NA, 1L, NA, NA, NA, NA, NA, NA,     NA, NA, NA, NA, NA, NA, NA, NA, NA, 1L, NA, NA, NA, NA, NA,     NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = ""ZUR (3)"", class = ""factor""),     DD = structure(c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,     NA, NA, NA, NA, 2L, NA, NA, NA, 2L, NA, NA, NA, 2L, NA, NA, 
NA, 2L, NA, NA, NA, 2L, NA, NA, NA, 2L, NA, NA, NA, 2L, NA,     NA, NA, 1L, NA, NA, NA, NA, NA, NA, NA), .Label = c(""LIK (3)"",     ""SLB (3)""), class = ""factor"")), .Names = c(""index"", ""AA"", ""BB"", ""CC"", ""DD""), row.names = c(NA, -50L), class = ""data.frame"")
</code></pre>
","<p>This could be done with <code>melt/dcast</code> from <code>reshape2</code> (or the same functions in <code>data.table</code>)</p>

<pre><code>library(reshape2)
#convert the dataset to long format
d2 &lt;- melt(d1, id.var='index', na.rm=TRUE)
#create a sequence column grouped by value, index
d2$i1 &lt;- with(d2, ave(seq_along(index), value, index,FUN=seq_along))
#convert from long to wide format
dcast(d2, i1+index~value, value.var='variable')[-1]
#  index   vv   ww   xx   yy   zz
#1    a &lt;NA&gt; &lt;NA&gt;    C    B    A
#2    b &lt;NA&gt;    C &lt;NA&gt;    A &lt;NA&gt;
#3    c    B &lt;NA&gt;    A &lt;NA&gt; &lt;NA&gt;
#4    c    C &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;
</code></pre>

<hr>

<p>If we need to do with <code>dplyr/tidyr</code>, use <code>gather/spread</code></p>

<pre><code>library(dplyr)
library(tidyr)
gather(d1, variable, value, -index, na.rm=TRUE) %&gt;% 
           group_by(value, index) %&gt;%                
           mutate(i1= row_number()) %&gt;%
           spread(value, variable) %&gt;%
           select(-i1)
#  index    vv    ww    xx    yy    zz
#   (chr) (chr) (chr) (chr) (chr) (chr)
# 1     a    NA    NA     C     B     A
# 2     b    NA     C    NA     A    NA
# 3     c     B    NA     A    NA    NA
# 4     c     C    NA    NA    NA    NA
</code></pre>

<hr>

<p>Based on the updated dataset, the <code>dcast</code> output is</p>

<pre><code>dcast(d2, i1+index~value, value.var='variable')[-1]
#    index BER (3) GEC (1) LIK (3) SLB (3) ZUR (3)
#1   16fA    &lt;NA&gt;      AA    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;
#2   16fB    &lt;NA&gt;      AA    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;
#3   16gA    &lt;NA&gt;      AA    &lt;NA&gt;      DD      CC
#4   16gB    &lt;NA&gt;      AA    &lt;NA&gt;      DD      CC
#5   16gC      BB      AA    &lt;NA&gt;      DD    &lt;NA&gt;
#6   16gD    &lt;NA&gt;      AA    &lt;NA&gt;      DD    &lt;NA&gt;
#7   16gE    &lt;NA&gt;      AA    &lt;NA&gt;      DD    &lt;NA&gt;
#8   16gF    &lt;NA&gt;      AA    &lt;NA&gt;      DD      CC
#9   16gG      BB      AA    &lt;NA&gt;      DD    &lt;NA&gt;
#10  16gP      BB      AA      DD    &lt;NA&gt;    &lt;NA&gt;
</code></pre>

<p>and the <code>gather/spread</code> output is</p>

<pre><code>#     index BER (3) GEC (1) LIK (3) SLB (3) ZUR (3)
#   (fctr)   (chr)   (chr)   (chr)   (chr)   (chr)
#1    16fA      NA      AA      NA      NA      NA
#2    16fB      NA      AA      NA      NA      NA
#3    16gA      NA      AA      NA      DD      CC
#4    16gB      NA      AA      NA      DD      CC
#5    16gC      BB      AA      NA      DD      NA
#6    16gD      NA      AA      NA      DD      NA
#7    16gE      NA      AA      NA      DD      NA
#8    16gF      NA      AA      NA      DD      CC
#9    16gG      BB      AA      NA      DD      NA
#10   16gP      BB      AA      DD      NA      NA
</code></pre>
"
11546256,two-way density plot combined with one way density plot with selected regions in r,1,3,1,"<pre><code># data 
set.seed (123)
xvar &lt;- c(rnorm (1000, 50, 30), rnorm (1000, 40, 10), rnorm (1000, 70, 10))
yvar &lt;-   xvar + rnorm (length (xvar), 0, 20)
myd &lt;- data.frame (xvar, yvar)


# density plot for xvar
            upperp = 80   # upper cutoff
            lowerp = 30   # lower cutoff
            x &lt;- myd$xvar
            plot(density(x))
            dens &lt;- density(x)
            x11 &lt;- min(which(dens$x &lt;= lowerp))
            x12 &lt;- max(which(dens$x &lt;= lowerp))
            x21 &lt;- min(which(dens$x &gt; upperp))
            x22 &lt;- max(which(dens$x &gt; upperp))
            with(dens, polygon(x = c(x[c(x11, x11:x12, x12)]),
                y = c(0, y[x11:x12], 0), col = ""green""))
             with(dens, polygon(x = c(x[c(x21, x21:x22, x22)]),
                y = c(0, y[x21:x22], 0), col = ""red""))
            abline(v = c(mean(x)), lwd = 2, lty = 2, col = ""red"")
# density plot with yvar
    upperp = 70  # upper cutoff
    lowerp = 30   # lower cutoff
    x &lt;- myd$yvar
    plot(density(x))
    dens &lt;- density(x)
    x11 &lt;- min(which(dens$x &lt;= lowerp))
    x12 &lt;- max(which(dens$x &lt;= lowerp))
    x21 &lt;- min(which(dens$x &gt; upperp))
    x22 &lt;- max(which(dens$x &gt; upperp))
    with(dens, polygon(x = c(x[c(x11, x11:x12, x12)]),
        y = c(0, y[x11:x12], 0), col = ""green""))
     with(dens, polygon(x = c(x[c(x21, x21:x22, x22)]),
        y = c(0, y[x21:x22], 0), col = ""red""))
    abline(v = c(mean(x)), lwd = 2, lty = 2, col = ""red"")
</code></pre>

<p>I need to plot two way density plot, I am not sure there is better way than the following:</p>

<pre><code>ggplot(myd,aes(x=xvar,y=yvar))+
    stat_density2d(aes(fill=..level..), geom=""polygon"") +
    scale_fill_gradient(low=""blue"", high=""green"") + theme_bw()
</code></pre>

<p>I want to combine all three types in to one (I did not know if I can create two-way plot in ggplot), there is not prefrence on whether the solution be  plots are in ggplot or base or mixed. I hope this is doable project, considering robustness of R.  I personally prefer ggplot2. </p>

<p><img src=""https://i.stack.imgur.com/SgaBQ.jpg"" alt=""enter image description here""></p>

<p>Note: the lower shading in this plot is not right, red should be always lower and green upper in xvar and yvar graphs, corresponding to shaded region in xy density plot. </p>

<p><strong>Edit:</strong> Ultimate expectation on the graph (thanks seth and jon for very close answer)
(1) removing space and axis tick labels etc to make it compact<br>
(2) alignments of grids so that middle plot ticks and grids should align with side ticks and labels and size of plots look the same. 
<img src=""https://i.stack.imgur.com/Ig2KR.jpg"" alt=""enter image description here""></p>
","<p>Here is the example for combining multiple plots with alignment:</p>

<pre><code>library(ggplot2)
library(grid)

set.seed (123)
xvar &lt;- c(rnorm (100, 50, 30), rnorm (100, 40, 10), rnorm (100, 70, 10))
yvar &lt;-   xvar + rnorm (length (xvar), 0, 20)
myd &lt;- data.frame (xvar, yvar)

p1 &lt;- ggplot(myd,aes(x=xvar,y=yvar))+
  stat_density2d(aes(fill=..level..), geom=""polygon"") +
  coord_cartesian(c(0, 150), c(0, 150)) +
  opts(legend.position = ""none"")

p2 &lt;- ggplot(myd, aes(x = xvar)) + stat_density() +
  coord_cartesian(c(0, 150))
p3 &lt;- ggplot(myd, aes(x = yvar)) + stat_density() + 
  coord_flip(c(0, 150))

gt &lt;- ggplot_gtable(ggplot_build(p1))
gt2 &lt;- ggplot_gtable(ggplot_build(p2))
gt3 &lt;- ggplot_gtable(ggplot_build(p3))

gt1 &lt;- ggplot2:::gtable_add_cols(gt, unit(0.3, ""null""), pos = -1)
gt1 &lt;- ggplot2:::gtable_add_rows(gt1, unit(0.3, ""null""), pos = 0)

gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt2$grobs[[which(gt2$layout$name == ""panel"")]],
                                  1, 4, 1, 4)
gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt2$grobs[[which(gt2$layout$name == ""axis-l"")]],
                                 1, 3, 1, 3, clip = ""off"")

gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt3$grobs[[which(gt3$layout$name == ""panel"")]],
                                 4, 6, 4, 6)
gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt3$grobs[[which(gt3$layout$name == ""axis-b"")]],
                                 5, 6, 5, 6, clip = ""off"")
grid.newpage()
grid.draw(gt1)
</code></pre>

<p><img src=""https://i.stack.imgur.com/TL3w0.png"" alt=""enter image description here""></p>

<p>note that this works with gglot2 0.9.1, and in the future release you may do it more easily.</p>

<p><em>And finally</em></p>

<p>you can do that by:</p>

<pre><code>library(ggplot2)
library(grid)

set.seed (123)
xvar &lt;- c(rnorm (100, 50, 30), rnorm (100, 40, 10), rnorm (100, 70, 10))
yvar &lt;-   xvar + rnorm (length (xvar), 0, 20)
myd &lt;- data.frame (xvar, yvar)

p1 &lt;- ggplot(myd,aes(x=xvar,y=yvar))+
  stat_density2d(aes(fill=..level..), geom=""polygon"") +
  geom_polygon(aes(x, y), 
               data.frame(x = c(-Inf, -Inf, 30, 30), y = c(-Inf, 30, 30, -Inf)),
               alpha = 0.5, colour = NA, fill = ""red"") +
  geom_polygon(aes(x, y), 
               data.frame(x = c(Inf, Inf, 80, 80), y = c(Inf, 80, 80, Inf)),
               alpha = 0.5, colour = NA, fill = ""green"") +
  coord_cartesian(c(0, 120), c(0, 120)) +
  opts(legend.position = ""none"")

xd &lt;- data.frame(density(myd$xvar)[c(""x"", ""y"")])
p2 &lt;- ggplot(xd, aes(x, y)) + 
  geom_area(data = subset(xd, x &lt; 30), fill = ""red"") +
  geom_area(data = subset(xd, x &gt; 80), fill = ""green"") +
  geom_line() +
  coord_cartesian(c(0, 120))

yd &lt;- data.frame(density(myd$yvar)[c(""x"", ""y"")])
p3 &lt;- ggplot(yd, aes(x, y)) + 
  geom_area(data = subset(yd, x &lt; 30), fill = ""red"") +
  geom_area(data = subset(yd, x &gt; 80), fill = ""green"") +
  geom_line() +
  coord_flip(c(0, 120))

gt &lt;- ggplot_gtable(ggplot_build(p1))
gt2 &lt;- ggplot_gtable(ggplot_build(p2))
gt3 &lt;- ggplot_gtable(ggplot_build(p3))

gt1 &lt;- ggplot2:::gtable_add_cols(gt, unit(0.3, ""null""), pos = -1)
gt1 &lt;- ggplot2:::gtable_add_rows(gt1, unit(0.3, ""null""), pos = 0)

gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt2$grobs[[which(gt2$layout$name == ""panel"")]],
                                  1, 4, 1, 4)
gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt2$grobs[[which(gt2$layout$name == ""axis-l"")]],
                                 1, 3, 1, 3, clip = ""off"")

gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt3$grobs[[which(gt3$layout$name == ""panel"")]],
                                 4, 6, 4, 6)
gt1 &lt;- ggplot2:::gtable_add_grob(gt1, gt3$grobs[[which(gt3$layout$name == ""axis-b"")]],
                                 5, 6, 5, 6, clip = ""off"")
grid.newpage()
grid.draw(gt1)
</code></pre>

<p><img src=""https://i.stack.imgur.com/dBYgM.png"" alt=""enter image description here""></p>
"
8251632,Amazon Product API with R,1,1,4,"<p>I would like to use R to send requests to the Amazon Product API service.</p>

<p>Is there a way to authenticate and query the Amazon Product API with R without getting the following error:</p>

<p>""The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.""</p>
","<p>Try this</p>

<p>This should perform a search using the Product Advertising API, which I think you mean.</p>

<p>You need to supply the AWSAccessKeyId and AWSsecretkey,</p>

<p>which can be acquired on: <a href=""http://docs.amazonwebservices.com/AWSECommerceService/2011-08-01/GSG/"" rel=""nofollow"">http://docs.amazonwebservices.com/AWSECommerceService/2011-08-01/GSG/</a></p>

<pre><code>search.amazon &lt;- function(Keywords, SearchIndex = 'All', AWSAccessKeyId, AWSsecretkey, AssociateTag, ResponseGroup = 'Small', Operation = 'ItemSearch'){
     library(digest)
     library(RCurl)

 base.html.string &lt;- ""http://ecs.amazonaws.com/onca/xml?""
 SearchIndex &lt;- match.arg(SearchIndex, c('All',
                                             'Apparel',
                                             'Appliances',
                                             'ArtsAndCrafts',
                                             'Automotive',
                                             'Baby',
                                             'Beauty',
                                             'Blended',
                                             'Books',
                                             'Classical',
                                             'DigitalMusic',
                                             'DVD',
                                             'Electronics',
                                             'ForeignBooks',
                                             'Garden',
                                             'GourmetFood',
                                             'Grocery',
                                             'HealthPersonalCare',
                                             'Hobbies',
                                             'HomeGarden',
                                             'HomeImprovement',
                                             'Industrial',
                                             'Jewelry',
                                             'KindleStore',
                                             'Kitchen',
                                             'Lighting',
                                             'Magazines',
                                             'Marketplace',
                                             'Miscellaneous',
                                             'MobileApps',
                                             'MP3Downloads',
                                             'Music',
                                             'MusicalInstruments',
                                             'MusicTracks',
                                             'OfficeProducts',
                                             'OutdoorLiving',
                                             'Outlet',
                                             'PCHardware',
                                             'PetSupplies',
                                             'Photo',
                                             'Shoes',
                                             'Software',
                                             'SoftwareVideoGames',
                                             'SportingGoods',
                                             'Tools',
                                             'Toys',
                                             'UnboxVideo',
                                             'VHS',
                                             'Video',
                                             'VideoGames',
                                             'Watches',
                                             'Wireless',
                                             'WirelessAccessories'))
 Operation &lt;- match.arg(Operation, c('ItemSearch',
                                             'ItemLookup',
                                             'BrowseNodeLookup',
                                             'CartAdd',
                                             'CartClear',
                                             'CartCreate',
                                             'CartGet',
                                             'CartModify',
                                             'SimilarityLookup'))
 ResponseGroup &lt;- match.arg(ResponseGroup, c('Accessories',
                                             'AlternateVersions',
                                             'BrowseNodeInfo',
                                             'BrowseNodes',
                                             'Cart',
                                             'CartNewReleases',
                                             'CartTopSellers',
                                             'CartSimilarities',
                                             'Collections',
                                             'EditorialReview',
                                             'Images',
                                             'ItemAttributes',
                                             'ItemIds',
                                             'Large',
                                             'Medium',
                                             'MostGifted',
                                             'MostWishedFor',
                                             'NewReleases',
                                             'OfferFull',
                                             'OfferListings',
                                             'Offers',
                                             'OfferSummary',
                                             'PromotionSummary',
                                             'RelatedItems',
                                             'Request',
                                             'Reviews',
                                             'SalesRank',
                                             'SearchBins',
                                             'Similarities',
                                             'Small',
                                             'TopSellers',
                                             'Tracks',
                                             'Variations',
                                             'VariationImages',
                                             'VariationMatrix',
                                             'VariationOffers',
                                             'VariationSummary'),
                            several.ok = TRUE)
 version.request = '2011-08-01'
 Service = 'AWSECommerceService'
 if(!is.character(AWSsecretkey)){
  message('The AWSsecretkey should be entered as a character vect, ie be qouted')
 }

 pb.txt &lt;- Sys.time()

 pb.date &lt;- as.POSIXct(pb.txt, tz = Sys.timezone)

 Timestamp = strtrim(format(pb.date, tz = ""GMT"", usetz = TRUE, ""%Y-%m-%dT%H:%M:%S.000Z""), 24)

 str = paste('GET\necs.amazonaws.com\n/onca/xml\n',
        'AWSAccessKeyId=', curlEscape(AWSAccessKeyId),
             '&amp;AssociateTag=', AssociateTag,
             '&amp;Keywords=', curlEscape(Keywords),
             '&amp;Operation=', curlEscape(Operation),
             '&amp;ResponseGroup=', curlEscape(ResponseGroup),
             '&amp;SearchIndex=', curlEscape(SearchIndex),
             '&amp;Service=AWSECommerceService',
             '&amp;Timestamp=', gsub('%2E','.',gsub('%2D', '-', curlEscape(Timestamp))),
             '&amp;Version=', version.request,
             sep = '')

 ## signature test
 Signature = curlEscape(base64(hmac( enc2utf8((AWSsecretkey)), enc2utf8(str1), algo = 'sha256', serialize = FALSE,  raw = TRUE)))

 AmazonURL &lt;- paste(base.html.string,
             'AWSAccessKeyId=', AWSAccessKeyId,
             '&amp;AssociateTag=', AssociateTag,
             '&amp;Keywords=', Keywords,
             '&amp;Operation=',Operation,
             '&amp;ResponseGroup=',ResponseGroup,
             '&amp;SearchIndex=', SearchIndex,
             '&amp;Service=AWSECommerceService',
             '&amp;Timestamp=', Timestamp,
             '&amp;Version=', version.request,
             '&amp;Signature=', Signature
             sep = '')
 AmazonResult &lt;- getURL(AmazonURL)
 return(AmazonResult)
}
</code></pre>

<h1>The URL which we get from running this code wont give a signature address. To get a signature address use the following web address and paste the URL over there and click on Display Signed URL.</h1>

<h1><a href=""http://associates-amazon.s3.amazonaws.com/signed-requests/helper/index.html"" rel=""nofollow"">http://associates-amazon.s3.amazonaws.com/signed-requests/helper/index.html</a></h1>
"
42654132,r pie chart labels overlap ggplot2,1,1,1,"<p>I'm trying to make a pie chart with several slices, and many of them have low values. The problem is that when I make the chart most of the labels overlap each other. </p>

<p>The graphic is this: </p>

<p><img src=""https://i.stack.imgur.com/vrqYE.jpg"" alt=""graphic""></p>

<p>The data: </p>

<pre><code>           Descripcion  Freq
               Sumarios   17
    Previsiones Legales   34
          Multas SICORE   19
           Multas ANSeS    7
            Multas AFIP    5
  Gastos Corresponsalía   22
      Faltantes de Caja  470
    Cargos Jubilaciones 2185
            ATM Fraudes   10
        ATM Diferencias  201
</code></pre>

<p>And the code:</p>

<pre><code>#armo el grafico
pmas &lt;- ggplot(cant_masivos_trim, aes(x=1, y=Freq, fill=Descripcion)) +
        geom_bar(stat=""identity"") +
        ggtitle(paste(""Cantidad de Reportes - Carga Masiva""))
pmas &lt;- pmas + coord_polar(theta='y')
pmas &lt;- ggplot(cant_masivos_trim, aes(x=1, Freq, fill=Descripcion)) +
        ggtitle(paste(""Cantidad de Reportes - Carga Masiva"")) +
        coord_polar(theta='y')
pmas &lt;- pmas + geom_bar(stat=""identity"", color='black') + guides(fill=guide_legend

(override.aes=list(colour=NA)))
pmas &lt;- pmas + theme(axis.ticks=element_blank(),  # the axis ticks
          axis.title=element_blank(),  # the axis labels
          axis.text.y=element_blank()) # the 0.75, 1.00, 1.25 labels.
y.breaks &lt;- cumsum(cant_masivos_trim$Freq) - cant_masivos_trim$Freq/2
pmas &lt;- pmas +
    # prettiness: make the labels black
    theme(axis.text.x=element_text(color='black')) +
    scale_y_continuous(
        breaks=y.breaks,   # where to place the labels
        labels= (paste(cant_masivos_trim$Freq, percent(cant_masivos_trim$Freq/sum (cant_masivos_trim$Freq)), sep='\n'))) # the labels
</code></pre>

<p>I try to find a solution here, but have no luck. Does anybody have an idea?</p>
","<p>Here is an attempt using <code>ggrepel</code>. The result for the pie chart is not really pretty, but I can't improve it. And afterwards, I provide another solution without pie charts at all.</p>

<pre><code>library(ggplot2)
library(tibble)
library(scales)
library(ggrepel)
library(forcats)

df &lt;- tribble(
  ~Descripcion,  ~Freq,
   ""Sumarios"",   17,
   ""Previsiones Legales"",   34,
   ""Multas SICORE"",   19,
   ""Multas ANSeS"",    7,
   ""Multas AFIP"",    5,
   ""Gastos Corresponsalía"",   22,
   ""Faltantes de Caja"",  470,
   ""Cargos Jubilaciones"", 2185,
   ""ATM Fraudes"",   10,
   ""ATM Diferencias"",  201)
</code></pre>

<p>I change <code>df$Descripcion</code>to a factor, and ordered by <code>df$Freq</code>, using <code>forcats::fct_reorder</code>. And then I change the order in the data frame, so the function to position the labels works correctly.</p>

<pre><code>df$Descripcion &lt;- fct_reorder(df$Descripcion, df$Freq)

df &lt;- df[order(df$Freq, decreasing = TRUE), ]
df
# A tibble: 10 × 2
#               Descripcion  Freq
#                   &lt;fctr&gt; &lt;dbl&gt;
#  1               Sumarios    17
#  2    Previsiones Legales    34
#  3          Multas SICORE    19
#  4           Multas ANSeS     7
#  5            Multas AFIP     5
#  6  Gastos Corresponsalía    22
#  7      Faltantes de Caja   470
#  8    Cargos Jubilaciones  2185
#  9            ATM Fraudes    10
# 10        ATM Diferencias   201
</code></pre>

<p>I then define another data frame to place the labels. I chose the x.breaks through trial and error.</p>

<pre><code>my_labels &lt;- tibble(x.breaks = seq(1, 1.5, length.out = 10),
                    y.breaks = cumsum(df$Freq) - df$Freq/2,
                    labels = paste(df$Freq, percent(df$Freq/sum (df$Freq)), sep='\n'),
                    Descripcion = df$Descripcion)
</code></pre>

<p>And then the plot (note that I changed the <code>theme(axis.x.text)</code> to <code>element_blank()</code> as I add the labels through <code>geom_label_repel()</code> now)</p>

<pre><code>pmas &lt;- ggplot(df, aes(x = 1, y = Freq, fill = Descripcion)) +
  ggtitle(paste(""Cantidad de Reportes - Carga Masiva"")) +
  geom_bar(stat=""identity"", color='black') + 
  coord_polar(theta='y') + 
  guides(fill=guide_legend(override.aes=list(colour=NA)))+ 
  theme(axis.ticks=element_blank(),  # the axis ticks
        axis.title=element_blank(),  # the axis labels
        axis.text.y=element_blank(), # the 0.75, 1.00, 1.25 labels.
        axis.text.x = element_blank(), 
        panel.grid = element_blank()) +
  scale_fill_brewer(palette = ""Set3"", direction = -1)+
  geom_label_repel(data = my_labels, aes(x = x.breaks, y = y.breaks, 
                                        label = labels, fill = Descripcion),
                   label.padding = unit(0.1, ""lines""),
                   size = 2,
                   show.legend = FALSE,
                   inherit.aes = FALSE)

pmas
</code></pre>

<p><a href=""https://i.stack.imgur.com/uSJAZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uSJAZ.png"" alt=""Pie Chart""></a></p>

<p>Here is another version of the plot, where you do not need to provide another data frame for the labels. I chose to put the labels before the bars, but it is up to you. Note the <code>expand_limits(y = -150)</code> to ensure that the label is visible, and the <code>coord_flip()</code> so as the labels are more readable. I also use <code>geom_col()</code> in place of <code>geom_bar(stat = ""identity"")</code>.</p>

<pre><code>pmas2 &lt;- ggplot(data = df, aes(x = Descripcion, y = Freq)) +
  geom_col(aes(fill = Descripcion) , show.legend = FALSE) +
  ggtitle(paste(""Cantidad de Reportes - Carga Masiva"")) +
  coord_flip() +
  geom_label(aes(label = paste(df$Freq, percent(df$Freq/sum(df$Freq)), sep = ""\n""),
                y = -150, fill = Descripcion),
             show.legend = FALSE,
             size = 3, label.padding = unit(0.1, ""lines"")) +
  expand_limits(y = -150) +
  scale_fill_brewer(palette = ""Set3"", direction = -1) 

pmas2
</code></pre>

<p><a href=""https://i.stack.imgur.com/EArgI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EArgI.png"" alt=""Bar chart""></a></p>
"
26837910,"randomForest using R for regression, make sense?",2,3,2,"<p>I want to exam which variable impacts most on the outcome, in my data, which is the stock yield. My data is like below.</p>

<p><img src=""https://i.stack.imgur.com/gaXLT.jpg"" alt=""enter image description here""></p>

<p>And my code is also attached. </p>

<pre><code>library(randomForest)
require(data.table)
data = fread(""C:/stockcrazy.csv"")
PEratio &lt;- data$offeringPE/data$industryPE
data_update &lt;- data.frame(data,PEratio)

train &lt;- data_update[1:47,]
test &lt;- data_update[48:57,]
</code></pre>

<p>For the above subset data set train and test, I am not sure if I need to do a cross validation on this data. And I don't know how to do it.</p>

<pre><code>data.model &lt;- randomForest(yield ~ offerings + offerprice + PEratio + count + bingo 
                           + purchase , data=train, importance=TRUE)

par(mfrow = c(1, 1))
varImpPlot(data.model, n.var = 6, main = ""Random Forests: Top 6 Important Variables"")
importance(data.model)
plot(data.model)


model.pred &lt;- predict(data.model, newdata=test)
model.pred

d &lt;- data.frame(test,model.pred)
</code></pre>

<p>I am sure not sure if the result of IncMSE is good or bad. Can someone interpret this?<img src=""https://i.stack.imgur.com/utYds.jpg"" alt=""enter image description here""></p>

<p>Additionally, I found the predicted values of the test data is not a good prediction of the real data. So how can I improve this?</p>

<p><img src=""https://i.stack.imgur.com/FmRbb.jpg"" alt=""enter image description here""></p>
","<p>Let's see. Let's start with %IncMSE: </p>

<p>I found <a href=""https://stats.stackexchange.com/questions/12605/measures-of-variable-importance-in-random-forests"">this</a> really good answer on cross validated about %IncMSE which I quote: </p>

<blockquote>
  <p>if a predictor is important in your current model, then assigning
  other values for that predictor randomly but 'realistically' (i.e.:
  permuting this predictor's values over your dataset), should have a
  negative influence on prediction, i.e.: using the same model to
  predict from data that is the same except for the one variable, should
  give worse predictions.</p>
  
  <p>So, you take a predictive measure (MSE) with the original dataset and
  then with the 'permuted' dataset, and you compare them somehow. One
  way, particularly since we expect the original MSE to always be
  smaller, the difference can be taken. Finally, for making the values
  comparable over variables, these are scaled.</p>
</blockquote>

<p>This means that in your case the most important variable is purchase i.e. when the variable purchase was permuted (i.e. the order of the values randomly changed) the resulting model was 12% worse than having the variable in its original order in terms of calculating the mean square error. The MSE was 12% higher using a permuted purchase variable meaning that the this variable is the most important. Variable importance is just a measure of how important your predictor variables were in the model you used. In your case purchase was the most important and P/E ratio was the least (for those 6 variables). This is not something you can interpret as good or bad because it doesn't show you how well the model fits unseen data. I hope this is clear now.</p>

<p>For the cross-validation:</p>

<p>You do not need to do a cross validation during the training phase because it happens automatically. Approximately, 2/3 of the records are used for the creation of a tree and the 1/3 that is left out (out-of-bag data) is used to assess the tree afterwards (the R squared for the tree is computed using the oob data)</p>

<p>As for the improvement of the model:</p>

<p>By showing just the 10 first lines of the predicted and the actual values of yield, you cannot make a safe decision on whether the model is good or bad. What you need is a test of fitness. The most common one is the R squared. It is simplistic but for comparing models and getting a first opinion about your model it does its job. This is calculated by the model for every tree that you make and can be accessed by data.model$rsq. This ranges from 0 to 1 with 1 being the perfect model and 0 showing really poor fit ( it can sometimes even take negative values which shows a bad fit). If your rsq is bad then you can try the following to improve your model although it is not certain that you will get the results you wish for:</p>

<ol>
<li>Calibrate your trees in a different way.  Change the number of trees grown and prune the trees by specifying a big nodesize number. (here you use the default 500 trees and a nodesize of 5 which might overfit your model.)</li>
<li>Increase the number of variables if possible.</li>
<li>Choose a different model. There are cases were a random Forest would not work well</li>
</ol>
"
40788972,Why is bam from mgcv slow for some data?,2,2,2,"<p>I am fitting the same Generalized Additive Model on multiple data sets using the <code>bam</code> function from <code>mgcv</code>. While for most of my data sets the fit completes within a reasonable time between 10 and 20 minutes. For a few data sets the run take more than 10 hours to complete. I cannot find any similarities between the slow cases, the final fit is neither exceptionally good nor bad, nor do they contain any noticeable outliers.</p>

<p>How can I figure out why the fit is so slow for these instances? And how might I be able to speed these up?</p>

<p>My model contains two smooth terms (using a cyclic  cubic spline basis) and some additional numerical and factor variables. In total 300 coefficients (including those for smooth terms) are estimated. I keep the number of knots intentionally below information theoretically optimal numbers to speed up the fitting process. My data sets contain around 850k rows.</p>

<p>This is the function call:</p>

<pre><code>bam(
    value
    ~ 0
    + weekday_x
    + weekday
    + time
    + ""a couple of factor variables encoding special events""
    + delta:weekday
    + s(share_of_year, k=length(knotsYear), bs=""cc"")
    + s(share_of_year_x, k=length(knotsYear), bs=""cc"")
    , knots=list(
      share_of_year=knotsYear
      , share_of_year_x=knotsYear
    )
    , family=quasipoisson()
    , data=data
)
</code></pre>

<p>knotsYears contains 26 knots.</p>

<p>This model converges reasonably fast for most cases but incredibly slow for a few.</p>
","<h1>A most likely cause: ""fREML"" failure</h1>

<p>In a typical model like above, without tensor smooth <code>te</code> or <code>ti</code>, my experience is that REML iteration fails for some cases.</p>

<p>The canonical <code>bam</code> implementation uses <code>fast.REML.fit</code>. The convergence test of this routine needs a fix, but as Simon has moved on and focus more on the <code>discrete</code> method now, he is not keen on fixing it. The fixed version is (at the moment) only available in an extension pack for testing, ""Zheyuan add-on"", supplemented to my PhD thesis. But <code>fast.REML.fit</code> is also not that fragile, and such convergence failure is not often, otherwise piles of big report would get this issue fixed back in 2012.</p>

<p>The following just helps you make a check not a fix.</p>

<p>Let <code>fit</code> be your fitted model that takes 10 hours, check <code>fit$outer.info</code>. This gives number of REML iterations it takes, and the convergence information like gradient and Hessian. If you see <code>iter = 200</code>, or any information saying some failure like ""step failed"", then you know why it takes so long. But if you look at gradient, most likely you will see that it is almost zero. In other words, REML iteration has actually converged but <code>fast.REML.fit</code> fails to detect it.</p>

<hr>

<h1>Another check: ""performance iteration""</h1>

<p>Since you are fitting a GAM not an AM (additive model with Gaussian response), there is another P-IRLS (penalized iteratively re-weighed least squares) outside the REML iteration. Yes, the whole (canonical) <code>bam</code> estimation is a double loop nest, called ""performance iteration"". This may fail, too, but such failure is more intrinsic and may not be overcome, as the ""performance iteration"" is not guaranteed to converge. So, check <code>fit$iter</code> to see if it is also very big (in the worst case it can be 200). <code>mgcv</code> manual has a dedicated section <code>?gam.convergence</code> discussing this type of convergence failure, and it is the driving reason that ""outer iteration"" is desirable. However, for large dataset, ""outer iteration"" is too expensive to implement. So, bear with ""performance iteration"".</p>

<p><code>mgcv</code> has a ""tracing"" option. If you set <code>control = gam.control(trace = TRUE)</code> when calling <code>bam</code>, the deviance information and iteration counter will be printed to screen as ""performance iteration"" goes. This gives you a clear path of the penalized deviance, so you can inspect whether it is cycling around or trapped at some point. This is more informative than a single iteration number stored in <code>fit$iter</code>.</p>

<hr>

<h1>Perhaps try the new method?</h1>

<p>Maybe you want to try the new <code>discrete = TRUE</code> (added in 2015; paper formally published in 2017). It uses a new fitting iteration. We are (MUCH) happier to test its practical convergence capability, than the old method. When using it, turn on ""trace"", too. If it fails to converge, think about reporting it, but we need a reproducible case.</p>
"
16313109,Is there a better reference for r formulas than ?formula?,2,2,2,"<p>There are many redundant, and sometimes conflicting, ways of specifying formulae in R. Is there a comprehensive yet concise reference for mapping a conceptual models to R syntax than <code>?formula</code>?</p>

<p>I am interested in a broad overview, including the syntax used to specify formulas in non-linear and hierarchical models such as <code>glm</code>, <code>lmer</code>, <code>gam</code>, <code>earth</code>, including (<code>/</code>) for nesting, <code>random</code> and <code>fixed</code> effects in mixed models, and <code>s</code> and <code>te</code> for splines, and others found in popular contributed packages.</p>
","<p>R comes with several manuals, which are accessible from vanilla R's ""Help"" menu at the top right when running R and are also in several places on-line.</p>

<p>Chapter 11 of ""<a href=""http://cran.r-project.org/doc/manuals/R-intro.pdf"" rel=""noreferrer"">An Introduction to R</a>"" has a couple of pages on formulas, for example. </p>

<p>I don't know that it constitutes a ""comprehensive"" resource but it covers much* of what you need to know about how formulas work.</p>

<p>* Indeed, pretty much all of what perhaps 95% of users will ever use</p>

<p>The canonical reference to formulas in the S language might be </p>

<p>Chambers J.M., and Hastie T.J., eds. (1992),
<em>Statistical Models in S</em>. Chapman &amp; Hall, London.</p>

<p>though the origin of the approach comes from </p>

<p>Wilkinson G.N., and Rogers C.E. (1973). ""Symbolic Description of Factorial Models for Analysis of Variance.""  <em>Applied Statistics</em>, <strong>22</strong>, 392–399</p>

<p>A number of recent books related to R discuss formulas but I don't know that I'd call any of them comprehensive. </p>

<p>There are also numerous on-line resources (for example <a href=""http://science.nature.nps.gov/im/datamgmt/statistics/r/formulas/"" rel=""noreferrer"">here</a>) often with a good deal of very useful information.</p>

<p>That said, once you get comfortable with using formulas in R and so have a context into which more knowledge can be placed, the help page contains a surprising amount of information (along with other pages it links to). It is a bit terse and cryptic, but once you have the broader base of knowledge of R's particular way of working, it can be quite useful.</p>

<p>Specific questions relating to R formulas (depending on their content) are likely to be on topic either at StackOverflow or at CrossValidated - indeed there are some quite advanced questions relating to formulas to be found already (use of searches like <code>[r] formula</code> might be fruitful), and it would be handy to have more such questions to help users struggling with these issues; if you have specific questions I'd encourage you to ask.</p>

<p>As for 'redundant' and 'conflicting', I suppose you mean things like the fact that there is more than one way to specify a no-intercept model : <code>y ~ . -1</code> and <code>y ~ . +0</code> both work, for example, but in slightly different contexts each makes sense.</p>

<p>In addition, there's the common bugbear of having to isolate quadratic and higher order terms from the formula interface (to use <code>I(x^2)</code> as a predictor so it's passed through the formula interface unharmed and survives far enough to be interpreted as an algebraic expression). Again, once you get a picture of what's going on 'behind the scenes' that seems much less of a nuisance.</p>

<p>Specific examples of the things I just mentioned: </p>

<pre><code>lm(dist ~ . -1, data=cars) # ""remove-intercept-term"" form of no-intercept
lm(dist ~ . +0, data=cars) # ""make-intercept-zero"" form of no-intercept
lm(dist ~ speed + speed^2, data=cars) # doesn't do what we want here
lm(dist ~ speed + I(speed^2), data=cars) # gets us a quadratic term
lm(dist ~ poly(speed,2), data=cars) # avoid potential multicollinearity
</code></pre>

<p>I agree that the formula interface could at least use a little further guidance and better examples in the <code>?formula</code> help.</p>
"
35923155,Automate tick max and min in faceted ggplot,1,3,1,"<p>I am trying to just mark the max and min of each x-axis in a faceted ggplot. I have several facets with different x scales and the same y scale, and the x axis tick labels overlap each other. Rather than having to manually determine the limits and breaks for each facet x axis, I am looking for a way to just label the min and max values for each.</p>

<p>Code using example data of the <code>CO2</code> dataset (see <code>?CO2</code>):</p>

<pre><code>CO2$num &lt;- 1:nrow(CO2)
library(reshape2)
CO2.melt &lt;- melt(CO2,
                 id.var=c(""Type"",
                          ""Plant"",
                          ""Treatment"",
                          ""num""))
CO2.melt &lt;- CO2.melt[order(CO2.melt$num),]

library(ggplot2)
ggplot(CO2.melt, 
       aes(x = value, 
           y = num)) +
  geom_path(aes(color = Treatment)) +
  facet_wrap( ~ variable, scales = ""free_x"",nrow=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/nLZmM.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nLZmM.jpg"" alt=""enter image description here""></a></p>

<p>Purpose is to replicate well log displays such as <a href=""http://certmapper.cr.usgs.gov/data/PubArchives/OF00-200/WELLS/TULAGEA1/LAS/TL1LOG.JPG"" rel=""nofollow noreferrer"">this one</a>.</p>
","<p>When you want to implemented this for the tick-labels, the use of <code>scales = ""free_x""</code> in a faceted plot makes this hard to automate this. However, with a bit of tinkering and the help of several other packages, you could also use the following approach:</p>

<p><strong>1)</strong> Summarise the data in order to get an idea which tick-labels / breaks you need on the x-axis:</p>

<pre><code>library(data.table)
minmax &lt;- melt(setDT(CO2.melt)[, .(min.val = min(value), max.val = max(value),
                                   floor.end = 10*ceiling(min(value)/10),
                                   ceil.end = 10*floor((max(value)-1)/10)),
                               variable][],
               measure.vars = patterns('.val','.end'),
               variable.name = 'var',
               value.name = c('minmax','ends'))
</code></pre>

<p>which gives:</p>

<pre><code>&gt; minmax
   variable var minmax ends
1:     conc   1   95.0  100
2:   uptake   1    7.7   10
3:     conc   2 1000.0  990
4:   uptake   2   45.5   40
</code></pre>

<p><strong>2)</strong> Create break vecors for each facet:</p>

<pre><code>brks1 &lt;- c(95,250,500,750,1000)
brks2 &lt;- c(7.7,10,20,30,40,45.5)
</code></pre>

<p><strong>3)</strong> Create the facets:</p>

<pre><code>p1 &lt;- ggplot(CO2.melt[CO2.melt$variable==""conc"",], 
             aes(x = value, y = num, colour = Treatment)) +
  geom_path() +
  scale_x_continuous(breaks = brks1) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(colour = c('red','black')[c(1,2,2,2,1)],
                                   face = c('bold','plain')[c(1,2,2,2,1)]),
        axis.title = element_blank(),
        panel.grid.major = element_line(colour = ""grey60""),
        panel.grid.minor = element_blank())

p2 &lt;- ggplot(CO2.melt[CO2.melt$variable==""uptake"",], 
             aes(x = value, y = num, colour = Treatment)) +
  geom_path() +
  scale_x_continuous(breaks = brks2) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(colour = c('red','black')[c(1,2,2,2,2,1)],
                                   face = c('bold','plain')[c(1,2,2,2,2,1)]),
        axis.title = element_blank(),
        panel.grid.major = element_line(colour = ""grey60""),
        panel.grid.minor = element_blank())
</code></pre>

<p><strong>4)</strong> Extract the legend into a separate object:</p>

<pre><code>library(grid)
library(gtable)
fill.legend &lt;- gtable_filter(ggplot_gtable(ggplot_build(p2)), ""guide-box"")
legGrob &lt;- grobTree(fill.legend)
</code></pre>

<p><strong>5)</strong> Create the final plot:</p>

<pre><code>library(gridExtra)
grid.arrange(p1 + theme(legend.position=""none""), 
             p2 + theme(legend.position=""none""), 
             legGrob, ncol=3, widths = c(4,4,1))
</code></pre>

<p>which results in:</p>

<p><a href=""https://i.stack.imgur.com/aaUC3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aaUC3.png"" alt=""enter image description here""></a></p>

<hr>

<p>A possible alternative solution to do this automatically, is either use <code>geom_text</code> or <code>geom_label</code>. An example to show how you can achieve this:</p>

<pre><code># create a summary
library(dplyr)
library(tidyr)
minmax &lt;- CO2.melt %&gt;% 
  group_by(variable) %&gt;% 
  summarise(minx = min(value), maxx = max(value)) %&gt;%
  gather(lbl, val, -1)

# create the plot
ggplot(CO2.melt, aes(x = value, y = num, color = Treatment)) +
  geom_path() +
  geom_text(data = minmax, 
            aes(x = val, y = -3, label = val), 
            colour = ""red"", fontface = ""bold"", size = 5) +
  facet_wrap( ~ variable, scales = ""free_x"", nrow=1) +
  theme_minimal()
</code></pre>

<p>which gives:</p>

<p><a href=""https://i.stack.imgur.com/eY9Nu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eY9Nu.png"" alt=""enter image description here""></a></p>

<p>You can also get the minimum and maximum values on the fly inside <code>ggplot</code> (credit to <a href=""https://stackoverflow.com/a/35923085/2204410"">@eipi10</a>). Another example using <code>geom_label</code>:</p>

<pre><code>ggplot(CO2.melt, aes(x = value, y = num, color = Treatment)) +
  geom_path() +
  geom_label(data = CO2.melt %&gt;% 
               group_by(variable) %&gt;% 
               summarise(minx = min(value), maxx = max(value)) %&gt;%
               gather(lbl, val, -1), 
             aes(x = val, y = -3, label = val), 
             colour = ""red"", fontface = ""bold"", size = 5) +
  facet_wrap( ~ variable, scales = ""free_x"", nrow=1) +
  theme_minimal()
</code></pre>

<p>which gives:</p>

<p><a href=""https://i.stack.imgur.com/kxuyL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kxuyL.png"" alt=""enter image description here""></a></p>
"
33454122,Plotting standard error on radial.plot(),1,3,3,"<p>I am using the <code>radial.plot</code> function of the <code>plotrix-package</code> in <code>R</code>. Does anyone know of a straightforward way to implement standard-error bars. The solution would have to work even when there is more than one datapoint per radial position which could lead to partial overlap of the <code>SE-bar</code> (see graph below).</p>

<p>Now the graph looks like this:</p>

<p><a href=""https://i.stack.imgur.com/0odSZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0odSZ.jpg"" alt=""test""></a></p>

<p>Used Code: </p>

<pre><code>library(plotrix)
ppp &lt;- matrix(runif(1:16, 10, 60), nrow=2, ncol=8)
kl &lt;- c(0, pi/4, pi/2, pi*0.75,pi, pi+pi/4,pi+pi/2, pi+pi*0.75)
plot_rt_soa7 &lt;- radial.plot(ppp,rp.type=""p"",radial.pos=kl,
                label.pos=kl,start=pi/2,
                labels=1:8,radial.lim=c(-10,65),main=""SOA 7"")
                legend(45,50,c(""T-oben"", ""T-unten""),col=1:2,lty=1)
</code></pre>

<p>The errorbars could look e.g. like this:
(from <a href=""https://stackoverflow.com/questions/26583620/how-to-plot-error-bars-in-polar-coordinates-in-python"">How to plot error bars in polar coordinates in python?</a>)</p>

<p><a href=""https://i.stack.imgur.com/XMr1t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XMr1t.png"" alt=""enter image description here""></a></p>

<p>Any help would be much appreciated</p>
","<p>Here is some basic code that will plot error bars for both the 'x' (orthogonal to radius) and 'y' (parallel to radius) dimensions, and a point for the center value. It does not use the <code>plotrix</code> package for the error bar plotting, but instead uses <code>R</code> base graphics. You must provide the errors for the dimensions or comment out the part of the code that plots undesired errors. There are several graphical parameters for line weight, color, point color, and point shape. A sample graph is provided below.</p>

<pre><code>library(plotrix)
set.seed(10) # seed for reproducable graph
ppp &lt;- matrix(runif(1:16, 10, 60), nrow=2, ncol=8)
kl &lt;- c(0, pi/4, pi/2, pi*0.75,pi, pi+pi/4,pi+pi/2, pi+pi*0.75)
start &lt;- pi/2 # know starting value for plotting points angularl
rad_low_lim &lt;- -10 # used when computing values of the error lines and in plot limits
plot_rt_soa7 &lt;- radial.plot(ppp,rp.type=""p""
                            ,radial.pos=kl
                            ,label.pos=kl
                            ,start=start
                            ,labels=1:8
                            ,radial.lim=c(rad_low_lim,65)
                            ,main=""SOA 7"")
legend(40,120,c(""T-oben"", ""T-unten""),col=1:2,lty=1)

# generating random error values for both x and y
error_ppp_y &lt;- matrix(rnorm(16, 15, 5), nrow=2, ncol=8)
error_ppp_x &lt;- matrix(rnorm(16, 10, 3), nrow=2, ncol=8)

bar_cols &lt;- c('blue','green') # colors for bars
lwds &lt;- c(4,2) # line weights for bars
pts_cols &lt;- c('black','red') # colors for points
pts_pch &lt;- c(19,17) # point pch

# loop over the number of rows (T-oben and T-unten)
for(j in 1:2){

  # loop over the observations
  for(i in 1:ncol(ppp)){

    # plotting the errors of the 'y' value
    # center value is determined and errors are rotated to make
    # parallel to the radius
    lines(c(ppp[j,i]+error_ppp_y[j,i]-rad_low_lim,ppp[j,i]-error_ppp_y[j,i]-rad_low_lim)*cos(kl[i]+start)
          ,c(ppp[j,i]+error_ppp_y[j,i]-rad_low_lim,ppp[j,i]-error_ppp_y[j,i]-rad_low_lim)*sin(kl[i]+start)
          ,lwd=lwds[j]
          ,col=bar_cols[j]
    )

    # plotting the 'x' errors that are orthognal to the radius
    # points are the ""center"" with the error values rotated to make them orthognal to the radius
    # comment out if not desired
    lines((ppp[j,i]-rad_low_lim)*cos(kl[i]+start)+c(error_ppp_x[j,i],-error_ppp_x[j,i])*cos(kl[i])
          ,(ppp[j,i]-rad_low_lim)*sin(kl[i]+start)+c(error_ppp_x[j,i],-error_ppp_x[j,i])*sin(kl[i])
          ,lwd=lwds[j]
          ,col=bar_cols[j]
    )

    # plotting points for the center
    # comment out if not desired
    points((ppp[j,i]-rad_low_lim)*cos(kl[i]+start)
          ,(ppp[j,i]-rad_low_lim)*sin(kl[i]+start)
          ,col=pts_cols[j]
          ,pch=pts_pch[j]
    )
  }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/FURbz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FURbz.png"" alt=""enter image description here""></a></p>
"
21858221,List of packages that need an update,2,2,2,"<p>How can I produce a list of packages that need an update, i.e. a table with package name, version currently installed and version available at the repository?</p>

<p>I have tried to <a href=""http://cran.r-project.org/doc/manuals/R-admin.html#Updating-packages"" rel=""nofollow"">hack <code>packageStatus()</code></a>, <code>installed.packages()</code>, <code>update.packages()</code> but I am not able to force these functions to produce the desired output.</p>

<p>Note that I <em>don't</em> want to actually update these packages; I just want to see the mentioned list.</p>
","<p>Take a look at <code>old.packages()</code>. on my system I have:</p>

<pre><code>R&gt; old.packages()
              Package         LibPath                                   Installed   Built   ReposVer    Repository                           
bnlearn       ""bnlearn""       ""/home/gavin/R/build/3.0-patched/library"" ""3.4""       ""3.0.2"" ""3.5""       ""http://cran.rstudio.com/src/contrib""
deldir        ""deldir""        ""/home/gavin/R/build/3.0-patched/library"" ""0.1-4""     ""3.0.2"" ""0.1-5""     ""http://cran.rstudio.com/src/contrib""
devtools      ""devtools""      ""/home/gavin/R/build/3.0-patched/library"" ""1.3""       ""3.0.2"" ""1.4.1""     ""http://cran.rstudio.com/src/contrib""
digest        ""digest""        ""/home/gavin/R/build/3.0-patched/library"" ""0.6.3""     ""3.0.2"" ""0.6.4""     ""http://cran.rstudio.com/src/contrib""
extrafont     ""extrafont""     ""/home/gavin/R/build/3.0-patched/library"" ""0.15""      ""3.0.2"" ""0.16""      ""http://cran.rstudio.com/src/contrib""
forecast      ""forecast""      ""/home/gavin/R/build/3.0-patched/library"" ""4.8""       ""3.0.2"" ""5.1""       ""http://cran.rstudio.com/src/contrib""
foreign       ""foreign""       ""/home/gavin/R/build/3.0-patched/library"" ""0.8-57""    ""3.0.2"" ""0.8-59""    ""http://cran.rstudio.com/src/contrib""
Matrix        ""Matrix""        ""/home/gavin/R/build/3.0-patched/library"" ""1.1-0""     ""3.0.2"" ""1.1-2""     ""http://cran.rstudio.com/src/contrib""
matrixStats   ""matrixStats""   ""/home/gavin/R/build/3.0-patched/library"" ""0.8.12""    ""3.0.2"" ""0.8.14""    ""http://cran.rstudio.com/src/contrib""
mgcv          ""mgcv""          ""/home/gavin/R/build/3.0-patched/library"" ""1.7-27""    ""3.0.2"" ""1.7-28""    ""http://cran.rstudio.com/src/contrib""
mvtnorm       ""mvtnorm""       ""/home/gavin/R/build/3.0-patched/library"" ""0.9-9996""  ""3.0.2"" ""0.9-9997""  ""http://cran.rstudio.com/src/contrib""
party         ""party""         ""/home/gavin/R/build/3.0-patched/library"" ""1.0-11""    ""3.0.2"" ""1.0-13""    ""http://cran.rstudio.com/src/contrib""
R.methodsS3   ""R.methodsS3""   ""/home/gavin/R/build/3.0-patched/library"" ""1.5.2""     ""3.0.2"" ""1.6.1""     ""http://cran.rstudio.com/src/contrib""
raster        ""raster""        ""/home/gavin/R/build/3.0-patched/library"" ""2.2-5""     ""3.0.2"" ""2.2-12""    ""http://cran.rstudio.com/src/contrib""
Rcpp          ""Rcpp""          ""/home/gavin/R/build/3.0-patched/library"" ""0.10.6""    ""3.0.2"" ""0.11.0""    ""http://cran.rstudio.com/src/contrib""
RcppArmadillo ""RcppArmadillo"" ""/home/gavin/R/build/3.0-patched/library"" ""0.3.920.1"" ""3.0.2"" ""0.4.000.2"" ""http://cran.rstudio.com/src/contrib""
rgl           ""rgl""           ""/home/gavin/R/build/3.0-patched/library"" ""0.93.991""  ""3.0.2"" ""0.93.996""  ""http://cran.rstudio.com/src/contrib""
rpart         ""rpart""         ""/home/gavin/R/build/3.0-patched/library"" ""4.1-3""     ""3.0.2"" ""4.1-5""     ""http://cran.rstudio.com/src/contrib""
scatterplot3d ""scatterplot3d"" ""/home/gavin/R/build/3.0-patched/library"" ""0.3-34""    ""3.0.2"" ""0.3-35""    ""http://cran.rstudio.com/src/contrib""
survival      ""survival""      ""/home/gavin/R/build/3.0-patched/library"" ""2.37-4""    ""3.0.2"" ""2.37-7""    ""http://cran.rstudio.com/src/contrib""
</code></pre>

<p>For your specific requirements:</p>

<pre><code>R&gt; old.packages()[, c(""Package"",""Installed"",""ReposVer"")]
              Package         Installed   ReposVer   
bnlearn       ""bnlearn""       ""3.4""       ""3.5""      
deldir        ""deldir""        ""0.1-4""     ""0.1-5""    
devtools      ""devtools""      ""1.3""       ""1.4.1""    
digest        ""digest""        ""0.6.3""     ""0.6.4""    
extrafont     ""extrafont""     ""0.15""      ""0.16""     
forecast      ""forecast""      ""4.8""       ""5.1""      
foreign       ""foreign""       ""0.8-57""    ""0.8-59""   
Matrix        ""Matrix""        ""1.1-0""     ""1.1-2""    
matrixStats   ""matrixStats""   ""0.8.12""    ""0.8.14""   
mgcv          ""mgcv""          ""1.7-27""    ""1.7-28""   
mvtnorm       ""mvtnorm""       ""0.9-9996""  ""0.9-9997"" 
party         ""party""         ""1.0-11""    ""1.0-13""   
R.methodsS3   ""R.methodsS3""   ""1.5.2""     ""1.6.1""    
raster        ""raster""        ""2.2-5""     ""2.2-12""   
Rcpp          ""Rcpp""          ""0.10.6""    ""0.11.0""   
RcppArmadillo ""RcppArmadillo"" ""0.3.920.1"" ""0.4.000.2""
rgl           ""rgl""           ""0.93.991""  ""0.93.996"" 
rpart         ""rpart""         ""4.1-3""     ""4.1-5""    
scatterplot3d ""scatterplot3d"" ""0.3-34""    ""0.3-35""   
survival      ""survival""      ""2.37-4""    ""2.37-7"" 
</code></pre>
"
40156061,White space between tiles in heatplot ggplot,1,3,1,"<p>I have this heatplot that I've generated using ggplot2. </p>

<p><a href=""https://i.stack.imgur.com/PH5hS.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/PH5hS.png"" alt=""enter image description here""></a>
What I would like to do is to add white space between the variables listed in capital letters and the variables listed in lower case letters so as to separate these two categories. I still want the tiles to be of equal width. </p>

<p>The code I've used to generate the plot is: </p>

<pre><code>ggplot(mockdata, aes(variable, Measurement)) + 
  geom_tile(aes(fill = mockdata$plotval), colour = ""dark red"")  + scale_fill_gradient2(limits=c(-20, 20),high = ""firebrick3"", low = ""dodgerblue4"") + 
  theme_minimal() + theme(axis.text.x=element_text(size=28, angle=90), axis.text.y=element_text(size=28)) + 
  labs(title="""", x="""", y="""", fill="""") 
</code></pre>

<p>And my data looks as follows:</p>

<pre><code>&gt; head(mockdata)    Measurement variable         Pval       effect
&gt; direction    plotval category 21          20        A 4.511987e+04
&gt; -0.004892941        -1 -4.6543678 Measured 22          19        A 3.436853e-08  0.054344854         1  7.4638390 Measured 23          18        A 1.465755e+00  0.355139910         1 -0.1660613 Measured 24         
&gt; 17        A 7.006222e-04 -0.079390247        -1  3.1545161 Measured 25
&gt; 16        A 4.705051e-04 -0.017708611        -1  3.3274357 Measured 26
&gt; 15        A 2.301113e+03 -0.022934623        -1 -3.3619379 Measured
</code></pre>

<p>PS. While it is strictly another question, I would also like to edit the legend so that it ranges simply from 0-20 and is split in two vertically, showing on one side the red gradient and the blue gradient on the other. </p>

<p>Thank you very much for your help. </p>

<p>Edit: dput output as requested</p>

<pre><code>    dput(mockdata)
structure(list(Measurement = structure(c(20L, 19L, 18L, 17L, 
16L, 15L, 14L, 13L, 12L, 11L, 10L, 9L, 8L, 7L, 6L, 5L, 4L, 3L, 
2L, 1L, 20L, 19L, 18L, 17L, 16L, 15L, 14L, 13L, 12L, 11L, 10L, 
9L, 8L, 7L, 6L, 5L, 4L, 3L, 2L, 1L, 20L, 19L, 18L, 17L, 16L, 
15L, 14L, 13L, 12L, 11L, 10L, 9L, 8L, 7L, 6L, 5L, 4L, 3L, 2L, 
1L, 20L, 19L, 18L, 17L, 16L, 15L, 14L, 13L, 12L, 11L, 10L, 9L, 
8L, 7L, 6L, 5L, 4L, 3L, 2L, 1L, 20L, 19L, 18L, 17L, 16L, 15L, 
14L, 13L, 12L, 11L, 10L, 9L, 8L, 7L, 6L, 5L, 4L, 3L, 2L, 1L, 
20L, 19L, 18L, 17L, 16L, 15L, 14L, 13L, 12L, 11L, 10L, 9L, 8L, 
7L, 6L, 5L, 4L, 3L, 2L, 1L, 20L, 19L, 18L, 17L, 16L, 15L, 14L, 
13L, 12L, 11L, 10L, 9L, 8L, 7L, 6L, 5L, 4L, 3L, 2L, 1L, 20L, 
19L, 18L, 17L, 16L, 15L, 14L, 13L, 12L, 11L, 10L, 9L, 8L, 7L, 
6L, 5L, 4L, 3L, 2L, 1L, 20L, 19L, 18L, 17L, 16L, 15L, 14L, 13L, 
12L, 11L, 10L, 9L, 8L, 7L, 6L, 5L, 4L, 3L, 2L, 1L), .Label = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", 
""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", 
""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"", 
""36"", ""37"", ""38"", ""39"", ""40""), class = ""factor""), variable = structure(c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L), .Label = c(""A"", ""B"", ""C"", ""a"", ""b"", ""c"", ""d"", ""e"", 
""f""), class = ""factor""), Pval = c(45119.8699435164, 3.43685309800565e-08, 
1.46575455003134, 0.000700622204341352, 0.000470505115493356, 
2301.1126328306, 4.54797845872279e-09, 2.82713540460051, 7.27455376527109, 
0.000708124149686762, 2.42898037459018, 129.848099736546, 0.199979430821282, 
3.14108095159057, 1.27073870790998, 107.060952494286, 5853.27997745224, 
0.0742582348172777, 0.179994821770633, 896.77644657471, 23.2401456908433, 
1.04257158400361e-05, 0.818717078694074, 0.000516863241751892, 
0.445544049614064, 37.9699212955842, 4.87750761132064, 0.138732186303325, 
0.0185924820003653, 30.8203248121754, 2.40190754411345, 61.2579350060232, 
6.44618209352764, 1047821.38958175, 704454.528072486, 1184.40676026588, 
9.27799786552194, 244051.746539605, 375048.779613815, 9134.54101073437, 
3.11905288061898e-08, 2.00150860109179e-05, 7.0931638758186e-05, 
2.43242155771004, 1602.73424833609, 6.02923321145412, 3.21794638623681, 
39545.9388361236, 475.781444413131, 455.686197237482, 4.67829129583755, 
909.58298359142, 2.02989433872695e-12, 0.000760836775114312, 
0.000112864936813818, 0.00189775331609849, 0.0815921243521161, 
31.0427655620544, 64.1785732227039, 0.00217933417639979, 0.32663419999951, 
2.54891270451988e-07, 8.48612397192046e-08, 0.370184441863085, 
0.0773444299723239, 1580.93083600716, 25.4862794021994, 30.2642707469218, 
224.214639928028, 14.7414363362227, 18.0511919451453, 21037.258121794, 
21659.968905852, 17721957.0100965, 41240.6588986806, 111.028537460958, 
21.7975275086975, 276.44178388089, 171.535750708364, 6.54776065462672, 
151.823636593007, 77.7232628471612, 378439.528032194, 8.9932059191815e-10, 
7.49689679316569e-08, 0.0155924907385915, 1032.31959077079, 69.010587250471, 
3136.70323628637, 249713.871697334, 19360.5708508639, 40.7258690163879, 
0.0162957051889601, 1.02869393872855e-06, 0.025268197376688, 
0.528481476061038, 0.982629085096339, 9.45655313066933e-07, 4.57557582847333e-05, 
7.45403294058092e-05, 4.53918933391971, 3.30530837062386e-13, 
0.630234341296706, 9.82668496529192e-08, 8.53389810467837e-08, 
9.02891336492619, 5451.02888390797, 3.90681590706977, 329.545150676593, 
103.491400305633, 697.038514954349, 222.73520900148, 10798.3042798457, 
21.0644243649671, 43176.6607108169, 4235.53654423626, 3100.32036580987, 
0.474678829666085, 6.40621457829452, 122.637549472055, 43.0666245228938, 
12.0250322640284, 61440.278302552, 6788575.58160297, 51.9360282881212, 
769.96899739087, 7.59005145509618e-09, 2.43526615569376e-14, 
7.39725489778816e-07, 0.000232784090617743, 2.71678851958226e-05, 
7.42801688358956e-06, 0.000419885957835749, 13.5780839769886, 
3.32442709394189, 0.335486082296544, 3.94230896978352, 1.46425278783307, 
8.69549979650646, 596385.229504125, 0.000115943040096764, 3628.23773839244, 
49454.7726336256, 0.000504974333790079, 0.00995546823577904, 
0.233933121119438, 0.363364289185792, 9193.93256769758, 354.456908896329, 
5661.68504768727, 0.121589829029783, 940.93343158607, 2.03003608173289e-18, 
0.0149649659043151, 4.52682955802236e-10, 7.15334079727642e-09, 
0.000310851843411754, 1.93389728243117e-06, 2.03883508956486e-07, 
0.00284517176384008, 540.061334125012, 21318282.1015595, 14894.4660549342, 
0.000266471541489485, 0.246672590081899, 0.102617416010794, 30584.5294303485, 
1.6644151457974, 0.274144521451231, 5.61250055231602, 801087.819256331, 
649321.765215902, 1.09526223361541, 3.19905699111348e-05, 0.383890848761449, 
895.643897487244, 2.68459116542856, 26.4804232740359, 5.94041739131986, 
1.53119475830081e-07), effect = c(-0.0048929408170183, 0.0543448544236634, 
0.355139910284011, -0.0793902472151885, -0.0177086113171934, 
-0.0229346227548993, 0.0859476877708055, 0.0616530901499479, 
0.0255808327961239, 0.0587050814255305, 0.0141476490245787, 0.016845032477233, 
0.0534824934671667, 0.109055235276048, 0.111112652283139, 0.0234941463950778, 
0.0509358202565552, 0.0252735688725701, 0.0359067099796669, 0.00562271637183446, 
0.0507704293161921, 0.17283228087345, 0.16140675832248, -0.0818286747635847, 
-0.00755469414427661, -0.0104447567093411, -0.128088271682644, 
-0.0534210973538898, -0.373370146687058, -0.0158548424704536, 
-0.0748163315974356, -0.0904460324042191, -0.0028512716452736, 
0.014271561838011, -0.00190156753813359, -0.0239279177026479, 
-0.0094336807819733, 0.0222425549129021, 0.0292506342181443, 
0.0325754202368439, 0.0148015986703938, 0.0421882264651577, 0.423260142208342, 
0.0203382101649578, 0.00402969463133637, 0.0866038959569813, 
-0.0126180724522637, -0.00101624925914612, 0.0424729539424711, 
0.0109268890743347, -0.100768815837666, -0.00606344260837149, 
-0.0909174554780339, -0.0959700647301915, -0.0768705808220001, 
-0.0662807953036616, -0.00706464147110935, -0.0260825101925435, 
-0.116360501719279, -0.0685817739681672, 0.0190371136722137, 
0.314176452865903, 0.0852019981707266, -0.109955683402951, -0.0429494359040651, 
-0.018893257986317, 0.0359280715736875, -0.00777719486360404, 
0.0268854711195253, 0.035717165605315, -0.0185632636448169, 0.00219893106540051, 
-0.0575168050187894, -0.0268860255228866, -0.0120997481434518, 
-0.0174770295188239, -0.0365387268315564, 0.00427667014560775, 
0.0290413811465955, -0.0303874959875708, 0.0494496371834744, 
-0.0172878730121681, -0.0596619493875312, 0.0674472930186462, 
0.0581172695200586, 0.0352865160196343, -0.0390338901505756, 
0.0201101954340249, 0.0968644367520139, -0.00587675128187206, 
-0.0990223012750421, -0.022532243059551, -0.109508938534005, 
-0.0828800247402709, -0.0224991372217077, -0.0864470271893071, 
-0.0120276456354962, -0.171616504240665, -0.0405716645109222, 
-0.150207882114913, 0.019187309212901, 0.0583618216547288, 0.312977089073896, 
-0.116708039798045, -0.0727212548658722, -0.0334904589242441, 
0.0250211767009604, -0.0262773156800457, -0.00638328497889965, 
0.0207436409328599, 0.00526977878443117, 0.0121248749949723, 
0.0372084648778052, 0.0119773388777849, 0.0299841410899367, 0.0179852238184209, 
0.00615922907815808, 0.0923011852715508, 0.0286655081413075, 
0.0615892570124506, -0.040101427234219, 0.00471273799312188, 
-0.0695325837078327, -0.0194435878635383, -0.0113447270023325, 
-0.052896201189637, 0.15391622949438, 0.0625317280746324, 0.305195811975115, 
0.0175140430499281, 0.0944330305254164, 0.125486034995771, 0.204097960624017, 
0.0139041133925463, 0.115503245160239, 0.0744749299835659, 0.0309401153645095, 
0.0774734185854047, 0.128577840642144, -0.00523940764293137, 
-0.0826725654792668, 0.00061718469555555, -0.139447399226457, 
-0.015892803158233, -0.0776351210087576, -0.114375795737603, 
0.506655172546014, 0.0139569906287469, 0.032593809131997, 0.0838209562382717, 
0.026889024005808, 0.114847700394033, 0.0344957168306127, 0.309849172983949, 
0.224583305802426, 0.0920680732068372, 0.20205450223499, 0.129261938040113, 
0.118741527574873, 0.0666598681717314, -0.0259685736746482, -0.00493768230442141, 
-0.0513043909367941, -0.0228292902998043, -0.125120892334107, 
-0.0354061460089046, -0.145910823420589, -0.0505094213129978, 
-0.357522550450534, -0.0492725484140125, -0.00672879449498781, 
-0.00498836470563034, 0.0857101915123005, 0.245202106389044, 
0.0652610111098569, 0.00803793757485241, 0.0456354806905821, 
0.0567597911088563, 0.155069881077418, 0.0398552868265476), direction = c(-1, 
1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 
-1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 
-1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 
1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 
-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 
-1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 
-1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1), 
    plotval = c(-4.65436783913808, 7.4638390305887, -0.166061251006543, 
    3.15451610300161, 3.32743565041216, -3.36193787665198, 8.34218160099397, 
    -0.45134660936061, -0.861806358039495, 3.14989059429165, 
    -0.385424005854277, -2.11343559832442, 0.699014672037075, 
    -0.497079129132047, -0.104056259146589, -2.02963110287305, 
    -3.7673992980343, 1.12925537854411, 0.744739988834407, -2.95268419318977, 
    -1.36623884628847, 4.98189411623867, 0.0868661499973576, 
    3.28662435286223, 0.351109352101632, -1.57943969693098, -0.688197955780953, 
    0.857822769551858, 1.73066263028667, -1.48883721139058, -0.380556286225031, 
    -1.78716235272839, -0.809302569419764, -6.02028725962301, 
    -5.84785296501876, -3.073500877557, -0.967454268269813, -5.38748192000237, 
    -5.57408775663275, -3.96068672995749, 7.50597726212444, 4.69864253925926, 
    4.14916000637992, -0.386038843759309, -3.20486151733282, 
    -0.780262082752201, -0.507578804102515, -4.59710189032113, 
    -2.67740750044583, -2.65866587397551, -0.670087259849411, 
    -2.95884232700787, 11.6925265676543, 3.11870850390026, 3.94744095721937, 
    2.72176024102353, 1.08835175932808, -1.49196040507446, -1.8073900579076, 
    2.66167617047025, 0.485938344726535, 6.59364503804761, 7.0712906280104, 
    0.431581837757606, 1.11157095694052, -3.19891287043981, -1.40630643990113, 
    -1.48093021354005, -2.35066396611971, -1.16853980119637, 
    -1.25650588425321, -4.32298913566366, -4.3356578288347, -7.24851167871111, 
    -4.61532559535481, -2.04543461904724, -1.33840723441989, 
    -2.4416036868115, -2.23435464751167, -0.816092795927033, 
    -2.18133938975974, -1.89055102413018, -5.57799649218455, 
    9.04608546234764, 7.12511846787433, 1.80708450523289, -3.01381416921109, 
    -1.83891572308132, -3.49647343200987, -5.39744266823951, 
    -4.28691815843339, -1.6098703601382, 1.7879268409032, 5.98771381910823, 
    1.59742573937338, 0.276970230656392, 0.00761038521452666, 
    6.02426713306055, 4.33955424293266, 4.12760869236016, -0.656978297958022, 
    12.4807880165375, 0.200497935929844, 7.00759296681644, 7.06885254695129, 
    -0.955635485860221, -3.73647848327309, -0.591822947332438, 
    -2.51791492529225, -2.01490426327141, -2.84325677575944, 
    -2.34778887382083, -4.03335556106781, -1.32354959550492, 
    -4.63524905089898, -3.62690843318892, -3.49140657316158, 
    0.323600137070818, -0.806601481409993, -2.08862346391581, 
    -1.63414083395139, -1.08008625029083, -4.78845317422705, 
    -6.83177865764632, -1.71546873469442, -2.88647323876581, 
    8.11975527988995, 13.6134535668385, 6.13092941593436, 3.63304670440456, 
    4.56594416664396, 5.1291271177969, 3.37686864914492, -1.13283849034736, 
    -0.521716813134629, 0.474325491909851, -0.595750658150867, 
    -0.165616059566562, -0.939294549263021, -5.77552687887169, 
    3.93575531635149, -3.55969573624066, -4.6942082095516, 3.29673069510235, 
    2.0019383089733, 0.630908284813054, 0.439657756641714, -3.96350131412125, 
    -2.54956344582082, -3.75294570645333, 0.915102752209537, 
    -2.97355889939168, 17.6924962428955, 1.82492426830489, 9.34420585704084, 
    8.14549108393769, 3.50744655283763, 5.71356659688074, 6.6906179005526, 
    2.54589150999432, -2.732443084933, -7.32875220486331, -4.17302493900107, 
    3.57434916575198, 0.607879106033299, 0.98877892542788, -4.48550180288597, 
    -0.221261659119412, 0.562020428705443, -0.749156396749516, 
    -5.90368012822885, -5.81245996063456, -0.039518112762786, 
    4.49497802290257, 0.415792240525927, -2.95213537116121, -0.428878156658466, 
    -1.42292492276573, -0.773816960869839, 6.81496956627369), 
    category = c(""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Measured"", ""Measured"", ""Measured"", ""Measured"", 
    ""Measured"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", ""Derived"", 
    ""Derived"")), .Names = c(""Measurement"", ""variable"", ""Pval"", 
""effect"", ""direction"", ""plotval"", ""category""), row.names = c(21L, 
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 
35L, 36L, 37L, 38L, 39L, 40L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 
68L, 69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 
101L, 102L, 103L, 104L, 105L, 106L, 107L, 108L, 109L, 110L, 111L, 
112L, 113L, 114L, 115L, 116L, 117L, 118L, 119L, 120L, 141L, 142L, 
143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 
154L, 155L, 156L, 157L, 158L, 159L, 160L, 181L, 182L, 183L, 184L, 
185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 
196L, 197L, 198L, 199L, 200L, 221L, 222L, 223L, 224L, 225L, 226L, 
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 
238L, 239L, 240L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 
269L, 270L, 271L, 272L, 273L, 274L, 275L, 276L, 277L, 278L, 279L, 
280L, 301L, 302L, 303L, 304L, 305L, 306L, 307L, 308L, 309L, 310L, 
311L, 312L, 313L, 314L, 315L, 316L, 317L, 318L, 319L, 320L, 341L, 
342L, 343L, 344L, 345L, 346L, 347L, 348L, 349L, 350L, 351L, 352L, 
353L, 354L, 355L, 356L, 357L, 358L, 359L, 360L), class = ""data.frame"")
</code></pre>
","<h1>Option 1</h1>

<p>Use facets</p>

<pre><code>mockdata$type = ifelse(mockdata$variable %in% LETTERS[1:3], ""1"", ""2"")

ggplot(mockdata) + 
  facet_wrap(~ type, scales='free_x') +
  geom_tile(aes(variable, Measurement, fill = mockdata$plotval), colour = ""dark red"")  + 
  scale_fill_gradient2(limits=c(-20, 20),high = ""firebrick3"", low = ""dodgerblue4"") + 
  theme_minimal() + 
  theme(axis.text.x=element_text(size=28, angle=90), axis.text.y=element_text(size=28)) + 
  labs(title="""", x="""", y="""", fill="""") 
</code></pre>

<p><a href=""https://i.stack.imgur.com/tdh4T.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/tdh4T.png"" alt=""enter image description here""></a></p>

<p>Although this is not quite perfect, because dropping the unused x-axis categories from each facet requires specifying <code>scales='free_x'</code> - which unfortunately does not keep the boxes the same width in each facet.</p>

<h1>Option2</h1>

<p>To remedy this we can also set <code>space=""free_x""</code> in facet_grid. Which I think gives what you are looking for:</p>

<pre><code>ggplot(mockdata) + 
  facet_grid(~ type, scales='free_x', space=""free_x"") +
  geom_tile(aes(variable, Measurement, fill = mockdata$plotval), colour = ""dark red"")  + 
  scale_fill_gradient2(limits=c(-20, 20),high = ""firebrick3"", low = ""dodgerblue4"") + 
  theme_minimal() + 
  theme(axis.text.x=element_text(size=28, angle=90), axis.text.y=element_text(size=28)) + 
  labs(title="""", x="""", y="""", fill="""") 
</code></pre>

<p><a href=""https://i.stack.imgur.com/Ue5JM.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/Ue5JM.png"" alt=""enter image description here""></a></p>

<h1>Option 3</h1>

<p>An alternative solution is to create separate plots for each category, and arrange them together. Here I put them together using <code>cowplot::plot_grid</code>, which allows us to set the relative widths how we want them (although you may need to tweak the rel_widths values a bit to get it just right):</p>

<pre><code>library(cowplot)
p1 &lt;- ggplot(mockdata[which(mockdata$type==""1""),]) + 
  geom_tile(aes(variable, Measurement, fill = plotval), colour = ""dark red"")  + 
  scale_fill_gradient2(limits=c(-20, 20), high = ""firebrick3"", low = ""dodgerblue4"") + 
  theme_minimal() + 
  theme(axis.text.x=element_text(size=28, angle=90), axis.text.y=element_text(size=28),
        legend.position=""none"") + 
  labs(title="""", x="""", y="""", fill="""") 

p2 &lt;- ggplot(mockdata[which(mockdata$type==""2""),]) + 
  geom_tile(aes(variable, Measurement, fill = plotval), colour = ""dark red"")  + 
  scale_fill_gradient2(limits=c(-20, 20),high = ""firebrick3"", low = ""dodgerblue4"") + 
  theme_minimal() + 
  theme(axis.text.x=element_text(size=28, angle=90), axis.text.y=element_blank()) + 
  labs(title="""", x="""", y="""", fill="""") 

plot_grid(p1,p2, nrow = 1,  rel_widths = c(1,2))
</code></pre>

<p><a href=""https://i.stack.imgur.com/v7S9W.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/v7S9W.png"" alt=""enter image description here""></a></p>

<h1>Option 4</h1>

<p>You can create an extra factor level in the data, which lies between the columns you want to separate, and then plot tiles for that column in white.</p>

<pre><code>mockdata = rbind(mockdata, 
                 data.frame(Measurement=1:20, 
                            variable="""", 
                            Pval = NA,
                            effect = NA,
                            direction = NA,
                            plotval = 0,
                            category = ""Measured""))
mockdata$variable = factor(mockdata$variable, levels = c(""A"", ""B"", ""C"", """", ""a"", ""b"", ""c"", ""d"", ""e"",""f""))

ggplot(mockdata, aes(variable, Measurement)) + 
  geom_tile(fill = NA, colour = NA)  + 
  geom_tile(data = mockdata[which(mockdata$variable==""""),], fill = ""white"", colour = ""white"")  + 
  geom_tile(data = mockdata[which(mockdata$variable!=""""),], aes(fill = plotval), colour = ""dark red"")  + 
  scale_fill_gradient2(limits=c(-20, 20),high = ""firebrick3"", low = ""dodgerblue4"") + 
  theme_minimal() + 
  theme(axis.text.x=element_text(size=28, angle=90), axis.text.y=element_text(size=28)) + 
  labs(title="""", x="""", y="""", fill="""") 
</code></pre>

<p><a href=""https://i.stack.imgur.com/pJ7vC.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/pJ7vC.png"" alt=""enter image description here""></a></p>
"
31768695,setting query params to API with getURL in RCurl,2,2,4,"<p>I am trying to use R with Hotwire Hotels API:
Below is the original request:</p>

<pre><code>getURL(http://api.hotwire.com/v1/deal/hotel?dest=chicago&amp;apikey=##############&amp;format=JSON&amp;limit=50&amp;startdate=08/27/2015&amp;enddate=08/28/2015)
</code></pre>

<p>However, when I try to pass the ‘startdate<code>and ‘enddate</code> as parameters (see below), it does not seem to pick the date.</p>

<pre><code>getURL (""http://api.hotwire.com/v1/deal/hotel?dest=chicago&amp;apikey=##############&amp;format=JSON&amp;limit=50"",     
 httpheader=list(startdate=c_in,enddate=c_out))
</code></pre>

<p>Would appreciate any help on this.</p>

<p>Subra</p>
","<p>You should really try to switch to <code>httr</code> as it makes all the web calls much cleaner (and the API docs say the date params go in the query part):</p>

<pre><code>library(httr)

c_in &lt;- '08/02/2015'
c_out &lt;- '08/05/2015'

hotwire_api_url &lt;- ""http://api.hotwire.com/v1/deal/hotel""

res &lt;- GET(hotwire_api_url,
           query=list(dest=""chicago"",
                      apikey=""########"",
                      format=""JSON"",
                      limit=50,
                      startdate=c_in,
                      enddate=c_out))

# since return val is ""text/x-json""

recs &lt;- jsonlite::fromJSON(content(res, as=""text""))

head(recs$Result)
##                   FoundDate CurrencyCode NightDuration    EndDate
## 1 2015-08-01T23:38:48-07:00          USD           3.0 08/05/2015
## 2 2015-08-01T23:38:04-07:00          USD           3.0 08/05/2015
## 3 2015-08-01T23:38:47-07:00          USD           3.0 08/05/2015
## 4 2015-08-01T23:38:48-07:00          USD           3.0 08/05/2015
## 5 2015-08-01T23:38:36-07:00          USD           3.0 08/05/2015
## 6 2015-08-01T23:38:18-07:00          USD           3.0 08/05/2015
##                                 Headline IsWeekendStay Price  StartDate
## 1    Naperville 5 Star Hotel, $143/night         false 143.0 08/02/2015
## 2     Northbrook 4 Star Hotel, $67/night         false  67.0 08/02/2015
## 3     Chicago 4.5 Star Hotel, $208/night         false 208.0 08/02/2015
## 4 Schiller Park 4 Star Hotel, $140/night         false 140.0 08/02/2015
## 5     Chicago 4.5 Star Hotel, $142/night         false 142.0 08/02/2015
## 6     Naperville 4 Star Hotel, $98/night         false  98.0 08/02/2015
##                                                                                                                                                                                                                 Url
## 1     http://www.hotwire.com/hotel/superPage.jsp?encDealHash=MTAwOjE4NTAzOjE0MDUwOjUuMDoxNDMuMDpZOlk6WQ--&amp;rs=20500&amp;xid=x-103&amp;wid=w-3&amp;rid=r-69820702248&amp;startDate=08/02/2015&amp;endDate=08/05/2015&amp;bid=B311402&amp;sid=S298
## 2         http://www.hotwire.com/hotel/superPage.jsp?encDealHash=MTAwOjM1MTEwOjg3NTQ3OjQuMDo2Ny4wOlk6WTpZ&amp;rs=20500&amp;xid=x-103&amp;wid=w-3&amp;rid=r-69820702248&amp;startDate=08/02/2015&amp;endDate=08/05/2015&amp;bid=B311402&amp;sid=S298
## 3 http://www.hotwire.com/hotel/superPage.jsp?encDealHash=MTAwOjQ4Njk6ODc1NDU6NC41OjIwOC4wMDAwMjpZOlk6WQ--&amp;rs=20500&amp;xid=x-103&amp;wid=w-3&amp;rid=r-69820702248&amp;startDate=08/02/2015&amp;endDate=08/05/2015&amp;bid=B311402&amp;sid=S298
## 4     http://www.hotwire.com/hotel/superPage.jsp?encDealHash=MTAwOjM1MTI3Ojg5ODg5OjQuMDoxNDAuMDpZOlk6WQ--&amp;rs=20500&amp;xid=x-103&amp;wid=w-3&amp;rid=r-69820702248&amp;startDate=08/02/2015&amp;endDate=08/05/2015&amp;bid=B311402&amp;sid=S298
## 5         http://www.hotwire.com/hotel/superPage.jsp?encDealHash=MTAwOjQ4Njk6ODc5NTE6NC41OjE0Mi4wOlk6WTpZ&amp;rs=20500&amp;xid=x-103&amp;wid=w-3&amp;rid=r-69820702248&amp;startDate=08/02/2015&amp;endDate=08/05/2015&amp;bid=B311402&amp;sid=S298
## 6         http://www.hotwire.com/hotel/superPage.jsp?encDealHash=MTAwOjE4NTAzOjE0MDUwOjQuMDo5OC4wOlk6WTpZ&amp;rs=20500&amp;xid=x-103&amp;wid=w-3&amp;rid=r-69820702248&amp;startDate=08/02/2015&amp;endDate=08/05/2015&amp;bid=B311402&amp;sid=S298
##            City CountryCode NeighborhoodLatitude NeighborhoodLongitude
## 1    Naperville          US              41.8008              -88.1328
## 2    Northbrook          US              42.1138              -87.8601
## 3       Chicago          US              41.8989              -87.6243
## 4 Schiller Park          US              41.9334              -87.9029
## 5       Chicago          US              41.8549              -87.6221
## 6    Naperville          US              41.8008              -88.1328
##                                   Neighborhood StarRating StateCode
## 1                           Lisle - Naperville        5.0        IL
## 2         Northbrook - Glenview - Mt. Prospect        4.0        IL
## 3         Magnificent Mile North - Water Tower        4.5        IL
## 4                O'Hare Intl Airport ORD South        4.0        IL
## 5 McCormick Place - South Loop - Soldier Field        4.5        IL
## 6                           Lisle - Naperville        4.0        IL
</code></pre>
"
27907750,How to write unit tests for suggested packages?,2,2,2,"<p>Packages in R can have different types of dependencies on other packages. Some of these types indicate hard requirements, i.e. <code>Depends</code>, <code>Imports</code> and <code>LinkingTo</code>.</p>

<p>However, there is a second category that indicate a softer dependency, i.e. <code>Suggests</code> and <code>Enhances</code>.  In both these cases, the package provides additional functionality if the suggested / enhanced package is available.</p>

<p>Here is a concrete example: The package <a href=""http://cran.r-project.org/web/packages/checkpoint/index.html"" rel=""noreferrer""><code>checkpoint</code></a> imports <code>knitr</code> because <code>knitr</code> helps <code>checkpoint</code> to parse <code>rmarkdown</code> files.</p>

<p>But now I am considering changing <code>knitr</code> to a <code>Suggests</code> dependency, i.e. only provide this functionality if <code>knitr</code> is actually installed.</p>

<p>For proper unit testing, this means I have to test both scenarios:</p>

<ol>
<li>If <code>knitr</code> is available, then do stuff.</li>
<li>If <code>knitr</code> is <strong>not</strong> available, then throw a warning and do nothing.</li>
</ol>

<p>The actual R code is simple:</p>

<pre><code>if(require(knitr)) {
  do_stuff()
} else {
  message(""blah"")
}
</code></pre>

<p><strong>Question</strong></p>

<p>But how can I set up unit tests for both scenarios?</p>

<p>The way I see it, the simple fact of checking for <code>require(knitr)</code> will load the <code>knitr</code> package if it is available in the local library.</p>

<p>So, to test for case 1, I have to install <code>knitr</code> locally, meaning I can't test for case 2.</p>

<p><strong>Is there a way of configuring <code>testthat</code> (or any other unit testing framework) for this use case?</strong></p>
","<h2>tl;dr</h2>

<p>To test the branch followed when use <code>require(knitr)</code> fails, use <code>trace()</code> to temporarily modify <code>require()</code> so that it won't find <strong>knitr</strong>, even if it is present on <code>.libPaths()</code>. Specifically, in the body of <code>require()</code>, reset the value of <code>lib.loc=</code> to point to <code>R.home()</code> -- an existing directory that does not contain a <code>knitr</code> package. </p>

<p>This seems to work just as well in a package as it will in an interactive session in which you run the following:</p>

<pre><code>find.package(""knitr"")

trace(""require"", quote(lib.loc &lt;- R.home()), at=1)
isTRUE(suppressMessages(suppressWarnings(require(knitr))))

untrace(""require"")
isTRUE(suppressMessages(suppressWarnings(require(knitr))))
</code></pre>

<hr>

<p>As I understand this, you have a function with two branches, one to be performed in R sessions for which <code>require(knitr)</code> succeeds, and the other to be performed in sessions where it fails. You are then wanting to test this function ""both ways"" from a single R instance in which <strong>knitr</strong> actually is on <code>.libPaths()</code>.</p>

<p>So basically you are needing some way to temporarily blind the call <code>require(knitr)</code> to the actual presence of <strong>knitr</strong>. Completely and temporarily resetting the value returned by <code>.libPaths()</code> looked promising, but doesn't seem to be possible. </p>

<p>Another promising avenue is to somehow reset the default value of <code>lib.loc</code> in calls to <code>require()</code> from <code>NULL</code> (which means ""use the value of <code>.libPaths()</code>) to some other location where <strong>knitr</strong> is not available. You can't accomplish this by overwriting <code>base::require()</code>, nor (in a package) can you get there by defining a local masking version of <code>require()</code> with the desired value of <code>lib.loc</code>. </p>

<p>It does, though, look like you can get away with using <code>trace()</code> to temporarily modify <code>require()</code> (blinding it to <strong>knitr</strong>'s availability by setting <code>lib.loc=R.home()</code>). Then do <code>untrace()</code> to restore <code>require()</code> to the vanilla version which will  go ahead and find <strong>knitr</strong>.  </p>

<p>Here's what that looked like in the dummy package I tested this with. First an R function that allows us to test for success along the two branches</p>

<pre><code>## $PKG_SRC/R/hello.R

hello &lt;- function(x=1) {
    if(require(knitr)) {
        x==2
    } else {
        x==3
    }
}
</code></pre>

<p>Then a couple of tests, one for each branch:</p>

<pre><code>## $PKG_SRC/inst/tests/testme.R

## Test the second branch, run when require(knitr) fails
trace(""require"", quote(lib.loc &lt;- R.home()), at=1)
stopifnot(hello(3))
untrace(""require"")

## Test the first branch, run when require(knitr) succeeds
stopifnot(hello(2))
</code></pre>

<p>To test this, I used <code>pkgKitten::kitten(""dummy"")</code> to set up a source directory, copied in these two files, added <code>Suggests: knitr</code> to the <code>DESCRIPTION</code> file, and then ran <code>devtools::install()</code> and <code>devtools::check()</code> from the appropriate directory. The package installs just fine, and passes all of the checks.</p>
"
8750402,Setting output resolution of kde object,1,3,3,"<p>I'm trying to figure out how to correctly set the output resolution of a kde object created using the kde function in the ks library in R. Basically, I have some SpatialPoints objects that I am feeding into kde and converting the output to a raster. I want the cells of this raster to have a specific resolution. </p>

<p>Here's an example using the meuse data set, as requested.</p>

<pre><code>library(ks)
library(raster)
data(meuse)
points = data.frame(meuse$x,meuse$y)
raster(kde(points,Hlscv(points)))
</code></pre>

<p>The output I get from this code is:</p>

<pre><code>class       : RasterLayer 
dimensions  : 151, 151, 22801  (nrow, ncol, ncell)
resolution  : 31.37394, 46.03558  (x, y) 
extent      : 177628.8, 182366.2, 328186.8, 335138.2  (xmin, xmax, ymin, ymax)
projection  : NA 
values      : in memory
min value   : 0 
max value   : 2.925851e-07 
</code></pre>

<p>I want to find a way to set the output resolution (third row of output) to be what I want.</p>

<p>Now I know that kde has 'gridsize' and 'bgridsize' options and to use these you set the number of points/cells you want in each dimension. However, without knowing the extent of the output you cannot calculate the number of cells to get a specific resolution.</p>

<p>One thought I've had is to use the H-value in the appropriate dimension to buffer the min and max coordinates of each dimension and pre-derive the extent for the kde output. However, I think this would only work with diagonal matrices for H and so I am not sure it could be implementable with a full 2x2 H-value matrix.</p>

<p>I am also aware that you can resample a raster but I want to ensure that I am not coming from a lower resolution kde object.</p>
","<p>I was poking around and noticed that there was a consistent relationship between the increase in extent of the output kde raster compared to the input points and the H parameters. I don't know why this is the case (if it is due to the implementation or the math behind things), so it may not hold for all cases, but it works consistently for me, so I'm passing it on. Specifically, I found that (this is an equation, not code): </p>

<pre><code>(range(coordinates(output)) - range(coordinates(input))) / H = ~7.4 for each dimension.
</code></pre>

<p>Consider the following code:</p>

<pre><code>for (i in seq(1:10)){
    pts = data.frame(x=rnorm(100,300000,2500),y=rnorm(100,4000000,2500))
    rangePtsX &lt;- diff(range(pts$x))
    rangePtsY &lt;- diff(range(pts$y))
    H  &lt;- Hlscv(pts)
    ras &lt;- raster(kde(pts,H))
    rangeRasX &lt;- xmax(ras) - xmin(ras)
    rangeRasY &lt;- ymax(ras) - ymin(ras)
    rangeDiffX &lt;- rangeRasX - rangePtsX
    rangeDiffY &lt;- rangeRasY - rangePtsY
    print(paste(""rangeDiffX/hX:"",rangeDiffX/sqrt(H[1,1]),""rangeDiffY/hY:"",rangeDiffY/sqrt(H[2,2])))
}
</code></pre>

<p>The output of this is:</p>

<pre><code>[1] ""rangeDiffX/hX: 7.37104456534156 rangeDiffY/hY: 7.37763153209006""
[1] ""rangeDiffX/hX: 7.39015683159216 rangeDiffY/hY: 7.39151926375274""
[1] ""rangeDiffX/hX: 7.39492769120192 rangeDiffY/hY: 7.39414077521017""
[1] ""rangeDiffX/hX: 7.39909462708713 rangeDiffY/hY: 7.39917867776494""
[1] ""rangeDiffX/hX: 7.39801448966617 rangeDiffY/hY: 7.39779576937998""
[1] ""rangeDiffX/hX: 7.39679742756067 rangeDiffY/hY: 7.39745249174806""
[1] ""rangeDiffX/hX: 7.39405975028797 rangeDiffY/hY: 7.39368126656615""
[1] ""rangeDiffX/hX: 7.3913950522465 rangeDiffY/hY: 7.38980236385133""
[1] ""rangeDiffX/hX: 7.39988585440102 rangeDiffY/hY: 7.39988850314936""
[1] ""rangeDiffX/hX: 7.39529001635855 rangeDiffY/hY: 7.39475036015628""
</code></pre>

<p>From there it was simple to write a function to calculate the optimum number of cells:</p>

<pre><code>gridSize &lt;- function(pts,H,res){
    sizeX &lt;- (diff(range(pts[,1])) + (7.4 * sqrt(H[1,1]))) / res
    sizeY &lt;- (diff(range(pts[,2])) + (7.4 * sqrt(H[2,2]))) / res
    c(sizeX,sizeY)
}
</code></pre>

<p>This works for different selectors as well:</p>

<pre><code>for (hMethod in c(""Hlscv"",""Hlscv.diag"",""Hscv"",""Hpi"",""Hpi.diag"")){
    for (i in seq(1:5)){
        pts &lt;- data.frame(x=rnorm(100,300000,2500),y=rnorm(100,4000000,2500))
        ras &lt;- raster(kde(pts,eval(call(hMethod,x=pts)),gridsize=gridSize(pts,H,30)))
        print(paste(""xres:"",xres(ras),""yres:"",yres(ras)))
    }
}
</code></pre>

<p>produces:</p>

<pre><code>    [1] ""xres: 29.8498137761045 yres: 29.8456700392426""
    [1] ""xres: 29.9573491524671 yres: 29.9874090657282""
    [1] ""xres: 29.968525344047 yres: 29.9580897162408""
    [1] ""xres: 29.9498803408057 yres: 29.964382664777""
    [1] ""xres: 29.9711108728299 yres: 29.9773401860409""
    [1] ""xres: 29.9950831714231 yres: 29.9658642153949""
    [1] ""xres: 29.9905564968586 yres: 29.982738272666""
    [1] ""xres: 29.9527729769381 yres: 29.9855591986985""
    [1] ""xres: 29.9786535322154 yres: 29.9594421322198""
    [1] ""xres: 29.9461263582666 yres: 29.9587155045891""
    [1] ""xres: 32.1494143184879 yres: 31.9656619568261""
    [1] ""xres: 31.3425525696929 yres: 31.7046601198584""
    [1] ""xres: 31.9515186102478 yres: 31.9042586036464""
    [1] ""xres: 31.5043829006183 yres: 30.0677630099221""
    [1] ""xres: 31.1816325999623 yres: 30.782974425409""
    [1] ""xres: 29.4406922338173 yres: 28.70952461795""
    [1] ""xres: 31.0945577583419 yres: 31.2995894646556""
    [1] ""xres: 29.3060327529272 yres: 29.4708662690499""
    [1] ""xres: 29.502732054192 yres: 29.3034911939017""
    [1] ""xres: 30.0529058397693 yres: 29.2893540247008""
    [1] ""xres: 27.7898933275596 yres: 29.1928490584976""
    [1] ""xres: 27.7628745096943 yres: 27.5794864810828""
    [1] ""xres: 28.9513504972817 yres: 29.7665791290592""
    [1] ""xres: 28.9569389967698 yres: 29.2103688932787""
    [1] ""xres: 28.6005579365073 yres: 29.3701912564357""
</code></pre>

<p>Which produces results very close to the value of 30 that I wanted (there is error inherent in choosing an integer number of grid cells in the kde). If I want a more exact resolution, I can resample / disaggregate the raster from here.</p>
"
36191297,Storing a value in a matrix using a loop,1,3,3,"<p>My code is:</p>

<pre><code>set.seed(100)
n &lt;- 10
d &lt;- rep(NA, n)
d[1] &lt;- 0
y &lt;- runif(n)
a &lt;- 10

for (i in 2:length(y)) {
    d[i] &lt;- d[i-1] + y[i-1]
} #This creates an interval with endpoints at my y uniform RVs

store.x &lt;- NULL
for (j in 1:a) {
    x &lt;- runif(1, min =0, max =sum(y))
    for (i in 1:length(y)) {
        if (x &lt;= d[i+1] &amp;&amp; x &gt; d[i]) {
            store.x[j] &lt;- i
            break
        }
    }
}  #This tells you which interval my x uniform RV is in
</code></pre>

<p>Now instead of store.x being a vector that tells me what intervals x falls into, I want it to be stored in the corresponding row and column of a matrix with the value of 1.  So for my first x since it falls into interval 7, my matrix will be all zeros except for a one in the first row, seventh column and a one in the seventh row, first column.</p>

<p>Any ideas on how to do this would be much appreciated!
Thanks</p>
","<p>You can more efficiently implement this algorithm using vectorized calls to <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Uniform.html"" rel=""nofollow""><code>runif()</code></a>, <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/cumsum.html"" rel=""nofollow""><code>cumsum()</code></a>, and <a href=""https://stat.ethz.ch/R-manual/R-devel/library/base/html/findInterval.html"" rel=""nofollow""><code>findInterval()</code></a>:</p>

<pre><code>set.seed(1L); ## seed the PRNG for reproducible results
n &lt;- 10L; ## length of x, y, and result matrix dimensions
y &lt;- runif(n); ## produce random interval lengths
yb &lt;- cumsum(c(0,y)); ## calculate interval boundaries
x &lt;- runif(n,0,yb[length(yb)]); ## generate random x values
i &lt;- findInterval(x,yb); ## find which intervals contain the x values
m &lt;- matrix(0,n,n); ## init the result matrix
m[matrix(c(seq_len(n),i,i,seq_len(n)),ncol=2L)] &lt;- 1; ## symmetric 1 assignment
m; ## print result
##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,]    0    0    1    0    0    0    0    0    0     0
##  [2,]    0    0    1    0    0    0    0    0    0     0
##  [3,]    1    1    0    0    0    0    1    0    0     0
##  [4,]    0    0    0    1    0    0    0    0    1     0
##  [5,]    0    0    0    0    0    0    0    1    0     0
##  [6,]    0    0    0    0    0    1    0    0    0     0
##  [7,]    0    0    1    0    0    0    1    0    0     0
##  [8,]    0    0    0    0    1    0    0    0    0     1
##  [9,]    0    0    0    1    0    0    0    0    0     0
## [10,]    0    0    0    0    0    0    0    1    0     0
</code></pre>

<p>Technically, the way you've written the interval test <code>x &lt;= d[i+1] &amp;&amp; x &gt; d[i]</code> means that you want the left boundary to be <em>open</em> and the right boundary to be <em>closed</em> for each interval. It appears that <code>findInterval()</code> has only recently added support for this variation on its logic, specifically in the <code>r69814</code> development snapshot (which will eventually become <code>R-3.3.0</code>). So you probably don't have access to it yet, but when you do, you can pass <code>left.open=T</code> to get that behavior.</p>

<hr>

<p>If we have <code>a</code> elements of <code>x</code> allowing for <code>a != n</code> then we have to decide how to map the index of each element in <code>x</code> to the index of the result matrix. The above solution assumes that <code>a == n</code> and that there is a direct mapping between the two index domains.</p>

<p>If we consider the elements of <code>x</code> to correspond to result matrix indexes <code>1:n</code> in a cyclic fashion, then we can define <code>xi</code> to be the cyclic index, taking <code>yi</code> as <code>i</code>, i.e. the <code>y</code> interval index in which the <code>x</code> element landed. We can then aggregate all <code>(xi,yi)</code> hits to produce a count for each cell.</p>

<p>If you still want symmetry, then we can further accumulate the count for each <code>(yi,xi)</code>, thus double-counting each hit, once for each of the two symmetric cells.</p>

<pre><code>set.seed(1L); ## seed the PRNG for reproducible results
n &lt;- 10L; ## length of y and result matrix dimensions
a &lt;- 100L; ## length of x
y &lt;- runif(n); ## produce random interval lengths
yb &lt;- cumsum(c(0,y)); ## calculate interval boundaries
x &lt;- runif(a,0,yb[length(yb)]); ## generate random x values
i &lt;- findInterval(x,yb); ## find which intervals contain the x values
m &lt;- matrix(0,n,n); ## init the result matrix
hit &lt;- as.matrix(aggregate(n~xi+yi,cbind(xi=seq_len(n),yi=i,n=1),sum)); ## aggregate hits
m[hit[,c('xi','yi')]] &lt;- m[hit[,c('xi','yi')]]+hit[,'n']; ## add n to xi,yi
m[hit[,c('yi','xi')]] &lt;- m[hit[,c('yi','xi')]]+hit[,'n']; ## add n to yi,xi
m; ## print result
##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,]    0    0    1    2    0    3    3    1    2     0
##  [2,]    0    2    2    2    1    1    3    3    1     0
##  [3,]    1    2    0    4    1    5    4    2    0     1
##  [4,]    2    2    4   10    1    2    1    1    3     2
##  [5,]    0    1    1    1    0    3    2    6    0     2
##  [6,]    3    1    5    2    3    2    3    5    1     0
##  [7,]    3    3    4    1    2    3    4    2    3     3
##  [8,]    1    3    2    1    6    5    2    2    3     2
##  [9,]    2    1    0    3    0    1    3    3    2     2
## [10,]    0    0    1    2    2    0    3    2    2     0
</code></pre>
"
46240619,A way to extract the name of subarray that was sampled in R,1,3,3,"<p>I have a simulation that I have constructed and need to refer to a given subarray that was randomly sampled from in order to create some plots using a <code>for</code> loop. I am not quite certain how to go about accomplishing this...</p>

<p>Here is my code:</p>

<pre><code>K &lt;- 2       # number of subarrays
N &lt;- 100 
Hstar &lt;- 10

perms &lt;- 10000

p &lt;- 0.95

specs &lt;- 1:N 

pop &lt;- array(dim = c(c(perms, N), K))

haps &lt;- as.character(1:Hstar)

probs &lt;- rep(1/Hstar, Hstar) 

for(j in 1:perms){
    for(i in 1:K){ 
        if(i == 1){
            pop[j, specs, i] &lt;- sample(haps, size = N, replace = TRUE, prob = probs)
    }
        else{
            K1 &lt;- c(1:5)
            K2 &lt;- c(6:10)
            pop[j ,, 1] &lt;- sample(haps[K1], size = N, replace = TRUE, prob = probs[K1])
            pop[j ,, 2] &lt;- sample(haps[K2], size = N, replace = TRUE, prob = probs[K2]) 
        }
    }
}

HAC.mat &lt;- array(dim = c(c(perms, N), K))

for(k in specs){
    for(j in 1:perms){
        for(i in 1:K){ 
            ind.index &lt;- sample(specs, size = k, replace = FALSE)
            hap.plot &lt;- pop[sample(1:nrow(pop), size = 1, replace = TRUE),     ind.index, sample(1:K, size = 1, replace = TRUE)]
            HAC.mat[j, k, i] &lt;- length(unique(hap.plot))
    }
  }
}

means &lt;- apply(HAC.mat, MARGIN = 2, mean)
lower &lt;- apply(HAC.mat, MARGIN = 2, function(x) quantile(x, 0.025))
upper &lt;- apply(HAC.mat, MARGIN = 2, function(x) quantile(x, 0.975))

par(mfrow = c(1, 2))

for(i in 1:K){
    if(i == 1){
        plot(specs, means, type = ""n"", xlab = ""Specimens sampled"", ylab = ""Unique haplotypes"", ylim = c(1, Hstar))
        polygon(x = c(specs, rev(specs)), y = c(lower, rev(upper)), col = ""gray"")
        lines(specs, means, lwd = 2)
        HAC.bar &lt;- barplot(N*probs, xlab = ""Unique haplotypes"", ylab = ""Specimens sampled"", names.arg = 1:Hstar)

}
    else{
        plot(specs, means, type = ""n"", xlab = ""Specimens sampled"", ylab = ""Unique haplotypes"", ylim = c(1, max(means)))
        polygon(x = c(specs, rev(specs)), y = c(lower, rev(upper)), col = ""gray"")
        lines(specs, means, lwd = 2)
        HAC.bar &lt;- barplot(max(HAC.mat)*probs[K1], xlab = ""Unique haplotypes"", ylab = ""Specimens sampled"", names.arg = min(means):ceiling(max(means)))
    }
}
</code></pre>

<p>The issue is in the last else statement where I indicate</p>

<pre><code>max(HAC.mat)*probs[K1]
</code></pre>

<p>Above, I just put K1 as an illustration. However, in reality, I don't know which row of K1 or K2 was sampled. Is there a way to specify this generically? Something like K[i]?</p>

<p>Thanks! Please let me know if more clarification is needed. </p>
","<p>You can use <code>get</code> and <code>paste0</code>. I made your example have less permutations so it finishes faster (since it's just for testing this) and I made the <code>probs</code> different so that you can see that it works.</p>

<pre><code>K &lt;- 2       # number of subarrays
N &lt;- 10 
Hstar &lt;- 10

perms &lt;- 1000

p &lt;- 0.95

specs &lt;- 1:N 

pop &lt;- array(dim = c(c(perms, N), K))

haps &lt;- as.character(1:Hstar)

probs &lt;- seq(1/Hstar, 1,.1) 

for(j in 1:perms){
  for(i in 1:K){ 
    if(i == 1){
      pop[j, specs, i] &lt;- sample(haps, size = N, replace = TRUE, prob = probs)
    }
    else{
      K1 &lt;- c(1:5)
      K2 &lt;- c(6:10)
      pop[j ,, 1] &lt;- sample(haps[K1], size = N, replace = TRUE, prob = probs[K1])
      pop[j ,, 2] &lt;- sample(haps[K2], size = N, replace = TRUE, prob = probs[K2]) 
      print(paste(""j is: "", j))
    }
  }
}

HAC.mat &lt;- array(dim = c(c(perms, N), K))

for(k in specs){
  for(j in 1:perms){
    for(i in 1:K){ 
      ind.index &lt;- sample(specs, size = k, replace = FALSE)
      hap.plot &lt;- pop[sample(1:nrow(pop), size = 1, replace = TRUE),     ind.index, sample(1:K, size = 1, replace = TRUE)]
      HAC.mat[j, k, i] &lt;- length(unique(hap.plot))
    }
  }
}

means &lt;- apply(HAC.mat, MARGIN = 2, mean)
lower &lt;- apply(HAC.mat, MARGIN = 2, function(x) quantile(x, 0.025))
upper &lt;- apply(HAC.mat, MARGIN = 2, function(x) quantile(x, 0.975))

par(mfrow = c(1, 2))

for(i in 1:K){
  print(paste(""i is:"",i))
  print(paste(""probs[...] is:"", probs[get(paste0(""K"",i))]))
  if(i == 1){
    plot(specs, means, type = ""n"", xlab = ""Specimens sampled"", ylab = ""Unique haplotypes"", ylim = c(1, Hstar))
    polygon(x = c(specs, rev(specs)), y = c(lower, rev(upper)), col = ""gray"")
    lines(specs, means, lwd = 2)
    HAC.bar &lt;- barplot(N*probs, xlab = ""Unique haplotypes"", ylab = ""Specimens sampled"", names.arg = 1:Hstar)

  }
  else{
    plot(specs, means, type = ""n"", xlab = ""Specimens sampled"", ylab = ""Unique haplotypes"", ylim = c(1, max(means)))
    polygon(x = c(specs, rev(specs)), y = c(lower, rev(upper)), col = ""gray"")
    lines(specs, means, lwd = 2)
    HAC.bar &lt;- barplot(max(HAC.mat)*probs[get(paste0(""K"",i))], xlab = ""Unique haplotypes"", ylab = ""Specimens sampled"", names.arg = min(means):ceiling(max(means)))
  }
}
</code></pre>

<p>I added the print statements so that we can see that it works:</p>

<pre><code>[1] ""i is: 1""
[1] ""probs[...] is: 0.1"" ""probs[...] is: 0.2"" ""probs[...] is: 0.3"" ""probs[...] is: 0.4"" ""probs[...] is: 0.5""
[1] ""i is: 2""
[1] ""probs[...] is: 0.6"" ""probs[...] is: 0.7"" ""probs[...] is: 0.8"" ""probs[...] is: 0.9"" ""probs[...] is: 1""
</code></pre>
"
15432529,Polynomial roots,2,3,3,"<p>I have a small issue with a polynomial:</p>

<pre><code>z²+alpha1*z + alpha2 = 0
</code></pre>

<p>I need to fins the values of alpha1 and alpha2 wihtin the roots of |z| &lt; 1. Is there any program in R or Matlab able to do it?
the thing is that the alpha values are not known. I need to find the allowed area where the roots of the polynomial are &lt;= |1|</p>
","<p>@Jonel_R, your problem can be solved analytically.</p>

<p>First I'll rename your variables to make it easier to type. I'll also use some notation abuse...</p>

<p>We want to find the values <code>(a, b)</code> such that the roots of <code>z^2 + a z + b == 0</code> satisfy the property <code>|z|&lt;=1</code>.</p>

<p>The roots are given by <code>(-a +- sqrt(d))/2</code>, where <code>d = a^2 - 4b</code></p>

<p>There are 3 possibilities. Two real distinct roots, one real root or two complex conjugate roots.</p>

<p>The middle case happens when <code>d = 0</code>, i. e., <code>b = a^2 / 4</code>. This is a parabola in the <code>a vs. b</code> plane. Not all points in this parabola generate polynomials whose root satisfy <code>|z|&lt;=1</code>, however. The root, is this case, is simply <code>-a/2</code>, so we mus add the condition <code>-1 &lt;= a/2 &lt;=1</code>, i. e., <code>-2 &lt;= a &lt;= 2</code>.</p>

<p>Now let's consider the first case. The points in the <code>a vs. b</code> plae that generate polynomials with two distinct real roots lie <em>below</em> the parabola, i. e., they must satisfy <code>b &lt; a^2/4</code>. The additional condition is that <code>|z| = |(-a +- sqrt(d))/2| &lt;= 1</code>.</p>

<p>The condition can be written as <code>-1 &lt;= (-a +- sqrt(d))/2 &lt;= 1</code>, where <code>+-</code> means both roots must satisfy the condition. Working this out we get:<br>
<code>a-2 &lt;= sqrt(d) &lt;= a+2</code> &amp; <code>a-2 &lt;= -sqrt(d) &lt;= a+2</code>  </p>

<p>Since both <code>sqrt(d)</code> and <code>-sqrt(d)</code> must lie in the interval <code>[a-2, a+2]</code>, and <code>d &gt; 0</code>, then this interval must contain zero in its interior. This means <code>-2 &lt; a &lt; 2</code>.</p>

<p>The conditions can be joined as:
<code>a-2 &lt;= -sqrt(d) &lt; 0 &lt; sqrt(d) &lt;= a+2</code>  </p>

<p>Squaring gives:
<code>(a-2)^2 &gt;= d</code> &amp; <code>d &lt;= (a+2)^2</code>  </p>

<p><code>d &lt;=  a^2 - 4a + 4</code> &amp; <code>d &lt;= a^2 + 4a + 4</code>  </p>

<p><code>-4b &lt;= -4a + 4</code> &amp; <code>-4b &lt;= +4a + 4</code>  </p>

<p><code>b &gt;= a-1</code> &amp; <code>b &gt;= -a-1</code>  </p>

<p>This means that <code>b</code> must be located above the lines <code>b = a-1</code> and <code>b=-a-1</code>. Also, <code>a</code> must be in <code>[-2,2]</code>. And, of course, we must have <code>b &lt; a^2/4</code>. Wow...</p>

<p>Now the last case: complex roots. This is easier. Since <code>d &lt; 0</code>, the roots are <code>-a/2 +- i * sqrt(-d)/2</code>. The absolute value of this is <code>a^2/4 - d/4</code>. This equals <code>b</code>, simply. So the condition is <code>b &lt;= 1</code>, and, as always, <code>b</code> lying above that parabola.</p>

<p>That's it... Quite interesting problem. :-)</p>

<p>You can try the following test function: It'll plot the points with real roots in blue and complex roots in red.</p>

<pre><code>test &lt;- function(x=2, n=10000)
{
    plot(c(-x,x), c(-x,x), type=""n"")

    plot(function(a) (a^2)/4, from=-x, to=x, add=T)

    plot(function(a) a-1, from=-x, to=x, add=T)

    plot(function(a) -a-1, from=-x, to=x, add=T)

    a &lt;- runif(n, -x, x)

    b &lt;- runif(n, -x, x)

    for( i in 1:n )
    {
        if( all(abs(polyroot(c(b[i],a[i],1))) &lt;= 1) )
        {
            col &lt;- ifelse(b[i] &lt; 0.25*a[i]^2, ""blue"", ""red"")

            points(a[i], b[i], pch=""."", col=col)
        }
    }
}
</code></pre>

<p>BTW: the syntax for <code>polyroot</code> is <code>polyroot(c(C, B, A))</code> gives the roots of <code>Ax^2 + Bx + C</code>. I believe @agstudy response got it wrong.</p>
"
14949437,How can I recreate this 2d surface + contour + glyph plot in R?,1,3,3,"<p>I've run a 2d simulation in some modelling software from which i've got an export of x,y point locations with a set of 6 attributes. I wish to recreate a figure that combines the data, like this:
<img src=""https://i.stack.imgur.com/Cprni.png"" alt=""enter image description here""></p>

<p>The ellipses and the background are shaded according to attribute 1 (and the borders of these are of course representing the model geometry, but I don't think I can replicate that), the isolines are contours of attribute 2, and the arrow glyphs are from attributes 3 (x magnitude) and 4 (y magnitude).</p>

<p>The x,y points are centres of the triangulated mesh I think, and look like this:
<img src=""https://i.stack.imgur.com/j4Nx6.png"" alt=""enter image description here""></p>

<p>I want to know how I can recreate a plot like this with R. To start with I have irregularly-spaced data due to it being exported from an irregular mesh. That's immediately where I get stuck with R, having only ever used it for producing box-and-whisper plots and the like.</p>

<p>Here's the data:
<a href=""https://dl.dropbox.com/u/22417033/Ellipses_noheader.txt"" rel=""nofollow noreferrer"">https://dl.dropbox.com/u/22417033/Ellipses_noheader.txt</a><br>
Edit: fields: x, y, heat flux (x), heat flux (y), thermal conductivity, Temperature, gradT (x), gradT (y).</p>

<pre><code>names(Ellipses) &lt;- c('x','y','dfluxx','dfluxy','kxx','Temps','gradTx','gradTy')
</code></pre>
","<p>@DWin is right to say that your graph don't represent faithfully your data, so I would advice to follow his answer. However here is how to reproduce (the closest I could) your graph:</p>

<pre><code>Ellipses &lt;- read.table(file.choose())
names(Ellipses) &lt;- c('x','y','dfluxx','dfluxy','kxx','Temps','gradTx','gradTy')
require(splancs)
require(akima)
</code></pre>

<p>First preparing the data:</p>

<pre><code>#First the background layer (the 'kxx' layer):
# Here the regular grid on which we're gonna do the interpolation
E.grid &lt;- with(Ellipses, 
               expand.grid(seq(min(x),max(x),length=200),
                           seq(min(y),max(y),length=200)))
names(E.grid) &lt;- c(""x"",""y"") # Without this step, function inout throws an error
E.grid$Value &lt;- rep(0,nrow(E.grid))
#Split the dataset according to unique values of kxx
E.k &lt;- split(Ellipses,Ellipses$kxx)
# Find the convex hull delimiting each of those values domain
E.k.ch &lt;- lapply(E.k,function(X){X[chull(X$x,X$y),]}) 
for(i in unique(Ellipses$kxx)){ # Pick the value for each coordinate in our regular grid
    E.grid$Value[inout(E.grid[,1:2],E.k.ch[names(E.k.ch)==i][[1]],bound=TRUE)]&lt;-i
}

# Then the regular grid for the second layer (Temp)
T.grid &lt;- with(Ellipses,
               interp(x,y,Temps, xo=seq(min(x),max(x),length=200),        
                      yo=seq(min(y),max(y),length=200), 
                      duplicate=""mean"", linear=FALSE))
# The regular grids for the arrow layer (gradT)
dx &lt;- with(Ellipses,
           interp(x,y,gradTx,xo=seq(min(x),max(x),length=15),
                  yo=seq(min(y),max(y),length=10),
                  duplicate=""mean"", linear=FALSE))
dy &lt;- with(Ellipses,
           interp(x,y,gradTy,xo=seq(min(x),max(x),length=15),
                  yo=seq(min(y),max(y),length=10),
                  duplicate=""mean"", linear=FALSE))
T.grid2 &lt;- with(Ellipses,
               interp(x,y,Temps, xo=seq(min(x),max(x),length=15),        
                      yo=seq(min(y),max(y),length=10), 
                      duplicate=""mean"", linear=FALSE))
gradTgrid&lt;-expand.grid(dx$x,dx$y)
</code></pre>

<p>And then the plotting:</p>

<pre><code>palette(grey(seq(0.5,0.9,length=5)))
par(mar=rep(0,4))
plot(E.grid$x, E.grid$y, col=E.grid$Value, 
     axes=F, xaxs=""i"", yaxs=""i"", pch=19)
contour(T.grid, add=TRUE, col=colorRampPalette(c(""blue"",""red""))(15), drawlabels=FALSE)
arrows(gradTgrid[,1], gradTgrid[,2],  # Here I multiply the values so you can see them
       gradTgrid[,1]-dx$z*40*T.grid2$z, gradTgrid[,2]-dy$z*40*T.grid2$z, 
       col=""yellow"", length=0.05)
</code></pre>

<p><img src=""https://i.stack.imgur.com/7hW7v.png"" alt=""enter image description here""></p>

<p>To understand in details how this code works, I advise you to read the following help pages: <code>?inout</code>, <code>?chull</code>, <code>?interp</code>, <code>?expand.grid</code> and <code>?contour</code>.</p>
"
42430047,Faster coding than using for loop,1,3,3,"<p>Suppose I have the following data frame </p>

<pre><code>set.seed(36)    

n &lt;- 300

dat &lt;- data.frame(x = round(runif(n,0,200)), y =  round(runif(n, 0, 500)))
d &lt;- dat[order(dat$y),]
</code></pre>

<p>For each value of <code>d$y&lt;=300</code>, I have to create a variable <code>res</code> in which the numerator is the sum of the indicator <code>(d$x &lt;= d$y[i])</code> and the denominator is the sum of the indicator <code>(d$y &gt;= d$y[i])</code>. I have written the codes in <code>for loop</code>:</p>

<pre><code>res &lt;- NULL

for( i in seq_len(sum(d$y&lt;=300)) ){

    numerator   &lt;- sum(d$x &lt;= d$y[i])
    denominator &lt;- sum(d$y &gt;= d$y[i])

    res[i] &lt;- numerator / denominator
 } 
</code></pre>

<p>But my concern is when the number of observations of <code>x</code> and <code>y</code> is large, that is, the number of rows of the data frame increases, the <code>for loop</code> will work slowly. Additionally, if I simulate data 1000 times and each time run the <code>for loop</code>, the program will be inefficient.</p>

<p>What can be the more efficient solution of the code?</p>
","<p>This depends on <code>d</code> already being sorted as it is:</p>

<pre><code># example data
set.seed(36)    
n &lt;- 1e5
dat &lt;- data.frame(x = round(runif(n,0,200)), y =  round(runif(n, 0, 500)))
d &lt;- dat[order(dat$y),]
</code></pre>

<p>My suggestion (thanks to @alexis_laz for the denominator):</p>

<pre><code>system.time(res3 &lt;- {
  xs &lt;- sort(d$x)                                 # sorted x
  yt &lt;- d$y[d$y &lt;= 300]                           # truncated y

  num = findInterval(yt, xs)
  den = length(d$y) - match(yt, d$y) + 1L

  num/den
})
#    user  system elapsed 
#       0       0       0 
</code></pre>

<p>OP's approach:</p>

<pre><code>system.time(res &lt;- {
  res &lt;- NULL
  for( i in seq_len(sum(d$y&lt;=300)) ){
    numerator   &lt;- sum(d$x &lt;= d$y[i])
    denominator &lt;- sum(d$y &gt;= d$y[i])
    res[i] &lt;- numerator / denominator
  }
  res
})
#    user  system elapsed 
#   50.77    1.13   52.10 

# verify it matched
all.equal(res,res3) # TRUE
</code></pre>

<p>@d.b's approach:</p>

<pre><code>system.time(res2 &lt;- {
  numerator = rowSums(outer(d$y, d$x, ""&gt;=""))
  denominator = rowSums(outer(d$y, d$y, ""&lt;=""))
  res2 = numerator/denominator

  res2 = res2[d$y &lt;= 300]
  res2
})
# Error: cannot allocate vector of size 74.5 Gb

# ^ This error is common when using outer() on large-ish problems
</code></pre>

<p><strong>Vectorization.</strong> Generally, tasks are faster in R if they can be vectorized. The key functions related to ordered vectors have confusing names (<code>findInterval</code>, <code>sort</code>, <code>order</code> and <code>cut</code>), but fortunately they all work on vectors.</p>

<p><strong>Continuous vs discrete</strong>. The <code>match</code> above should be a fast way to compute the denominator whether the data is continuous or has mass points / repeating values. If the data is continuous (and so has no repeats), the denominator can just be <code>seq(length(xs), length = length(yt), by=-1)</code>. If it is fully discrete and has a lot of repetition (like the example here), there might be some way to make that faster as well, maybe like one of these:</p>

<pre><code> den2 &lt;- inverse.rle(with(rle(yt), list(
    values = length(xs) - length(yt) +  rev(cumsum(rev(lengths))), 
    lengths = lengths)))

 tab  &lt;- unname(table(yt))
 den3 &lt;- rep(rev(cumsum(rev(tab))) + length(xs) - length(yt), tab)

 # verify
 all.equal(den,den2) # TRUE
 all.equal(den,den3) # TRUE
</code></pre>

<p><code>findInterval</code> will still work for the numerator for continuous data. It's not ideal for the repeated-values case considered here I guess (since we're redundantly finding the interval for many repeated <code>yt</code> values). Similar ideas for speeding that up likely apply.</p>

<p><strong>Other options.</strong> As @chinsoon suggested, the data.table package might be a good fit if <code>findInterval</code> is too slow, since it has a lot of features focused on sorted data, but it's not obvious to me how to apply it here.</p>
"
25875536,coerce column names from column factors and populate,1,1,1,"<p>I have data like</p>

<pre><code>set.seed(6)
df &lt;- data.frame(t = as.Date(""2014/1/1"")+seq(0,100.25,.25),
    name = paste(sample(c(""Alert_"",""NonOp_""),402,replace=TRUE),
                  sample(1:10,402,replace=TRUE),sep=""""),
    unit = c(rep(1:10,each=40),10,10))
head(df)
</code></pre>

<p>Here is some representative data</p>

<pre><code>head(df)
            t     name   unit
1  2014-01-01  NonOp_3      1
2  2014-01-01  NonOp_6      1
3  2014-01-01  Alert_5      1
4  2014-01-01  Alert_7      1
5  2014-01-02  NonOp_4      1
6  2014-01-02  NonOp_2      1
</code></pre>

<p>How to generate a table from the names, where the table has columns of unit, t, and then names in the name column gets coerced into columns of name factored without the Alert/NonOp, and the values in the factored names columns should be NA, A (for alert), and N (for NonOp).  Here is the type of table I'm looking for, if all the numbers above were for unit 1.</p>

<pre><code>unit               t   name_1 name_2 name_3 name_4 name_5 name_6 name_7 name_8 ...
   1      2014-01-01       NA     NA      N     NA      A      A      A     NA
   1      2014-01-02       NA      N     NA      N     NA     NA     NA     NA
</code></pre>

<p>The goal is to the named alerts / nonops into a table ordered by unit / t and write the table to a file.  And read the file into excel.</p>
","<p>It <em>sounds</em> like the following is what you're looking for:</p>

<pre><code>library(reshape2)
newdf &lt;- cbind(df, colsplit(df$name, ""_"", c(""V1"", ""V2"")))
newdf$V1 &lt;- factor(newdf$V1, c(""NonOp"", ""Alert""), c(""N"", ""A""))
newdf$V2 &lt;- paste0(""name_"", newdf$V2)
head(newdf)
#            t    name unit V1     V2
# 1 2014-01-01 NonOp_3    1  N name_3
# 2 2014-01-01 NonOp_6    1  N name_6
# 3 2014-01-01 Alert_5    1  A name_5
# 4 2014-01-01 Alert_7    1  A name_7
# 5 2014-01-02 NonOp_4    1  N name_4
# 6 2014-01-02 NonOp_2    1  N name_2

head(dcast(newdf, t ~ V2, value.var = ""V1""))
#            t name_1 name_10 name_2 name_3 name_4 name_5 name_6 name_7 name_8 name_9
# 1 2014-01-01   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;      N   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
# 2 2014-01-01   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;      N   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
# 3 2014-01-01   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;      A   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
# 4 2014-01-01   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;      A   &lt;NA&gt;   &lt;NA&gt;
# 5 2014-01-02   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;      N   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
# 6 2014-01-02   &lt;NA&gt;    &lt;NA&gt;      N   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;
</code></pre>

<p>Basically, start by splitting the ""name"" column into two columns, and then use <code>dcast</code>. The other steps are mostly cosmetic.</p>

<hr>

<p>Another approach would be to use <a href=""https://gist.github.com/mrdwab/11380733/"" rel=""nofollow"">my <code>cSplit</code> function</a> and <code>dcast.data.table</code> from the ""data.table"" package. </p>

<p>Skipping things like creating ""name_blah"" and replacing ""NonOp"" with ""N"" and so on, you can just directly do:</p>

<pre><code>dcast.data.table(cSplit(df, ""name"", ""_""), t ~ name_2, value.var = ""name_1"")
#               t     1 10     2     3     4     5     6     7  8  9
#   1: 2014-01-01    NA NA    NA NonOp    NA    NA    NA    NA NA NA
#   2: 2014-01-01    NA NA    NA    NA    NA    NA NonOp    NA NA NA
#   3: 2014-01-01    NA NA    NA    NA    NA Alert    NA    NA NA NA
#   4: 2014-01-01    NA NA    NA    NA    NA    NA    NA Alert NA NA
#   5: 2014-01-02    NA NA    NA    NA NonOp    NA    NA    NA NA NA
#  ---                                                              
# 398: 2014-04-10    NA NA    NA    NA    NA    NA NonOp    NA NA NA
# 399: 2014-04-10    NA NA    NA    NA NonOp    NA    NA    NA NA NA
# 400: 2014-04-10 NonOp NA    NA    NA    NA    NA    NA    NA NA NA
# 401: 2014-04-11    NA NA    NA NonOp    NA    NA    NA    NA NA NA
# 402: 2014-04-11    NA NA Alert    NA    NA    NA    NA    NA NA NA
</code></pre>
"
34292438,Conditional shiny UI when multiple conditions need to be handled,2,2,2,"<h2>Actual question</h2>

<p>How do I design(*) a shiny app where certain UI elements depend on <em>multiple</em> conditions that need to be systematically handled?</p>

<p>(*) in a maintainable way that won't drive you mad ;-)</p>

<hr>

<h2>Details</h2>

<p>I've read <a href=""http://shiny.rstudio.com/articles/dynamic-ui.html"" rel=""nofollow noreferrer"">Build a dynamic UI that reacts to user input</a> and like <code>conditionalPanel()</code>, but I have the feeling it's too ""one-dimensional"" for the <a href=""https://rappster.shinyapps.io/timetrackr"" rel=""nofollow noreferrer"">timetracking app</a> I would like to build (<a href=""https://github.com/rappster/timetrackr/tree/stackoverflow_20151215"" rel=""nofollow noreferrer"">source code on GitHub</a>).</p>

<p><strong>What I want to be able to do:</strong></p>

<ol>
<li><p>Have one (or more) UI element(s) that can trigger conditional UI parts:</p>

<p><em>State 1</em></p>

<p><a href=""https://i.stack.imgur.com/J0V57.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J0V57.png"" alt=""State 1""></a></p></li>
<li><p>Those conditional UI parts usually have some input fields and at least two action buttons: <code>Create</code> and <code>Cancel</code>:</p>

<p><em>State 2</em></p>

<p><a href=""https://i.stack.imgur.com/cu8ex.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cu8ex.png"" alt=""State 2""></a></p></li>
<li><p>If <code>Create</code> is clicked, the input should be appropriately processed (e.g. writing stuff to a DB) and then the conditional UI part should ""disappear"" again as its condition ""expired"":</p>

<p><em>State 3</em></p>

<p><a href=""https://i.stack.imgur.com/pbxOF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pbxOF.png"" alt=""State 3""></a></p>

<p><em>State 4</em></p>

<p><a href=""https://i.stack.imgur.com/4YD7K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4YD7K.png"" alt=""State 4""></a></p></li>
<li><p>If <code>Cancel</code> is clicked, the UI part should ""disappear"" again as its condition ""expired"":</p>

<p><em>State 4</em></p>

<p><a href=""https://i.stack.imgur.com/4YD7K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4YD7K.png"" alt=""State 4""></a></p></li>
<li><p>A subsequent click on <code>Trigger</code> should ""start the cycle"" again</p></li>
</ol>

<p><strong>Problem with multiple dependencies and dynamic dependency states:</strong></p>

<p>AFAIU, if I simply put the dependencies (i.e. <code>input$action_trigger</code>, <code>input$action_create</code> and <code>input$action_cancel</code> below) into the reactive context that builds the conditional UI, then I face multiple rounds of invalidation until all dependencies have reached a stable state (see <code>output$ui_conditional &lt;- renderUI({})</code> below). </p>

<p>From a UX-perspective, this feels like having to click on elements multiple times until you get what you want (check out an example of this ""multiple-clicks-necessary"" behavior in my <a href=""https://rappster.shinyapps.io/timetrackr"" rel=""nofollow noreferrer"">timetracking app</a>).</p>

<p>That's why I came up with the idea of introducing sort of a ""dependency state clearance"" layer (see <code>ui_decision &lt;- reactive({})</code> below)</p>

<h2>Current solution</h2>

<p>My current solution feels very wrong, very fragile and very high maintenance. You can also find it at <a href=""https://github.com/rappster/stackoverflow/blob/master/shiny/conditional/2015-12-15/app.R"" rel=""nofollow noreferrer"">GitHub</a></p>

<p><strong>Globals:</strong></p>

<pre><code>library(shiny)

GLOBALS &lt;- list()
GLOBALS$debug$enabled &lt;- TRUE

# Auxiliary functions -----------------------------------------------------

createDynamicUi_conditional &lt;- function(
  input,
  output,
  ui_decision,
  debug = GLOBALS$debug$enabled
) {
  if (debug) {
    message(""Dynamic UI: conditional ----------"")
    print(Sys.time())
  }

  ## Form components //
  container &lt;- list()

  field &lt;- ""title""
  name &lt;- ""Title""
  value &lt;- """"
  container[[field]] &lt;- textInput(field, name, value)

  field &lt;- ""description""
  name &lt;- ""Description""
  value &lt;- """"
  container[[field]] &lt;- textInput(field, name, value)

  ## Bundle in box //
  value &lt;- if (ui_decision == ""hide"") {
    div()
  } else if (ui_decision == ""show"" || ui_decision == ""create"") {
    container$buttons &lt;- div(style=""display:inline-block"",
      actionButton(""action_create"", ""Create""),
      actionButton(""action_cancel"", ""Cancel"")
    )
    do.call(div, args = list(container, title = ""conditional dynamic UI""))
  } else {
    ""Not implemented yet""
  }
  # print(value)
  value
}
</code></pre>

<p><strong>UI part:</strong></p>

<pre><code># UI ----------------------------------------------------------------------

ui &lt;- fluidPage(
  actionButton(""action_trigger"", ""Trigger 1""),
  h3(""Database state""),
  textOutput(""result""),
  p(),
  uiOutput(""ui_conditional"")
)
</code></pre>

<p><strong>Server part:</strong></p>

<pre><code># Server ------------------------------------------------------------------

server &lt;- function(input, output, session) {
  #####################
  ## REACTIVE VALUES ##
  #####################

  db &lt;- reactiveValues(
    title = """",
    description = """"
  )

  ui_control &lt;- reactiveValues(
    action_trigger = 0,
    action_trigger__last = 0,
    action_create = 0,
    action_create__last = 0,
    action_cancel = 0,
    action_cancel__last = 0
  )

  #################
  ## UI DECISION ##
  #################

  ui_decision &lt;- reactive({
    ## Dependencies //
    ## Trigger button:
    value &lt;- input$action_trigger
    if (ui_control$action_trigger != value) ui_control$action_trigger &lt;- value

    ## Create button:
    ## Dynamically created within `createDynamicUi_conditional`
    value &lt;- input$action_create
    if (is.null(value)) {
      value &lt;- 0
    }
    if (ui_control$action_create != value) {
      ui_control$action_create &lt;- value
    }

    ## Cancel button:
    ## Dynamically created within `createDynamicUi_conditional`
    value &lt;- input$action_cancel
    if (is.null(value)) {
      value &lt;- 0
    }
    if (ui_control$action_cancel != value) {
      ui_control$action_cancel &lt;- value
    }

    if (GLOBALS$debug$enabled) {
      message(""Dependency clearance -----"")
      message(""action_trigger:"")
      print(ui_control$action_trigger)
      print(ui_control$action_trigger__last)
      message(""action_create:"")
      print(ui_control$action_create)
      print(ui_control$action_create__last)
      message(""action_cancel:"")
      print(ui_control$action_cancel)
      print(ui_control$action_cancel__last)
    }
    ui_decision &lt;- if (
      c (ui_control$action_trigger == 0 &amp;&amp; ui_control$action_trigger == 0) ||
        c(
          ui_control$action_trigger &gt; 0 &amp;&amp;
            ui_control$action_trigger &lt;= ui_control$action_trigger__last &amp;&amp;

            ui_control$action_cancel &gt; 0 &amp;&amp;
            ui_control$action_cancel &gt; ui_control$action_cancel__last
        ) ||
        c(
          ui_control$action_create == 0 &amp;&amp;
            ui_control$action_create__last &gt; 0
        )
    ) {
      ""hide""
    } else if (
      ui_control$action_trigger &gt;= ui_control$action_trigger__last &amp;&amp;
        ui_control$action_create == ui_control$action_create__last
    ) {
      ## Synchronize //
      ui_control$action_cancel__last &lt;- ui_control$action_cancel
      ""show""
    } else if (
      ui_control$action_create &gt; ui_control$action_create__last
    ) {
      ""create""
    } else {
      ""Not implemented yet""
    }
    if (GLOBALS$debug$enabled) {
      print(ui_decision)
    }
    ## Synchronize //
    ui_control$action_trigger__last &lt;- ui_control$action_trigger
    ui_control$action_create__last &lt;- ui_control$action_create

    ui_decision
  })

  output$ui_conditional &lt;- renderUI({
    createDynamicUi_conditional(input, output, ui_decision = ui_decision())
  })

  #################
  ## WRITE TO DB ##
  #################

  writeToDb &lt;- reactive({
    ui_decision &lt;- ui_decision()
    if (ui_decision == ""create"") {
      db$title &lt;- input$title
      db$description &lt;- input$description
    }
  })

  ###################
  ## RENDER RESULT ##
  ###################

  output$result &lt;- renderText({
    writeToDb()
    c(
      paste0(""Title: "", db$title),
      paste0(""Description: "", db$description)
    )
  })
}
</code></pre>

<p><strong>Running the app:</strong></p>

<pre><code>shinyApp(ui, server)
</code></pre>

<h2>Big picture</h2>

<p>This is the app that I'm actually having in mind: <a href=""https://rappster.shinyapps.io/timetrackr"" rel=""nofollow noreferrer""><code>timetrackr</code></a></p>

<p><a href=""https://github.com/rappster/timetrackr/tree/stackoverflow_20151215"" rel=""nofollow noreferrer"">Source code on GitHub</a>.</p>

<p>It has been build without introducing a clearance layer as drafted above. While it does provide the desired functionality, very often, you need to click UI elements more than once until a stable dependency state is reached which is really irritating.</p>
","<p>I'll start with the solution:</p>

<pre><code>library(shiny)

ui &lt;- fluidPage(
  actionButton(""action_trigger"", ""Trigger 1""),
  h3(""Database state""),
  textOutput(""result""),
  p(),
  uiOutput(""ui_conditional"")
)

server &lt;- function(input, output, session) {
  ui_control &lt;- reactiveValues(show = FALSE)

  output$ui_conditional &lt;- renderUI({
    if (!ui_control$show) return()

    tagList(
      textInput(""title"", ""Title""),
      textInput(""description"", ""Description""),
      div(style=""display:inline-block"",
        actionButton(""action_create"", ""Create""),
        actionButton(""action_cancel"", ""Cancel"")
      )
    )
  })

  observeEvent(input$action_trigger, {
    ui_control$show &lt;- TRUE
  })
  observeEvent(input$action_create, {
    writeToDb()
    ui_control$show &lt;- FALSE
  })
  observeEvent(input$action_cancel, {
    ui_control$show &lt;- FALSE
  })

  writeToDb &lt;- function() {
    # ...
  }
}

shinyApp(ui, server)
</code></pre>

<p>I hope that this is sufficiently simple as to be self-explanatory. Let me know if it is not.</p>

<p>There are several principles that you can follow to make your Shiny reactive code much more robust and maintainable--and usually simpler, too.</p>

<ol>
<li>Each action button should have its own <code>observeEvent</code>, and you generally shouldn't need to use the action button value anywhere but as the first argument to <code>observeEvent</code>. It's rarely advisable to use an action button any other way, even though it can be tempting; especially if you're comparing the action button's value to its previous value, that's a pretty sure sign that you're on the wrong track.</li>
<li>Reactive expressions should never have side effects--e.g. writing to disk, or assigning to non-local variables (and reactive value objects like ui_control count as non-local variables when you set them from inside a reactive expression). These type of actions should instead be done in an <code>observe()</code> or <code>observeEvent()</code>. I will elaborate much more on this in early 2016.</li>
<li>Like regular functions, reactive expressions and observers should ideally have a single responsibility--one calculation or coherent set of calculations (in the case of reactive expressions), or one action or coherent set of actions (in the case of observers). If you're having trouble thinking of an informative and specific name for a function, that can be a sign that the function is doing too much; the same is true for reactive expressions (in this case, <code>ui_decision</code> is pretty vague).</li>
<li>In response to your general concern about instability while dynamically built UI/inputs come online, when you need to use such inputs, you can guard their invocations with <code>validate(need(input$foo, FALSE))</code>. You can put this in e.g. the beginning of a reactive expression, and it will silently abort execution of itself and any callers if <code>input$foo</code> is not yet available (i.e. it's <code>NULL</code>, <code>FALSE</code>, <code>""""</code>, or a number of other falsy values). This is a hugely helpful feature of Shiny that we have done a notably poor job of promoting. I also think we made the API too general and not easy enough to use, which I hope to rectify soon. In the meantime, see <a href=""http://shiny.rstudio.com/articles/validation.html"" rel=""nofollow"">http://shiny.rstudio.com/articles/validation.html</a> and/or <a href=""https://www.youtube.com/watch?v=7sQ6AEDFjZ4"" rel=""nofollow"">https://www.youtube.com/watch?v=7sQ6AEDFjZ4</a>.</li>
</ol>
"
4811995,R and Data Mining,2,2,2,"<p>Instead starting to code in Matlab, I recently started learning R, mainly because it is open-source. I am currently working in data mining and machine learning field. I found many machine learning algorithms implemented in R, and I am still exploring different packages implemented in R.</p>

<p>I have quick question: how do you compare R to Matlab for data mining application, its popularity, pros and cons, industry and academic acceptance etc.? Which one would you choose and why?</p>

<p>I went through various comparisons for Matlab vs R against various metrics but I am specifically interested to get answer for its applicability in  Data Mining and ML. 
Since both language are pretty new for me I was just wondering if R would be a good choice or not.</p>

<p>I appreciate any kind of suggestions.</p>
","<p>For the past three years or so, i have used R daily, and the largest portion of that daily use is spent on Machine Learning/Data Mining problems.</p>

<p>I was an exclusive Matlab user while in University; at the time i thought it was
an excellent set of tools/platform. I am sure it is today as well.</p>

<p>The Neural Network Toolbox, the Optimization Toolbox, Statistics Toolbox, 
and Curve Fitting Toolbox are each highly desirable (if not essential) 
for someone using MATLAB for ML/Data Mining work, yet they are all <em>separate</em> from 
the base MATLAB environment--in other words, they have to be purchased separately.</p>

<p>My <strong>Top 5 list for Learning ML/Data Mining in R</strong>:
<br/><br/></p>

<ul>
<li><a href=""http://lyle.smu.edu/IDA/arules/"" rel=""noreferrer"">Mining Association Rules in R</a></li>
</ul>

<p>This refers to a couple things: First, a group of R Package that all begin <strong><em>arules</em></strong> (available from CRAN); you can find the complete list (arules, aruluesViz, etc.) on the <a href=""http://lyle.smu.edu/IDA/arules/"" rel=""noreferrer"">Project Homepage</a>. Second, all of these packages are based on a data-mining technique known as <em>Market-Basked Analysis</em> and alternatively as <em>Association Rules</em>. In many respects, this family of algorithms is the essence of data-mining--exhaustively traverse large transaction databases and find above-average associations or correlations among the fields (variables or features) in those databases. In practice, you connect them to a data source and let them run overnight. The central R Package in the set mentioned above is called <strong><em>arules</em></strong>; On the CRAN Package page for <em>arules</em>, you will find links to a couple of excellent secondary sources (<em>vignettes</em> in R's lexicon) on the arules package and on Association Rules technique in general.<br/><br/></p>

<ul>
<li>The standard reference, <strong><em>T<a href=""http://www-stat.stanford.edu/~tibs/ElemStatLearn/"" rel=""noreferrer"">he Elements of Statistical
Learning</a></em></strong> by Hastie et al.</li>
</ul>

<p>The most current edition of this book is available in digital form for <em>free</em>. Likewise, at the book's website (linked to just above) are all data sets used in ESL, available for free download. (As an aside, i have the free digital version; i also purchased the hardback version from BN.com; all of the color plots in the digital version are reproduced in the hardbound version.) ESL contains thorough introductions to at least one exemplar from most of the major
ML rubrics--e.g., neural metworks, SVM, KNN; unsupervised
techniques (LDA, PCA, MDS, SOM, clustering), numerous flavors of regression, CART, 
Bayesian techniques, as well as model aggregation techniques (Boosting, Bagging) 
and model tuning (regularization). Finally, get the R Package that accompanies the book from CRAN (which will save the trouble of having to download the enter the datasets).
<br/>
<br/></p>

<ul>
<li>CRAN <strong><em>Task View: Machine Learning</em></strong></li>
</ul>

<p>The +3,500 Packages available
for R are divided up by domain into about 30 package families or '<a href=""http://cran.r-project.org/web/views/"" rel=""noreferrer"">Task Views</a>'. Machine Learning
is one of these families. The Machine Learning Task View contains about 50 or so
Packages. Some of these Packages are part of the core distribution, including e1071
(a sprawling ML package that includes working code for quite a few of 
the usual ML categories.)
<br/><br/></p>

<ul>
<li><a href=""http://blog.revolutionanalytics.com/predictive-analytics/page/4/"" rel=""noreferrer"">Revolution Analytics Blog</a></li>
</ul>

<p>With particular focus on the posts tagged with Predictive Analytics 
<br/><br/></p>

<ul>
<li><strong><em><a href=""http://blog.revolutionanalytics.com/2009/09/machine-learning-in-r-in-a-nutshell.html"" rel=""noreferrer"">ML in R tutorial</a></em></strong> comprised of slide deck and R code by Josh Reich</li>
</ul>

<p>A thorough study of the code would, by itself, be an excellent introduction to ML in R.
<br/><br/></p>

<p>And one final resource that i think is excellent, but didn't make in the top 5:</p>

<ul>
<li><a href=""http://abeautifulwww.com/2009/10/11/guide-to-getting-started-in-machine-learning/"" rel=""noreferrer"">A Guide to Getting Stared in Machine Learning [in R]</a></li>
</ul>

<p>posted at the blog <em>A Beautiful WWW</em></p>
"
33380714,Column & row names for matrix values greater than the column mean of their corresponding columns,1,1,3,"<p>I am looking for a way to get the column and row names for values in the matrix in which the value is <code>&gt;=</code> the mean of the column which it is in. I am new to R matrices.</p>

<pre><code>m &lt;- matrix(c(1:20), rnow=4)
colnames(m) &lt;- c(""A1"",""A2"", ""B1"", ""B2"")
rownames(m) &lt;- c(""Y1"",""Y2"", ""Z1"", ""Z2"", ""Z3"")

&gt; m
      A1 A2 B1 B2
   Y1  1  6 11 16
   Y2  2  7 12 17
   Z1  3  8 13 18
   Z2  4  9 14 19
   Z3  5 10 15 20
</code></pre>

<p>The means are given below.</p>

<pre><code>&gt; colMeans(dummy_expr)
   A1 A2 B1 B2 
   3  8 13 18 
</code></pre>

<p>I want to get the following:</p>

<pre><code>row col
Z1  A1
Z2  A1
Z3  A1
Z1  A2
Z2  A2
Z3  A2
Z1  B1
Z2  B1
Z3  B1
Z1  B2
Z2  B2
Z3  B2
</code></pre>

<p>Here is what I've got so far:</p>

<pre><code>apply(m, 1:2, function(x) x&gt;=colMeans(m))
</code></pre>
","<p>You can determine which elements of <code>m</code> are no smaller than the column means with <code>which</code> using <code>arr.ind=TRUE</code>, and then you can convert these indices to the row and column names using standard indexing into <code>rownames(m)</code> and <code>colnames(m)</code>.</p>

<pre><code>indices &lt;- which(t(t(m) &gt;= colMeans(m)), arr.ind=TRUE)
indices[,""row""] &lt;- rownames(m)[as.numeric(indices[,""row""])]
indices[,""col""] &lt;- colnames(m)[as.numeric(indices[,""col""])]
indices
#    row  col 
# Z1 ""Z1"" ""A1""
# Z2 ""Z2"" ""A1""
# Z3 ""Z3"" ""A1""
# Z1 ""Z1"" ""A2""
# Z2 ""Z2"" ""A2""
# Z3 ""Z3"" ""A2""
# Z1 ""Z1"" ""B1""
# Z2 ""Z2"" ""B1""
# Z3 ""Z3"" ""B1""
# Z1 ""Z1"" ""B2""
# Z2 ""Z2"" ""B2""
# Z3 ""Z3"" ""B2""
</code></pre>

<p>The most involved command was the first one, so I'll walk through it in pieces. </p>

<ul>
<li><code>t(m)</code> takes the transpose of the <code>m</code> matrix.</li>
</ul>



<pre><code>t(m)
#    Y1 Y2 Z1 Z2 Z3
# A1  1  2  3  4  5
# A2  6  7  8  9 10
# B1 11 12 13 14 15
# B2 16 17 18 19 20
</code></pre>

<ul>
<li><code>t(m) &gt;= colMeans(m)</code> returns whether the element in each row of <code>t(m)</code> is no smaller than the corresponding element in <code>colMeans(m)</code>; since we transposed <code>m</code> this result is the transposed version of the logical matrix indicating whether each element in <code>m</code> is no smaller than its associated column mean.</li>
</ul>



<pre><code>t(m) &gt;= colMeans(m)
#       Y1    Y2   Z1   Z2   Z3
# A1 FALSE FALSE TRUE TRUE TRUE
# A2 FALSE FALSE TRUE TRUE TRUE
# B1 FALSE FALSE TRUE TRUE TRUE
# B2 FALSE FALSE TRUE TRUE TRUE
</code></pre>

<ul>
<li>With <code>t(t(m) &gt;= colMeans(m))</code>, we transpose the result; now we have a logical matrix indicating whether each element in <code>m</code> exceeds its column mean:</li>
</ul>



<pre><code>t(t(m) &gt;= colMeans(m))
#       A1    A2    B1    B2
# Y1 FALSE FALSE FALSE FALSE
# Y2 FALSE FALSE FALSE FALSE
# Z1  TRUE  TRUE  TRUE  TRUE
# Z2  TRUE  TRUE  TRUE  TRUE
# Z3  TRUE  TRUE  TRUE  TRUE
</code></pre>

<ul>
<li>Finally <code>which</code> with <code>arr.ind=TRUE</code> will return the row and column numbers that are set:</li>
</ul>



<pre><code>which(t(t(m) &gt;= colMeans(m)), arr.ind=TRUE)
#    row col
# Z1   3   1
# Z2   4   1
# Z3   5   1
# Z1   3   2
# Z2   4   2
# Z3   5   2
# Z1   3   3
# Z2   4   3
# Z3   5   3
# Z1   3   4
# Z2   4   4
# Z3   5   4
</code></pre>

<p>All that remains is converting the column and row numbers to their associated names, which we do in the next two lines of code with simple indexing.</p>
"
28124623,Flatten (reassign values of) hotspots in a matrix,1,3,3,"<p>I have a matrix in the following format:</p>

<pre><code>set.seed(1)
m = matrix(sample(c(0,0,0,1),25,rep=T), nrow=5)
m[13] = 4
print(m)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1    0    0    1
[2,]    0    1    0    0    0
[3,]    0    0    4    1    0
[4,]    1    0    0    0    0
[5,]    0    0    1    1    0
</code></pre>

<p>Consider <code>[3,3]</code> is some hotspot that we want to 'flatten' by spreading it's value across the nearest neighbouring/nearby cells of zero value.   In this case that means assigning 1 to cells <code>[2,3]</code>, <code>[3,2]</code> and <code>[4,3]</code> so that <code>[3,3]</code> can also be reduced to 1:</p>

<pre><code>     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1    0    0    1
[2,]    0    1    1    0    0
[3,]    0    1    1    1    0
[4,]    1    0    1    0    0
[5,]    0    0    1    1    0
</code></pre>

<p>Is anyone aware of a matrix/raster operation that can achieve this efficiently, while preserving the sum total of all the cells?</p>
","<p>I got interested in this question, so I made an attempt. There, probably, exists a ""rastery"" tool for what you're trying but I'm not aware.</p>

<p>First, a helper function that finds the indices of elements of a square surrounding a specific element in a matrix:</p>

<pre><code>find_neighbors = function(i, j, n)
{
    tmp = expand.grid(replicate(2, -n:n, simplify = F))
    tmp2 = tmp[rowSums(abs(tmp) &lt; n) &lt; 2, ]
    inds = cbind(tmp2[, 1] + i, tmp2[, 2] + j)
    inds[order(rowSums(abs(cbind(inds[, 1] - i,        ##so that up/down/right/left are filled before diagonal neighbors
                                 inds[, 2] - j)))), ]
}
</code></pre>

<p>E.g.:</p>

<pre><code>m1 = matrix(0, 7, 8)
m1
#     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
#[1,]    0    0    0    0    0    0    0    0
#[2,]    0    0    0    0    0    0    0    0
#[3,]    0    0    0    0    0    0    0    0
#[4,]    0    0    0    0    0    0    0    0
#[5,]    0    0    0    0    0    0    0    0
#[6,]    0    0    0    0    0    0    0    0
#[7,]    0    0    0    0    0    0    0    0
m1[find_neighbors(3, 4, 1)] = 1
m1[find_neighbors(3, 4, 2)] = 2
m1[find_neighbors(3, 4, 3)] = 3
m1
#     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
#[1,]    3    2    2    2    2    2    3    0
#[2,]    3    2    1    1    1    2    3    0
#[3,]    3    2    1    0    1    2    3    0
#[4,]    3    2    1    1    1    2    3    0
#[5,]    3    2    2    2    2    2    3    0
#[6,]    3    3    3    3    3    3    3    0
#[7,]    0    0    0    0    0    0    0    0
</code></pre>

<p>And the function that flattens the hot-spots. There is a nested loop. The first ""for"" to loop over the hotspots and the second to iteratively flatten the hotspot to its neighbors. Nevertheless, the loop is exitted once the spot is flattened. </p>

<pre><code>ff = function(mat, thres = 1)
{    
    wh = which(mat &gt; thres, T)

    for(r in seq_len(nrow(wh))) {   
        for(n in seq_len(max(c(dim(mat) - wh[r, ], wh[r, ] - 1)))) {
            if(mat[wh[r, , drop = F]] &lt;= thres) break  #stop flattening if we are done

            inds = find_neighbors(wh[r, 1], wh[r, 2], n) #get indices of neighbours
            inds = inds[!((rowSums(inds &lt;= 0) &gt; 0) | #use valid indices..
                        inds[, 1] &gt; nrow(mat) |   
                        inds[, 2] &gt; ncol(mat)), ]

            inds = inds[mat[inds] &lt; thres, , drop = F] #use indices that are allowed to take values         
            tofill = nrow(inds) * thres #how many 'units' need to be taken from the hotspot?

            mat[wh[r, , drop = F]] = mat[wh[r, , drop = F]] + sum(mat[inds])  #in case the neighbors 
                                                                          #of the hotspot are &gt; 0,
                                                                          #the, just, increase the 
                                                                          #value of the hotspot                                                     
            if(mat[wh[r, , drop = F]] &lt;= tofill) tofill = mat[wh[r, , drop = F]] - thres #do we have enough 
                                                                                 #'units' in the hotspot?

            if(tofill &gt; 0) {
                if(tofill &lt; thres) {
                    mat[inds[1, , drop = F]] = tofill
                    mat[wh[r, , drop = F]] = mat[wh[r, , drop = F]] - tofill                    
                    next
                }
                nr = tofill %/% thres
                mat[inds[seq_len(nr), , drop = F]] = thres
                if((tofill %% thres) &gt; 0) mat[inds[nr + 1, , drop = F]] = tofill %% thres
                mat[wh[r, , drop = F]] = mat[wh[r, , drop = F]] - tofill
            }           
        }
    }

    mat
}
</code></pre>

<p>And an example:</p>

<pre><code>mm = matrix(0, 11, 9); mm[8, 2] = 12; mm[6, 7] = 4
mm
#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
# [1,]    0    0    0    0    0    0    0    0    0
# [2,]    0    0    0    0    0    0    0    0    0
# [3,]    0    0    0    0    0    0    0    0    0
# [4,]    0    0    0    0    0    0    0    0    0
# [5,]    0    0    0    0    0    0    0    0    0
# [6,]    0    0    0    0    0    0    4    0    0
# [7,]    0    0    0    0    0    0    0    0    0
# [8,]    0   12    0    0    0    0    0    0    0
# [9,]    0    0    0    0    0    0    0    0    0
#[10,]    0    0    0    0    0    0    0    0    0
#[11,]    0    0    0    0    0    0    0    0    0
ff(mm)
#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
# [1,]    0    0    0    0    0    0    0    0    0
# [2,]    0    0    0    0    0    0    0    0    0
# [3,]    0    0    0    0    0    0    0    0    0
# [4,]    0    0    0    0    0    0    0    0    0
# [5,]    0    0    0    0    0    0    1    0    0
# [6,]    0    1    0    0    0    1    1    0    0
# [7,]    1    1    1    0    0    0    1    0    0
# [8,]    1    1    1    1    0    0    0    0    0
# [9,]    1    1    1    0    0    0    0    0    0
#[10,]    0    1    0    0    0    0    0    0    0
#[11,]    0    0    0    0    0    0    0    0    0
ff(mm, 3)
ff(mm, 5)
ff(mm, 1500)
</code></pre>

<p>Hope any of these will be helpful.</p>
"
21564125,Recursive function: Save output of every call to list,1,3,3,"<p>I would like to save the output of every function call of the following recursive function to a list.  Moreover, I need to know which (j,l)-pair correspond to which entry of the resulting list.</p>

<p>I have created a stripped down version to reproduce the problem.  Please let me know if I should provide more information to help solve the problem.  Any help is highly appreciated.  Thank you.</p>

<pre><code>#the recursive function
phi &lt;- function(phik,j,l,k,d) {
  if(j==0) {
    diag(d)
  }
  else{
    if(j==1) {
      if(l&gt;k) {
        0 * diag(d)
      }
      else{
        phik[[l]]      
      }      
    }
    else {
      if(l&gt;k) {
        0 + phi(phik,j-1,l,k,d) %*% phik[[1]]
      }
      else {
        phi(phik,j-1,l+1,k,d) + phi(phik,j-1,l,k,d) %*% phik[[1]]
      }      
    }
  }
}

#related stuff
set.seed(123456)
phik &lt;- vector(mode=""list"", length=3)
phik[[1]] &lt;- matrix(rnorm(n=16,mean=0,s=1),nrow=4)
phik[[2]] &lt;- matrix(rnorm(n=16,mean=0,s=1),nrow=4)
phik[[3]] &lt;- matrix(rnorm(n=16,mean=0,s=1),nrow=4)
d &lt;- nrow(phik[[1]])
k &lt;- length(phik)

#function call
phiout &lt;- phi(phik,j=10,l=1,k=k,d=d)
</code></pre>
","<p>So, it is a little tricky with recursive functions, because if you want the results of the intermediate steps, you have glue them together in a list. Of course, that means when you use the results of the recursion in the function, you have to dig out the value that you want. That sounds a little convoluted, but in your case, I just mean that you have to return a little list of <code>phi</code>, <code>j</code>, and <code>l</code> at every step, but pull out just <code>phi</code> when you do the multiplications. Here is a little example:</p>

<pre><code>#the recursive function
phi &lt;- function(phik,j,l,k,d) {
  if(j==0) 
    list(list(phi=diag(d),j=j,l=l))
  else{
    if(j==1) {
      if(l&gt;k) 
        list(list(phi=0 * diag(d),j=j,l=l))
      else 
        list(list(phi=phik[[l]],j=j,l=l))
    }
    else {
      if(l&gt;k) {
        first&lt;-phi(phik,j-1,l,k,d)
        second&lt;-list(list(phi=0 + first[[1]]$phi %*% phik[[1]], j=j,l=l))
        c(second,first)
      }
      else {
        first&lt;-phi(phik,j-1,l+1,k,d) 
        second&lt;-phi(phik,j-1,l,k,d) 
        third&lt;-list(list(phi=first[[1]]$phi+(second[[1]]$phi %*% phik[[1]]), j=j, l=l))
        c(third,first,second)
      }
    }
  }
}
</code></pre>

<p>You might be interested in why I nested the results in the first to third cases (when <code>j</code> is 0 or 1). If you look at the other cases, it might become clear. When <code>l&gt;k</code> (and <code>j</code> is not 0 or 1), then there are two calls <code>phi</code> made. In this case, there will be a <code>list</code> returned, with two sets of <code>phi</code>, <code>i</code>, and <code>j</code>, so it is necessarily a <code>list</code> of <code>list</code>s. When I want to pull out <code>phi</code> from a returned value, it is difficult to tell whether it was going to be just a <code>list</code> or a <code>list</code> of <code>list</code>s, so I just standardized them all to the same thing.</p>

<p>I think <code>return</code> statements are ugly, but others disagree. You can add them in if you like, but they are strictly unnecessary (in this case).</p>

<p>Some sample output:</p>

<pre><code>set.seed(123456)
phik &lt;- vector(mode=""list"", length=3)
phik[[1]] &lt;- matrix(rnorm(n=16,mean=0,s=1),nrow=4)
phik[[2]] &lt;- matrix(rnorm(n=16,mean=0,s=1),nrow=4)
phik[[3]] &lt;- matrix(rnorm(n=16,mean=0,s=1),nrow=4)
d &lt;- nrow(phik[[1]])
k &lt;- length(phik)

phi(phik,j=2,l=3,k,d)
# [[1]]
# [[1]]$phi
#            [,1]      [,2]       [,3]       [,4]
# [1,] -0.9087417 -2.064341 -0.9962198  0.7713081
# [2,] -2.9595280 -5.330120 -4.0488408  2.3357631
# [3,] -1.3754167 -3.866457 -0.8592336  1.4135614
# [4,] -0.1021518 -4.332802  0.4883886 -2.2130314
# 
# [[1]]$j
# [1] 2
# 
# [[1]]$l
# [1] 3
# 
# 
# [[2]]
# [[2]]$phi
#      [,1] [,2] [,3] [,4]
# [1,]    0    0    0    0
# [2,]    0    0    0    0
# [3,]    0    0    0    0
# [4,]    0    0    0    0
# 
# [[2]]$j
# [1] 1
# 
# [[2]]$l
# [1] 4
# 
# 
# [[3]]
# [[3]]$phi
#            [,1]      [,2]       [,3]       [,4]
# [1,] -1.0461983  1.560074 -1.0713045  0.1582893
# [2,] -2.7488684  1.015088  0.9678209 -0.5019485
# [3,] -1.1298596  1.043994  0.1710325 -0.9659226
# [4,] -0.8616848 -1.115905 -0.8962503 -0.1137341
# 
# [[3]]$j
# [1] 1
# 
# [[3]]$l
# [1] 3
</code></pre>
"
25166743,Better Strategy for pulling elements from string,2,2,4,"<p>I have a string that looks like this: </p>

<pre><code>x &lt;- ""\r\n      Ticker Symbol: RBO\r\n  \t   Exchange: TSX \r\n\t   Assets ($mm) 36.26 \r\n\t   Units Outstanding: 1,800,000 \r\n\t   Mgmt. Fee** 0.25 \r\n      2013 MER* n/a \r\n\t   CUSIP: 74932K103""
</code></pre>

<p>What I need is this: </p>

<pre><code>list(Ticker = ""RBO"", Assets = 36.26, Shares = 1,800,000)
</code></pre>

<p>I've tried splitting, regex, etc. But I feel my string manipulation skills are not up to snuff.</p>

<p>Here's my ""best"" attempt so far. </p>

<pre><code>x &lt;- unlist(strsplit(unlist(strsplit(x, ""\r\n\t"") ),""\r\n""))
trim &lt;- function (x) gsub(""^\\s+|\\s+$"", """", x)
x &lt;- trim(x)
gsub(""[A-Z]+$"",""\\2"",x[2]) # bad attempt to get RBO
</code></pre>
","<p><strong>Update/better answer:</strong>   </p>

<p>A look at <code>cat(x)</code> and <code>readLines(x)</code> helps a lot here</p>

<pre><code>&gt; cat(x)
#
#      Ticker Symbol: RBO
#      Exchange: TSX 
#      Assets ($mm) 36.26 #
#      Units Outstanding: 1,800,000 
#      Mgmt. Fee** 0.25 
#      2013 MER* n/a 
#      CUSIP: 74932K103
&gt; readLines(textConnection(x))
# [1] """"                                   ""      Ticker Symbol: RBO""          
# [3] ""  \t   Exchange: TSX ""              ""\t   Assets ($mm) 36.26 ""          
# [5] ""\t   Units Outstanding: 1,800,000 "" ""\t   Mgmt. Fee** 0.25 ""            
# [7] ""      2013 MER* n/a ""               ""\t   CUSIP: 74932K103""
</code></pre>

<p>Now we know a few things.  One, we don't need the first line, and we <strong>do</strong> want the second line.  That makes things easier because now the first line matches our desired first line.  Next, it would be easier your list names matched the names in the string. I chose these.</p>

<pre><code>&gt; nm &lt;- c(""Symbol"", ""Assets"", ""Units"")
</code></pre>

<p>Now all we have to do use <code>grep</code> with <code>sapply</code>, and we'll get back a named vector of matches.  Setting <code>value = TRUE</code> in <code>grep</code> will return us the strings.  </p>

<pre><code>&gt; (y &lt;- sapply(nm, grep, x = readLines(textConnection(x))[-1], value = TRUE))
# b                              Symbol                               Assets 
#           ""      Ticker Symbol: RBO""           ""\t   Assets ($mm) 36.26 "" 
#                                Units 
# ""\t   Units Outstanding: 1,800,000 "" 
</code></pre>

<p>Then we <code>strsplit</code> that on <code>""[: ]""</code>, take the last element in each split, and we're done.</p>

<pre><code>&gt; lapply(strsplit(y, ""[: ]""), tail, 1)
$Symbol
[1] ""RBO""

$Assets
[1] ""36.26""

$Units
[1] ""1,800,000
</code></pre>

<p>You could achieve the same result with</p>

<pre><code>&gt; g &lt;- gsub(""[[:cntrl:]]"", """", capture.output(cat(x))[-1])
&gt; m &lt;- mapply(grep, nm, MoreArgs = list(x = g, value = TRUE))
&gt; lapply(strsplit(m, ""[: ]""), tail, 1)
</code></pre>

<p>Hope that helps.</p>

<hr>

<p><strong>Original Answer:</strong></p>

<p>It looks like if you're pulling these from a large table, that they'd all be in the same element ""slot"" each time, so maybe this might be a little easier.</p>

<pre><code>&gt; s &lt;- strsplit(x, ""[: ]|[[:cntrl:]]"")[[1]]
</code></pre>

<p>Explained:<br>
 - <code>[: ]</code> match a <code>"":""</code> character followed by a space character<br>
 - <code>|</code>  or<br>
 - <code>[[:cntrl:]]</code> any control character, which in this case is any of <code>\r</code>, <code>\t</code>, and <code>\n</code>. This is probably better explained <a href=""http://en.wikipedia.org/wiki/Control_character"" rel=""nofollow"">here</a> </p>

<p>Then, <code>nzchar</code> looks in the above result for non-zero length character strings, and returns TRUE if matched, FALSE otherwise. So we can look at the result of the first line, determine where the matches are, and subset based on that.</p>

<pre><code>&gt; as.list(s[nzchar(s)][c(3, 8, 11)])
[[1]]
[1] ""RBO""

[[2]]
[1] ""36.26""

[[3]]
[1] ""1,800,000""
</code></pre>

<p>You could put is into one line by assigning <code>s</code> as the inner call.  Since functions and calls are evaluated from the inside out, <code>s</code> is assigned before R reaches the outside <code>s</code> subset.  This is a bit less readable though.</p>

<pre><code>s[nzchar(s &lt;- strsplit(x, ""[: ]|[[:cntrl:]]"")[[1]])][c(3,8,11)]
</code></pre>

<p>So this would go <code>s &lt;- strsplit(...)</code> -> <code>[[</code> -> <code>nzchar</code> -> <code>s[..</code> >- <code>[c(3,8,11)]</code> </p>
"
37550993,RStudio installation failure under Debian sid: libgstreamer dependency problems,2,2,2,"<p>I use Debian sid (amd64), rolling updates as often as weekly. I downloaded recently the desktop version 0.99.902 of RStudio from their offical site and issued (as root, of course):</p>

<p>dpkg -i rstudio-0.99.902-amd64.deb</p>

<p>to no avail:</p>

<p>dpkg: dependency problems prevent configuration of rstudio:
 rstudio depends on libgstreamer0.10-0; however:
  Package libgstreamer0.10-0 is not installed.
 rstudio depends on libgstreamer-plugins-base0.10-0; however:
  Package libgstreamer-plugins-base0.10-0 is not installed.</p>

<p>Newer versions (1.0-0) of these 2 packages are installed on the system, but those older ones (0.10-0) are not available anymore on the official Debian repos.</p>

<p>What should be done to have RStudio installed and fully operational under
Debian sid? I have, of course, installed R debs, from official Debian
repositories, without any issues...</p>

<p>Thanks for any help!</p>
","<p>RStudio 1.0.153, released on July 20th 2017, depends on GStreamer 1.0 instead of GStreamer 0.10. It can be installed on modern Debian/Ubuntu without any additional setup, rendering this question and my answer obsolete.</p>

<p>To be more specific, there are two different DEB packages. One is aimed at Ubuntu 16.04 (or later) and Debian 9 (or later), comes only in 64-bit flavor and depends on newer GStreamer 1.0. Another package supports Ubuntu from 12.04 up to 15.10 and Debian 8 and it comes in both 32-bit and 64-bit flavors. This one still depends on older GStreamer 0.10.</p>

<p>Original answer remains below. </p>

<hr>

<p>As of mid-2016, RStudio has hard dependency on GStreamer 0.10 and there is no way around it. You have to install <code>libgstreamer0.10-0</code> and <code>libgstreamer-plugins-base0.10-0</code> to use RStudio.</p>

<p>These packages can be easily pulled in from Debian Jessie (stable). Just add Jessie repository to your sources.list and use apt-pinning to give it lower priority:</p>

<pre><code># /etc/apt/sources.list:
deb http://httpredir.debian.org/debian jessie main
</code></pre>



<pre><code># /etc/apt/preferences.d/01_release:
Package: *
Pin: release o=Debian,a=unstable
Pin-Priority: 600

Package: *
Pin: release o=Debian,n=jessie
Pin-Priority: 10
</code></pre>

<p>Then issue <code>apt-get update</code> and follow up with <code>apt-get install libgstreamer0.10-0 libgstreamer-plugins-base0.10-0</code>. </p>

<p>If you have happened to put RStudio .deb file into local repository, then use <code>apt-get install rstudio</code> and GStreamer0.10 will be pulled in by dependency resolver. This has additional advantage of marking these libraries as automatically installed - they will be subject to automatic removal once RStudio drop them as dependency.</p>

<h2>But will that break my system?</h2>

<p>No. </p>

<p>Upstream developers designed GStreamer0.10 and GStreamer1.0 as co-installable and able to run at the same time (<a href=""https://lwn.net/Articles/464270/"" rel=""nofollow noreferrer"">source</a>). In fact, both were available in Debian repository since <a href=""https://lists.debian.org/debian-devel-changes/2012/09/msg01120.html"" rel=""nofollow noreferrer"">September 2012</a> up to <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=822882;msg=10"" rel=""nofollow noreferrer"">April 2016</a>.</p>

<p>In this pinning setup, packages from Jessie repository will be pulled in only when Jessie is the only provider of requested package. There is no risk of overwriting any package from unstable with older version from stable.</p>

<h2>Why does RStudio depend on obsolete library?</h2>

<p>Because GStreamer0.10 is the newest version available in both Debian Jessie and Ubuntu 12.04, two distributions they want to support.</p>

<p>RStudio will eventually have to upgrade their dependency to GStreamer1.0, as it will gradually become the only version available. I guess this change may be introduced in spring 2017. First, support for Ubuntu 12.04 will end in April. Rstudio is likely to bump base system requirement to 14.04 - one that has both GStreamer0.10 and 1.0. Second, Debian Stretch - that will have only GStreamer1.0 available - is expected to be released around that time.</p>
"
26593508,How to update R data.frame column values efficiently?,1,1,1,"<p>Say I have this table A</p>

<pre><code>                mpg 
RX4            21.0  
Wag            21.0  
Datsun         22.8  
Drive          21.4   
Sportabout     18.7   
Valiant        18.1  
Duste          14.3   
Merc           24.4   
</code></pre>

<p>Now I have a table B</p>

<pre><code>              mpg
RX4           60.0  
Wag           60.0  
Datsun        70.8  
</code></pre>

<p>What I want to do is to update the mpg value of Table A according to Table B, I can do that easily using hashmap in Java, may I know what is the efficient way of doing that in R?</p>

<p>Thanks very much indeed. </p>
","<p>You could use <code>match</code> to match the rownames of <code>df1</code> (first dataset) and <code>df2</code> (second) and then use it as a index to replace values of <code>mpg</code> in <code>df1</code> with those from <code>df2</code></p>

<pre><code> indx &lt;- match(row.names(df2), row.names(df1))
 df1$mpg[indx] &lt;- df2$mpg[indx]
</code></pre>

<p>Or you could use the <code>compact</code> solution offered by @digEmAll </p>

<pre><code> df1[row.names(df2),'mpg'] &lt;- df2$mpg 
</code></pre>

<h3>Update</h3>

<p>Using the new info about some elements in <code>df2</code> are not in <code>df1</code> and wants to add those rows into `df1:</p>

<pre><code> indx &lt;- match(row.names(df2), row.names(df1))
 indx1 &lt;- indx[!is.na(indx)]

 indx2 &lt;- match(row.names(df1), row.names(df2))
 indx22 &lt;- indx2[!is.na(indx2)]

 df1$mpg[indx1] &lt;- df2$mpg[indx22]
 df1N &lt;- rbind(df1,df2[setdiff(rownames(df2), rownames(df1)),,drop=FALSE])
 df1N
 #           mpg
 #RX4        60.0
 #Wag        60.0
 #Datsun     70.8
 #Drive      21.4
 #Sportabout 18.7
 #Valiant    18.1
 #Duste      14.3
 #Merc       24.4
 #Mazda      45.0
 #Mercury    42.0
</code></pre>

<p>Or you could use <code>intersect</code> and <code>setdiff</code></p>

<pre><code>  indxN &lt;- intersect(row.names(df1), row.names(df2))
  df1[indxN, 'mpg']  &lt;- df2[indxN, 'mpg']
  rbind(df1,df2[setdiff(rownames(df2), rownames(df1)),,drop=FALSE])
</code></pre>

<h3>new data</h3>

<pre><code>  df1 &lt;- structure(list(mpg = c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 
   24.4)), .Names = ""mpg"", class = ""data.frame"", row.names = c(""RX4"", 
  ""Wag"", ""Datsun"", ""Drive"", ""Sportabout"", ""Valiant"", ""Duste"", ""Merc""
  ))


  df2 &lt;- structure(list(mpg = c(45, 60, 60, 42, 70.8)), .Names = ""mpg"",
   class =    ""data.frame"", row.names = c(""Mazda"", ""RX4"", ""Wag"",
  ""Mercury"", ""Datsun""))
</code></pre>

<h3>old data</h3>

<pre><code>  df1 &lt;- structure(list(mpg = c(60, 70, 80.8, 90.4, 18.7, 18.1, 14.3, 
  24.4, 22.8, 19.2, 17.8), cyl = c(6L, 6L, 4L, 6L, 8L, 6L, 8L, 
  4L, 4L, 6L, 6L), disp = c(160, 160, 108, 258, 360, 225, 360, 
 146.7, 140.8, 167.6, 167.6), hp = c(110L, 110L, 93L, 110L, 175L, 
 105L, 245L, 62L, 95L, 123L, 123L), drat = c(3.9, 3.9, 3.85, 3.08, 
 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92), wt = c(2.62, 2.875, 
 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44), qsec = c(16.46, 
 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9
 ), vs = c(0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L), am = c(1L, 
 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), gear = c(4L, 4L, 4L, 
 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L), carb = c(4L, 4L, 1L, 1L, 2L, 
 1L, 4L, 2L, 2L, 4L, 4L)), .Names = c(""mpg"", ""cyl"", ""disp"", ""hp"", 
 ""drat"", ""wt"", ""qsec"", ""vs"", ""am"", ""gear"", ""carb""), row.names = c(""Mazda RX4"", 
 ""Mazda RX4 Wag"", ""Datsun 710"", ""Hornet 4 Drive"", ""Hornet Sportabout"", 
 ""Valiant"", ""Duster 360"", ""Merc 240D"", ""Merc 230"", ""Merc 280"", 
 ""Merc 280C""), class = ""data.frame"")

 df2 &lt;- structure(list(mpg = c(60, 70, 80.8, 90.4), cyl = c(6L, 6L, 4L, 
 6L), disp = c(160, 160, 108, 258), hp = c(110L, 110L, 93L, 110L
 ), drat = c(3.9, 3.9, 3.85, 3.08), wt = c(2.62, 2.875, 2.32, 
 3.215), qsec = c(16.46, 17.02, 18.61, 19.44), vs = c(0L, 0L, 
 1L, 1L), am = c(1L, 1L, 1L, 0L), gear = c(4L, 4L, 4L, 3L), carb = c(4L, 
 4L, 1L, 1L)), .Names = c(""mpg"", ""cyl"", ""disp"", ""hp"", ""drat"", 
 ""wt"", ""qsec"", ""vs"", ""am"", ""gear"", ""carb""), class = ""data.frame"", row.names = 
 c(""Mazda RX4"",""Mazda RX4 Wag"", ""Datsun 710"", ""Hornet 4 Drive""))
</code></pre>
"
21370117,R-Advanced Web Scraping-bypassing aspNetHidden using xmlTreeParse(),2,2,4,"<p>This question takes a bit of time to introduce, bear with me. It will be fun to solve if you can get there. This scrape would be replicated over thousands of pages on this website using a loop. </p>

<p>I'm trying to scrape the website <a href=""http://www.digikey.com/product-detail/en/207314-1/A25077-ND/"" rel=""nofollow"">http://www.digikey.com/product-detail/en/207314-1/A25077-ND/</a> looking to capture the data in the table with Digi-Key Part Number, Quantity Available etc.. including the right hand side with Price Break, Unit Price, Extended Price. </p>

<p>Using the R function readHTMLTable() doesn't work and only returns NULL values. The reason for this (I believe) is because the website has hidden it's content using the tag ""aspNetHidden"" in the html code. </p>

<p>For this reason I also found difficulty using htmlTreeParse() and xmlTreeParse() with the whole section parented by  not appearing in the results. </p>

<p>Using the R function scrape() from the <a href=""http://cran.r-project.org/web/packages/scrapeR/scrapeR.pdf"" rel=""nofollow"">scrapeR package</a></p>

<pre><code>require(scrapeR)

URL&lt;-scrape(""http://www.digikey.com/product-detail/en/207314-1/A25077-ND/"")
</code></pre>

<p>does return the full html code including the lines of interest:</p>

<pre><code>&lt;th align=""right""&gt;Digi-Key Part Number&lt;/th&gt;
&lt;td id=""reportpartnumber""&gt;
&lt;meta itemprop=""productID"" content=""sku:A25077-ND""&gt;A25077-ND&lt;/td&gt;

&lt;th&gt;Price Break&lt;/th&gt;
&lt;th&gt;Unit Price&lt;/th&gt;
&lt;th&gt;Extended Price
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=""center""&gt;1&lt;/td&gt;
&lt;td align=""right""&gt;2.75000&lt;/td&gt;
&lt;td align=""right""&gt;2.75&lt;/td&gt;
</code></pre>

<p>However, I haven't been able to select the nodes out of this block of code with the error being returned: </p>

<pre><code>no applicable method for 'xpathApply' applied to an object of class ""list""
</code></pre>

<p>I've received that error using different functions such as:</p>

<pre><code>xpathSApply(URL,'//*[@id=""pricing""]/tbody/tr[2]')

getNodeSet(URL,""//html[@class='rd-product-details-page']"")
</code></pre>

<p>I'm not the most familiar with xpath but have been identifying the xpath using inspect element on the webpage and copy xpath. </p>

<p>Any help you can give on this would be much appreciated!</p>
","<p>You've not read the help for scrape have you? It returns a list, you need to get parts of that list (if parse=TRUE) and so on.</p>

<p>Also I think that web page is doing some heavy heavy browser detection. If I try and <code>wget</code> the page from the command line I get an error page, the <code>scrape</code> function gets something usable (but seems different to you) and Chrome gets the full junk with all the encoded stuff. Yuck. Here's what works for me:</p>

<pre><code>&gt; URL&lt;-scrape(""http://www.digikey.com/product-detail/en/207314-1/A25077-ND/"")
&gt; tables = xpathSApply(URL[[1]],'//table')
&gt; tables[[2]]
&lt;table class=""product-details"" border=""1"" cellspacing=""1"" cellpadding=""2""&gt;
  &lt;tr class=""product-details-top""/&gt;
  &lt;tr class=""product-details-bottom""&gt;
    &lt;td class=""pricing-description"" colspan=""3"" align=""right""&gt;All prices are in US dollars.&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th align=""right""&gt;Digi-Key Part Number&lt;/th&gt;
    &lt;td id=""reportpartnumber""&gt;&lt;meta itemprop=""productID"" content=""sku:A25077-ND""/&gt;A25077-ND&lt;/td&gt;
    &lt;td class=""catalog-pricing"" rowspan=""6"" align=""center"" valign=""top""&gt;
      &lt;table id=""pricing"" frame=""void"" rules=""all"" border=""1"" cellspacing=""0"" cellpadding=""1""&gt;
        &lt;tr&gt;
          &lt;th&gt;Price Break&lt;/th&gt;
          &lt;th&gt;Unit Price&lt;/th&gt;
          &lt;th&gt;Extended Price&amp;#13;
&lt;/th&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td align=""center""&gt;1&lt;/td&gt;
          &lt;td align=""right""&gt;2.75000&lt;/td&gt;
          &lt;td align=""right""&gt;2.75&lt;/td&gt;
</code></pre>

<p>Adjust to your use-case, here I'm getting all the tables and showing the second one, which has the info you want, some of it in the <code>pricing</code> table which you can get directly with:</p>

<pre><code>pricing = xpathSApply(URL[[1]],'//table[@id=""pricing""]')[[1]]

&gt; pricing
&lt;table id=""pricing"" frame=""void"" rules=""all"" border=""1"" cellspacing=""0"" cellpadding=""1""&gt;
  &lt;tr&gt;
    &lt;th&gt;Price Break&lt;/th&gt;
    &lt;th&gt;Unit Price&lt;/th&gt;
    &lt;th&gt;Extended Price&amp;#13;
&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=""center""&gt;1&lt;/td&gt;
    &lt;td align=""right""&gt;2.75000&lt;/td&gt;
    &lt;td align=""right""&gt;2.75&lt;/td&gt;
  &lt;/tr&gt;
</code></pre>

<p>and so on.</p>
"
35815845,"R, ggplot: Change linetype within a series",1,3,1,"<p>I am using ggplot geom_smooth to plot turnover data of a customer group from previous year against the current year (based on calendar weeks). As the last week is not complete, I would like to use a dashed linetype for the last week. However, I can't figure out how to that. I can either change the linetype for the entire plot or an entire series, but not within a series (depending on the value of x):</p>

<p><a href=""https://i.stack.imgur.com/eadPq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eadPq.png"" alt=""target graph""></a></p>

<p>To keep it simple, let's just use the following example:</p>

<pre><code>set.seed(42)
frame &lt;- data.frame(series = rep(c('a','b'),50),x = 1:100, y = runif(100))

ggplot(frame,aes(x = x,y = y, group = series, color=series)) + 
geom_smooth(size=1.5, se=FALSE)
</code></pre>

<p>How would I have to change this to get dashed lines for x >= 75?</p>

<p>The goal would be something like this:
<a href=""https://i.stack.imgur.com/89OGA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/89OGA.png"" alt=""example""></a></p>

<p>Thx very much for any help!</p>

<p><strong>Edit, 2016-03-05</strong></p>

<p>Of course I fail when trying to use this method on the original plot. The Problem lies with the ribbon, which is calculated using stat_summary and a predefined function. I tried to use use stat_summary on the original data (mdf), and geom_line on the smooth_data. Even when I comment out everything else, I still get ""Error: Continuous value supplied to discrete scale"". I believe the problem comes from the fact that the original x value (Kalenderwoche) was discrete, whereas the new, smoothed x is continuous. Do I have to somehow transform one into the other? What else could I do?</p>

<p>Here is what I tried (condensed to the essential lines):</p>

<pre><code>quartiles &lt;- function(x) {  
  x &lt;- na.omit(x) # remove NULL
  median &lt;- median(x)
  q1 &lt;- quantile(x,0.25)
  q3 &lt;- quantile(x,0.75)
  data.frame(y = median, ymin = median, ymax = q3)
}

g &lt;- ggplot(mdf, aes(x=Kalenderwoche, y=value, group=variable, colour=variable,fill=variable))+
geom_smooth(size=1.5, method=""auto"", se=FALSE)

# Take out the data for smooth line
smooth_data &lt;- ggplot_build(g)$data[[1]]

ggplot(mdf, aes(x=Kalenderwoche, y=value, group=variable, colour=variable,fill=variable))+
  stat_summary(fun.data = quartiles,geom=""ribbon"", colour=""NA"", alpha=0.25)+
  geom_line(data=smooth_data, aes(x=x, y=y, group=group, colour=group, fill=group))  
</code></pre>

<p>mdf looks like this:</p>

<pre><code>str(mdf)
'data.frame':   280086 obs. of  5 variables:
 $ konto_id     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Kalenderwoche: Factor w/ 14 levels ""2015-48"",""2015-49"",..: 4 12 1 3 7 13 10 6 5 9 ...
 $ variable     : Factor w/ 2 levels ""Umsatz"",""Umsatz Vorjahr"": 1 1 1 1 1 1 1 1 1 1 ...
 $ value        : num  0 428.3 97.8 76 793.1 ...
</code></pre>

<p>There are many accounts (konto_id), and for each account and calendar week (Kalenderwoche), there is a current turnover value (Umsatz) and a turnover value from last year (Umsatz Vorjahr). I can provide a smaller version of the data.frame and the entire code, if required.</p>

<p>Thx very much for any help!</p>

<p>P.S. I am a total novice in R, so my code probably looks rather stupid to pros, sorry for that :(</p>

<p><strong>Edit, 2016-03-06</strong></p>

<p>I have uploaded a subset of the data (mdf):
<a href=""https://drive.bitcasa.com/send/bsEpOfVFDwHO23XYF7XK8wsYh8p4MI1S_EEShsiBnLW1"" rel=""nofollow noreferrer"">mdf</a></p>

<p>The full code of the original graph is the following (looking somewhat weird with so little data, but that's not the point ;)</p>

<pre><code>library(dtw)
library(reshape2)
library(ggplot2)
library(RODBC)
library(Cairo)

# custom breaks for X axis
breaks.custom &lt;- unique(mdf$Kalenderwoche)[c(TRUE,rep(FALSE,0))] 

# function called by stat_summary
quartiles &lt;- function(x) {  
  x &lt;- na.omit(x)
  median &lt;- median(x)
  q1 &lt;- quantile(x,0.25)
  q3 &lt;- quantile(x,0.75)
  data.frame(y = median, ymin = median, ymax = q3)
}

# Positions for guidelines and labels
horizontal.center &lt;- (length(unique(mdf$Kalenderwoche))+1)/2
kw.horizontal.center &lt;- as.vector(sort(unique(mdf$Kalenderwoche))[c(horizontal.center-0.5,horizontal.center+0.5)])
vpos.P75.label &lt;- max(quantile(mdf$value[mdf$Kalenderwoche==kw.horizontal.center[1]],0.75)
                      ,quantile(mdf$value[mdf$Kalenderwoche==kw.horizontal.center[2]],0.75))+10
# use the higher P75 value of the two weeks around the center
vpos.mean.label &lt;- min(mean(mdf$value[mdf$Kalenderwoche==kw.horizontal.center[1]])
                       ,mean(mdf$value[mdf$Kalenderwoche==kw.horizontal.center[2]]))-10
vpos.median.label &lt;- min(median(mdf$value[mdf$Kalenderwoche==kw.horizontal.center[1]])
                         ,median(mdf$value[mdf$Kalenderwoche==kw.horizontal.center[2]]))-10

hpos.vline &lt;- which(as.vector(sort(unique(mdf$Kalenderwoche))==""2016-03""))

# custom colour palette (2 colors)
cbPaletteLine &lt;- c(""#DA2626"", ""#2626DA"")
cbPaletteFill &lt;- c(""#F0A8A8"", ""#7C7CE9"")


# ggplot
ggplot(mdf, aes(x=Kalenderwoche, y=value, group=variable, colour=variable,fill=variable))+
  geom_smooth(size=1.5, method=""auto"", se=FALSE)+ 
  # SE=FALSE to suppress drawing of the SE of the fit.SE of the data shall be used instead:
  stat_summary(fun.data = quartiles,geom=""ribbon"", colour=""NA"", alpha=0.25)+
  scale_x_discrete(breaks=breaks.custom)+
  scale_colour_manual(values=cbPaletteLine)+
  scale_fill_manual(values=cbPaletteFill)+
  #coord_cartesian(ylim = c(0, 250)) +
  theme(legend.title = element_blank(), title = element_text(face=""bold"", size=12))+
  #scale_color_brewer(palette=""Dark2"")+
  labs(title = ""Tranche 1"", x =  ""Kalenderwoche"", y = ""Konto-Umsatz [CHF]"")+
  geom_vline(xintercept = hpos.vline, linetype=2)+
  annotate(""text"", x=horizontal.center, y=vpos.median.label, label = ""Median"", size=4)+
  annotate(""text"", x=horizontal.center, y=vpos.mean.label, label= ""Mean"", size=4)+  
  annotate(""text"", x=horizontal.center, y=vpos.P75.label, label = ""P75%"", size=4)+
  theme(axis.text.x=element_text(angle = 90, hjust = 0.5, vjust = 0.5))
</code></pre>

<p><strong>Edit, 2016-03-06</strong></p>

<p>The final plot now looks like this (thx, Jason!!)
<a href=""https://i.stack.imgur.com/B4gy5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B4gy5.png"" alt=""enter image description here""></a></p>
","<p>I am not so sure how to smooth all data and use different line types for subsets by <code>geom_smooth</code> function. My idea is to pull out the data which ggplot used to construct the plot and use <code>geom_line</code> to reproduce it. This was the way I did it: </p>

<pre><code>set.seed(42)
frame &lt;- data.frame(series=rep(c('a','b'), 50),
                    x = 1:100, y = runif(100))
library(ggplot2)
g &lt;- ggplot(frame, aes(x=x, y=y, color=series)) + geom_smooth(se=FALSE) 

# Take out the data for smooth line
smooth_data &lt;- ggplot_build(g)$data[[1]]
ggplot(smooth_data[smooth_data$x &lt;= 76, ], aes(x=x, y=y, color=as.factor(group), group=group)) +
  geom_line(size=1.5) +
  geom_line(data=smooth_data[smooth_data$x &gt;= 74, ], linetype=""dashed"", size=1.5) +
  scale_color_discrete(""Series"", breaks=c(""1"", ""2""), labels=c(""a"", ""b""))
</code></pre>

<p><a href=""https://i.stack.imgur.com/AQzbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQzbv.png"" alt=""enter image description here""></a></p>

<p>You're right. The problem is that you add a continuous x to a discrete x in the original layer. One way to deal with it is to create a lookup table which in this case, it is easy because x is a sequence from 1 to 14. We can transform discrete x by indexing. In your code, it should work if you add:</p>

<pre><code>level &lt;- levels(mdf$Kalenderwoche)
ggplot(mdf, aes(x=Kalenderwoche, y=value, group=variable, colour=variable,fill=variable))+
  stat_summary(fun.data = quartiles,geom=""ribbon"", colour=""NA"", alpha=0.25) +
  geom_line(data=smooth_data, aes(x=level[x], y=y, group=group, colour=as.factor(group), fill=NA)) 
</code></pre>

<p>Here is my attempt for the question:</p>

<pre><code>g &lt;- ggplot(mdf, aes(x=Kalenderwoche, y=value, group=variable, colour=variable,fill=variable)) +
  geom_smooth(size=1.5, method=""auto"", se=FALSE) + 
  # SE=FALSE to suppress drawing of the SE of the fit.SE of the data shall be used instead:
  stat_summary(fun.data = quartiles,geom=""ribbon"", colour=""NA"", alpha=0.25)    

smooth_data &lt;- ggplot_build(g)$data[[1]]
ribbon_data &lt;- ggplot_build(g)$data[[2]]    

# Use them as lookup table
level &lt;- levels(mdf$Kalenderwoche)
clevel &lt;- levels(mdf$variable)    

ggplot(smooth_data[smooth_data$x &lt;= 13, ], aes(x=level[x], y=y, group=group, color=as.factor(clevel[group]))) +
  geom_line(size=1.5) + 
  geom_line(data=smooth_data[smooth_data$x &gt;= 13, ], linetype=""dashed"", size=1.5) +
  geom_ribbon(data=ribbon_data,
              aes(x=x, ymin=ymin, ymax=ymax, fill=as.factor(clevel[group]), color=NA), alpha=0.25) +
  scale_x_discrete(breaks=breaks.custom) +
  scale_colour_manual(values=cbPaletteLine) +
  scale_fill_manual(values=cbPaletteFill) +
  #coord_cartesian(ylim = c(0, 250)) +
  theme(legend.title = element_blank(), title = element_text(face=""bold"", size=12))+
  #scale_color_brewer(palette=""Dark2"")+
  labs(title = ""Tranche 1"", x =  ""Kalenderwoche"", y = ""Konto-Umsatz [CHF]"")+
  geom_vline(xintercept = hpos.vline, linetype=2)+
  annotate(""text"", x=horizontal.center, y=vpos.median.label, label = ""Median"", size=4)+
  annotate(""text"", x=horizontal.center, y=vpos.mean.label, label= ""Mean"", size=4)+  
  annotate(""text"", x=horizontal.center, y=vpos.P75.label, label = ""P75%"", size=4)+
  theme(axis.text.x=element_text(angle = 90, hjust = 0.5, vjust = 0.5))
</code></pre>

<p><a href=""https://i.stack.imgur.com/oIsBb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oIsBb.png"" alt=""enter image description here""></a></p>

<p>Note that the legend has borderline.</p>
"
26962642,Constraining norms with inequalities,2,3,3,"<p>I have time-series data for N stocks.</p>

<p><code>sample.data&lt;-replicate(10,rnorm(1000))</code>, where each column shows the returns of different stocks over time.</p>

<p>I am trying to construct a portfolio weight vector to minimize the variance of the returns.</p>

<p>the objective function:</p>

<pre><code>min w^{T}\sum w
s.t. e_{n}^{T}w=1
\left \| w \right \|\leq C
</code></pre>

<p>where w is the vector of weights, <code>\sum</code> is the covariance matrix, <code>e_{n}^{T}</code> is a vector of ones, <code>C</code> is a constant. Where the second constraint (<code>\left \| w \right \|</code>) is an inequality constraint. </p>

<p>I used the following code to solve this problem:</p>

<pre><code>library(Rsolnp) 
 gamma&lt;-1  
fn&lt;-function(x) {cov.Rt&lt;-cov(sample.data); return(t(x)%*%cov.Rt%*%x)}   #OBJECTIVE FUNCTION TO MINIMIZE 
eqn&lt;-function(x){one.vec&lt;-matrix(1,ncol=10,nrow=1); return(one.vec%*%x)} #EQUALITY CONSTRAINT
constraints&lt;-1   #EQUALITY CONSTRAINT
ineq&lt;-function(x){one.vec&lt;-matrix(1,ncol=10,nrow=1); #INEQUALITY CONSTRAINT
z1&lt;-one.vec%*%abs(x)
return(z1)  
    }   

uh&lt;-gamma #UPPER BOUND
lb&lt;-0 #LOWER BOUND
x0&lt;-matrix(0,10,1) #STARTING PARAMETER VECTOR (NOT SURE WHAT STARTING VALUES TO PUT HERE)

sol1&lt;-solnp(x0,fun=fn,eqfun=eqn,eqB=constraints, ineqfun=ineq,ineqUB=gamma,ineqLB=lb)
</code></pre>

<p>When running this code I get the following error message:</p>

<pre><code>Error in solve.default(cz,tol = 1e-25) : system is computationally singular: reciprocal condition number = 0 In addition: There were 50 warnings (use warnings() to see the first 50)

warnings()
1: In cbind(temp, funv): number of rows of result is not a multiple of vector length
</code></pre>

<p>Any ideas what I might be doing wrong?
Is there a problem with the starting parameter vector <code>x0</code>?</p>
","<p>I changed your code slightly:</p>

<pre><code>library(Rsolnp)
set.seed(1)
sample.data &lt;- matrix(rnorm(10*1000),ncol=10)
gamma &lt;- 1

#OBJECTIVE FUNCTION TO MINIMIZE 
fn &lt;- function(x){
  cov.Rt &lt;- cov(sample.data); 
  as.numeric(t(x) %*% cov.Rt %*% x) 
} 
#EQUALITY CONSTRAINT
eqn &lt;- function(x){
  one.vec &lt;- matrix(1, ncol=10, nrow=1)
  as.numeric(one.vec %*% x) 
} 
constraints &lt;- 1
#INEQUALITY CONSTRAINT
ineq &lt;- function(x){ 
  one.vec &lt;- matrix(1, ncol=10, nrow=1); 
  z1&lt;-one.vec %*% abs(x)
  as.numeric(z1)  
}   

uh &lt;- gamma #UPPER BOUND
lb &lt;- 0 #LOWER BOUND
x0 &lt;- matrix(1, 10, 1) #STARTING PARAMETER VECTOR (NOT SURE WHAT STARTING VALUES TO PUT HERE)

sol1 &lt;- solnp(x0, fun=fn, eqfun=eqn, eqB=constraints)
</code></pre>

<p>When we run the above code we get a solution of:</p>

<pre><code>Iter: 1 fn: 0.09624      Pars:  0.08355 0.08307 0.10154 0.09108 0.11745 0.12076 0.09020 0.09435 0.10884 0.10918
Iter: 2 fn: 0.09624      Pars:  0.08354 0.08308 0.10153 0.09107 0.11746 0.12078 0.09021 0.09434 0.10883 0.10918
solnp--&gt; Completed in 2 iterations
</code></pre>

<p>But when we add the inequality restriction to the optimization problem, then we run into problems:</p>

<pre><code>&gt; sol2 &lt;- solnp(x0, fun=fn, eqfun=eqn, eqB=constraints, 
          ineqfun=ineq, ineqUB=gamma, ineqLB=lb)

Iter: 1 fn: 0.09624      Pars:  0.08356 0.08305 0.10153 0.09111 0.11748 0.12078 0.09021 0.09431 0.10881 0.10916
solnp--&gt;The linearized problem has no feasible
solnp--&gt;solution.  The problem may not be feasible.

Iter: 2 fn: 272.5459     Pars:  4.44541 4.42066 5.40272 4.84595 6.25082 6.42718 4.80020 5.02004 5.79138 5.81029
solnp--&gt;The linearized problem has no feasible
solnp--&gt;solution.  The problem may not be feasible.

Iter: 3 fn: 272.5459     Pars:  4.44547 4.42070 5.40274 4.84596 6.25078 6.42712 4.80023 5.02006 5.79138 5.81023
Iter: 4 fn: 0.09624      Pars:  0.08357 0.08304 0.10157 0.09106 0.11744 0.12074 0.09021 0.09432 0.10886 0.10918
Iter: 5 fn: 0.09625      Pars:  0.08354 0.08308 0.10153 0.09107 0.11747 0.12078 0.09021 0.09434 0.10883 0.10919
Iter: 6 fn: 0.09717      Pars:  0.08394 0.08347 0.10201 0.09150 0.11803 0.12135 0.09064 0.09479 0.10935 0.10971
Iter: 7 fn: 0.09624      Pars:  0.08353 0.08307 0.10153 0.09106 0.11747 0.12078 0.09020 0.09433 0.10883 0.10919
Iter: 8 fn: 0.09624      Pars:  0.08353 0.08307 0.10153 0.09106 0.11747 0.12078 0.09020 0.09433 0.10883 0.10919
solnp--&gt; Solution not reliable....Problem Inverting Hessian.
Warning message:
In p0 * vscale[(neq + 2):(nc + np + 1)] :
  longer object length is not a multiple of shorter object length
</code></pre>

<p>Let's try to change gamma just a little:
<code>gamma &lt;- 1.01</code></p>

<pre><code>&gt; sol2 &lt;- solnp(x0, fun=fn, eqfun=eqn, eqB=constraints, 
          ineqfun=ineq, ineqUB=gamma, ineqLB=lb)
Iter: 1 fn: 0.09624      Pars:  0.08355 0.08307 0.10153 0.09108 0.11745 0.12076 0.09020 0.09435 0.10884 0.10918
Iter: 2 fn: 0.09624      Pars:  0.08354 0.08308 0.10153 0.09107 0.11746 0.12078 0.09021 0.09434 0.10883 0.10918
solnp--&gt; Completed in 2 iterations
</code></pre>

<p>So your inequality constraint seems to be binding exactly around the equality constraint. Also looking at the two constraints together, it seems a little strange to me. My guess is, that you probably specified your inequality constraint wrong and simply wants something like a shortselling constraint, eg. weights between 0 and 1. This could be archieved following your method by:</p>

<pre><code>ineq &lt;- function(x){ return(x) }   
uh &lt;- rep(gamma, 10) #UPPER BOUND
lb &lt;- rep(0, 10) #LOWER BOUND
sol3 &lt;- solnp(x0, fun=fn, eqfun=eqn, eqB=constraints, ineqfun=ineq, ineqUB=uh, ineqLB=lb)
</code></pre>
"
40919759,Stacked barplot using R base: how to add values inside each stacked bar,1,3,3,"<p>I found a plot in a stats book, which I want to reproduce with the base package. </p>

<p>The plot looks like this:</p>

<p><a href=""https://i.stack.imgur.com/etIoq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/etIoq.png"" alt=""enter image description here""></a></p>

<p>So far I have the plot, but I have problems to add a centred labels to each part of the bar. </p>

<p>My code looks like this:</p>

<pre><code>data   &lt;- sample( 5, 10 , replace = TRUE )

colors &lt;- c('yellow','violet','green','pink','red')

relative.frequencies &lt;- as.matrix( prop.table( table( data ) ) )

bc &lt;- barplot( relative.frequencies, horiz = TRUE, axes = FALSE, col = colors )
</code></pre>
","<p>For your given example, we can do (<strong>all readers can skip this part and jump to the next</strong>):</p>

<pre><code>set.seed(0)  ## `set.seed` for reproducibility
dat &lt;- sample( 5, 10 , replace = TRUE )
colors &lt;- c('yellow','violet','green','pink')
h &lt;- as.matrix( prop.table( table( dat ) ) )
## compute x-location of the centre of each bar
H &lt;- apply(h, 2L, cumsum) - h / 2
## add text to barplot
bc &lt;- barplot(h, horiz = TRUE, axes = FALSE, col = colors )
text(H, bc, labels = paste0(100 * h, ""%""))
</code></pre>

<p><a href=""https://i.stack.imgur.com/CwbiW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CwbiW.jpg"" alt=""strip""></a></p>

<hr>

<h1>For all readers</h1>

<p><strong>I will now construct a comprehensive example for you to digest the idea.</strong></p>

<p><strong>Step 1: generate a toy matrix of percentage for experiment</strong></p>

<pre><code>## a function to generate `n * p` matrix `h`, with `h &gt; 0` and `colSums(h) = 1`
sim &lt;- function (n, p) {
  set.seed(0)
  ## a positive random matrix of 4 rows and 3 columns
  h &lt;- matrix(runif(n * p), nrow = n)
  ## rescale columns of `h` so that `colSums(h)` is 1
  h &lt;- h / rep(colSums(h), each = n)
  ## For neatness we round `h` up to 2 decimals
  h &lt;- round(h, 2L)
  ## but then `colSums(h)` is not 1 again
  ## no worry, we simply reset the last row:
  h[n, ] &lt;- 1 - colSums(h[-n, ])
  ## now return this good toy matrix
  h
  }

h &lt;- sim(4, 3)
#     [,1] [,2] [,3]
#[1,] 0.43 0.31 0.42
#[2,] 0.13 0.07 0.40
#[3,] 0.18 0.30 0.04
#[4,] 0.26 0.32 0.14
</code></pre>

<p><strong>Step 2: understand a stacked bar-chart and get ""mid-height"" of each stacked bar</strong></p>

<p>For stacked bar-chart, the height of the bar is the cumulative sum of each column of <code>h</code>:</p>

<pre><code>H &lt;- apply(h, 2L, cumsum)
#     [,1] [,2] [,3]
#[1,] 0.43 0.31 0.42
#[2,] 0.56 0.38 0.82
#[3,] 0.74 0.68 0.86
#[4,] 1.00 1.00 1.00
</code></pre>

<p>We now shift back <code>h / 2</code> to get the mid / centre of each stacked bar:</p>

<pre><code>H &lt;- H - h / 2
#      [,1]  [,2] [,3]
#[1,] 0.215 0.155 0.21
#[2,] 0.495 0.345 0.62
#[3,] 0.650 0.530 0.84
#[4,] 0.870 0.840 0.93
</code></pre>

<p><strong>Step 3: producing a bar-chart with filled numbers</strong></p>

<p>For a vertical bar-chart, <code>H</code> above gives the <code>y</code> coordinate of the centre of each stacked bar. The <code>x</code> coordinate is returned by <code>barplot</code> (invisibly). Be aware, that we need to <strong>replicate</strong> each of <code>x</code>'s element <code>nrow(H)</code> times when using <code>text</code>:</p>

<pre><code>x &lt;- barplot(h, col = 1 + 1:nrow(h), yaxt = ""n"")
text(rep(x, each = nrow(H)), H, labels = paste0(100 * h, ""%""))
</code></pre>

<p><a href=""https://i.stack.imgur.com/KGWQf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KGWQf.jpg"" alt=""vertical barchart""></a></p>

<p>For a horizontal bar-chart, <code>H</code> above gives the <code>x</code> coordinate of the centre of each stacked bar. The <code>y</code> coordinate is returned by <code>barplot</code> (invisibly). Be aware, that we need to <strong>replicate</strong> each of <code>y</code>'s element <code>nrow(H)</code> times when using <code>text</code>:</p>

<pre><code>y &lt;- barplot(h, col = 1 + 1:nrow(h), xaxt = ""n"", horiz = TRUE)
text(H, rep(y, each = nrow(H)), labels = paste0(100 * h, ""%""))
</code></pre>

<p><a href=""https://i.stack.imgur.com/L5S9q.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L5S9q.jpg"" alt=""Horizontal bar-chart""></a></p>
"
40450608,Is `outer` fast enough for my double summation?,1,3,3,"<p>I'm trying to evaluate the following double sums in r:</p>

<p><a href=""https://i.stack.imgur.com/IOJXa.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IOJXa.gif"" alt=""enter image description here""></a></p>

<p>I know <code>outer</code> is a quick way to do it. I've tried the following</p>

<pre><code>sum(f(outer(X,X,function(x,y) (x-y)/c)))
</code></pre>

<p>and while it seems to work, I'm not sure just how fast it is compare to some alternatives? Does it make a difference in terms of speed to do <code>outer</code> first then my function or vice versa? is there a better way to do it?</p>
","<p>I would like to point out first that you can write you code as</p>

<pre><code>sum(f(outer(x, x, ""-"") / c))
</code></pre>

<p>This reduces function call overhead, as subtraction in R is already a function. Try <code>""-""(5, 2)</code>.</p>

<hr>

<p><code>outer</code> is fast enough for your application. The only case when it is suboptimal is when your function <code>f</code> is symmetric around 0, i.e., <code>f(-u) = f(u)</code>. In this case, optimal computation only sums over the lower triangular of combination matrix <code>outer(x, x, ""-"")</code>, and multiply the sum by 2 for summation over off-diagonals. Finally, diagonal results are added.</p>

<p>The following function does this. We generate <code>(i, j)</code> indices for the lower triangular part (excluding diagonal) of the combination matrix, then the lower triangular part of <code>outer(x, x, ""-"") / c</code> would be <code>dx &lt;- (x[i] - x[j]) / c</code>. Now,</p>

<ul>
<li>if <code>f</code> is symmetric, the result is <code>2 * sum(f(dx)) + n * f(0)</code>, and this is faster than <code>outer</code>;</li>
<li>if <code>f</code> is asymmetric, we have to do <code>sum(f(dx)) + sum(f(-dx)) + n * f(0)</code>, and this won't have any advantage over <code>outer</code>.</li>
</ul>

<hr>

<pre><code>## `x` is the vector, `f` is your function of interest, `c` is a constant
## `symmetric` is a switch; only set `TRUE` when `f` is symmetric around 0
g &lt;- function (x, f, c, symmetric = FALSE) {
  n &lt;- length(x)
  j &lt;- rep.int(1:(n-1), (n-1):1)
  i &lt;- sequence((n-1):1) + j
  dx &lt;- (x[i] - x[j]) / c
  if (symmetric) 2 * sum(f(dx)) + n * f(0)
  else sum(f(dx)) + sum(f(-dx)) + n * f(0)
  }
</code></pre>

<hr>

<p>Consider a small example here. Let's assume <code>c = 2</code> and a vector <code>x &lt;- 1:500</code>. We also consider a symmetric function <code>f1 &lt;- cos</code> and an asymmetric function <code>f2 &lt;- sin</code>. Let's do a benchmark:</p>

<pre><code>x &lt;- 1:500
library(microbenchmark)
</code></pre>

<p>We first consider the symmetric case with <code>f1</code>. Remember to set <code>symmetric = TRUE</code> for <code>g</code>.</p>

<pre><code>microbenchmark(sum(f1(outer(x,x,""-"")/2)), g(x, f1, 2, TRUE))

#Unit: milliseconds
#                        expr      min       lq     mean   median       uq
# sum(f2(outer(x, x, ""-"")/2)) 32.79472 35.35316 46.91560 36.78152 37.63580
#           g(x, f2, 2, TRUE) 20.24940 23.34324 29.97313 24.45638 25.33352
#      max neval cld
# 133.5494   100   b
# 120.3278   100  a 
</code></pre>

<p>Here we see that <code>g</code> is faster.</p>

<p>Now consider the asymmetric case with <code>f2</code>.</p>

<pre><code>microbenchmark(sum(f2(outer(x,x,""-"")/2)), g(x, f2, 2))

#Unit: milliseconds
#                        expr      min       lq     mean   median       uq
# sum(f2(outer(x, x, ""-"")/2)) 32.84412 35.55520 44.33684 36.95336 37.89508
#                 g(x, f2, 2) 36.71572 39.11832 50.54516 40.25590 41.75060
#      max neval cld
# 134.2991   100   a
# 142.5143   100   a
</code></pre>

<p>As expected, there is no advantage here.</p>

<hr>

<p>Yes, we also want to check that <code>g</code> is doing the correct computation. It is sufficient to consider a small example, with <code>x &lt;- 1:5</code>.</p>

<pre><code>x &lt;- 1:5

#### symmetric case ####

sum(f1(outer(x, x, ""-"") / 2))
# [1] 14.71313

g(x, f1, 2, TRUE)
# [1] 14.71313

#### asymmetric case ####

sum(f2(outer(x, x, ""-"") / 2))
# [1] 0

g(x, f2, 2)
# [1] 0
</code></pre>

<p>So <code>g</code> is correct.</p>
"
41570136,Parsing a text file in R with a variety of delimiters,1,1,4,"<p>I have a text file in this rather horrendous HTML format:</p>

<pre><code>A&lt;b&gt;Metabolism&lt;/b&gt;
B
B  &lt;b&gt;Overview&lt;/b&gt;
C    01200 Carbon metabolism [PATH:bpe01200]
D      BP3142 pgi; glucose-6-phosphate isomerase    K01810 GPI; glucose-6-phosphate isomerase [EC:5.3.1.9]
D      BP1971 pgi; glucose-6-phosphate isomerase    K01810 GPI; glucose-6-phosphate isomerase [EC:5.3.1.9]
D      BP1519 fba; fructose-1,6-bisphosphate aldolase   K01624 FBA; fructose-bisphosphate aldolase, class II [EC:4.1.2.13]
D      BP0801 tpiA; triosephosphate isomerase   K01803 TPI; triosephosphate isomerase (TIM) [EC:5.3.1.1]
D      BP1000 gap; glyceraldehyde-3-phosphate dehydrogenase K00134 GAPDH; glyceraldehyde 3-phosphate dehydrogenase [EC:1.2.1.12]
</code></pre>

<p>I would like to parse this file into columns in R.</p>

<p>such as:</p>

<pre><code>A,Metabolism
B,
B,Overview
C,01200,Carbon metabolism,Path,bpe01200
D,BP3142,Pgi,glucose-6-phosphate isomerase,GPI,glucose-6-phosphate isomerase,[EC:5.3.1.9]
...
D,BP1000,gap,glyceraldehyde-3-phosphate dehydrogenase,K00134,GAPDH,glyceraldehyde 3-phosphate dehydrogenase,[EC:1.2.1.12]
</code></pre>

<p>The problem is that the delimiter changes in each part of the line.
It seems to follow this pattern
e.g</p>

<pre><code>D      BP1971 pgi; glucose-6-phosphate isomerase    K01810 GPI; glucose-6-phosphate isomerase [EC:5.3.1.9]
 ^Tab        ^space^Semi colon                  ^tab      ^space^semi colon
</code></pre>

<p>I can think of the not so smart way to do it.By parsing 1 delimiter at a time. But does anyone have any smart solutions? or know of a tool that can interpret this nicely?</p>

<p>I would really appreciate some help :)</p>

<p>Thanks</p>
","<pre><code>library(stringr)
library(purrr)
file &lt;- ""A&lt;b&gt;Metabolism&lt;/b&gt;
B
B  &lt;b&gt;Overview&lt;/b&gt;
C\t01200 Carbon metabolism [PATH:bpe01200]
D\tBP3142 pgi; glucose-6-phosphate isomerase\tK01810 GPI; glucose-6-phosphate isomerase [EC:5.3.1.9]
D\tBP1971 pgi; glucose-6-phosphate isomerase\tK01810 GPI; glucose-6-phosphate isomerase [EC:5.3.1.9]
D\tBP1519 fba; fructose-1,6-bisphosphate aldolase\tK01624 FBA; fructose-bisphosphate aldolase, class II [EC:4.1.2.13]
D\tBP0801 tpiA; triosephosphate isomerase\tK01803 TPI; triosephosphate isomerase (TIM) [EC:5.3.1.1]
D\tBP1000 gap; glyceraldehyde-3-phosphate dehydrogenase\tK00134 GAPDH; glyceraldehyde 3-phosphate dehydrogenase [EC:1.2.1.12]
This line is to check behavior when parsing fails.""
cat(file)
data &lt;- readLines(con = textConnection(file))
# Pattern to capture ""A&lt;b&gt;Metabolism&lt;/b&gt;"" for instance
pattern_1 &lt;- ""^(\\w+)\\h*&lt;b&gt;\\h*(\\w+)\\h*&lt;/b&gt;\\h*$""
# Pattern to capture ""B"" for instance
pattern_2 &lt;- ""^(\\w+)$""
# Pattern to capture ""C\t01200 Carbon metabolism [PATH:bpe01200]"" for instance
pattern_3 &lt;- ""^(\\w+)\\t+(\\w+)\\s+([^\\[\\t;]*)\\h*(\\[[^\\]]*\\])$""
# Pattern to capture ""D\tBP3142 pgi; glucose-6-phosphate isomerase\tK01810 GPI; glucose-6-phosphate isomerase [EC:5.3.1.9]"" for instance
pattern_4 &lt;- ""^(\\w+)\\t+(\\w+)\\s+(\\w+);\\h*([^\\t]*)\\t+(\\w+)\\s+(\\w+);\\h*([^\\[]*)\\h*(\\[[^\\]]*\\])$""
# Some more explanations:
# Parens wrap groups to extract
# ""\\w+"" matches words
# ""\\t+"", ""\\s+"" or "";\\h*"" are specific separators of OP's original data
# ""([^\\t]*)"" matches anything until the next tab separator
# Convoluted patterns such as ""(\\[[^\\]]*\\])"" extract whatever is inside brackets
patterns &lt;- mget(paste0(""pattern_"", 1:4))
# A list of the data parsed 4 times, once for each pattern:
patterns %&gt;% 
  map(~ {
    extraction &lt;- str_match(data, .x)
    cbind(match = !is.na(extraction[, 1]), extraction[, - 1])
  })
# This is closer to your desired output: a list of [un]parsed rows:
data %&gt;%
  map(~ {
    # Find the first pattern that matches. 0 if none does
    pattern_index &lt;- detect_index(patterns, grepl, .x, perl = TRUE)
    # If failed to parse, return original row as length 1 character vector. Else return parsed row as character vector
    if (pattern_index == 0L) .x else str_match(.x, get(paste0(""pattern_"", pattern_index)))[- 1]
  })
</code></pre>

<p>Head of output looks like this:</p>

<pre><code>list(c(""A"", ""Metabolism""), ""B"", c(""B"", ""Overview""), c(""C"", ""01200"", 
""Carbon metabolism "", ""[PATH:bpe01200]""), c(""D"", ""BP3142"", ""pgi"", 
""glucose-6-phosphate isomerase"", ""K01810"", ""GPI"", ""glucose-6-phosphate isomerase "", 
""[EC:5.3.1.9]""))
</code></pre>
"
43627015,Difference by Group using dplyr and data.table,1,1,1,"<p>I want to calculate difference by groups. Although I referred <a href=""https://stackoverflow.com/questions/24569177/r-function-diff-over-various-groups?noredirect=1&amp;lq=1"">R: Function “diff” over various groups</a> thread on SO, for unknown reason, I am unable to find the difference. I have tried three methods : a) <code>spread</code> b) <code>dplyr::mutate</code> with <code>base::diff()</code> c) <code>data.table</code> with <code>base::diff()</code>. While a) works, I am unsure how I can solve this problem using b) and c).</p>

<p><strong>Description about the data:</strong>
I have revenue data for the product by year. I have categorized years >= 2013 as Period 2 (called <code>P2</code>), and years &lt; 2013 as Period 1 (called <code>P1</code>).</p>

<p><strong>Sample data:</strong></p>

<pre><code>dput(Test_File)
structure(list(Ship_Date = c(2010, 2010, 2012, 2012, 2012, 2012, 
2017, 2017, 2017, 2016, 2016, 2016, 2011, 2017), Name = c(""Apple"", 
""Apple"", ""Banana"", ""Banana"", ""Banana"", ""Banana"", ""Apple"", ""Apple"", 
""Apple"", ""Banana"", ""Banana"", ""Banana"", ""Mango"", ""Pineapple""), 
    Revenue = c(5, 10, 13, 14, 15, 16, 25, 25, 25, 1, 2, 4, 5, 
    7)), .Names = c(""Ship_Date"", ""Name"", ""Revenue""), row.names = c(NA, 
14L), class = ""data.frame"")
</code></pre>

<p><strong>Expected Output</strong></p>

<pre><code>dput(Diff_Table)
structure(list(Name = c(""Apple"", ""Banana"", ""Mango"", ""Pineapple""
), P1 = c(15, 58, 5, NA), P2 = c(75, 7, NA, 7), Diff = c(60, 
-51, NA, NA)), .Names = c(""Name"", ""P1"", ""P2"", ""Diff""), class = ""data.frame"", row.names = c(NA, 
-4L))
</code></pre>

<p><strong>Here's my code:</strong></p>

<p><strong>Method 1: Using <code>spread</code> [Works]</strong></p>

<pre><code>data.table::setDT(Test_File)
cutoff&lt;-2013
Test_File[Test_File$Ship_Date&gt;=cutoff,""Ship_Period""]&lt;-""P2""
Test_File[Test_File$Ship_Date&lt;cutoff,""Ship_Period""]&lt;-""P1""

Diff_Table&lt;- Test_File %&gt;%
  dplyr::group_by(Ship_Period,Name) %&gt;%
  dplyr::mutate(Revenue = sum(Revenue)) %&gt;%
  dplyr::select(Ship_Period, Name,Revenue) %&gt;%
  dplyr::ungroup() %&gt;%
  dplyr::distinct() %&gt;%
  tidyr::spread(key = Ship_Period,value = Revenue) %&gt;% 
  dplyr::mutate(Diff = `P2` - `P1`)
</code></pre>

<p><strong>Method 2: Using <code>dplyr</code> [Doesn't work: NAs are generated in <code>Diff</code> column.]</strong></p>

<pre><code>Diff_Table&lt;- Test_File %&gt;%
  dplyr::group_by(Ship_Period,Name) %&gt;%
  dplyr::mutate(Revenue = sum(Revenue)) %&gt;%
  dplyr::select(Ship_Period, Name,Revenue) %&gt;%
  dplyr::ungroup() %&gt;%
  dplyr::distinct() %&gt;%
  dplyr::arrange(Name,Ship_Period, Revenue) %&gt;%
  dplyr::group_by(Ship_Period,Name) %&gt;%
  dplyr::mutate(Diff = diff(Revenue))
</code></pre>

<p><strong>Method 3: Using <code>data.table</code> [Doesn't work: It generates all zeros in <code>Diff</code> column.]</strong></p>

<pre><code>Test_File[,Revenue1 := sum(Revenue),by=c(""Ship_Period"",""Name"")]
Diff_Table&lt;-Test_File[,.(Diff = diff(Revenue1)),by=c(""Ship_Period"",""Name"")]
</code></pre>

<p><strong>Question:</strong> Can someone please help me with method 2 and method 3 above? I am fairly new to R so I apologize if my work sounds too basic. I am still learning this language.</p>
","<p>We can do this with <code>data.table</code>.  Convert the 'data.frame' to 'data.table' (<code>setDT(Test_File)</code>), grouped by the run-length-id of 'Name' and 'Name', get the <code>sum</code> of 'Revenue', reshape it to 'wide' format with <code>dcast</code>, get the difference between 'P2' and 'P1' and assign (<code>:=</code>) it to 'Diff'</p>

<pre><code>library(data.table)
dcast(setDT(Test_File)[, .(Revenue = sum(Revenue)),
   .(grp=rleid(Name), Name)], Name~ paste0(""P"", rowid(Name)), 
        value.var = ""Revenue"")[, Diff := P2 - P1][]
#        Name P1 P2 Diff
#1:     Apple 15 75   60
#2:    Banana 58  7  -51
#3:     Mango  5 NA   NA
#4: Pineapple  7 NA   NA
</code></pre>

<hr>

<p>Or for third case, i.e. <code>base R</code>, we create a grouping column based on whether the adjacent elements in 'Name' are the same or not ('grp'), then <code>aggregate</code> the 'Revenue' by 'Name' and 'grp' to find the <code>sum</code>, create a sequence column, <code>reshape</code> it to 'wide' and <code>transform</code> the dataset to create the 'Diff' column </p>

<pre><code>Test_File$grp &lt;- with(Test_File, cumsum(c(TRUE, Name[-1]!=Name[-length(Name)])))
d1 &lt;- aggregate(Revenue~Name +grp, Test_File, sum)
d1$Seq &lt;- with(d1, ave(seq_along(Name), Name, FUN = seq_along))
transform(reshape(d1[-2], idvar = ""Name"", timevar = ""Seq"", 
            direction = ""wide""), Diff = Revenue.2- Revenue.1)
</code></pre>

<hr>

<p>The <code>tidyverse</code> method can also be done using</p>

<pre><code>library(dplyr)
library(tidyr)
Test_File %&gt;% 
       group_by(grp = cumsum(c(TRUE, Name[-1]!=Name[-length(Name)])), Name)  %&gt;%
       summarise(Revenue = sum(Revenue)) %&gt;%
       group_by(Name) %&gt;% 
       mutate(Seq = paste0(""P"", row_number()))  %&gt;% 
       select(-grp) %&gt;% 
       spread(Seq, Revenue) %&gt;% 
       mutate(Diff = P2-P1)
 #Source: local data frame [4 x 4]
 #Groups: Name [4]

#      Name    P1    P2  Diff
#      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
#1     Apple    15    75    60
#2    Banana    58     7   -51
#3     Mango     5    NA    NA
#4 Pineapple     7    NA    NA
</code></pre>

<h3>Update</h3>

<p>Based on the OP's comments to use only <code>diff</code> function</p>

<pre><code>library(data.table)
setDT(Test_File)[, .(Revenue = sum(Revenue)), .(Name, grp = rleid(Name))
  ][, .(P1 = Revenue[1L], P2 = Revenue[2L], Diff = diff(Revenue)) , Name]
#        Name P1 P2 Diff
#1:     Apple 15 75   60
#2:    Banana 58  7  -51
#3:     Mango  5 NA   NA
#4: Pineapple  7 NA   NA
</code></pre>

<hr>

<p>Or with <code>dplyr</code></p>

<pre><code>Test_File %&gt;% 
   group_by(grp = cumsum(c(TRUE, Name[-1]!=Name[-length(Name)])), Name)  %&gt;%
   summarise(Revenue = sum(Revenue)) %&gt;%
   group_by(Name) %&gt;% 
   summarise(P1 = first(Revenue), P2 = last(Revenue)) %&gt;%
   mutate(Diff = P2-P1)
</code></pre>
"
15912817,Computing the circularity of a binary image,1,3,3,"<p>I am trying to compute the circularity of a given binary image. After some research its clear to me that the formula for circularity is </p>

<pre><code>4π*area/perimeter^2
</code></pre>

<p>Which should range from 0 to 1, 1 being most circular.</p>

<p>Given the binary matrix <code>im</code></p>

<p>Computing the area is trivial</p>

<p>area = sum(im)</p>

<p>I am computing the perimeter following this rule: <code>A pixel is part of the perimeter if it is nonzero and it is connected to at least one zero-valued pixel</code></p>

<pre><code>per = matrix(0, nrow(im), ncol(im))
for(i in 2:(nrow(im)-1)){
  for(j in 2:(ncol(im)-1)){
    if(im[i,j] != 0){
      x=c(im[i-1,j],im[i+1,j],im[i,j-1], im[i,j+1])
      if(0 %in% x) per[i,j] = 1
    }
  }
}
perimeter = sum(per)
</code></pre>

<p>Then i compute circularity like this:</p>

<pre><code>circ = (4*pi*area)/(perimiter^2)
</code></pre>

<p>However, I get values larger than 1 sometimes and things dont add up. For example: </p>

<p>this image gives me <code>circ=1.155119</code></p>

<p><img src=""https://i.stack.imgur.com/ks6MV.jpg"" alt=""enter image description here""></p>

<p>and this image gives me <code>circ=1.148728</code></p>

<p><img src=""https://i.stack.imgur.com/7OUs0.jpg"" alt=""enter image description here""></p>

<p>Any idea whats going on?
Shouldn't the values be more like <code>0.95</code> and <code>0.7</code></p>
","<p>Your definition for the ""binary perimeter"" is not a good approximation
of the smooth perimeter.</p>

<pre><code># Sample data
n &lt;- 100
im &lt;- matrix(0, 3*n, 3*n+1)
x &lt;- ( col(im) - 1.5*n ) / n
y &lt;- ( row(im) - 1.5*n ) / n
im[ x^2 + y^2 &lt;= 1 ] &lt;- 1
image(im)

# Shift the image in one direction
s1 &lt;- function(z) cbind(rep(0,nrow(z)), z[,-ncol(z)] )
s2 &lt;- function(z) cbind(z[,-1], rep(0,nrow(z)) )
s3 &lt;- function(z) rbind(rep(0,ncol(z)), z[-nrow(z),] )
s4 &lt;- function(z) rbind(z[-1,], rep(0,ncol(z)) )

# Area, perimeter and circularity
area &lt;- function(z) sum(z)
perimeter &lt;- function(z) sum( z != 0 &amp; s1(z)*s2(z)*s3(z)*s4(z) == 0)
circularity &lt;- function(z) 4*pi*area(z) / perimeter(z)^2

circularity(im)
# [1] 1.241127

area(im)
# [1] 31417
n^2*pi
# [1] 31415.93

perimeter(im)
# [1] 564
2*pi*n
# [1] 628.3185
</code></pre>

<p>One worrying feature is that this perimeter is not rotation-invariant:
when you rotate a square of side 1 (with sides parallel to the axes)
by 45 degrees, its area remains the same, but its perimeter is divided by sqrt(2)...</p>

<pre><code>square1 &lt;- -1 &lt;= x &amp; x &lt;= 1 &amp; -1 &lt;= y &amp; y &lt;= 1
c( perimeter(square1), area(square1) )
# [1]   800 40401

square2 &lt;- abs(x) + abs(y) &lt;= sqrt(2)
c( perimeter(square2), area(square2) )
# [1]   564 40045
</code></pre>

<p>Here is a slightly better approximation of the perimeter.
For each point on the perimeter, 
look at which points in its 8-neighbourhood are also in the perimeter;
if they form a vertical or horizontal segment, 
the contribution of the pair to the perimeter is 1, 
if they are in diagonal, the contribution is sqrt(2).</p>

<pre><code>edge &lt;- function(z) z &amp; !(s1(z)&amp;s2(z)&amp;s3(z)&amp;s4(z))
perimeter &lt;- function(z) {
  e &lt;- edge(z)
  ( 
    # horizontal and vertical segments
    sum( e &amp; s1(e) ) + sum( e &amp; s2(e) ) + sum( e &amp; s3(e) ) + sum( e &amp; s4(e) ) + 
    # diagonal segments
    sqrt(2)*( sum(e &amp; s1(s3(e))) + sum(e &amp; s1(s4(e))) + sum(e &amp; s2(s3(e))) + sum(e &amp; s2(s4(e))) )
  ) / 2  # Each segment was counted twice, once for each end
}

perimeter(im)
# [1] 661.7544
c( perimeter(square1), area(square1) )
# [1]   805.6569 40401.0000
c( perimeter(square2), area(square2) )
# [1]   797.6164 40045.0000

circularity(im)
# [1] 0.9015315
circularity(square1)
# [1] 0.7821711
circularity(square2)
# [1] 0.7909881
</code></pre>
"
43866046,Reuse input in Rshiny app,2,2,2,"<p>i'd like to reuse an input field in a tabbed shiny app. my code is below.</p>

<pre><code>library(shiny)

ui &lt;- navbarPage(""Iris data browser"",
    tabPanel(""Panel 1"",
             selectInput(""species"", ""Species"",
                         unique(iris$Species)),
             sliderInput(""sepal.length"", ""Sepal length"",
                         4.3,7.9,4.5,.1),
             tableOutput(""table1"")),

    tabPanel(""Panel 2"",
             selectInput(""species"", ""Species"",
                         unique(iris$Species)),
             tableOutput(""table2"")))


server &lt;- function(input, output) {
    output$table1 &lt;- renderTable({
        iris[iris$Species == input$species &amp; iris$Sepal.Length &lt;= input$sepal.length,c(""Sepal.Length"",""Sepal.Width"")]
    })

    output$table2 &lt;- renderTable({
        iris[iris$Species == input$species,c(""Petal.Length"",""Petal.Width"")]
    })
}

# Run the application 
shinyApp(ui = ui, server = server)
</code></pre>

<p>i'd like to use the same <code>selectInput()</code> on both panels. the expected result is that when i change the input value in ""Panel 1"" it will take on the same value in ""Panel 2"" and vice versa. of course, the filtering should also be applied to the tables on both panels. additionally, the input for species is shared on both panels, but the slider for sepal length should only appear on panel 1. therefore, sidebarLayout() is no solution.</p>

<p>thanks!</p>
","<p>Here is a solution that uses 2 <code>selectInput</code>s but links them so that they have the same choices selected. Explanation of changes is below the code:</p>

<pre><code>library(shiny)

ui &lt;- navbarPage(""Iris data browser"",
                 tabPanel(""Panel 1"",
                          selectInput(""species1"", ""Species"", choices=unique(iris$Species)),
                          sliderInput(""sepal.length"", ""Sepal length"",
                                      4.3,7.9,4.5,.1),
                          tableOutput(""table1"")),

                 tabPanel(""Panel 2"",
                          selectInput(""species2"", ""Species"", choices=unique(iris$Species) ),
                          uiOutput(""select2""),
                          tableOutput(""table2"")))


server &lt;- function(session, input, output) {

  Selected&lt;-reactiveValues(Species=NULL)



  observeEvent(input$species1, Selected$Species&lt;-(input$species1))
  observeEvent(input$species2, Selected$Species&lt;-(input$species2))

  observeEvent(Selected$Species, updateSelectInput(session, ""species1"", selected=Selected$Species))
  observeEvent(Selected$Species, updateSelectInput(session, ""species2"", selected=Selected$Species))

  output$table1 &lt;- renderTable({
    iris[iris$Species == Selected$Species &amp; iris$Sepal.Length &lt;= input$sepal.length,c(""Sepal.Length"",""Sepal.Width"")]
  })

  output$table2 &lt;- renderTable({
    iris[iris$Species == Selected$Species ,c(""Petal.Length"",""Petal.Width"")]
  })
}

# Run the application 
shinyApp(ui = ui, server = server)
</code></pre>

<p><strong>1)</strong> In the <code>ui</code> I changed the <code>inputId</code>s to ""species1"" and ""species2""<br>
<strong>2)</strong> I added the <code>session</code> parameter to your <code>server</code> function.<br>
<strong>3)</strong> I created a <code>reactiveValues</code> object called <code>Selected</code> with an element called <code>Species</code> to store the currently selected species, it starts out as <code>NULL</code>.<br>
<strong>4)</strong> The first two <code>observeEvents</code> will fire when the user chooses a species and stores that choice in <code>Selected$Species</code>. It does not matter which selector is used and will always have the value selected last.<br>
<strong>5)</strong> The next two <code>observeEvent</code>s update the two <code>selectInput</code>s to have the the selected choice be <code>Selected$Species</code> so that when you change the value in one tab it will change in the other automatically. You need to use the <code>session</code> argument here which is why I added it earlier.<br>
<strong>6)</strong> I changed the tables to filter based on <code>Selected$Species</code></p>

<p>There are a few advantages of this system. It would be easy to add more tabs with more <code>selecteInput</code>s and just add new <code>observeEvent</code> statements for them. If you have a bunch of these it might be worth you while to look into shiny modules. </p>

<p>Here, the tables just use <code>Selected$Species</code> but if you wanted to you could add more logic and they could sometimes update and sometimes not if that made sense for your app. That allows you to produce complicated behavior -for example if some values don't make sense for one of your displays you could catch that ahead of time and alert the user or display something else.</p>
"
16213029,More efficient strategy for which() or match(),2,1,3,"<p>I have a vector of positive and negative numbers</p>

<pre><code>vec&lt;-c(seq(-100,-1), rep(0,20), seq(1,100))
</code></pre>

<p>the vector is larger than the example, and takes on a random set of values. I have to repetitively find the number of negative numbers in the vector... I am finding this is quite inefficient.</p>

<p>Since I only need to find the number of negative numbers, and the vector is sorted, I only need to know the index of the first 0 or positive number (there may be no 0s in the actual random vectors).</p>

<p>Currently I am using this code to find the length</p>

<pre><code>length(which(vec&lt;0))
</code></pre>

<p>but this forces R to go through the entire vector, but since it is sorted, there is no need. </p>

<p>I could use</p>

<pre><code>match(0, vec)
</code></pre>

<p>but my vector does not always have 0s</p>

<p>So my question is, is there some kind of match() function that applies a condition instead of finding a specific value? Or is there a more efficient way to run my which() code?</p>

<p>Thank you</p>
","<p>The solutions offered so far all imply creating a <code>logical(length(vec))</code> and doing a full or partial scan on this. As you note, the vector is sorted. We can exploit this by doing a binary search. I started thinking I'd be super-clever and implement this in C for even greater speed, but had trouble with debugging the indexing of the algorithm (which is the tricky part!). So I wrote it in R:</p>

<pre><code>f3 &lt;- function(x) {
    imin &lt;- 1L
    imax &lt;- length(x)
    while (imax &gt;= imin) {
        imid &lt;- as.integer(imin + (imax - imin) / 2)
        if (x[imid] &gt;= 0)
            imax &lt;- imid - 1L
        else
            imin &lt;- imid + 1L
    }
    imax
}
</code></pre>

<p>For comparison with the other suggestions</p>

<pre><code>f0 &lt;- function(v) length(which(v &lt; 0))
f1 &lt;- function(v) sum(v &lt; 0)
f2 &lt;- function(v) which.min(v &lt; 0) - 1L
</code></pre>

<p>and for fun</p>

<pre><code>library(compiler)
f3.c &lt;- cmpfun(f3)
</code></pre>

<p>Leading to</p>

<pre><code>&gt; vec &lt;- c(seq(-100,-1,length.out=1e6), rep(0,20), seq(1,100,length.out=1e6))
&gt; identical(f0(vec), f1(vec))
[1] TRUE
&gt; identical(f0(vec), f2(vec))
[1] TRUE
&gt; identical(f0(vec), f3(vec))
[1] TRUE
&gt; identical(f0(vec), f3.c(vec))
[1] TRUE
&gt; microbenchmark(f0(vec), f1(vec), f2(vec), f3(vec), f3.c(vec))
Unit: microseconds
      expr       min        lq     median         uq       max neval
   f0(vec) 15274.275 15347.870 15406.1430 15605.8470 19890.903   100
   f1(vec) 15513.807 15575.229 15651.2970 17064.8830 18326.293   100
   f2(vec) 21473.814 21558.989 21679.3210 22733.1710 27435.889   100
   f3(vec)    51.715    56.050    75.4495    78.5295   100.730   100
 f3.c(vec)    11.612    17.147    28.5570    31.3160    49.781   100
</code></pre>

<p>Probably there are some tricky edge cases that I've got wrong! Moving to C, I did</p>

<pre><code>library(inline)
f4 &lt;- cfunction(c(x = ""numeric""), ""
    int imin = 0, imax = Rf_length(x) - 1, imid;
    while (imax &gt;= imin) {
        imid = imin + (imax - imin) / 2;
        if (REAL(x)[imid] &gt;= 0)
            imax = imid - 1;
        else
            imin = imid + 1;
    }
    return ScalarInteger(imax + 1);
"")
</code></pre>

<p>with</p>

<pre><code>&gt; identical(f3(vec), f4(vec))
[1] TRUE
&gt; microbenchmark(f3(vec), f3.c(vec), f4(vec))
Unit: nanoseconds
      expr   min      lq  median      uq   max neval
   f3(vec) 52096 53192.0 54918.5 55539.0 69491   100
 f3.c(vec) 10924 12233.5 12869.0 13410.0 20038   100
   f4(vec)   553   796.0   893.5  1004.5  2908   100
</code></pre>

<p><code>findInterval</code> came up when a similar question was asked on the <a href=""https://stat.ethz.ch/pipermail/r-help/2013-April/352460.html"">R-help</a> list. It is slow but safe, checking that <code>vec</code> is actually sorted and dealing with NA values. If one wants to live on the edge (arguably no worse that implementing f3 or f4) then</p>

<pre><code>f5.i &lt;- function(v)
    .Internal(findInterval(v, 0 - .Machine$double.neg.eps, FALSE, FALSE))
</code></pre>

<p>is nearly as fast as the C implementation, but likely more robust and vectorized (i.e., look up a vector of values in the second argument, for easy range-like calculations).</p>
"
40952582,R: conditional aggregate based on factor level and year,1,1,1,"<p>I have a dataset in R which I am trying to aggregate by column level and year which looks like this:</p>

<pre><code>    City  State   Year   Status      Year_repealed   PolicyNo
    Pitt   PA     2001   InForce                        6
    Phil.  PA     2001   Repealed        2004           9
    Pitt   PA     2002   InForce                        7
    Pitt   PA     2005   InForce                        2
</code></pre>

<p>What I would like to create is where for each Year, I aggregate the PolicyNo across states taking into account the date the policy was repealed. The results I would then get is:</p>

<pre><code>    Year    State PolicyNo
    2001     PA     15  
    2002     PA     22
    2003     PA     22
    2004     PA     12 
    2005     PA     14
</code></pre>

<p>I am not sure how to go about splitting and aggregating the data conditional on the repeal data and was wondering if there is a way to achieve this is R easily. </p>
","<p>It may help you to break this up into two distinct problems. </p>

<ol>
<li>Get a table that shows the change in PolicyNo in every city-state-year. </li>
<li>Summarize that table to show the PolicyNo in each state-year.</li>
</ol>

<p>To accomplish (1) we add the missing years with <code>NA</code> PolicyNo, and add repeals as negative <code>PolicyNo</code> observations.</p>

<pre><code>library(dplyr)

df = structure(list(City = c(""Pitt"", ""Phil."", ""Pitt"", ""Pitt""), State = c(""PA"", ""PA"", ""PA"", ""PA""), Year = c(2001L, 2001L, 2002L, 2005L), Status = c(""InForce"", ""Repealed"", ""InForce"", ""InForce""), Year_repealed = c(NA, 2004L, NA, NA), PolicyNo = c(6L, 9L, 7L, 2L)), .Names = c(""City"", ""State"", ""Year"", ""Status"", ""Year_repealed"", ""PolicyNo""), class = ""data.frame"", row.names = c(NA, -4L))

repeals = df %&gt;%
  filter(!is.na(Year_repealed)) %&gt;%
  mutate(Year = Year_repealed, PolicyNo = -1 * PolicyNo)
repeals
#    City State Year   Status Year_repealed PolicyNo
# 1 Phil.    PA 2004 Repealed          2004       -9

all_years = expand.grid(City = unique(df$City), State = unique(df$State),
                        Year = 2001:2005)

df = bind_rows(df, repeals, all_years)
#     City State Year   Status Year_repealed PolicyNo
# 1   Pitt    PA 2001  InForce            NA        6
# 2  Phil.    PA 2001 Repealed          2004        9
# 3   Pitt    PA 2002  InForce            NA        7
# 4   Pitt    PA 2005  InForce            NA        2
# 5  Phil.    PA 2004 Repealed          2004       -9
# 6   Pitt    PA 2001     &lt;NA&gt;            NA       NA
# 7  Phil.    PA 2001     &lt;NA&gt;            NA       NA
# 8   Pitt    PA 2002     &lt;NA&gt;            NA       NA
# 9  Phil.    PA 2002     &lt;NA&gt;            NA       NA
# 10  Pitt    PA 2003     &lt;NA&gt;            NA       NA
# 11 Phil.    PA 2003     &lt;NA&gt;            NA       NA
# 12  Pitt    PA 2004     &lt;NA&gt;            NA       NA
# 13 Phil.    PA 2004     &lt;NA&gt;            NA       NA
# 14  Pitt    PA 2005     &lt;NA&gt;            NA       NA
# 15 Phil.    PA 2005     &lt;NA&gt;            NA       NA
</code></pre>

<p>Now the table shows every city-state-year and incorporates repeals. This is a table we can summarize.</p>

<pre><code>df = df %&gt;%
  group_by(Year, State) %&gt;%
  summarize(annual_change = sum(PolicyNo, na.rm = TRUE))
df
# Source: local data frame [5 x 3]
# Groups: Year [?]
# 
#    Year State annual_change
#   &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;
# 1  2001    PA            15
# 2  2002    PA             7
# 3  2003    PA             0
# 4  2004    PA            -9
# 5  2005    PA             2
</code></pre>

<p>That gets us PolicyNo change in each state-year. A cumulative sum over the changes gets us levels.  </p>

<pre><code>df = df %&gt;%
  ungroup() %&gt;%
  mutate(PolicyNo = cumsum(annual_change))
df
# # A tibble: 5 × 4
#    Year State annual_change PolicyNo
#   &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;
# 1  2001    PA            15       15
# 2  2002    PA             7       22
# 3  2003    PA             0       22
# 4  2004    PA            -9       13
# 5  2005    PA             2       15
</code></pre>
"
33814656,Different pages in Shiny App,2,2,2,"<p>I want to have different pages in my shiny dashboard. First of all I created a login page to give authentication to user and admin. After that if admin login to system want to see some options that the user cannot access to them. 
 Question:  when I login as user or admin I can see the main ui.r page in the background how can I fix this problem to see only admin.R or user.R?
When the user login the dashboard shows and when the admin login the dashboard and widget show.
So I decided  to create 4 pages in R as following:
ui.R</p>

<pre><code>library(shiny)
library(shinydashboard)
shinyUI( 
  dashboardPage(
    dashboardHeader(title = ""Navigational Support System""),
    dashboardSidebar(),
    dashboardBody(
      box(
        uiOutput(""page"")
      )
    )
  )
)
</code></pre>

<p>server.R</p>

<pre><code>library(shiny)
library(shinydashboard)
source(""user.R"")
source(""admin.R"")
############################################################################################################
#Login USER and ADMIN TO the System
my_username &lt;- c(""test"",""admin"")
my_password &lt;- c(""test"",""123"")
get_role=function(user){
  if(user==""test"") {
    return(""TEST"")
  }else{
    return(""ADMIN"")
  }
}

get_ui=function(role){
  if(role==""TEST""){
    return(list_field_user)
  }else{
    return(list_field_admin)
  }
}


shinyServer(function(input, output,session) {

  USER &lt;- reactiveValues(Logged = FALSE,role=NULL)

  ui1 &lt;- function(){
    tagList(
      div(id = ""login"",
          wellPanel(textInput(""userName"", ""Username""),
                    passwordInput(""passwd"", ""Password""),
                    br(),actionButton(""Login"", ""Log in"")))
      #tags$style(type=""text/css"", '#login{ width:750px; float:left;}')

    )}

  ui2 &lt;- function(){tagList(tabPanel(""NSS"",get_ui(USER$role)))}

  observe({ 
    if (USER$Logged == FALSE) {
      if (!is.null(input$Login)) {
        if (input$Login &gt; 0) {
          Username &lt;- isolate(input$userName)
          Password &lt;- isolate(input$passwd)
          Id.username &lt;- which(my_username == Username)
          Id.password &lt;- which(my_password == Password)
          if (length(Id.username) &gt; 0 &amp; length(Id.password) &gt; 0) {
            if (Id.username == Id.password) {
              USER$Logged &lt;- TRUE
              USER$role=get_role(Username)

            }
          } 
        }
      }
    }
  })
  observe({
    if (USER$Logged == FALSE) {

      output$page &lt;- renderUI({
        div(class=""outer"",do.call(bootstrapPage,c(""Please Login"",ui1())))
      })
    }
    if (USER$Logged == TRUE)    {
      output$page &lt;- renderUI({
        div(class=""outer"",do.call(navbarPage,c(inverse=TRUE,title = ""Welcome Admin!"",ui2())))
      })
      #print(ui)
    }
  })
  ##################################################################################################

})
</code></pre>

<p>admin.r</p>

<pre><code>list_field_admin =

  shinyUI( 
  dashboardPage(
    dashboardHeader(title = ""Decison Support System""),
    dashboardSidebar( sidebarMenu(
      menuItem(""Dashboard"", tabName = ""dashboard"", icon = icon(""dashboard"")),
      menuItem(""Widgets"", tabName = ""widgets"", icon = icon(""th""))
    )
    ),
    dashboardBody(
  )

))
</code></pre>

<p>user.r</p>

<pre><code>list_field_user =  shinyUI( 
  dashboardPage(
    dashboardHeader(title = ""Decison Support System""),
    dashboardSidebar( sidebarMenu(
      menuItem(""Dashboard"", tabName = ""dashboard"", icon = icon(""dashboard""))

    )
    ),
    dashboardBody(
    )

  ))
</code></pre>
","<p>If i undertand you right you can create different lists in you additional files </p>

<p>1) admin.r </p>

<pre><code>admin_title=""Decison Support System""
admin_side=list(sidebarMenu(
  menuItem(""Dashboard"", tabName = ""dashboard"", icon = icon(""dashboard"")),
  menuItem(""Widgets"", tabName = ""widgets"", icon = icon(""th""))
))
admin_main=list(

  tabItems(
    tabItem(tabName = ""dashboard"", list(h1(""1234""),h2(""234""))),
    tabItem(tabName = ""widgets"", list(fluidRow(column(6,numericInput(""inputtest"", ""test"", value = 0),column(6,actionButton(inputId =""test1"",label =""go"")))))
  )
  ))
</code></pre>

<p>2) user.r</p>

<pre><code>test_title=""Decison Support System""
test_side=list(sidebarMenu(
  menuItem(""Dashboard"", tabName = ""dashboard"", icon = icon(""dashboard""))))

test_main=list(

  tabItems(
    tabItem(tabName = ""dashboard"", list(h1(""user""),h2(""user"")))
  ))
</code></pre>

<p>Then change a bit you UI and server</p>

<p>UI :</p>

<pre><code>library(shiny)
library(shinydashboard)
shinyUI( 
  dashboardPage(
    dashboardHeader( title=textOutput(""title"")),
    dashboardSidebar(uiOutput(""side"")),
    dashboardBody(
          uiOutput(""page"")

    )
  )

)
</code></pre>

<p>server :</p>

<pre><code>library(shiny)
library(shinydashboard)
source(""user.R"")
source(""admin.R"")

my_username &lt;- c(""test"",""admin"")
my_password &lt;- c(""test"",""123"")
get_role=function(user){

  if(user==""test"") {

    return(""TEST"")
  }else{

    return(""ADMIN"")
  }
}

get_ui=function(role){
  itog=list()
  if(role==""TEST""){
    itog$title=test_title
    itog$main=test_main
    itog$side=test_side
    return(itog)
  }else{
    itog$title=admin_title
    itog$main=admin_main
    itog$side=admin_side
    return(itog)
  }
}


shinyServer(function(input, output,session) {

  USER &lt;- reactiveValues(Logged = FALSE,role=NULL)

  ui1 &lt;- function(){
    tagList(
      div(id = ""login"",
          wellPanel(textInput(""userName"", ""Username""),
                    passwordInput(""passwd"", ""Password""),
                    br(),actionButton(""Login"", ""Log in"")))
      ,tags$style(type=""text/css"", ""#login {font-size:10px;   text-align: left;position:absolute;top: 40%;left: 50%;margin-top: -10px;margin-left: -150px;}"")
    )}


  observe({ 
    if (USER$Logged == FALSE) {
      if (!is.null(input$Login)) {
        if (input$Login &gt; 0) {
          Username &lt;- isolate(input$userName)
          Password &lt;- isolate(input$passwd)
          Id.username &lt;- which(my_username == Username)
          Id.password &lt;- which(my_password == Password)
          if (length(Id.username) &gt; 0 &amp; length(Id.password) &gt; 0) {
            if (Id.username == Id.password) {
              USER$Logged &lt;- TRUE
              USER$role=get_role(Username)

          }
        } 
      }
    }
    }
  })
  observe({
    if (USER$Logged == FALSE) {

      output$page &lt;- renderUI({
        box(
        div(class=""outer"",do.call(bootstrapPage,c("""",ui1()))))
      })
    }
    if (USER$Logged == TRUE)    {
      itog=get_ui(USER$role)
      output$title&lt;- renderText({
        itog$title
      })
      output$side &lt;- renderUI({
        itog$side
      })
      output$page &lt;- renderUI({
        itog$main
      })
      }
  })
})
</code></pre>
"
24728445,R: How to do this matrix operation without loops or more efficient?,2,3,3,"<p>I'm trying to make this operation matrices, multiplying the first column with 2, 3 and 4, the first hold value, and then multiply the second column with 3 and 4, keep the value of the third and multiply the third column with 4. I want to do this without using a ""for"" loop, wanted to use functions like sapply or mapply. Does anyone have an idea how to do it?</p>

<p>Example with one line:</p>

<pre><code>a[1,1]*(a[1,2], a[1,3], a[1,4]) = 2 4 4 4
a[1,1] a[1,2]*(a[1,3], a[1,4]) = 2 4 16 16 #keep a[1,1] a[1,2] 
a[1,1] a[1,2] a[1,3] a[1,3]*(a[1,4]) = 2 4 16 256 # #keep a[1,1] a[1,2] a[1,3] 
</code></pre>

<p>Input:</p>

<pre><code>&gt; a&lt;- matrix(2,4,4) # or any else matrix like a&lt;- matrix(c(1,8,10,1,4,1),3,3)
&gt; a
     [,1] [,2] [,3] [,4]
[1,]    2    2    2    2
[2,]    2    2    2    2
[3,]    2    2    2    2
[4,]    2    2    2    2
</code></pre>

<p>Output:</p>

<pre><code>&gt; a
     [,1] [,2] [,3] [,4]
[1,]    2    4    16    256
[2,]    2    4    16    256
[3,]    2    4    16    256
[4,]    2    4    16    256
</code></pre>

<p><strong>EDIT: LOOP VERSION</strong></p>

<pre><code>a&lt;- matrix(2,4,4); 
ai&lt;-a[,1,drop=F]; 
b&lt;- matrix(numeric(0),nrow(a),ncol(a)-1); 

i&lt;- 1; 

for ( i in 1:(ncol(a)-1)){ 

  a&lt;- a[,1]*a[,-1,drop=F]; 
  b[,i]&lt;- a[,1]; 

}

b&lt;- cbind(ai[,1],b); 

b
</code></pre>

<p><img src=""https://i.stack.imgur.com/qttdS.png"" alt=""enter image description here""></p>
","<p>If I understand correctly, what you are trying to do is, starting with a matrix A with N columns, perform the following steps:</p>

<p>Step 1. Multiply columns 2 through N of A by column 1 of A. Call the resulting matrix A1.</p>

<p>Step 2. Multiply columns 3 through N of A1 by column 2 of A1. Call the resulting matrix A2.</p>

<p>...</p>

<p>Step (N-1). Multiply column N of A(N-2) by column (N-1) of A(N-2). This is the desired result.</p>

<p>If this is indeed what you are trying to do, you need to either write a double <code>for</code> loop (which you want to avoid, as you say) or come up with some iterative method of performing the above steps.</p>

<p>The double <code>for</code> way would look something like this </p>

<pre><code>DoubleFor &lt;- function(m) {
    res &lt;- m
    for(i in 1:(ncol(res)-1)) {
        for(j in (i+1):ncol(res)) {
            res[, j] &lt;- res[, i] * res[, j]
        }
    }
    res
}
</code></pre>

<p>Using R's vectorized operations, you can avoid the inner <code>for</code> loop </p>

<pre><code>SingleFor &lt;- function(m) {
    res &lt;- m
    for(i in 1:(ncol(res)-1)) 
        res[, (i+1):ncol(res)] &lt;- res[, i] * res[, (i+1):ncol(res)]
    res
}
</code></pre>

<p>When it comes to iterating a procedure, you may want to define a recursive function, or use <code>Reduce</code>. The recursive function would be something like</p>

<pre><code>RecursiveFun &lt;- function(m, i = 1) {
    if (i == ncol(m)) return(m)
    n &lt;- ncol(m)
    m[, (i+1):n] &lt;- m[, (i+1):n] * m[, i]
    Recall(m, i + 1) # Thanks to @batiste for suggesting using Recall()!
}
</code></pre>

<p>while <code>Reduce</code> would use a similar function without the recursion (which is provided by <code>Reduce</code>)</p>

<pre><code>ReduceFun &lt;- function(m) { 
    Reduce(function(i, m) {
            n &lt;- ncol(m)
            m[, (i+1):n] &lt;- m[, (i+1):n] * m[, i]
            m
        }, c((ncol(m)-1):1, list(m)), right = T)
}
</code></pre>

<p>These will all produce the same result, e.g. testing on your matrix     </p>

<pre><code>a &lt;- matrix(c(1, 8, 10, 1, 4, 1), 3, 3)
DoubleFor(a)
#      [,1] [,2] [,3]
# [1,]    1    1    1
# [2,]    8   32 2048
# [3,]   10   10 1000
all(DoubleFor(a) == SingleFor(a) &amp; SingleFor(a) == RecursiveFun(a) &amp; 
    RecursiveFun(a) == ReduceFun(a))
# [1] TRUE
</code></pre>

<p>Just out of curiosity, I did a quick speed comparison, but I don't think any one of the above will be significantly faster than the others for your size of matrices, so I would just go with the one you think is more readable.</p>

<pre><code>a &lt;- matrix(rnorm(1e6),  ncol = 1e3)
system.time(DoubleFor(a))
#    user  system elapsed 
#  22.158   0.012  22.220 
system.time(SingleFor(a))
#    user  system elapsed 
#  27.349   0.004  27.415 
system.time(RecursiveFun(a))
#    user  system elapsed 
#  25.150   1.336  26.534 
system.time(ReduceFun(a))
#    user  system elapsed 
#  26.574   0.004  26.626 
</code></pre>
"
25114546,Bar-charts plot order depending on values (ggplot2),1,3,1,"<p>Considering two series of values <code>rr1</code> and <code>rr2</code>:</p>

<pre><code>rr1 &lt;- c(-1, -0.6, 1.7, 6.3, 9, 10, 8.8, 6.2, 4.5, 4, 3.4)
rr2 &lt;- c(-2.3, -1.8, -4, -5.7, -7.2, -5.6, -2.3, 0.2, -0.3, -1.4, -1.3)
</code></pre>

<p>Figure 1:
<img src=""https://i.stack.imgur.com/Mf8Pc.png"" alt=""enter image description here""></p>

<p><code>rr1</code> is mainly positive (except for x=-5 and x=-4) while <code>rr2</code> is negative (except for x=2).</p>

<p>Using <code>ggplot2</code> to group <code>rr1</code> and <code>rr2</code> in the same bar-chart (Figure 2):</p>

<pre><code>dat &lt;- data.frame(
  group = rep(c(""rr1"", ""rr2""), each=11),
  x = rep(-5:5, 2),
  y = c(rr1, rr2)
)
ggplot(dat, aes(x=x, y=y, fill=group)) + 
  geom_bar(stat=""identity"", position=""identity"", width=0.25) +
  scale_x_continuous(breaks=-5:5) +
  scale_y_continuous(breaks=seq(-10,10,2.5), limits=c(-10,10)) +
  theme(axis.text.x=element_text(size=14), axis.text.y=element_text(size=14), legend.text=element_text(size=14))
</code></pre>

<p>Figure 2:
<img src=""https://i.stack.imgur.com/pXcWg.png"" alt=""enter image description here""></p>

<p>Is there a way for <code>rr1</code> to be plotted over <code>rr2</code> when x is equal to -5 and -4?</p>
","<p>The problem is that <code>rr2</code> values prints over <code>rr1</code>, so there is manual solution - first print all positive rr1 then all negative rr2, negative rr1 and positive rr2.</p>

<pre><code>rr1 &lt;- c(-1, -0.6, 1.7, 6.3, 9, 10, 8.8, 6.2, 4.5, 4, 3.4)
rr2 &lt;- c(-2.3, -1.8, -4, -5.7, -7.2, -5.6, -2.3, 0.2, -0.3, -1.4, -1.3)

dat &lt;- data.frame(
  group = rep(c(""rr1"", ""rr2""), each=11),
  x = rep(-5:5, 2),
  y = c(rr1, rr2)
)

positive_rr1 &lt;- subset(dat,group==""rr1"" &amp; y &gt;=0)
negative_rr1 &lt;- subset(dat,group==""rr1"" &amp; y &lt;0)
positive_rr2 &lt;- subset(dat,group==""rr2"" &amp; y &gt;=0)
negative_rr2 &lt;- subset(dat,group==""rr2"" &amp; y &lt;0)

ggplot(dat, aes(x=x, y=y, fill=group)) + 
  geom_bar(data=positive_rr1,stat=""identity"", position=""identity"", width=0.25) +
  geom_bar(data=negative_rr2,stat=""identity"", position=""identity"", width=0.25) +
  geom_bar(data=negative_rr1,stat=""identity"", position=""identity"", width=0.25) +
  geom_bar(data=positive_rr2,stat=""identity"", position=""identity"", width=0.25) +
  scale_x_continuous(breaks=-5:5) +
  scale_y_continuous(breaks=seq(-10,10,2.5), limits=c(-10,10)) +
  theme(axis.text.x=element_text(size=14), axis.text.y=element_text(size=14), legend.text=element_text(size=14))
</code></pre>

<p><img src=""https://i.stack.imgur.com/rbJQu.png"" alt=""enter image description here""></p>

<p>The chart above is not stacked chart, because rr1 is printed over rr2, to get stacked chart we can modify <code>dat</code></p>

<pre><code>for(i in -5:5){
  if(dat[dat$x==i &amp; dat$group==""rr1"",""y""] &lt; 0 &amp; dat[dat$x==i &amp; dat$group==""rr2"",""y""] &lt; 0){
    dat[dat$x==i &amp; dat$group==""rr2"",""y""] &lt;- dat[dat$x==i &amp; dat$group==""rr2"",""y""]+
      dat[dat$x==i &amp; dat$group==""rr1"",""y""]
  }
  if(dat[dat$x==i &amp; dat$group==""rr1"",""y""] &gt; 0 &amp; dat[dat$x==i &amp; dat$group==""rr2"",""y""] &gt; 0){
    dat[dat$x==i &amp; dat$group==""rr1"",""y""] &lt;- dat[dat$x==i &amp; dat$group==""rr2"",""y""]+
      dat[dat$x==i &amp; dat$group==""rr1"",""y""]
  }
}

positive_rr1 &lt;- subset(dat,group==""rr1"" &amp; y &gt;=0)
negative_rr1 &lt;- subset(dat,group==""rr1"" &amp; y &lt;0)
positive_rr2 &lt;- subset(dat,group==""rr2"" &amp; y &gt;=0)
negative_rr2 &lt;- subset(dat,group==""rr2"" &amp; y &lt;0)

ggplot(dat, aes(x=x, y=y, fill=group)) + 
  geom_bar(data=positive_rr1,stat=""identity"", position=""identity"", width=0.25) +
  geom_bar(data=negative_rr2,stat=""identity"", position=""identity"", width=0.25) +
  geom_bar(data=negative_rr1,stat=""identity"", position=""identity"", width=0.25) +
  geom_bar(data=positive_rr2,stat=""identity"", position=""identity"", width=0.25) +
  scale_x_continuous(breaks=-5:5) +
  scale_y_continuous(breaks=seq(-10,10,2.5), limits=c(-10,10)) +
  theme(axis.text.x=element_text(size=14), axis.text.y=element_text(size=14), legend.text=element_text(size=14))
</code></pre>

<p><img src=""https://i.stack.imgur.com/Op4X5.png"" alt=""enter image description here""></p>
"
26856665,Can't figure out how to conditionally lag a variable,1,1,1,"<p>I am having problems creating a lagged variable that resets itself on two different conditions: 1) whether there is an onset of war (1 or a 0); 2) whether a country switches.</p>

<p>Here is example data (EDITED):</p>

<pre><code>   year wartype war_onset country neighborWar
1  1970    &lt;NA&gt;         0      US           1
2  1971    &lt;NA&gt;         0      US           1
3  1972    &lt;NA&gt;         0      US           1
4  1973    &lt;NA&gt;         0      US           0
5  1974   civil         1      US           0
6  1975    &lt;NA&gt;         0      US           1
7  1976    &lt;NA&gt;         0      US           0
8  1970    &lt;NA&gt;         0     Rus           1
9  1971    &lt;NA&gt;         0     Rus           1
10 1972    &lt;NA&gt;         0     Rus           0
11 1973   civil         1     Rus           0
12 1974    &lt;NA&gt;         0     Rus           1
13 1975    &lt;NA&gt;         0     Rus           1
14 1976    &lt;NA&gt;         0     Rus           1
15 1977    &lt;NA&gt;         0     Rus           0
</code></pre>

<p>I want to create a lagged variable on neighbor war with the above conditioning so that the data looks like this (EDITED):</p>

<pre><code>    year wartype war_onset country neighborWar ideal_lag_behavior
 1  1970    &lt;NA&gt;         0      US           1                 NA
 2  1971    &lt;NA&gt;         0      US           1                 NA
 3  1972    &lt;NA&gt;         0      US           1                 NA
 4  1973    &lt;NA&gt;         0      US           0                  3
 5  1974   civil         1      US           0                  2
 6  1975    &lt;NA&gt;         0      US           1                 NA
 7  1976    &lt;NA&gt;         0      US           0                 NA
 8  1970    &lt;NA&gt;         0     Rus           1                 NA
 9  1971    &lt;NA&gt;         0     Rus           1                 NA
 10 1972    &lt;NA&gt;         0     Rus           0                 NA
 11 1973   civil         1     Rus           0                  2
 12 1974    &lt;NA&gt;         0     Rus           1                 NA
 13 1975    &lt;NA&gt;         0     Rus           1                 NA
 14 1976    &lt;NA&gt;         0     Rus           1                 NA
 15 1977    &lt;NA&gt;         0     Rus           0                  3
</code></pre>

<p>Rather than this (EDITED):</p>

<pre><code>  &gt; df
   year wartype war_onset country neighborWar lagged_variable
1  1970    &lt;NA&gt;         0      US           1              NA
2  1971    &lt;NA&gt;         0      US           1              NA
3  1972    &lt;NA&gt;         0      US           1              NA
4  1973    &lt;NA&gt;         0      US           0               3
5  1974   civil         1      US           0               2
6  1975    &lt;NA&gt;         0      US           1               2
7  1976    &lt;NA&gt;         0      US           0               1
8  1970    &lt;NA&gt;         0     Rus           1               2
9  1971    &lt;NA&gt;         0     Rus           1               3
10 1972    &lt;NA&gt;         0     Rus           0               2
11 1973   civil         1     Rus           0               2
12 1974    &lt;NA&gt;         0     Rus           1               2
13 1975    &lt;NA&gt;         0     Rus           1               2 
14 1976    &lt;NA&gt;         0     Rus           1               3
15 1977    &lt;NA&gt;         0     Rus           0               3
</code></pre>

<p>Data generating code:</p>

<pre><code>b &lt;- c(1970, 1971, 1972, 1973, 1974, 1975, 1976, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977)
c &lt;- c(NA, NA, NA, NA, ""civil"", NA, NA, NA, NA, NA, ""civil"", NA, NA, NA, NA)
d &lt;- c(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0)
s &lt;- c(""US"", ""US"", ""US"", ""US"", ""US"", ""US"", ""US"", ""Rus"", ""Rus"", ""Rus"", ""Rus"", ""Rus"", ""Rus"", ""Rus"", ""Rus"")
v &lt;- c(1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0)
v3 &lt;- v + lag(v,1) + lag(v,2) + lag(v,3)
df &lt;- data.frame(b,c,d,s,v,v3)
colnames(df) &lt;-c(""year"", ""wartype"", ""war_onset"", ""country"", 
             ""neighborWar"", ""lagged variable"")
</code></pre>

<p>The lagged variable decomposed:</p>

<pre><code>v &lt;- c(1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0)
lag1 &lt;- lag(v,1)
lag2 &lt;- lag(v,2)
lag3 &lt;- lag(v,3)
lags_plus_v &lt;- v + lag(v,1) + lag(v,2) + lag(v,3)
df &lt;- data.frame(v, lag1, lag2, lag3, lags_plus_v)
df

  v lag1 lag2 lag3 lags_plus_v
1  1   NA   NA   NA          NA
2  1    1   NA   NA          NA
3  1    1    1   NA          NA
4  0    1    1    1           3
5  0    0    1    1           2
6  1    0    0    1           2
7  0    1    0    0           1
8  1    0    1    0           2
9  1    1    0    1           3
10 0    1    1    0           2
11 0    0    1    1           2
12 1    0    0    1           2
13 1    1    0    0           2
14 1    1    1    0           3
15 0    1    1    1           3
</code></pre>

<p>Any ideas. I haven't.</p>

<p>I have tried the plm package and couldn't figure out any solutions. I have been slowly hacking away at Hadley Wickham's book Advanced R for ideas on generating a function but haven't put 2 and 2 together yet. </p>
","<p>I think this produces what you are looking for, but it was a bit tricky to figure this out. Please try to be more explicit about the desired behavior (and the reasoning for it) in future questions.</p>

<p>The main point was to create a (lagged) grouping variable from <code>war_onset</code>. </p>

<p>I group the data by <code>country</code> and by the newly generated variable <code>x</code>. <code>x</code> equals the first lag of the cumulative sum of <code>war_onset</code>. I'll add a version of the sample data including the <code>x</code> variable to show what it looks like. Then, the rest is easily computed based on the description in the sample - it is the sum of <code>neighborWar</code> and the first 3 lags of the same variable. The values are <code>NA</code> whenever any of the lags of <code>neighborWar</code> is also NA.</p>

<pre><code>require(dplyr)
df %&gt;%
  group_by(country, x = lag(cumsum(war_onset), 1, default = 0)) %&gt;%
  mutate(lagged_variable = neighborWar + lag(neighborWar,1) + lag(neighborWar,2) + lag(neighborWar,3)) %&gt;%
  ungroup() %&gt;% select(-x)

#Source: local data frame [15 x 6]
#
#   year wartype war_onset country neighborWar lagged_variable
#1  1970    &lt;NA&gt;         0      US           1              NA
#2  1971    &lt;NA&gt;         0      US           1              NA
#3  1972    &lt;NA&gt;         0      US           1              NA
#4  1973    &lt;NA&gt;         0      US           0               3
#5  1974   civil         1      US           0               2
#6  1975    &lt;NA&gt;         0      US           1              NA
#7  1976    &lt;NA&gt;         0      US           0              NA
#8  1970    &lt;NA&gt;         0     Rus           1              NA
#9  1971    &lt;NA&gt;         0     Rus           1              NA
#10 1972    &lt;NA&gt;         0     Rus           0              NA
#11 1973   civil         1     Rus           0               2
#12 1974    &lt;NA&gt;         0     Rus           1              NA
#13 1975    &lt;NA&gt;         0     Rus           1              NA
#14 1976    &lt;NA&gt;         0     Rus           1              NA
#15 1977    &lt;NA&gt;         0     Rus           0               3
</code></pre>

<hr>

<p>For demonstration of the grouping variable <code>x</code> and the lags of neighborWar, here's an example showing all the columns:</p>

<pre><code>df %&gt;%
  group_by(country, x = lag(cumsum(war_onset), 1, default = 0)) %&gt;%
  mutate(lag1 =  lag(neighborWar,1),
         lag2 =  lag(neighborWar,2),
         lag3 =  lag(neighborWar,3),
         lagged_variable = neighborWar + lag1 + lag2 + lag3)

#Source: local data frame [15 x 10]
#Groups: country, x
#
#   year wartype war_onset country neighborWar x lag1 lag2 lag3 lagged_variable
#1  1970    &lt;NA&gt;         0      US           1 0   NA   NA   NA              NA
#2  1971    &lt;NA&gt;         0      US           1 0    1   NA   NA              NA
#3  1972    &lt;NA&gt;         0      US           1 0    1    1   NA              NA
#4  1973    &lt;NA&gt;         0      US           0 0    1    1    1               3
#5  1974   civil         1      US           0 0    0    1    1               2
#6  1975    &lt;NA&gt;         0      US           1 1   NA   NA   NA              NA
#7  1976    &lt;NA&gt;         0      US           0 1    1   NA   NA              NA
#8  1970    &lt;NA&gt;         0     Rus           1 1   NA   NA   NA              NA
#9  1971    &lt;NA&gt;         0     Rus           1 1    1   NA   NA              NA
#10 1972    &lt;NA&gt;         0     Rus           0 1    1    1   NA              NA
#11 1973   civil         1     Rus           0 1    0    1    1               2
#12 1974    &lt;NA&gt;         0     Rus           1 2   NA   NA   NA              NA
#13 1975    &lt;NA&gt;         0     Rus           1 2    1   NA   NA              NA
#14 1976    &lt;NA&gt;         0     Rus           1 2    1    1   NA              NA
#15 1977    &lt;NA&gt;         0     Rus           0 2    1    1    1               3
</code></pre>
"
19172904,"Add a popup with error, warning to shiny",2,2,2,"<p>Is there any way to add a <strong>popup</strong> (a closable window) with a warning or other message in <strong>Shiny</strong> - the R package I use to build my web application?</p>

<p>I have been searching for some time but without any results.</p>
","<p>Although I don't think there is anything natively available in <code>shiny</code>, you can try adding <code>jQueryUI</code> to your application and using the Dialog widget. See <a href=""http://jqueryui.com/dialog/"" rel=""noreferrer"">http://jqueryui.com/dialog/</a>.</p>

<p>(Un?)fortunately, you'll be forced to write some JavaScript to make it work.</p>

<hr>

<p>Per @GSee's suggestion, here's a very minimal example of what it takes to make it work.</p>

<p>You'll need to download <a href=""http://jqueryui.com/"" rel=""noreferrer"">jQueryUI</a> and set up a shiny project with a structure like so:</p>

<pre><code>.
├── server.R
├── ui.R
└── www
    ├── css
    │   └── jquery-ui.css
    ├── images
    │   ├── animated-overlay.gif
    │   ├── ui-bg_flat_0_aaaaaa_40x100.png
    │   ├── ui-bg_flat_75_ffffff_40x100.png
    │   ├── ui-bg_glass_55_fbf9ee_1x400.png
    │   ├── ui-bg_glass_65_ffffff_1x400.png
    │   ├── ui-bg_glass_75_dadada_1x400.png
    │   ├── ui-bg_glass_75_e6e6e6_1x400.png
    │   ├── ui-bg_glass_95_fef1ec_1x400.png
    │   ├── ui-bg_highlight-soft_75_cccccc_1x100.png
    │   ├── ui-icons_222222_256x240.png
    │   ├── ui-icons_2e83ff_256x240.png
    │   ├── ui-icons_454545_256x240.png
    │   ├── ui-icons_888888_256x240.png
    │   └── ui-icons_cd0a0a_256x240.png
    └── js
        └── jquery-ui.js
</code></pre>

<p>(all of the image icons come part of jQueryUI)</p>

<p>Next, add a file called <code>scripts.js</code> (or whatever you like) to the <code>www/js</code> folder, containing the following</p>

<pre><code>$( function() {
  $(""#dialog"").dialog();
})
</code></pre>

<p>This calls the <code>jQueryUI</code> <code>dialog</code> initializer on the element with id <code>dialog</code>.</p>

<p>Next, have a <code>server.R</code> and <code>ui.R</code> as follows:</p>

<pre><code>server.R
--------
library(shiny)
shinyServer( function(input, output, session) {

  ## a very unsafe, basic access to the R console
  output$dialog &lt;- renderPrint({

    code &lt;- input$console
    output &lt;- eval( parse( text=code ) )
    return(output)

  })

})
</code></pre>

<p>and</p>

<pre><code>ui.R
----

library(shiny)

shinyUI(bootstrapPage(
  includeCSS(""www/css/jquery-ui.css""),  

  includeScript(""www/js/jquery-ui.js""),
  includeScript(""www/js/scripts.js""),

  textInput(""console"", ""Enter an R Command""),
  uiOutput(""dialog"")

))
</code></pre>

<p>Now, if you do <code>runApp()</code>, you should see the results of evaluation of any code you write into the text input <code>console</code> appearing in the <code>dialog</code> box.</p>

<p>Now, the question is, how can we minimize it, or only show it when, say, error code is produced? That I'll have to leave for you, because I think it'll be tricky. Some options:</p>

<ol>
<li><p>Figure out how to get our R code to send, or trigger, some JavaScript to show or hide the element. An example (not mine) using this to temporarily disable a button is <a href=""https://gist.github.com/xiaodaigh/6810928"" rel=""noreferrer"">here</a>.</p></li>
<li><p>Attach a (JavaScript) observer or trigger to the output produced, and if you see an error (or output otherwise conforming in some way), show the box; otherwise hide it.</p></li>
<li><p>Generate an actual Shiny input/output pair to handle behavior as desired. (Brief tutorial at <a href=""http://rstudio.github.io/shiny/tutorial/#building-inputs"" rel=""noreferrer"">http://rstudio.github.io/shiny/tutorial/#building-inputs</a>)</p></li>
</ol>

<p>If you want to get a bit more out of your jQueryUI dialog, you can also try the extension jQuery-dialogextend <a href=""https://github.com/ROMB/jquery-dialogextend"" rel=""noreferrer"">here</a>.</p>

<p>And, disclaimer: the console here is only for demonstrative purposes; please don't put any shiny apps that run unsanitized code from the user into the wild!</p>
"
43778821,Issue connecting RStudio (but not R) to Hive with Kerberos,2,2,2,"<p>I'me trying to connect RStudio to Hive that has Kerberos authentication. If I run the below in an R script called from the command line, it works. </p>

<pre><code>library(""DBI"")
library(""rJava"")
library(""RJDBC"")

cp = c(""/u01/cloudera/parcels/CDH/lib/hive/lib/hive-jdbc.jar""
, ""/u01/cloudera/parcels/CDH/lib/hadoop/hadoop-common.jar""
, ""/u01/cloudera/parcels/CDH/lib/hive/lib/libthrift-0.9.2.jar""
, ""/u01/cloudera/parcels/CDH/lib/hive/lib/hive-service.jar""
, ""/u01/cloudera/parcels/CDH/lib/hive/lib/httpclient-4.2.5.jar""
, ""/u01/cloudera/parcels/CDH/lib/hive/lib/httpcore-4.2.5.jar""
, ""/u01/cloudera/parcels/CDH/lib/hive/lib/hive-jdbc-standalone.jar"")
.jinit(classpath=cp)

drv &lt;- JDBC(""org.apache.hive.jdbc.HiveDriver"" , ""hive-jdbc.jar"" )

conn &lt;- dbConnect(drv , ""jdbc:hive2://XXXX:10000/default;principal=hive/XXXX@XXXXX"";auth-kerberos)
</code></pre>

<p>If I run the exact same script in RStudio, I get an error:</p>

<pre><code>javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
</code></pre>

<p>If I run system('klist') in RStudio, it shows I have a valid ticket. It seems RStudio isn't able to identify the ticket but R is. Any ideas?</p>
","<p>Some boring stuff first, to put things into context, then the solution.</p>

<ul>
<li>Kerberos: it's complicated by nature <em>(think cryptography <sup>network</sup>)</em>, even without considering that Microsoft has its own implementation and extensions</li>
<li>Java and Kerberos: it's even more complicated <em>(only partial support, subtle changes in Java versions, etc.)</em></li>
<li>Hadoop and Java and Kerberos: it's complicated and ugly <em>(read the GitBook ""Hadoop and Kerberos, the Madness beyond the Gate"" if you really want to lose your sanity)</em> and it's even worse on Windows cf. lack of an official build for the required Hadoop ""native libs"" </li>
<li>Hive and JDBC and Kerberos: the good news is that you don't need the Hadoop ""ugly"" part <strong>unless</strong> you are using the Apache JDBC driver on Windows <em>(hint: ditch it and opt for the Cloudera JDBC driver!)</em>; the bad news is that you may need raw JAAS configuration and specific Java system properties</li>
<li>R and Java/JDBC: it works quite well, except that sometimes you want to pass specific Java system properties to the JVM -- either at launch time or at run time -- but <code>.jinit</code> does not support that AFAIK, you must resort to a workaround</li>
</ul>

<p><hr>
There is one <strong>Java system property</strong> that must be set for Kerberos auth to work in JDBC, and it's not always set by default.<br>
But you can't set that Java property from R directly; you have to set an <strong>environment variable</strong> <em>(either before starting R, or from R code but before <code>.jinit</code>)</em></p>

<p><strong><em>Option 1:</em></strong> from a Linux shell script, before starting R...<br></p>

<pre><code>export JAVA_TOOL_OPTIONS=""-Djavax.security.auth.useSubjectCredsOnly‌​=false""
</code></pre>

<p><strong><em>Option 2:</em></strong> from your R code...<br></p>

<pre><code>Sys.setenv(JAVA_TOOL_OPTIONS=""-Djavax.security.auth.useSubjectCredsOnly‌​=false"")
.jinit(...)
</code></pre>

<p><hr>Now, that may not be sufficient in all cases. Maybe you need to use a specific Kerberos config because your Hadoop cluster uses its own KDC. Maybe you don't want to use the default Kerberos ticket, but instead authenticate as a service account, using a password stored in a keytab file.<br>
And maybe you need some debugging information because, well, shit happens <em>(and security libraries are quite secretive by default, not to make things too easy for hackers, I suppose...)</em></p>

<p>Please refer to <a href=""https://stackoverflow.com/questions/42477466/error-when-connect-to-impala-with-jdbc-under-kerberos-authrication/42506620#42506620"">that post</a> for more information about <strong>advanced Java configuration for Hive/Impala JDBC with Kerberos</strong>.</p>

<p>And be careful when setting the environment variable: simulate a Java command-line i.e. <code>-Dsome.key=value -Dsome.other.key=blahblah</code>; in shell script, use quotes (because of the separating space); in R code, use a single string, no array.</p>
"
38593287,Understanding influence of random start weights on neural network performance,2,2,2,"<p>Using R and the package <code>neuralnet</code>, I try to model data that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/m4Seq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m4Seq.png"" alt=""training data""></a></p>

<p>These are temperature readings in 10 min intervals over several days (above is a 2 day cutout). Using the code below, I fit a neural network to the data. There are probably simpler ways to model this exact data, but in the future the data might look quite different. Using a single hidden layer with 2 neurons gives me satisfactory results:</p>

<p><a href=""https://i.stack.imgur.com/D67DZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D67DZ.png"" alt=""good neural network performance""></a></p>

<p>This also works <em>most of the time</em> with more layers and neurons. However, with one hidden layer with one neuron and <em>occasionally</em> with two layers (in my case 3 and 2 neurons respectively), I get rather poor results, always in the same shape:</p>

<p><a href=""https://i.stack.imgur.com/FHCA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FHCA7.png"" alt=""poor neural network performance""></a></p>

<p>The only thing random is the initialization of start weights, so I assume it's related to that. However, I must admit that I have not fully grasped the theory of neural networks yet. What I would like to know is, whether the poor results are due to a local minimum ('neuralnet' uses resilient backpropagation with weight backtracking by default) and I'm simply out of luck, or if I can avoid such a scenario. I am under the impression that there is an optimal number of hidden nodes for fitting e.g. polynomials of degree 2, 5, 10. If not, what's my best course of action? A larger learning rate? Smaller error threshold? Thanks in advance.</p>

<p>I have <em>not</em> tried tuning the rprop parameters yet, so the solution might lie there.</p>

<p>Code: </p>

<pre><code># DATA ----------------------
minute &lt;- seq(0, 6*24 - 1)
temp &lt;- rep.int(17, 6*24)
temp[(6*7):(6*20)] &lt;- 20
n &lt;- 10
dta &lt;- data.frame(Zeit = minute, Status = temp)
dta &lt;- dta[rep(seq_len(nrow(dta)), n), ]
# Scale everything
maxs &lt;- apply(dta, 2, max) 
mins &lt;- apply(dta, 2, min)

nnInput &lt;- data.frame(Zeit = dta$Zeit, Status = dta$Status)
nnInput &lt;- as.data.frame(scale(nnInput, center = mins, scale = maxs - mins))
trainingData &lt;- nnInput[seq(1, nrow(nnInput), 2), ]
testData     &lt;- nnInput[seq(2, nrow(nnInput), 2), ]

# MODEL ---------------------
model &lt;- as.formula(""Status ~ Zeit"")
net &lt;- neuralnet::neuralnet(model, 
                            trainingData, 
                            hidden = 2, 
                            threshold = 0.01,
                            linear.output = TRUE,
                            lifesign = ""full"",
                            stepmax = 100000,
                            rep = 1)

net.results &lt;- neuralnet::compute(net, testData$Zeit)

results &lt;- net.results$net.result * (maxs[""Status""] - mins[""Status""]) + mins[""Status""]
testData &lt;- as.data.frame(t(t(testData) * (maxs - mins) + mins))

cleanOutput &lt;- data.frame(Actual = testData$Status, 
                          Prediction = results, 
                          diff = abs(results - testData$Status))

summary(cleanOutput)

plot(cleanOutput$Actual[1:144], main = ""Zeittabelle"", xlab = paste(""Min. seit 0:00 *"", n), ylab = ""Temperatur"")
lines(cleanOutput$Prediction[1:144], col = ""red"", lwd = 3)
</code></pre>
","<p>Basically - initialization is really important. If you don't initialize it randomly then you might make your network not working at all (e.g. by setting all the weights to <code>0</code>). It is also proven that for <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf"">sigmoid</a> and <a href=""http://arxiv.org/pdf/1502.01852v1.pdf"">relu</a> a certain kind of activation might help in training your network.</p>

<p>But in your case - I think that the differences are mostly made by the complexity of your problem. With a models with a complexity which seem to fit the complexity of your problem performs nice. The other models may suffer for the following reasons:</p>

<ol>
<li><strong>Too small complexity</strong> - with one node maybe you are basically not able to learn the proper function.</li>
<li><strong>Too big complexity</strong> - with two-layer network you might experience stucking in a local minimas. Increasing the number of parameters of your network is also increasing the size of parameter space. Of course - one hand you might get the better model - on the other hand - you may land in this region of a parameter space which will result in poor solution. Maybe trying the same model with different initialization - and choosing the best model might overcome this issue.</li>
</ol>

<p><strong>UPDATE:</strong></p>

<ol>
<li><p>With small network sizes - it is quite usual to stuck in a local minimum. Depending on the amount of time which you need to train your network you may use the following techniques to overcome that:</p>

<ul>
<li><strong>Dropout / Batch normalization / Batch learning randomization :</strong> when you are able to train your network for a little bit longer time - you might use a randomization properties of dropout or batch normalization. Due to this random fluctuations you are able to move from poor local minima (which are usually believed to be relatively shallow).</li>
<li><strong>Cross - validation / Multiple run:</strong> When you are starting your training multiple times - the probability that you will finish in a poor minimum significantly decreases.</li>
</ul></li>
<li><p>About the connection between layer size and polynomial degree - I think that the question is not clearly stated. You must specify more details like e.g. the activation function. I also think that the nature of a polynomials and functions which could be modelled by a classic neural networks differs a lot. In polynomials - the small change in parameters values usually tends to much higher difference than in neural network case. Usually a derivative of a neural network is a bounded function whereas the polynomial derivative is unbounded when the degree is bigger that 2. Due to this facts I think - that looking for a dependency between a polynomial degree and a size of a hidden layer might be not worth serious considerations.</p></li>
</ol>
"
25308913,Automatically escape unicode characters,2,2,4,"<p>How can you display a unicode string, say:</p>

<pre><code>x &lt;- ""•""
</code></pre>

<p>using its escaped equivalent?</p>

<pre><code>y &lt;- ""\u2022""

identical(x, y)
# [1] TRUE
</code></pre>

<p>(I'd like to be able to do this because CRAN packages must contain only ASCII, but sometimes you want to use unicode in an error message or similar)</p>
","<p>After digging into some documentation about <code>iconv</code>, I think you can accomplish this using only the <code>base</code> package. But you need to pay extra attention to the encoding of the string.</p>

<p>On a system with UTF-8 encoding: </p>

<pre><code>&gt; stri_escape_unicode(""你好世界"")
[1] ""\\u4f60\\u597d\\u4e16\\u754c""

# use big endian
&gt; iconv(x, ""UTF-8"", ""UTF-16BE"", toRaw=T)
[[1]]
[1] 4f 60 59 7d 4e 16 75 4c

&gt; x &lt;- ""•""
&gt; iconv(x, ""UTF-8"", ""UTF-16BE"", toRaw=T)    
[[1]]
[1] 20 22
</code></pre>

<p>But, if you are on a system with <code>latin1</code> encoding, things may go wrong.   </p>

<pre><code>&gt; x &lt;- ""•""
&gt; y &lt;- ""\u2022""
&gt; identical(x, y)
[1] FALSE
&gt; stri_escape_unicode(x)
[1] ""\\u0095"" # &lt;- oops!

# culprit
&gt; Encoding(x)
[1] ""latin1""

# and it causes problem for iconv
&gt; iconv(x, Encoding(x), ""Unicode"")
Error in iconv(x, Encoding(x), ""Unicode"") : 
  unsupported conversion from 'latin1' to 'Unicode' in codepage 1252
&gt; iconv(x, Encoding(x), ""UTF-16BE"")
Error in iconv(x, Encoding(x), ""UTF-16BE"") : 
  embedded nul in string: '\0•'
</code></pre>

<p>It is safer to cast the string into UTF-8 before converting to Unicode:</p>

<pre><code>&gt; iconv(enc2utf8(enc2native(x)), ""UTF-8"", ""UTF-16BE"", toRaw=T)
[[1]]
[1] 20 22
</code></pre>

<p><strong>EDIT: This may cause some problems for strings already in UTF-8 encoding on some particular systems. Maybe it's safer to check the encoding before conversion.</strong></p>

<pre><code>&gt; Encoding(""•"")
[1] ""latin1""
&gt; enc2native(""•"")
[1] ""•""
&gt; enc2native(""\u2022"")
[1] ""•""
# on a Windows with default latin1 encoding
&gt; Encoding(""测试"") 
[1] ""UTF-8""
&gt; enc2native(""测试"") 
[1] ""&lt;U+6D4B&gt;&lt;U+8BD5&gt;""   # &lt;- BAD! 
</code></pre>

<p>For some characters or lanuages, <code>UTF-16</code> may not be enough. So probably you should be using <code>UTF-32</code> since</p>

<blockquote>
  <p>The UTF-32 form of a character is a direct representation of its codepoint.</p>
</blockquote>

<p>Based on above trial and error, below is probably one safer escape function we can write:</p>

<pre><code>unicode_escape &lt;- function(x, endian=""big"") {
  if (Encoding(x) != 'UTF-8') {
    x &lt;- enc2utf8(enc2native(x))
  }
  to.enc &lt;- ifelse(endian == 'big', 'UTF-32BE', 'UTF-32LE')

  bytes &lt;- strtoi(unlist(iconv(x, ""UTF-8"", ""UTF-32BE"", toRaw=T)), base=16)
  # there may be some better way to do thibs.
  runes &lt;- matrix(bytes, nrow=4)
  escaped &lt;- apply(runes, 2, function(rb) {
    nonzero.bytes &lt;- rb[rb &gt; 0]
    ifelse(length(nonzero.bytes) &gt; 1, 
           # convert back to hex
           paste(""\\u"", paste(as.hexmode(nonzero.bytes), collapse=""""), sep=""""),
           rawToChar(as.raw(nonzero.bytes))
           )
  })
  paste(escaped, collapse="""")
}
</code></pre>

<h3>Tests:</h3>

<pre><code>&gt; unicode_escape(""•••ERROR!!!•••"")
[1] ""\\u2022\\u2022\\u2022ERROR!!!\\u2022\\u2022\\u2022""
&gt; unicode_escape(""Hello word! 你好世界！"")
[1] ""Hello word! \\u4f60\\u597d\\u4e16\\u754c!""
&gt; ""\u4f60\u597d\u4e16\u754c""
[1] ""你好世界""
</code></pre>
"
32346302,Apply a rule to calculate sum of specific,1,1,1,"<p>Hi I have a data set like this.</p>

<pre><code>Num   C     Pr      Value   Volume
111   aa    Alen      111    222
111   aa    Paul      100    200
222   vv    Iva       444    555
222   vv    John      333    444
</code></pre>

<p>I would like to filter the data according to Num and to add a new row where take the sum of column Value and Volume but to keep the information of column Num and C, but in column Pr to put Total. It should look like this way.</p>

<pre><code>Num   C     Pr      Value   Volume
222   vv   Total     777     999
</code></pre>

<p>Could you suggest me how to do it? I would like only for Num 222.</p>

<p>When I try to use <code>res</code> command I end up with this result.</p>

<pre><code>#  Num  C    Pr   Value Volume 
1: 111 aa  Alen   111    222
2: 111 aa  Paul   100    200
3: 111 aa  Total   NA     NA
4: 222 vv   Iva   444    555
5: 222 vv  John   333    444
6: 222 vv Total    NA     NA
</code></pre>

<p>What cause this?</p>

<p>The structure of my data is the following one.</p>

<pre><code>'data.frame':   4 obs. of  5 variables:
  $ Num   : Factor w/ 2 levels ""111"",""222"": 1 1 2 2
  $ C     : Factor w/ 2 levels ""aa"",""vv"": 1 1 2 2
  $ Pr    : Factor w/ 4 levels ""Alen"",""Iva"",""John"",..: 1 4 2 3
  $ Value : Factor w/ 4 levels ""100"",""111"",""333"",..: 2 1 4 3
  $ Volume: Factor w/ 4 levels ""200"",""222"",""444"",..: 2 1 4 3
</code></pre>
","<p>We could use <code>data.table</code>.  We convert the 'data.frame' to 'data.table' (<code>setDT(df1)</code>), grouped by 'Num', 'C' columns and specifying the columns to do the <code>sum</code> in <code>.SDcols</code>, we loop those columns using <code>lapply</code>, get the <code>sum</code>, and create the 'Pr' column.  We can <code>rbind</code> the original dataset with the new summarised output ('DT1') and <code>order</code> the result based on 'Num'.</p>

<pre><code>library(data.table)#v1.9.5+
DT1 &lt;- setDT(df1)[,lapply(.SD, sum) , by = .(Num,C), 
              .SDcols=Value:Volume][,Pr:='Total'][]
rbind(df1, DT1)[order(Num)]
#   Num  C    Pr Value Volume
#1: 111 aa  Alen   111    222
#2: 111 aa  Paul   100    200
#3: 111 aa Total   211    422
#4: 222 vv   Iva   444    555
#5: 222 vv  John   333    444
#6: 222 vv Total   777    999
</code></pre>

<hr>

<p>This can be done using <code>base R</code> methods as well.  We get the <code>sum</code> of 'Value', 'Volume' columns grouped by 'Num', 'C', using the formula method of <code>aggregate</code>, <code>transform</code> the output by creating the 'Pr' column, <code>rbind</code> with original dataset and <code>order</code> the output ('res') based on 'Num'.</p>

<pre><code>res &lt;- rbind(df1,transform(aggregate(.~Num+C, df1[-3], FUN=sum), Pr='Total'))
res[order(res$Num),]
#  Num  C    Pr Value Volume
#1 111 aa  Alen   111    222
#2 111 aa  Paul   100    200
#5 111 aa Total   211    422
#3 222 vv   Iva   444    555
#4 222 vv  John   333    444
#6 222 vv Total   777    999
</code></pre>

<p>EDIT: Noticed that the OP mentioned <code>filter</code>.  If this is for a single 'Num', we <code>subset</code> the data, and then do the <code>aggregate</code>, <code>transform</code> steps.  </p>

<pre><code>transform(aggregate(.~Num+C, subset(df1, Num==222)[-3], FUN=sum), Pr='Total')
#  Num  C Value Volume    Pr
#1 222 vv   777    999 Total
</code></pre>

<p>Or we may not need <code>aggregate</code>.  After <code>subset</code>ting the data, we convert the 'Num' to 'factor', loop through the output dataset ('df2') get the <code>sum</code> if it the column is <code>numeric</code> class or else we get the first element and wrap with <code>data.frame</code>.</p>

<pre><code>df2 &lt;- transform(subset(df1, Num==222), Num=factor(Num))
data.frame(c(lapply(df2[-3], function(x) if(is.numeric(x)) 
                   sum(x) else x[1]), Pr='Total'))
#  Num  C Value Volume    Pr
#1 222 vv   777    999 Total
</code></pre>

<h3>data</h3>

<pre><code>df1 &lt;- structure(list(Num = c(111L, 111L, 222L, 222L), C = c(""aa"", ""aa"", 
""vv"", ""vv""), Pr = c(""Alen"", ""Paul"", ""Iva"", ""John""), Value = c(111L, 
100L, 444L, 333L), Volume = c(222L, 200L, 555L, 444L)), .Names = c(""Num"", 
""C"", ""Pr"", ""Value"", ""Volume""), class = ""data.frame"",
row.names = c(NA, -4L))
</code></pre>
"
41435077,R: instructions for unbundling and using a packrat snapshot,2,2,2,"<p>I used packrat (v 0.4.8.-1) to to create a snapshot and bundle of the R package dependencies that go along with the corresponding R code. I want to provide the R code and packrat bundle to others to make the work I am doing (including the R environment) fully reproducible. </p>

<p>I tested unbundling using a different computer from the one I used to write R code and create the bundle. I opened an R code file in R studio, and called library(packrat) to load packrat (also v 0.4.8-1). I then called packrat::unbundle(bundle = ""directory"", where = ""directory""), which unbundled successfully. But subsequently calling packrat::restore() gave me the error ""This project has not yet been packified. Run 'packrat::init()' to init packrat"". It seems like init() should not be necessary because I am not trying to create a new snapshot, but rather utilize the one in the bundle. The packrat page (<a href=""https://rstudio.github.io/packrat/"" rel=""nofollow noreferrer"">https://rstudio.github.io/packrat/</a>) and CRAN provide very little documentation about unbundling to help troubleshoot this, or that I could point users of my code to for instructions (who likely will be familiar with R, but may not have used packrat).</p>

<p>So, can someone please provide clear step-by-step instructions for how users of a bundled snapshot should unbundle, and then use that saved snapshot to run a R code file? </p>
","<p>After some experimenting, I found an approach that seems to have worked so far. </p>

<p>I have provided users with three files:</p>

<pre><code>-tar.gz (packrat bundle file)
-unbundle.R (R code file that includes a library statement to load 
 the packrat library, and the unbundle command for the tar.gz file)
-unbundle_readme.txt  
</code></pre>

<p>The readme file includes instructions similar to those below, and so far users have been able to run R code using the package dependencies. The readme file tells users about requirements (R, R studio, packrat, R package development prerequisites (Rtools for Windows, XCode for Mac)), and includes output of sessionInfo() to document R package versions that the R code should use after instructions are followed. In the example below 'code_folder' refers to a folder within the tar.gz file that contains R. code and associated input files. </p>

<p>Example unbundle instructions:</p>

<h1>Step 1</h1>

<p>Save, but do not expand/unzip, the tar file to a directory.
Problems with accessing the saved package dependencies
are more likely when a program other than R or R studio
is used to unbundle the tar file.</p>

<p>If the tar file has already been expanded, re-save the
tar file to a new directory, which should not be a the same
directory as the expanded tar file, or a subdirectory of
the expanded tar file. </p>

<h1>Step 2</h1>

<p>Save unbundle.R in the same directory as the tar file</p>

<h1>Step 3</h1>

<p>Open unbundle.R using R studio</p>

<h1>Step 4</h1>

<p>Execute unbundle.R
(This will create a subfolder ‘code_folder’. 
Please note that this step may take 5-15 minutes to run.)</p>

<h1>Step 5</h1>

<p>Close R studio</p>

<h1>Step 6</h1>

<p>Navigate to the subfolder ‘cold_folder’</p>

<h1>Step 7</h1>

<p>Open a R script using R studio
(The package library should correspond to that listed below.
This will indicate R studio is accessing the saved package
dependencies.)</p>

<h1>Step 8</h1>

<p>Execute the R code, which will utilize the project package library.
After the package library has been loaded using the above 
steps, it is not necessary to re-load the package library for each 
script. R studio will continue to access the package dependencies
for each script you open within the R studio session. If you
subsequently close R-studio, and then open scripts from within
the unbundle directory, R studio should still access the
dependencies without requiring re-loading of the saved package
snapshot.</p>
"
22255339,Adding a bar chart to a ggplot2 legend,1,1,1,"<p>I have the following <code>data.frame</code>:</p>

<pre><code>my.df = data.frame(mean = c(0.045729661,0.030416531,0.043202944,0.025600973,0.040526913,0.046167044,0.029352414,0.021477789,0.027580529,0.017614864,0.020324659,0.027547972,0.0268722,0.030804717,0.021502093,0.008342398,0.02295506,0.022386184,0.030849534,0.017291356,0.030957321,0.01871551,0.016945678,0.014143042,0.026686185,0.020877973,0.028612298,0.013227244,0.010710895,0.024460647,0.03704981,0.019832982,0.031858501,0.022194059,0.030575241,0.024632496,0.040815748,0.025595652,0.023839083,0.026474704,0.033000706,0.044125751,0.02714219,0.025724641,0.020767752,0.026480009,0.016794441,0.00709195), std.dev = c(0.007455271,0.006120299,0.008243454,0.005552582,0.006871527,0.008920899,0.007137174,0.00582671,0.007439398,0.005265133,0.006180637,0.008312494,0.006628951,0.005956211,0.008532386,0.00613411,0.005741645,0.005876588,0.006640122,0.005339993,0.008842722,0.006246828,0.005532832,0.005594483,0.007268493,0.006634795,0.008287031,0.00588119,0.004479003,0.006333063,0.00803285,0.006226441,0.009681048,0.006457784,0.006045368,0.006293256,0.008062195,0.00857954,0.008160441,0.006830088,0.008095485,0.006665062,0.007437581,0.008599525,0.008242957,0.006379928,0.007168385,0.004643819), parent.origin = c(""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal""), group = c(""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F""), replicate = c(1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6))
</code></pre>

<p>For which I produce a ggplot with this code:</p>

<pre><code>p1 = ggplot(data = my.df, aes(factor(replicate), color = factor(parent.origin)))
p1 = p1 + geom_boxplot(aes(fill = factor(parent.origin),lower = mean - std.dev, upper = mean + std.dev, middle = mean, ymin = mean - 3*std.dev, ymax = mean + 3*std.dev), position = position_dodge(width = 0), width = 0.5, alpha = 0.5, stat=""identity"") + facet_wrap(~group, ncol = 4)+scale_fill_manual(values = c(""red"",""blue""),labels = c(""maternal"",""paternal""),name = ""parental allele"")+scale_colour_manual(values = c(""red"",""blue""),labels = c(""maternal"",""paternal""),name = ""parental allele"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/AE2DM.png"" alt=""ggplot""></p>

<p>Now I want to add this data.frame, which gives the following bar chart:</p>

<pre><code>bar.df = data.frame(bar=c(""paternal.fraction"",""maternal.fraction"",""parental.effect"",""cross.effect"",""gender.effect""),vals=c(0.4,0.6,0.82,0.91,0.77))

#order bars and specify colors
bar.df$ord = factor(bar.df$bar,as.character(bar.df$bar))
cols = c(paterna=""blue"",maternal=""red"",parental=""purple"",cross=""gray40"",gender=""gray70"")

bar.p = ggplot(bar.df, aes(y=vals))+geom_bar(aes(x=ord),data=bar.df,stat=""identity"",fill=cols)+labs(x="""",y="""")
</code></pre>

<p><img src=""https://i.stack.imgur.com/v2ywV.png"" alt=""bar chart""></p>

<p>to be located as a small plot below the legend of the original plot (the one with the ""parental allele"" title.)</p>

<p>Any idea?</p>
","<p>Here's the grob way of doing it:</p>

<pre><code>my.df = data.frame(mean = c(0.045729661,0.030416531,0.043202944,0.025600973,0.040526913,0.046167044,0.029352414,0.021477789,0.027580529,0.017614864,0.020324659,0.027547972,0.0268722,0.030804717,0.021502093,0.008342398,0.02295506,0.022386184,0.030849534,0.017291356,0.030957321,0.01871551,0.016945678,0.014143042,0.026686185,0.020877973,0.028612298,0.013227244,0.010710895,0.024460647,0.03704981,0.019832982,0.031858501,0.022194059,0.030575241,0.024632496,0.040815748,0.025595652,0.023839083,0.026474704,0.033000706,0.044125751,0.02714219,0.025724641,0.020767752,0.026480009,0.016794441,0.00709195), std.dev = c(0.007455271,0.006120299,0.008243454,0.005552582,0.006871527,0.008920899,0.007137174,0.00582671,0.007439398,0.005265133,0.006180637,0.008312494,0.006628951,0.005956211,0.008532386,0.00613411,0.005741645,0.005876588,0.006640122,0.005339993,0.008842722,0.006246828,0.005532832,0.005594483,0.007268493,0.006634795,0.008287031,0.00588119,0.004479003,0.006333063,0.00803285,0.006226441,0.009681048,0.006457784,0.006045368,0.006293256,0.008062195,0.00857954,0.008160441,0.006830088,0.008095485,0.006665062,0.007437581,0.008599525,0.008242957,0.006379928,0.007168385,0.004643819), parent.origin = c(""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""maternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal"",""paternal""), group = c(""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:M"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1r:F"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:M"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F"",""F1i:F""), replicate = c(1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6,1,2,3,4,5,6))

library(ggplot2)
p1 = ggplot(data = my.df, aes(factor(replicate), color = factor(parent.origin)))
p1 = p1 + geom_boxplot(aes(fill = factor(parent.origin),lower = mean - std.dev, upper = mean + std.dev, middle = mean, ymin = mean - 3*std.dev, ymax = mean + 3*std.dev), position = position_dodge(width = 0), width = 0.5, alpha = 0.5, stat=""identity"") + facet_wrap(~group, ncol = 4)+scale_fill_manual(values = c(""red"",""blue""),labels = c(""maternal"",""paternal""),name = ""parental allele"")+scale_colour_manual(values = c(""red"",""blue""),labels = c(""maternal"",""paternal""),name = ""parental allele"")

library(grid)
bar.df = data.frame(bar=c(""paternal.fraction"",""maternal.fraction"",""parental.effect"",""cross.effect"",""gender.effect""),vals=c(0.4,0.6,0.82,0.91,0.77))
#order bars and specify colors
bar.df$ord = factor(bar.df$bar,as.character(bar.df$bar))
#specify bar colors
cols = c(paterna=""blue"",maternal=""red"",parental=""purple"",cross=""gray40"",gender=""gray70"")
bar.p = ggplot(bar.df, aes(y=vals))+geom_bar(aes(x=ord),data=bar.df,stat=""identity"",fill=cols)+labs(x="""",y="""") +theme(axis.text.x = element_text(angle = 90),plot.margin=unit(c(2,2,2,2),""mm""))

library(gridExtra)
g_legend &lt;- function(a.gplot){
            tmp &lt;- ggplot_gtable(ggplot_build(a.gplot))
            leg &lt;- which(sapply(tmp$grobs, function(x) x$name) == ""guide-box"")
            legend &lt;- tmp$grobs[[leg]]
            return(legend)
    }
bar.p.grob=ggplotGrob(bar.p)
p2 = arrangeGrob(p1 + theme(legend.position = ""none""),widths=c(3/4, 1/4),arrangeGrob(g_legend(p1),bar.p.grob), ncol = 2)
</code></pre>

<p>Which gives:
<img src=""https://i.stack.imgur.com/0GBpy.png"" alt=""enter image description here""></p>
"
26004302,shiny app busy indicator,2,2,2,"<p>Note : I have read almost all the discussions on this object in shiny googlegroups and here in SO.</p>

<p>I need an indicator that shows the shiny server is busy. I have tried shiny incubator, however, the problem is that I can't set a max for progress bar. 
I don't want something like this : <a href=""http://shiny.rstudio.com/gallery/progress-example.html"" rel=""noreferrer"">http://shiny.rstudio.com/gallery/progress-example.html</a>
What I need is something that:
1- shows a busy indicator message and bar (i.e just a simple animated bar, do not need to show a filling bar) as long as the server is calculating
2- it is shown in no matter which tab you are viewing. (not only in the related tab, but on top of the tabset)</p>

<p>Thanks</p>
","<p>I've been looking for this as well. Most people suggest a conditional panel like so:</p>

<pre><code>conditionalPanel(
            condition=""!($('html').hasClass('shiny-busy'))"",
            img(src=""images/busy.gif"")
)
</code></pre>

<p>You could always give yourself more control and create the conditional handling (maybe depending on more stuff) like this in your ui.R:</p>

<pre><code>div(class = ""busy"",
    p(""Calculation in progress..""),
    img(src=""images/busy.gif"")
)
</code></pre>

<p>where some JavaScript handles the showing and hiding of that div:</p>

<pre><code>setInterval(function(){
  if ($('html').attr('class')=='shiny-busy') {
    $('div.busy').show()
  } else {
    $('div.busy').hide()
  }
},100)
</code></pre>

<p>with some extra css you could make sure your animated busy image gets a fixed postion where it will always be visible.</p>

<p>In any of the above cases i found that the ""shiny-busy"" condition is somewhat imprecise and unreliable: the div shows for a split second and disappears while computations are still going on...
I found a dirty solution to fix that problem, at least in my apps. Feel free to try it out and maybe someone could give an insight to how and why this solves the issue.</p>

<p>In your server.R you'll need to add two reactiveValues:</p>

<pre><code>shinyServer(function(input, output, session) {

    # Reactive Value to reset UI, see render functions for more documentation
    uiState &lt;- reactiveValues()
    uiState$readyFlag &lt;- 0
    uiState$readyCheck &lt;- 0
</code></pre>

<p>then, in your renderPlot function (or other output function where computations go on), you use these reactive values to reset the function:</p>

<pre><code>output$plot&lt;- renderPlot({

    if (is.null(input$file)){
        return()
    }
    if(input$get == 0){
        return()
    }

    uiState$readyFlag

    # DIRTY HACK:
    # Everytime ""Get Plot"" is clicked we get into this function
    # In order for the ui to be able show the 'busy' indicator we
    # somehow need to abort this function and then of course seamlessly
    # call it again.
    # We do this by using a reactive value keeping track of the ui State:
    # renderPlot is depending on 'readyFlag': if that gets changed somehow
    # the reactive programming model will call renderPlot
    # If readyFlag equals readyCheck we exit the function (= ui reset) but in the
    # meantime we change the readyFlag, so the renderHeatMap function will 
    # immediatly be called again. At the end of the function we make sure 
    # readyCheck gets the same value so we are back to the original state

    isolate({
        if (uiState$readyFlag == uiState$readyCheck) {
            uiState$readyFlag &lt;- uiState$readyFlag+1
            return(NULL)
        }
    })

    isolate({plot &lt;- ...})

    # Here we make sure readyCheck equals readyFlag once again
    uiState$readyCheck &lt;- uiState$readyFlag

    return(plot)
})
</code></pre>
"
44320008,Parse HTML data using R,2,2,4,"<p>I have a html data set  as below, which I want to parse and convert into a tabular format which I can use .</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;

&lt;head&gt;
    &lt;title&gt;Page Title&lt;/title&gt;
&lt;/head&gt;

&lt;body&gt;
    &lt;div class=""brewery"" id=""brewery""&gt;
        &lt;ul class=""vcard simple""&gt;
            &lt;li class=""name""&gt; Bradley Farm / RB Brew, LLC&lt;/li&gt;
            &lt;li class=""address""&gt;317 Springtown Rd &lt;/li&gt;
            &lt;li class=""address_2""&gt;New Paltz, NY 12561-3020 | &lt;a href='http://www.google.com/maps/place/317 Springtown Rd++New Paltz+NY+United States' target='_blank'&gt;Map&lt;/a&gt; &lt;/li&gt;
            &lt;li class=""telephone""&gt;Phone: (845) 255-8769&lt;/li&gt;
            &lt;li class=""brewery_type""&gt;Type: Micro&lt;/li&gt;
            &lt;li class=""url""&gt;&lt;a href=""http://www.raybradleyfarm.com"" target=""_blank""&gt;www.raybradleyfarm.com&lt;/a&gt; &lt;/li&gt;
        &lt;/ul&gt;
        &lt;ul class=""vcard simple col2""&gt;&lt;/ul&gt;
    &lt;/div&gt;
    &lt;div class=""brewery""&gt;
        &lt;ul class=""vcard simple""&gt;
            &lt;li class=""name""&gt;(405) Brewing Co&lt;/li&gt;
            &lt;li class=""address""&gt;1716 Topeka St &lt;/li&gt;
            &lt;li class=""address_2""&gt;Norman, OK 73069-8224 | &lt;a href='http://www.google.com/maps/place/1716 Topeka St++Norman+OK+United States' target='_blank'&gt;Map&lt;/a&gt; &lt;/li&gt;
            &lt;li class=""telephone""&gt;Phone: (405) 816-0490&lt;/li&gt;
            &lt;li class=""brewery_type""&gt;Type: Micro&lt;/li&gt;
            &lt;li class=""url""&gt;&lt;a href=""http://www.405brewing.com"" target=""_blank""&gt;www.405brewing.com&lt;/a&gt; &lt;/li&gt;
        &lt;/ul&gt;
        &lt;ul class=""vcard simple col2""&gt;&lt;/ul&gt;
    &lt;/div&gt;
&lt;/body&gt;
</code></pre>

<p>Below is the code which I have used. The issue I am facing is it converts into text file using Rvest but cant seem to make it of any useful format.</p>

<pre><code>library(dplyr)
library(rvest)

url&lt;-html(""beer.html"")
selector_name&lt;-"".brewery""
fnames&lt;-html_nodes(x = url, css = selector_name) %&gt;%
html_text()
head(fnames)
fnames
</code></pre>

<p>Would this be a correct approach or should I be doing it using some other package to go through each div and the inner elements.</p>

<p>The out put I would like to see it is</p>

<pre><code>No.  Name  Address Type Website
</code></pre>

<p>Thank You.</p>
","

<pre class=""lang-r prettyprint-override""><code>library(rvest)
library(dplyr)

html_file &lt;- '&lt;!DOCTYPE html&gt;
&lt;html&gt;

&lt;head&gt;
    &lt;title&gt;Page Title&lt;/title&gt;
&lt;/head&gt;

&lt;body&gt;
    &lt;div class=""brewery"" id=""brewery""&gt;
        &lt;ul class=""vcard simple""&gt;
            &lt;li class=""name""&gt; Bradley Farm / RB Brew, LLC&lt;/li&gt;
            &lt;li class=""address""&gt;317 Springtown Rd &lt;/li&gt;
            &lt;li class=""address_2""&gt;New Paltz, NY 12561-3020 | &lt;a href=""http://www.google.com/maps/place/317 Springtown Rd++New Paltz+NY+United States"" target=""_blank""&gt;Map&lt;/a&gt; &lt;/li&gt;
            &lt;li class=""telephone""&gt;Phone: (845) 255-8769&lt;/li&gt;
            &lt;li class=""brewery_type""&gt;Type: Micro&lt;/li&gt;
            &lt;li class=""url""&gt;&lt;a href=""http://www.raybradleyfarm.com"" target=""_blank""&gt;www.raybradleyfarm.com&lt;/a&gt; &lt;/li&gt;
        &lt;/ul&gt;
        &lt;ul class=""vcard simple col2""&gt;&lt;/ul&gt;
    &lt;/div&gt;
    &lt;div class=""brewery""&gt;
        &lt;ul class=""vcard simple""&gt;
            &lt;li class=""name""&gt;(405) Brewing Co&lt;/li&gt;
            &lt;li class=""address""&gt;1716 Topeka St &lt;/li&gt;
            &lt;li class=""address_2""&gt;Norman, OK 73069-8224 | &lt;a href=""http://www.google.com/maps/place/1716 Topeka St++Norman+OK+United States"" target=""_blank""&gt;Map&lt;/a&gt; &lt;/li&gt;
            &lt;li class=""telephone""&gt;Phone: (405) 816-0490&lt;/li&gt;
            &lt;li class=""brewery_type""&gt;Type: Micro&lt;/li&gt;
            &lt;li class=""url""&gt;&lt;a href=""http://www.405brewing.com"" target=""_blank""&gt;www.405brewing.com&lt;/a&gt; &lt;/li&gt;
        &lt;/ul&gt;
        &lt;ul class=""vcard simple col2""&gt;&lt;/ul&gt;
    &lt;/div&gt;
&lt;/body&gt;'

page &lt;- read_html(html_file) 

tibble(
  name = page %&gt;% html_nodes("".vcard .name"") %&gt;% html_text(),
  address = page %&gt;% html_nodes("".vcard .address"") %&gt;% html_text(),
  type = page %&gt;% html_nodes("".vcard .brewery_type"") %&gt;% html_text() %&gt;% stringr::str_replace_all(""^Type: "", """"),
  website = page %&gt;% html_nodes("".vcard .url a"") %&gt;% html_attr(""href"")
)

#&gt; # A tibble: 2 x 4
#&gt;                           name            address  type                       website
#&gt;                          &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;                         &lt;chr&gt;
#&gt; 1  Bradley Farm / RB Brew, LLC 317 Springtown Rd  Micro http://www.raybradleyfarm.com
#&gt; 2             (405) Brewing Co    1716 Topeka St  Micro     http://www.405brewing.com
</code></pre>
"
30943439,Defining the name of a text/html file like the name of a data.frame in R,2,2,4,"<p>I have a data.frame CLI_CAP_X_PROD_AG:</p>

<pre><code>      FAMILIA_PRD NUM_CLI CAPITAL_SOL PART_CAP_PER ATRASADA ATR_PER
      A             536     2616925        33.62   467830   17.88
      B             151     1613035        20.72   268223   16.63
</code></pre>

<p>when I try to save the file using the name of the data.frame I get error:</p>

<pre><code>      print(temp,type = ""html"", include.rownames = FALSE, file = paste(CLI_CAP_X_PROD_AG,""_tab.html"", sep=""""))
</code></pre>

<p>because R read the elements of CLI_CAP_X_PROD_AG.</p>

<p>Any help will be much appreciated!</p>
","<p>You should store your xtables in a named list and iterate over the list. Demo:</p>

<pre><code>library('xtable');
li &lt;- list(t1=xtable(data.frame(a=letters[1:3],b=1:3)),t2=xtable(data.frame(a=letters[4:6],b=4:6)));
names(li);
## [1] ""t1"" ""t2""
li;
## $t1
## % latex table generated in R 3.1.3 by xtable 1.7-4 package
## % Fri Jun 19 13:48:30 2015
## \begin{table}[ht]
## \centering
## \begin{tabular}{rlr}
##   \hline
##  &amp; a &amp; b \\
##   \hline
## 1 &amp; a &amp;   1 \\
##   2 &amp; b &amp;   2 \\
##   3 &amp; c &amp;   3 \\
##    \hline
## \end{tabular}
## \end{table}
##
## $t2
## % latex table generated in R 3.1.3 by xtable 1.7-4 package
## % Fri Jun 19 13:48:30 2015
## \begin{table}[ht]
## \centering
## \begin{tabular}{rlr}
##   \hline
##  &amp; a &amp; b \\
##   \hline
## 1 &amp; d &amp;   4 \\
##   2 &amp; e &amp;   5 \\
##   3 &amp; f &amp;   6 \\
##    \hline
## \end{tabular}
## \end{table}
##
for (n in names(li)) print(li[[n]],type='html',include.rownames=F,file=paste0(n,'_tab.html'));
file.show('t1_tab.html',pager='cat');
## &lt;!-- html table generated in R 3.1.3 by xtable 1.7-4 package --&gt;
## &lt;!-- Fri Jun 19 13:48:34 2015 --&gt;
## &lt;table border=1&gt;
## &lt;tr&gt; &lt;th&gt; a &lt;/th&gt; &lt;th&gt; b &lt;/th&gt;  &lt;/tr&gt;
##   &lt;tr&gt; &lt;td&gt; a &lt;/td&gt; &lt;td align=""right""&gt;   1 &lt;/td&gt; &lt;/tr&gt;
##   &lt;tr&gt; &lt;td&gt; b &lt;/td&gt; &lt;td align=""right""&gt;   2 &lt;/td&gt; &lt;/tr&gt;
##   &lt;tr&gt; &lt;td&gt; c &lt;/td&gt; &lt;td align=""right""&gt;   3 &lt;/td&gt; &lt;/tr&gt;
##    &lt;/table&gt;
##
file.show('t2_tab.html',pager='cat');
## &lt;!-- html table generated in R 3.1.3 by xtable 1.7-4 package --&gt;
## &lt;!-- Fri Jun 19 13:48:34 2015 --&gt;
## &lt;table border=1&gt;
## &lt;tr&gt; &lt;th&gt; a &lt;/th&gt; &lt;th&gt; b &lt;/th&gt;  &lt;/tr&gt;
##   &lt;tr&gt; &lt;td&gt; d &lt;/td&gt; &lt;td align=""right""&gt;   4 &lt;/td&gt; &lt;/tr&gt;
##   &lt;tr&gt; &lt;td&gt; e &lt;/td&gt; &lt;td align=""right""&gt;   5 &lt;/td&gt; &lt;/tr&gt;
##   &lt;tr&gt; &lt;td&gt; f &lt;/td&gt; &lt;td align=""right""&gt;   6 &lt;/td&gt; &lt;/tr&gt;
##    &lt;/table&gt;
##
</code></pre>
"
37733800,ggplot2: how to add lines and p-values on a grouped barplot?,1,3,1,"<p>I tried unsuccessfully to solve my problem reading the answers of these posts:
<a href=""https://stackoverflow.com/questions/14958159/indicating-the-statistically-significant-difference-in-bar-graph-using-r"">Indicating the statistically significant difference in bar graph USING R</a>, 
<a href=""https://stackoverflow.com/questions/29263046/how-to-draw-the-boxplot-with-significant-level"">How to draw the boxplot with significant level?</a> and
<a href=""https://stackoverflow.com/questions/17084566/put-stars-on-ggplot-barplots-and-boxplots-to-indicate-the-level-of-significanc"">Put stars on ggplot barplots and boxplots - to indicate the level of significance (p-value)</a>.</p>

<p>I would like to add some lines and labels to show the level of significance in a grouped barplot using R like the ones inside the red rectangle.</p>

<p><img src=""https://i.stack.imgur.com/2bIYv.jpg"" alt=""plot""></p>

<p>Here is a simpler version of the code that I came up:</p>

<pre><code>#### DATA
g &lt;- as.factor(c('Kit1_A', 'Kit2_A', 'Kit1_B', 'Kit2_B','Kit1_C', 'Kit2_C'))
groups  &lt;- rep(g, 3)
targets &lt;- c(rep('X', 6), rep('Y', 6), rep('Z', 6))
mean &lt;- c(20.8, 23.8, 21.61667, 23.54583, 22.26250, 25.41250, 20.39583, 23.82917, 20.70000, 23.82917, 21.52083, 24.83333, 20.68750, 24.60000, 20.78750, 24.42083, 22.86667, 25.28750)
sd &lt;- c(1.249251, 1.137451, 2.372480, 2.439704, 2.149715, 1.465997, 1.579936, 0.944777, 2.320555, 1.419932, 2.636766, 2.820217, 2.014647, 1.384187, 2.193378, 1.685869, 3.456228, 2.197052)
df  &lt;-data.frame(groups, targets, mean, sd)

#### Barplot
library(ggplot2)
f &lt;- ggplot(df, aes(x=targets, y=mean, fill=groups))
f &lt;- f + geom_bar(position=""dodge"", stat=""identity"", colour='black')
f &lt;- f + geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2,position=position_dodge(.9))
f &lt;- f + theme(legend.title = element_blank())
f &lt;- f + scale_fill_manual(values=c('#D6EAF8','#5DADE2','#2874A6','#D5F5E3','#58D68D','#239B56'))
f &lt;- f + coord_cartesian(ylim = c(0, 35))
</code></pre>

<p>Thanks for any help.</p>
","<p>Maybe this is not the best answer but you could do it with <code>annotate(""rect"")</code>/<code>annotate(""segment"")</code> and <code>grid.text</code> or another option would be <code>annotation_custom(grob = linesGrob())</code></p>

<p>I used your data frame with <code>annotate(""rect"")</code> and <code>grid.text</code>.</p>

<p><strong>The updated code is:</strong></p>

<pre><code># Add rectangles 
f + annotate(""rect"", xmin = 0.6, xmax = 1.07, ymin = 27, ymax =27, alpha=1,colour = ""black"")+
    annotate(""rect"", xmin = 0.6, xmax = 0.6, ymin = 26.7, ymax =27, alpha=1, colour = ""black"")+
    annotate(""rect"", xmin = 1.07, xmax = 1.07, ymin = 26.7, ymax =27, alpha=1, colour = ""black"")+
    annotate(""rect"", xmin = 0.778, xmax = 1.2, ymin = 29.5, ymax =29.5, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 0.778, xmax = 0.778, ymin = 29.2, ymax =29.5, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 1.2, xmax = 1.2, ymin = 29.2, ymax =29.5, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 0.925, xmax = 1.4, ymin = 32, ymax =32, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 0.925, xmax = 0.925, ymin = 31.5, ymax =32, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 1.4, xmax = 1.4, ymin = 31.5, ymax =32, alpha=1,colour = ""black"") +

  # Second two lines
    annotate(""rect"", xmin = 1.61, xmax = 2.08, ymin = 27, ymax =27, alpha=1,colour = ""black"")+
    annotate(""rect"", xmin = 1.61, xmax = 1.61, ymin = 26.7, ymax =27, alpha=1, colour = ""black"")+
    annotate(""rect"", xmin = 2.08, xmax = 2.08, ymin = 26.7, ymax =27, alpha=1, colour = ""black"")+
    annotate(""rect"", xmin = 1.76, xmax = 2.2, ymin = 29.5, ymax =29.5, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 1.76, xmax = 1.76, ymin = 29.2, ymax =29.5, alpha=1,colour = ""black"") +
    annotate(""rect"", xmin = 2.2, xmax = 2.2, ymin = 29.2, ymax =29.5, alpha=1,colour = ""black"")


# Add text   

  grid.text((paste(""p&lt;0.001"")),
              x = unit(0.15, ""npc""), y = unit(0.77, ""npc""), just = c(""left"", ""bottom""), 
              gp = gpar(fontface = ""bold"", fontsize = 8, col = ""black""))

  grid.text((paste(""p&lt;0.001"")),
              x = unit(0.185, ""npc""), y = unit(0.839, ""npc""), just = c(""left"", ""bottom""), 
              gp = gpar(fontface = ""bold"", fontsize = 8, col = ""black""))

  grid.text((paste(""p&lt;0.001"")),
              x = unit(0.233, ""npc""), y = unit(0.902, ""npc""), just = c(""left"", ""bottom""), 
              gp = gpar(fontface = ""bold"", fontsize = 8, col = ""black""))
  # Second two lines
  grid.text((paste(""p&lt;0.001"")),
              x = unit(0.42, ""npc""), y = unit(0.77, ""npc""), just = c(""left"", ""bottom""), 
              gp = gpar(fontface = ""bold"", fontsize = 8, col = ""black""))

  grid.text((paste(""p&lt;0.001"")),
            x = unit(0.45, ""npc""), y = unit(0.839, ""npc""), just = c(""left"", ""bottom""), 
            gp = gpar(fontface = ""bold"", fontsize = 8, col = ""black""))
</code></pre>

<p>And the output:
<a href=""https://i.stack.imgur.com/Tr801.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tr801.png"" alt=""enter image description here""></a></p>
"
17966055,makeCluster function in R snow hangs indefinitely,2,2,2,"<p>I am using <code>makeCluster</code> function from R package <code>snow</code>  from Linux machine to start a SOCK cluster on a remote Linux machine. All seems settled for the two machines to communicate succesfully (I am able to estabilish ssh connections between the two). But:</p>

<pre><code>makeCluster(""192.168.128.24"",type=""SOCK"")
</code></pre>

<p>does not throw any result, just hangs indefinitely. </p>

<p>What am I doing wrong? </p>

<p>Thanks a lot</p>
","<p>Unfortunately, there are a lot of things that can go wrong when creating a snow (or parallel) cluster object, and the most common failure mode is to hang indefinitely. The problem is that <code>makeSOCKcluster</code> launches the cluster workers one by one, and each worker (if successfully started) must make a socket connection back to the master before the master proceeds to launch the next worker. If any of the workers fail to connect back to the master, <code>makeSOCKcluster</code> will hang without any error message. The worker may issue an error message, but by default any error message is redirected to <code>/dev/null</code>.</p>

<p>In addition to ssh problems, <code>makeSOCKcluster</code> could hang because:</p>

<ul>
<li>R not installed on a worker machine</li>
<li>snow not installed on a the worker machine</li>
<li>R or snow not installed in the same location as the local machine</li>
<li>current user doesn't exist on a worker machine</li>
<li>networking problem</li>
<li>firewall problem</li>
</ul>

<p>and there are many more possibilities.</p>

<p>In other words, no one can diagnose this problem without further information, so you have to do some troubleshooting in order to get that information.</p>

<p>In my experience, the single most useful troubleshooting technique is <em>manual mode</em> which you enable by specifying <code>manual=TRUE</code> when creating the cluster object.  It's also a good idea to set <code>outfile=""""</code> so that error messages from the workers aren't redirected to <code>/dev/null</code>:</p>

<pre><code>cl &lt;- makeSOCKcluster(""192.168.128.24"", manual=TRUE, outfile="""")
</code></pre>

<p><code>makeSOCKcluster</code> will display an Rscript command to execute in a terminal on the specified machine, and then it will wait for you to execute that command. In other words, <strong>makeSOCKcluster will hang until you manually start the worker on host 192.168.128.24</strong>, in your case. Remember that this is a troubleshooting technique, not a solution to the problem, and the hope is to get more information about why the workers aren't starting by trying to start them manually.</p>

<p>Obviously, the use of manual mode bypasses any ssh issues (since you're not using ssh), so if you can create a SOCK cluster successfully in manual mode, then probably ssh is your problem. If the Rscript command isn't found, then either R isn't installed, or it's installed in a different location. But hopefully you'll get some error message that will lead you to the solution.</p>

<p>If <code>makeSOCKcluster</code> still just hangs after you've executed the specified Rscript command on the specified machine, then you probably have a networking or firewall issue.</p>

<p>For more troubleshooting advice, see my answer for <a href=""https://stackoverflow.com/a/17925618/2109128"">making cluster in doParallel / snowfall hangs</a>.</p>
"
28006813,How I can find out 1st and last observation with in group in R for every by group,1,1,1,"<p>Hi my data set is as follows</p>

<pre><code>dialled     Ringing     state   duration
NA  NA  NA  0
NA  NA  NA  0
NA  NA  NA  0
NA  NA  NA  0
123 NA  NA  0
123 NA  NA  0
123 NA  NA  0
123 NA  NA  60
NA  NA  active  0
NA  NA  active  0
NA  NA  inactive    0
NA  NA  inactive    0
NA  145 inactive    0
NA  145 inactive    0
NA  145 inactive    56
NA  NA  active  0
NA  NA  active  0
NA  NA  inactive    0
222 NA  inactive    0
222 NA  inactive    0
222 NA  inactive    37
NA  NA  active  0
NA  NA  active  0
NA  NA  inactive    0
123 NA  inactive    0
123 NA  inactive    0
123 NA  active  60
NA  NA  active  0
</code></pre>

<p>I want to get 1st and last obs. for every <code>dialled</code> number (repeated one as well, because every call is different). Answer I am looking for is</p>

<pre><code>dialled     Ringing     state   duration
123 NA  NA  0
123 NA  NA  60
222 NA  inactive    0
222 NA  inactive    37
123 NA  NA  0
123 NA  NA  60   
</code></pre>

<p>I was using the following</p>

<pre><code>library(plyr)
ddply(DF, .(Dialled_nbr), function(x) x[c(1,nrow(x)), ]) which gave me

dialled     Ringing     state   duration
123 NA  NA  0
123 NA  NA  60
222 NA  inactive    0
222 NA  inactive    37
</code></pre>

<p>But answer is not correct. Please help
</p>

<p><strong>New data is</strong></p>

<pre>

dialled     Ringing     state   duration
123 NA  NA  0
123 NA  NA  0
123 NA  NA  60
123 NA  NA  0
123 NA  NA  0
123 NA  NA  70
222 NA  inactive    0
222 NA  inactive    0
222 NA  inactive    37
123 NA  inactive    0
123 NA  inactive    0
123 NA  active  60


Answer to be
dialled     Ringing     state   duration
123 NA  NA  0
123 NA  NA  60
123 NA  NA  0
123 NA  NA  70
222 NA  inactive    0
222 NA  inactive    37
123 NA  inactive    0
123 NA  active  60
</pre>
","<p>Here is an option with <a href=""https://github.com/Rdatatable/data.table/"" rel=""nofollow""><code>data.table_1.9.5</code></a>.  Create the ""data.table"" from ""data.frame"" using <code>setDT</code>, remove the <code>NA</code> values in ""dialled"" column (<code>!is.na(dialled)</code>), generate grouping variable by using <code>rleid</code> on ""Dialled_nbr"", get the row index of the first and last rows for the levels of grouping variable (<code>.I(c(1L, .N)]</code>), finally subset the ""dt1"" based on the row index.</p>

<pre><code>library(data.table)
dt1 &lt;- setDT(df)[!is.na(dialled)]
dt1[dt1[,.I[c(1L, .N)],rleid(dialled)]$V1]
#    dialled Ringing    state duration
#1:     123      NA       NA        0
#2:     123      NA       NA       60
#3:     222      NA inactive        0
#4:     222      NA inactive       37
#5:     123      NA inactive        0
#6:     123      NA   active       60
</code></pre>

<p>Or using <code>base R</code></p>

<pre><code>df1 &lt;- df[!is.na(df$dialled),]
grp&lt;-  inverse.rle(within.list(rle(df1$dialled), 
                    values &lt;- seq_along(values)))

df1[!duplicated(grp)|!duplicated(grp,fromLast=TRUE),]
#    dialled Ringing    state duration
#5      123      NA     &lt;NA&gt;        0
#8      123      NA     &lt;NA&gt;       60
#19     222      NA inactive        0
#21     222      NA inactive       37
#25     123      NA inactive        0
#27     123      NA   active       60
</code></pre>

<h3>Update</h3>

<p>Based on the new dataset, </p>

<pre><code>grp &lt;- cumsum(c(TRUE,df$duration[-nrow(df)]!=0))
df[!duplicated(grp)|!duplicated(grp,fromLast=TRUE),]
#   dialled Ringing    state duration
#1      123      NA     &lt;NA&gt;        0
#3      123      NA     &lt;NA&gt;       60
#4      123      NA     &lt;NA&gt;        0
#6      123      NA     &lt;NA&gt;       70
#7      222      NA inactive        0
#9      222      NA inactive       37
#10     123      NA inactive        0
#12     123      NA   active       60
</code></pre>

<h3>data</h3>

<pre><code> df &lt;- structure(list(dialled = c(NA, NA, NA, NA, 123L, 123L, 123L, 
 123L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 222L, 222L, 222L, 
 NA, NA, NA, 123L, 123L, 123L, NA), Ringing = c(NA, NA, NA, NA, 
 NA, NA, NA, NA, NA, NA, NA, NA, 145L, 145L, 145L, NA, NA, NA, 
 NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), state = c(NA, NA, NA, 
 NA, NA, NA, NA, NA, ""active"", ""active"", ""inactive"", ""inactive"", 
 ""inactive"", ""inactive"", ""inactive"", ""active"", ""active"", ""inactive"", 
 ""inactive"", ""inactive"", ""inactive"", ""active"", ""active"", ""inactive"", 
 ""inactive"", ""inactive"", ""active"", ""active""), duration = c(0L, 
 0L, 0L, 0L, 0L, 0L, 0L, 60L, 0L, 0L, 0L, 0L, 0L, 0L, 56L, 0L, 
 0L, 0L, 0L, 0L, 37L, 0L, 0L, 0L, 0L, 0L, 60L, 0L)), .Names = 
 c(""dialled"", ""Ringing"", ""state"", ""duration""), class = ""data.frame"", 
 row.names = c(NA, -28L))
</code></pre>

<h3>newdata</h3>

<pre><code> df &lt;- structure(list(dialled = c(123L, 123L, 123L, 123L, 123L, 123L, 
 222L, 222L, 222L, 123L, 123L, 123L), Ringing = c(NA, NA, NA, 
 NA, NA, NA, NA, NA, NA, NA, NA, NA), state = c(NA, NA, NA, NA, 
 NA, NA, ""inactive"", ""inactive"", ""inactive"", ""inactive"", ""inactive"", 
 ""active""), duration = c(0L, 0L, 60L, 0L, 0L, 70L, 0L, 0L, 37L, 
 0L, 0L, 60L)), .Names = c(""dialled"", ""Ringing"", ""state"", ""duration""
 ), class = ""data.frame"", row.names = c(NA, -12L))
</code></pre>
"
36560304,how to hide menuItem and its contain for particular user in shiny r?,2,2,2,"<p>I am creating my app where I want to hide particular menuItem and its contains depending upon user credential. I want to show everything for admin/tester but not all user. I found question in stack overflow, <a href=""https://stackoverflow.com/questions/33027756/hide-an-element-box-tabs-in-shiny-dashbaord"">Hide an element (box/tabs) in shiny dashbaord</a>, I am modified code as below </p>

<pre><code>library(shiny)
library(shinydashboard)
library(shinyjs)

ui &lt;- dashboardPage(

dashboardHeader(title = ""Set"")
,dashboardSidebar(
   sidebarSearchForm(label = ""Search..."", ""searchText"", ""searchButton""),
  sidebarMenu(
    menuItem(""Port"", tabName = ""P"", icon = icon(""cog""))
    ,menuItem(""Rank"", tabName = ""Rank"", icon = icon(""cog""))
    ,menuItem(""Mark"", tabName = ""Mark"", icon = icon(""bar-chart""))
    ,menuItem(""Read"", tabName = ""Read"", icon = icon(""file-pdf-o""))
    ,useShinyjs()
    ,menuItem(""Port, tabName = ""Ocean"", icon = icon(""heart""))
  )
  ,uiOutput('Super')
  ,uiOutput('AList')
  ,uiOutput('BList')
  ,uiOutput('DList')
  ,uiOutput('SList')
)


,dashboardBody(
....
)
)

server &lt;- shinyServer(function(input, output, session) {
  observeEvent(session$user,{
   if (session$user == ""tester"") return(NULL)
     hide(id = ""Port"", anim = TRUE)
 })
}

shinyApp(ui = ui, server = server) 
</code></pre>

<p>However, it is not working, Any tips ?</p>
","<p>So, your approach has one problem: The way that the <code>menuItems</code> look like in the final document.</p>

<p>The code you provided is saying: Hide the element with Id ""Port""!
 This would all be fine, if the <code>menuItems</code> actually had an Id, but when you look at them (right click + inspect) you'd see that's not the case. </p>

<p>Close inspection shows, that your <code>menuItems</code> can be identified in the document by tag name (= <code>a</code>) and the <code>data-value</code> (which correponds to the assigned <code>tabName</code>). This is the selecting argument for your hiding command. I don't know if Shinyjs offers searching by other attributes, but you might as well write the JS code youself. </p>

<p>In the code below, I faked the user login with a <code>textInput</code>. You can observe, that the first <code>menuItem</code> only shows, if you insert ""tester"" into the text field. </p>

<p>The way it's done: You send a message to the client to show/hide an item with certain <code>tabName</code>. The JS script searches through all 
<code>a</code> tags for the one with your given name stored in its <code>data-value</code> attribute. Hiding is done by <code>display: none</code>.</p>

<pre><code>library(shiny)
library(shinydashboard)

ui &lt;- dashboardPage(
  dashboardHeader(title = ""Set""),
  dashboardSidebar(
    sidebarSearchForm(label = ""Search..."", ""searchText"", ""searchButton""),
    tags$head(tags$script(HTML(""
      Shiny.addCustomMessageHandler('manipulateMenuItem', function(message){
        var aNodeList = document.getElementsByTagName('a');

        for (var i = 0; i &lt; aNodeList.length; i++) {
          if(aNodeList[i].getAttribute('data-value') == message.tabName) {
            if(message.action == 'hide'){
              aNodeList[i].setAttribute('style', 'display: none;');
            } else {
              aNodeList[i].setAttribute('style', 'display: block;');
            };
          };
        }
      });
    ""))),
    sidebarMenu(
      menuItem(""Port"", tabName = ""P"", icon = icon(""cog"")),
      menuItem(""Rank"", tabName = ""Rank"", icon = icon(""cog"")),
      menuItem(""Mark"", tabName = ""Mark"", icon = icon(""bar-chart"")),
      menuItem(""Read"", tabName = ""Read"", icon = icon(""file-pdf-o"")),
      menuItem(""Port"", tabName = ""Ocean"", icon = icon(""heart""))
    ),
    uiOutput('Super'),
    uiOutput('AList'),
    uiOutput('BList'),
    uiOutput('DList'),
    uiOutput('SList')
  ),
  dashboardBody(
    textInput(""user"", ""User ID fake."")
  )
)

server &lt;- function(input, output, session) {
  observeEvent(input$user,{
    if(input$user != ""tester""){
      session$sendCustomMessage(type = ""manipulateMenuItem"", message = list(action = ""hide"", tabName = ""P""))
    }else{
      session$sendCustomMessage(type = ""manipulateMenuItem"", message = list(action = ""show"", tabName = ""P""))
    }
 })
}

shinyApp(ui, server)
</code></pre>
"
8550563,R: combining mutiple library locations with most up-to-date packages,2,2,2,"<p>Question: How do I move all of the most up-to-date R packages into one simple location that R (and everything else) will use from now and forever for my packages?</p>

<p>I have been playing around with R on Ubuntu 10.04 using variously RGedit, RCmdr, R shell, and RStudio. Meanwhile, I have installed packages, updated packages, and re-updated packages via apt, synaptic, install.packages(), etc... which apparently means these packages get placed everywhere, and (with the occasional sudo tossed in) with different permissions.</p>

<p>Currently I have different versions of different (and repeated) packages in:</p>

<pre><code>/home/me/R/i486-pc-linux-gnu-library/2.10
/home/me/R/i486-pc-linux-gnu-library/2.14
/home/me/R/i486-pc-linux-gnu-library/
/usr/local/lib/R/site-library               
/usr/lib/R/site-library                     
/usr/lib/R/library 
</code></pre>

<p>First - I'm a single user, on a single machine - I don't want multiple library locations, I just want it to work.</p>

<p>Second - I am on an extremely slow connection, and can't keep just downloading packages repeatedly.</p>

<p>So - is there an easy way to merge all these library locations into one simple location? Can I just copy the folders over? </p>

<p>How do I set it in concrete that this is and always will be where anything R related looks for and updates packages?</p>

<p>This is maddening.</p>

<p>Thanks for your help.</p>
","<p>After piecing together various bits of info here goes: A complete moron's guide to the R packages directory organization:</p>

<p><em>NB1 - this is my experience with Ubuntu - your mileage may vary 
NB2 - I'm a single user on a single machine, and I like things simple.</em></p>

<p>Ubuntu puts anything installed via apt, or synaptic in:</p>

<pre><code>/usr/lib/R/site-library                     
/usr/lib/R/library
</code></pre>

<p>directories. The default vanilla R install will try install packages here:</p>

<pre><code>/usr/local/lib/R/site-library
</code></pre>

<p>Since these are system directories the user does not have write privileges to, depending on what method you are interacting with R you might be prompted with a friendly - <em>""Hey buddy - we can't write there, you want us to put your packages in your home directory?""</em> which seems innocent and reasonable enough... assuming you never change your GUI, or your working environment. If you do, the new GUI / environment might not be looking in the directory where the packages were placed, so won't find them. (Most interfaces have a way for you to point where your personal library of packages is, but who wants to muck about in config files?)</p>

<p>What seems to be the best practice for me (and feel free to correct me if I'm wrong) with a default install setup on Ubuntu, is to do any package management from a basic R shell as sudo: <code>&gt; sudo R</code> and from there do your <code>install.packages()</code> voodoo. This seems to put packages in the <code>usr/local/lib/R/site-library</code> directory.</p>

<p>At the same time, <code>update.packages()</code> will update the files in <code>/usr/lib/R/site-library</code> and <code>usr/lib/R/library</code> directories, as well as <code>usr/local/lib/R/site-library</code></p>

<p>(As for <code>usr/lib/R/</code> division, it looks like <code>/library/</code> has the core packages, while <code>/site-library/</code> holds anything added, assuming they were installed by apt...)</p>

<p>Any packages previously installed and in the wrong place can be moved to the <code>/usr/local/lib/R/site-library</code> directory (assuming you are <code>sudo</code>ing it) just by moving the directories (thanks @Tommy), but as <code>usr/lib/R/</code> is controlled by apt - best not add or subtract anything from there...</p>

<p>Whew. Anyway - simple enough, and in simple language. Thanks everyone for the help.</p>
"
8860470,How to output a message in snowfall?,2,2,2,"<p>I am conducting a simulation study using snowfall package on Windows 7. </p>

<p>I like to print out a message for every 10 runs to main R console to monitor the progress, but it fails to do so. ie. nothing is printed</p>

<p>Any help will be much appreciated.</p>

<pre><code>runsim = function(nsim,n,mean,var){
cov = 0
for(i in 1:nsim){
if ( i %% 10==0) 
cat(""\n Running simulation"",i)
dat = function1(n,mean,var)

cov = ...
}
cov / nsim
}
sfExport(""function1"",""runsim"")
sfLibrary(library1)

wrapper = function(n){
runsim(100,n,0.5,0.25)
}

Out&lt;-sfLapply(1:100,wrapper)
</code></pre>
","<p>Check <code>?sfCat</code> and find the line where it says:</p>

<blockquote>
  <p>sfCat is a debugging function printing a message on all slaves (which appear in the logfiles). </p>
</blockquote>

<p>This tells us that we need to enable logging in the call to <code>sfInit</code> (argument <code>slaveOutfile</code>). Then, each call to <code>sfCat()</code> will dump stuff to the log file you specified. It took me a while to figure that out as well ;-)    </p>

<h2>Code Example</h2>

<pre><code>if (!require(""snowfall"")) {
    install.packages(""snowfall"")
    require(""snowfall"")
}

sfInit(parallel=TRUE, cpus=4, slaveOutfile=""test.txt"")
sfLibrary(""snowfall"", character.only=TRUE)

res &lt;- sfClusterApplyLB(1:100, function(x) {
    sfCat(paste(""Iteration "", x), sep=""\n"")
})
sfStop()
</code></pre>

<h2>Output of <code>./test.txt</code></h2>

<pre><code>[...]

Calling a snowfall function without calling 'sfInit' first or after sfStop().
'sfInit()' is called now.
snowfall 1.84 initialized: sequential execution, one CPU.

Iteration  4
Type: EXEC 
Iteration  92
Type: EXEC 
Iteration  94
Type: EXEC 
Iteration  96
Type: EXEC 
Iteration  98
Type: EXEC 
Iteration  100
ype: EXEC 
Iteration  29
Type: EXEC 
Iteration  31
Type: EXEC 
Iteration  33
Type: EXEC 
Iteration  35
Type: EXEC 
Iteration  37
Type: EXEC 
Iteration  39
Type: EXEC 
Iteration  41
Type: EXEC 
Iteration  43
Type: EXEC 
Iteration  45
Type: EXEC 
Iteration  47
Type: EXEC 
Iteration  49
Type: EXEC 
Iteration  51
Type: EXEC 
Iteration  53
Type: EXEC 
Iteration  55
Type: EXEC 
Iteration  57
Type: EXEC 
Iteration  59
Type: EXEC 
Iteration  61
Type: EXEC 
Iteration  63
Type: EXEC 
Iteration  65
Type: EXEC 
Iteration  67
Type: EXEC 
Iteration  69
Type: EXEC 
Iteration  71
Type: EXEC 
Iteration  73
Type: EXEC 
Iteration  75
Type: EXEC 
Iteration  77
Type: EXEC 
Iteration  79
Type: EXEC 
Iteration  81
Type: EXEC 
Iteration  83
Type: EXEC 
Iteration  85
Type: EXEC 
Iteration  87
Type: EXEC 
Iteration  89
Type: EXEC 
Iteration  91
Type: EXEC 
Iteration  93
Type: EXEC 
Iteration  95
Type: EXEC 
Iteration  97
Type: EXEC 
Iteration  99
 EXEC 
Iteration  74
Type: EXEC 
Iteration  76
Type: EXEC 
Iteration  78
Type: EXEC 
Iteration  80
Type: EXEC 
Iteration  82
Type: EXEC 
Iteration  84
Type: EXEC 
Iteration  86
Type: EXEC 
Iteration  88
Type: EXEC 
Iteration  90
</code></pre>

<h2>Own question</h2>

<p>Does anyone have a clue how to ""control"" the specific things that go to the log file? 
E.g., what would be nice is to include information about which worker finished which job etc.</p>
"
44023431,Clean way to do nested lazy evaluation in rlang,2,2,4,"<p>Let's say I have a function <code>f</code> that takes a bunch of arguments, along with an optional extra argument.</p>

<pre><code>f &lt;- function(..., extra)
{
    arglst &lt;- lapply(quos(...), get_expr)
    if(!missing(extra))
    {
        extra &lt;- get_expr(enquo(extra))
        arglst &lt;- c(arglst, extra=extra)
    }
    arglst
    ## ... do something with argument list ... ##
}

f(a, extra=foo)
# [[1]]
# a
# 
# $extra
# foo
</code></pre>

<p>Note that I don't want to evaluate the arguments as such, but I do want to get the expressions that were passed in, to be evaluated by other code down the line.</p>

<p>The new <a href=""https://cran.r-project.org/web/packages/rlang/index.html"" rel=""nofollow noreferrer"">rlang</a> package (which powers the next version of dplyr, to be released on CRAN Real Soon Now) provides extensive facilities for lazy evaluation which I'm using in <code>f</code> above. For example <code>quos</code>, <code>get_expr</code> and <code>enquo</code> are all functions from rlang.</p>

<p>In <code>f</code>, the part where I process <code>extra</code> is actually boilerplate code: I'll want to do this in other functions, not just in <code>f</code>. I don't want to rewrite it each time, so I thought I'd put it into its own function:</p>

<pre><code>doExtra &lt;- function(arglst, extra)
{
    if(!missing(extra))
    {
        extra &lt;- get_expr(enquo(extra))
        arglst &lt;- c(arglst, extra=extra)
    }
    arglst
}

f2 &lt;- function(..., extra)
{
    arglst &lt;- lapply(quos(...), get_expr)
    arglst &lt;- doExtra(arglst, extra)
    arglst
}
</code></pre>

<p>The problem is that when I do it this way, the value of <code>extra</code> that <code>doExtra</code> sees is what's passed in from <code>f2</code>, not the original:</p>

<pre><code>f2(a, extra=foo)
# [[1]]
# a
#
# $extra
# extra
</code></pre>

<p>How can I modify <code>f</code> to isolate the boilerplate code, without getting the wrong result? I can do something like manipulate the environment of <code>doExtra</code>'s calling frame directly, but that would be exceedingly ugly.</p>
","<ul>
<li><p>To forward a named argument to another enquoting function, you have to enquote then unquote: <code>!! enquo(arg)</code>. If you just pass <code>enquo(arg)</code>, the enquoting function will see just that: <code>enquo(arg)</code>. If you pass the argument symbol, that's what it will see as well. That's why you need to unquote within the argument it captures.</p>

<p><code>!! enquo(arg)</code> triggers the evaluation of <code>enquo(arg)</code>, which returns the expression supplied to the <code>arg</code> argument. Then it is unquoted inside the argument captured by your function.</p></li>
<li><p>If you are enquoting a potentially missing argument, it's better to enquote it and then check for missingness with <code>quo_is_missing()</code>. Enquoting a missing argument creates the same object returned by calling <code>quo()</code> without argument.</p></li>
<li><p>If you don't need quosures, you can use <code>exprs()</code> and <code>enexpr()</code>. However you're losing the environment and you are making further evaluation brittle.</p>

<p>If you are capturing the environment in some other way to evaluate it with <code>base::eval()</code> or similar, please note that quosures can contain other quosures. Only <code>eval_tidy()</code> will understand these nested quosures.</p></li>
</ul>

<p>IIUC your question, it's about passing an argument that should be enquoted to another function. One way to do it is to capture in the first function, then pass by value to the second function:</p>

<pre><code>library(""purrr"")
library(""rlang"")

f &lt;- function(..., extra) {
  exprs &lt;- exprs(...)

  # Pass the enquoted argument by value
  exprs &lt;- extra_by_value(exprs, enexpr(extra))

  exprs
}
extra_by_value &lt;- function(exprs, extra) {
  if (!is_missing(extra)) {
    c(exprs, extra = extra)
  } else {
    exprs
  }
}
</code></pre>

<p>If the second function has to take by expression rather than by value (perhaps because it is another user-facing verb), you have to unquote the enquoted expression:</p>

<pre><code>f &lt;- function(..., extra) {
  exprs &lt;- exprs(...)

  # Since the argument is captured by the function, we need
  # to unquote the relevant expression into the argument:
  exprs &lt;- extra_by_expression(exprs, !! enexpr(extra))

  exprs
}
extra_by_expression &lt;- function(exprs, extra) {
  extra &lt;- enexpr(extra)
  if (!is_missing(extra)) {
    c(exprs, extra = extra)
  } else {
    exprs
  }
}
</code></pre>

<p>All those concepts apply to quosures. Here is the equivalent code:</p>

<pre><code>f &lt;- function(..., extra) {
  quos &lt;- quos(...)

  # Since the argument is captured by the function, we need
  # to unquote the relevant expression into the argument:
  quos &lt;- extra_by_expression(quos, !! enquo(extra))

  quos
}
extra_by_expression &lt;- function(quos, extra) {
  extra &lt;- enquo(extra)
  if (!quo_is_missing(extra)) {
    c(quos, extra = extra)
  } else {
    quos
  }
}
</code></pre>

<p>It is almost always better to use quosures than raw expressions because they keep track of their context.</p>
"
29941797,.onLoad failed in loadNamespace() for 'rJava' when installing a package,2,2,2,"<p>I have a package ""javaOnLoadFailed"" (just a minimal package for testing my issue, hence the weird name) which imports rJava. I get 'rJava' errors when I try to either check() or install() the package, even though require(rJava) itself works fine. </p>

<p>install() gives the following errors:</p>

<pre><code>&gt; install()
Installing javaOnloadFailed
""C:/Program Files/R/R-3.2.0/bin/x64/R"" --no-site-file --no-environ --no-save  \
--no-restore CMD INSTALL  \
""C:/Projects/stackoverflow/javaOnloadFailed/javaOnLoadFailed""  \
--library=""C:/Users/adb2018/Documents/R/win-library/3.2"" --with-keep.source  \
--install-tests 

* installing *source* package 'javaOnloadFailed' ...
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
*** arch - i386
Error : .onLoad failed in loadNamespace() for 'rJava', details:
  call: inDL(x, as.logical(local), as.logical(now), ...)
  error: unable to load shared object 'C:/Users/adb2018/Documents/R/win-library/3.2/rJava/libs/i386/rJava.dll':
  LoadLibrary failure:  %1 is not a valid Win32 application.

Error: loading failed
Execution halted
*** arch - x64
ERROR: loading failed for 'i386'
* removing 'C:/Users/adb2018/Documents/R/win-library/3.2/javaOnloadFailed'
Error: Command failed (1)
</code></pre>

<p>I am using R 3.2.0 from within Architect, with sessionInfo():</p>

<pre><code>R version 3.2.0 (2015-04-16)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] devtools_1.7.0.9000 rj_2.0.3-2         

loaded via a namespace (and not attached):
[1] tools_3.2.0   rj.gd_2.0.0-1
</code></pre>
","<p>The Java environment variable is empty</p>

<pre><code>&gt; Sys.getenv('JAVA')
[1] """"
</code></pre>

<p>Based on a <a href=""http://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/"">suggestion</a>, I tried setting the JAVA environment variable to point to the 64 Bit version of Java (because I am running R 64 bit, as you could see from the sessionInfo, but that doesn't work:</p>

<pre><code>&gt; Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_45')
&gt; install()
Installing javaOnloadFailed
""C:/Program Files/R/R-3.2.0/bin/x64/R"" --no-site-file --no-environ --no-save  \
  --no-restore CMD INSTALL  \
  ""C:/Projects/stackoverflow/javaOnloadFailed/javaOnLoadFailed""  \
  --library=""C:/Users/adb2018/Documents/R/win-library/3.2"" --with-keep.source  \
  --install-tests 

* installing *source* package 'javaOnloadFailed' ...
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
*** arch - i386
Error : .onLoad failed in loadNamespace() for 'rJava', details:
  call: inDL(x, as.logical(local), as.logical(now), ...)
  error: unable to load shared object 'C:/Users/adb2018/Documents/R/win-library/3.2/rJava/libs/i386/rJava.dll':
  LoadLibrary failure:  %1 is not a valid Win32 application.

Error: loading failed
Execution halted
*** arch - x64
ERROR: loading failed for 'i386'
* removing 'C:/Users/adb2018/Documents/R/win-library/3.2/javaOnloadFailed'
Error: Command failed (1)
</code></pre>

<p>I then tried to set the JAVA environment variable such that it points to the 32 bit version of Java on my system, and then it works!</p>

<pre><code>&gt; Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre1.8.0_45\\')
&gt; install()
Installing javaOnloadFailed
""C:/Program Files/R/R-3.2.0/bin/x64/R"" --no-site-file --no-environ --no-save  \
  --no-restore CMD INSTALL  \
  ""C:/Projects/stackoverflow/javaOnloadFailed/javaOnLoadFailed""  \
  --library=""C:/Users/adb2018/Documents/R/win-library/3.2"" --with-keep.source  \
  --install-tests 

* installing *source* package 'javaOnloadFailed' ...
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
*** arch - i386
*** arch - x64
* DONE (javaOnloadFailed)

[INFO] Updating the R environment index started...

[INFO] The R environment index was updated successfully.
</code></pre>

<p>I don't quite understand why I need to point to Java 32 bit to make R 64 bit work, but that's what seems to be the case.</p>

<p>By the way, don't stumble over my package name ""javaOnLoadFailed"". I just created a minimal package with that name to test the problem.</p>
"
29082935,R Studio - Cloning local repository,2,2,2,"<p>I want to create a master repository on our server, from which I can clone a local version onto my computer.</p>

<p>I am using R Studio v0.98.994.</p>

<p>So far, this is what I have tried doing:</p>

<p>Create a folder for the master repository to live in.  I do this using 'new project' in R studio, and tell it to make a git repository.</p>

<p>I can then open up another new project, located on my C drive, and use R studio to clone, by telling it to open an existing project and setting the URL as the location of the master project.</p>

<p>However, then when I make changes and commit to my local repository (which works fine) I cannot push to the master repository, I get an error exactly as described in this question:  <a href=""https://stackoverflow.com/questions/8985782/git-push-fails-refusing-to-update-checked-out-branch-refs-heads-master"">git push fails: `refusing to update checked out branch: refs/heads/master`</a></p>

<p>So it appears that R Studio creates non-bare repositories?</p>

<p>Now I thought, well okay, I will use git bash to initialise the repository and then connect to that within R studio.</p>

<p>I do so, but cannot then find a way to use that repository in R Studio.</p>

<p>I am very new to Git, so it is entirely probable that this is one of those 'read the instructions' questions, in which case I am very sorry - and could someone possibly point me towards some guidance for this situation?  I have spent the better half of a day googling around this error and haven't yet managed to pull together the pieces :(  I also apologise; this doesn't feel like a very reproducible question.</p>
","<p>It sounds like you are using Windows Git, with a setup on a local Windows machine (C: drive) and a server of some kind, mounted as the S: drive. There's a few things you should be aware of when doing this.</p>

<h3>Shared Repositories</h3>

<p>If you are intending for multiple people to share the same repository, you want to initiate a <em>shared</em> repository. See the --shared option in <a href=""http://git-scm.com/docs/git-init"" rel=""nofollow"">git-init</a> for more details. Note that I'm not sure how having your repository on a Windows machine affects the sharing options. If you are just trying to keep your repository in two places, that makes things a lot easier.</p>

<h3>Bare Repositories</h3>

<p>Separate from the discussion of sharing is the discussion of bare repositories. If you don't intend to ever work with files in the server (i.e. it's just going to be a place to push changes so they are safely stored), you could initialize a <em>bare repository</em>. A bare repository contains the database structure of Git, but does not have the actual files in the directory.</p>

<p>A standard Git repository is a directory with a hidden folder in it named <code>.git</code>. This <code>.git</code> folder contains all the various data structures that Git uses to track changes. A bare repository is essentially a folder containing only the contents of <code>.git</code>.</p>

<p>The good thing about a bare repository is that no one can work in the repository itself (since there is no working directory, just the database). This means that no one could log into S: and edit the repository themselves. Instead, they would have to clone the repository, then push their changes back to the origin. The <a href=""http://www.gitguys.com/topics/shared-repositories-should-be-bare-repositories/"" rel=""nofollow"">GitGuys</a> have a good article about why this is ideal.</p>

<p>Note that shared repos and bare repos are not dependent or mutually exclusive. As a general practice, if you are having a ""server repo"" from which you pull and to which you push, you should have it be bare, regardless of whether the project is shared.</p>

<h3>A Non-Shared Workflow</h3>

<p>Since it's not clear if you are sharing or not sharing and you're on a Windows environment, which I don't know about from a sharing standpoint, I'm going to give you a simple example. Using git-bash, you should be able to change directories to wherever on S: you have your repositories. Then, use <code>git init</code> with the bare options as described by the link above to initialize a bare repository. Navigate to where you want your repository to live on C:, and then do <code>git clone</code> to get a working copy.</p>

<p>Add a README file or something else so you can do your initial commit, and then commit and do <code>git push origin master</code> to push your changes to the S: repository. Once all that is done, THEN initialize the RStudio Git project. RStudio should defer to your existing configuration, and things should <em>hopefully</em> work.</p>
"
44475877,Parsing versus Scraping in R,2,2,4,"<p>I am new to R and have a question about the most efficient way to contruct a database. I would like to build a database of NFL statistics. These statistics are readily available on the web at a number of locations, but I've found the most thorough analysis to be on Pro-Football-Reference (<a href=""http://www.pro-football-reference.com/"" rel=""nofollow noreferrer"">http://www.pro-football-reference.com/</a>). This will be panel data where the time intervals are each week of each season, my observations are each player in each game, and my collumns are the statistics tallied in all of the tables of Pro-Football-Reference's boxscores (<a href=""http://www.pro-football-reference.com/boxscores/201702050atl.htm"" rel=""nofollow noreferrer"">http://www.pro-football-reference.com/boxscores/201702050atl.htm</a>).</p>

<p>I could scrape each table of each game of each season with something like:</p>

<pre><code>#PACKAGES
library(rvest)
library(XML)
page.201702050atl = read_html(""http://www.pro-football-reference.com/boxscores/201702050atl.htm"")
comments.201702050atl = page.201702050atl %&gt;% html_nodes(xpath = ""//comment()"")
scoring.201702050atl = readHTMLTable(""http://www.pro-football-reference.com/boxscores/201702050atl.htm"", which = 2)
game.info.201702050atl = comments.201702050atl[17] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#game_info"") %&gt;% html_table()
officials.201702050atl = comments.201702050atl[21] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#officials"") %&gt;% html_table()
team.stats.201702050atl = comments.201702050atl[27] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#team_stats"") %&gt;% html_table()
scorebox.201702050atl = readHTMLTable(""http://www.pro-football-reference.com/boxscores/201702050atl.htm"", which = 1)
expected.points.201702050atl = comments.201702050atl[22] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#expected_points"") %&gt;% html_table()
player.offense.201702050atl = comments.201702050atl[31] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#player_offense"") %&gt;% html_table()
player.defense.201702050atl = comments.201702050atl[32] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#player_defense"") %&gt;% html_table()
returns.201702050atl = comments.201702050atl[33] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#returns"") %&gt;% html_table()
kicking.201702050atl = comments.201702050atl[34] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#kicking"") %&gt;% html_table()
home.starters.201702050atl = comments.201702050atl[35] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#home_starters"") %&gt;% html_table()
vis.starters.201702050atl = comments.201702050atl[36] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#vis_starters"") %&gt;% html_table()
home.snap.counts.201702050atl = comments.201702050atl[37] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#home_snap_counts"") %&gt;% html_table()
vis.snap.counts.201702050atl = comments.201702050atl[38] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#vis_snap_counts"") %&gt;% html_table()
targets.directions.201702050atl = comments.201702050atl[39] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#targets_directions"") %&gt;% html_table()
rush.directions.201702050atl = comments.201702050atl[40] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#rush_directions"") %&gt;% html_table()
pass.tackles.201702050atl = comments.201702050atl[41] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#pass_tackles"") %&gt;% html_table()
rush.tackles.201702050atl = comments.201702050atl[42] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#rush_tackles"") %&gt;% html_table()
home.drives.201702050atl = comments.201702050atl[43] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#home_drives"") %&gt;% html_table()
vis.drives.201702050atl = comments.201702050atl[44] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#vis_drives"") %&gt;% html_table()
pbp.201702050atl = comments.201702050atl[45] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#pbp"") %&gt;% html_table()
</code></pre>

<p>However, the number of lines of code needed to clean up each scraped table for 256 games each year seems to suggest a more efficient method might exist.</p>

<p>The NFL officially records stats in their game books (<a href=""http://www.nfl.com/liveupdate/gamecenter/57167/ATL_Gamebook.pdf"" rel=""nofollow noreferrer"">http://www.nfl.com/liveupdate/gamecenter/57167/ATL_Gamebook.pdf</a>). Since sites, like Pro-Football-Reference, include stats not tallied in the official game books, and since the identifying anguage necessary to do so is included in the game books' Play-by-Play, I deduce they are running a function to parse the Play-by-Play and tally their statistics. New as I am, I've never written a function or parsed anything in R before; but, I'd imagine one function I can apply to every game book would be a more efficient method than scraping each individual table. Am I on the right path here? I'd hate to invest a ton of effort in the wrong direction.</p>

<p>An additional problem arises because the game books are PDFs. Play-by-Plays exist on other websites in table format, but none are as complete. I've read some excellent tutorials on this site about how to convert a PDF into text using</p>

<pre><code>library(tm)
</code></pre>

<p>But, I've not yet figured it out for my own purposes.</p>

<p>Once I convert the entire PDF to text, do I simply identify the Play-by-Play portion, parse it out, and from there parse out each statistic? Are there adittional obstacles my limitted experience has prevented me from forseeing?</p>

<p>This might be too ""beginner"" of a question for this website; but, could anyone set me up here? Or, provide me with a resource that could? Thanks so much for the help.</p>
","<p>Consider generalizing your one game parsing to all games by storing html tables in a growing list for all 256 games. Below is example for Week 1.</p>

<pre><code>doc &lt;- htmlParse(readLines(""http://www.pro-football-reference.com/years/2016/week_1.htm""))

# EXTRACT ALL GAME PAGES
games &lt;- xpathSApply(doc, ""//a[text()='Final']/@href"")

# FUNCTION TO BUILD HTML TABLE LIST
getwebdata &lt;- function(url) {
    print(url)
    boxscoreurl &lt;- paste0(""http://www.pro-football-reference.com"", url)
    page &lt;- read_html(boxscoreurl)
    comments &lt;- page %&gt;% html_nodes(xpath = ""//comment()"")

    list(
      scoring = readHTMLTable(boxscoreurl, which = 2),
      game.info = comments[17] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#game_info"") %&gt;% html_table(),
      officials = comments[21] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#officials"") %&gt;% html_table(),
      team.stats = comments[27] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#team_stats"") %&gt;% html_table(),
      scorebox = readHTMLTable(boxscoreurl, which = 1),
      expected.points = comments[22] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#expected_points"") %&gt;% html_table(),
      player.offense = comments[31] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#player_offense"") %&gt;% html_table(),
      player.defense = comments[32] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#player_defense"") %&gt;% html_table(),
      returns = comments[33] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#returns"") %&gt;% html_table(),
      kicking = comments[34] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#kicking"") %&gt;% html_table(),
      home.starters = comments[35] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#home_starters"") %&gt;% html_table(),
      vis.starters = comments[36] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#vis_starters"") %&gt;% html_table(),
      home.snap.counts = comments[37] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#home_snap_counts"") %&gt;% html_table(),
      vis.snap.counts = comments[38] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#vis_snap_counts"") %&gt;% html_table(),
      targets.directions = comments[39] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#targets_directions"") %&gt;% html_table(),
      rush.directions = comments[40] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#rush_directions"") %&gt;% html_table(),
      pass.tackles = comments[41] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#pass_tackles"") %&gt;% html_table(),
      rush.tackles = comments[42] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#rush_tackles"") %&gt;% html_table(),
      home.drives = comments[43] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#home_drives"") %&gt;% html_table(),
      vis.drives = comments[44] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#vis_drives"") %&gt;% html_table(),
      pbp = comments[45] %&gt;% html_text() %&gt;% read_html() %&gt;% html_node(""#pbp"") %&gt;% html_table()
    )
}

# ALL WEEK ONE LIST OF HTML TABLE(S) DATA
week1datalist &lt;- lapply(games, getwebdata)

# TRY/CATCH VERSION (ANY PARSING ERROR TO RETURN EMPTY LIST)
week1datalist &lt;- lapply(games, function(g) {
   tryCatch({ return(getwebdata(g)) 
      }, error = function(e) return(list())
})

# NAME EACH LIST ELEMENT BY CORRESPONDING GAME
shortgames &lt;- gsub(""/"", """", gsub("".htm"", """", games))
week1datalist &lt;- setNames(week1datalist, shortgames)
</code></pre>

<p>Ultimately, you can then reference one game's specific stats table by name:</p>

<pre><code>week1datalist$boxscores201609080den$scoring

week1datalist$boxscores201609110atl$game.info
</code></pre>

<p>Also, you might need to include <a href=""https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r""><code>tryCatch</code></a> in <code>lapply</code> since some pages may not be consistent.</p>
"
29220167,What is the status of long-running remote R sessions in ESS/Emacs?,2,2,2,"<p>I routinely run R remotely and have had great success with RStudio server to do so.  However, Emacs/ESS is still preferable in many cases, particularly since I often work on multiple projects simultaneously.  What is the start-of-the-art when running ESS/R in emacs when the expectation is that the connection will be broken?  To be more concrete, I'd love to run a tmux session in Emacs so that I can connect to a long-running R session running in tmux (or screen).  What is the status of ESS/Emacs to support such a scenario?  This seems to be changing over time and I haven't found the ""definitive"" approach (perhaps there isn't one).</p>
","<p>I do that all the time.  At both home, and work.  </p>

<p>Key components:</p>

<ol>
<li><p>Start emacs in daemon mode:  <code>emacs --daemon &amp;</code>. Now emacs is long-running and persistent as it is disconnected from the front-end.</p></li>
<li><p>Connect using <code>emacsclient -nw</code> in text mode using tmux (or in my case, the byobu wrapper around tmux). As tmux persists, I can connect, disconnect, reconnect,... at will while having several tabs, split panes, ... from byobu/tmux.</p></li>
<li><p>When nearby -- on home desktop connecting to home server, or at work with several servers -- connect via <code>emacsclient -c</code>. Now I have the standard X11 goodness, plotting etc pp.  That is my default 'working' mode.</p></li>
<li><p>But because each emacs session has an R session (or actually several, particularly at work) I can actually get to them as I can ssh into the tmux/byobu session too.</p></li>
<li><p>Another nice feature is tramp-mode allowing you to edit a remote file (possibly used by a remote R session) in a local Emacs buffer as tramp wraps around ssh and scp making the remote file appear local.</p></li>
<li><p>Last but not least mosh is very nice on the (Ubuntu) laptop as it automagically resumes sessions when I am back on the local network at home or work.  In my case mosh from Debian/Ubuntu on server and client; may also work for you OS X folks.</p></li>
</ol>

<p>In short, works like a dream, but may require the extra step of ""disconnecting"" emacs from the particularly tmux shell in which you launch. Daemon mode is key. Some of these sessions run on for weeks.</p>

<p>I started working like this maybe half a decade ago. Possibly longer.  But using ESS to connect to remote Emacs session is much older -- I think the ESS manual already had entries for it when I first saw it in the late 1990s.  </p>

<p>But I find this easier as it gives me ""the whole emacs"" including whatever other buffers and session I may need.</p>

<p><strong>Edit:</strong> And just to be plain, I also use RStudio (Server) at home and work, but generally spend more time in Emacs for all the usual reasons.  </p>

<p><strong>More Edits:</strong> In follow-up to <a href=""https://twitter.com/kjhealy/status/580150117564571649"" rel=""nofollow"">@kjhealy</a> I added that I am also a fan of both tramp-mode (edit remote files locally in Emacs thanks to the magic that are ssh and scp) as well as mosh (sessions that magically resume when I get to work or back home).</p>
"
7121613,Recommendations using R with SimpleDB or BigQuery or using PHP with SimpleDB,2,2,2,"<p>I am currently working on system that generated product recommendations like those on <strong>Amazon</strong> : ""People who bought this also bought this..""</p>

<p>Current Scenario:</p>

<ul>
<li><p>Extract the Google Analytics data of the client and insert it in database.</p></li>
<li><p>On the website of the client, on load of product page the API call is made to get the recommendations of the product being viewed.</p></li>
<li><p>When API receives the product ID as request it looks in the database and retrieves (using association rules) the recommended product IDs and sends them as response.</p></li>
<li><p>The list of these product Ids will be processed to get the product details(image,price..) at the client end and displayed on website. </p></li>
<li><p>Currently I am using PHP and MYSQL with gapi package and REST api
storage on AMAZON EC2 .</p></li>
</ul>

<p><strong>My Question is:</strong>
Now, if I have to choose amongst the following, which will be the best choice to implement the above mentioned concept.</p>

<ul>
<li><p>PHP with SimpleDB or BIGQuery.</p></li>
<li><p>R language with BIGQuery.</p></li>
<li><p>RHIPE-(R and hadoop ) with SimpleDB.</p></li>
<li><p>Apache Mahout.</p></li>
</ul>

<p>Plese help!</p>
","<p>This isn't so easy to answer, because the constraints are fairly specialized.</p>

<p>The following considerations can be made, though:</p>

<ol>
<li>BIGQuery is not yet public.  Thus, with a small usage base, even if you are in the preview population, it will be harder to get advice on improvement.</li>
<li>Each of your answers asked about a modeling system &amp; a storage system.  Apache Mahout is not a storage mechanism, so it won't necessarily work on its own.  I used to believe that its machine learning implementations were a a pastiche of a few Google Summer of Code, but I've updated that view on the suggestion of a commenter.  It still looks like it has rather uneven and spotty coverage of different algorithms, and it's not particularly clear how the components are supported or maintained.  I encourage an evangelist for Mahout to address this.</li>
</ol>

<p>As a result, this eliminates the 1st, 2nd, and 4th options.</p>

<p>What I don't quite get is the need for a real-time server to utilize Hadoop and RHIPE.  That should be done in your batch processing for developing the recommendation models, not in real-time.  I suppose you could use RHIPE as a simple one-stop front end for firing off queries.</p>

<p>I'd recommend using RApache instead of RHIPE, because you can get your packages and models pre-loaded.  I see no advantage to using Hadoop in the front end, but it would be a very natural back end system for the model fitting.  </p>

<p>(Update 1) Other interface options include RServe (http://www.rforge.net/Rserve/) and possibly RStudio in server mode.  There are R/PHP interfaces (see comments below), but I suspect it would be better to access R through HTTP or TCP/IP.</p>

<p>(Update 2) Addressing the whole process, the basic idea I see is that you could query the data from PHP and pass to R or, if you wish to query from within R, look at the link in the comments (to the OmegaHat tools) or post a new question about R &amp; SimpleDB - I'm sure someone else on SO would be able to give better insight on this particular connection.  RApache would let you instantiate many R processes already prepared with packages loaded and data in RAM; thus you would only need to pass whatever data needs to be used for prediction.  If your new data is a small vector then RApache should be fine, and it seems this is correct for the data being processed in real-time.</p>
"
37430604,Extracting the outer-boundary of a set of grid points with some missing grids (holes),1,3,3,"<p>This question is about generalization of <a href=""https://stackoverflow.com/questions/28538161/find-the-perimeter-of-a-subset-of-a-near-regular-grid-of-points/28551704?noredirect=1#comment62340906_28551704"">this</a> question. The mentioned question is working well for the point set with no hole. In the present question I want to get the perimeter (outer boundary) of a subset of a near-regular grid of points where some of the grid point with in the polygon are missing (i.e., polygon with hole).  </p>

<p>The sample data set on grids is available <a href=""https://spaces.hightail.com/space/lY7n0/fi-c047ae46-d139-451f-b71a-104c2c90d269/fv-58222d76-4022-4595-a5bc-e76b4e9663e4/data"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I used the R-code as suggested as an answer in the above mentioned question (with no holes).</p>

<p>The following is the output of using those codes:<a href=""https://i.stack.imgur.com/Ywe3u.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ywe3u.jpg"" alt=""plot""></a></p>

<p>Now I want it ignore holes in inside the point set and want to consider the outer boundary of the point set as the required polygon.</p>

<p>Any suggestion!! Thanks.</p>
","<p>This slight variant on my previous code works if there's holes by finding all the loops and taking only the one with the largest X coordinate, which must be the outside loop. Unless the loops touch... Strictly perhaps it should take the loop with the largest area... Note also the need to use X and Y in one of the functions because of a bug (I've reported) in the igraph package.</p>

<pre><code>perimeterGrid &lt;- function(pts, maxdist=6000, mindist=1){
    g = edgeP(makegrid(pts, maxdist=maxdist, mindist=mindist))

    ## there might be holes. Find the loop with the largest X coordinate.
    parts = components(g)
    outer = which.max(tapply(V(g)$x, parts$membership, function(x){max(x)}))

    g = induced_subgraph(g, which(parts$membership==outer))

    loop = graph.dfs(minimum.spanning.tree(g),1)$order
    cbind(V(g)$x, V(g)$y)[loop,]
}

# haversine distance matrix
dmat &lt;- function(pts){
    n=nrow(pts)
    do.call(rbind,lapply(1:n,function(i){distHaversine(pts[i,],pts)}))
}

# make the grid cells given a maxdist (and a mindist to stop self-self edges)    
makegrid &lt;- function(pts, maxdist=6000,  mindist=1){
    dists = dmat(pts)
    g = graph.adjacency(dists&lt;maxdist &amp; dists&gt;mindist,
        mode=""undirected"")
    ## use X and Y here not x and y due to igraph bug
    ## these get copied to the output later...
    V(g)$X=pts[,1]
    V(g)$Y=pts[,2]

    g
}

# clever function that does the grid edge count etc
edgeP &lt;- function(g){
    # find all the simple squares
    square=graph.ring(4)
    subs = graph.get.subisomorphisms.vf2(g,square)
    # expand all the edges
    subs = do.call(rbind, lapply(subs, function(s){
        rbind(s[1:2], s[2:3], s[3:4], s[c(4,1)])
    }))
    # make new graph of the edges of all the squares
    e = graph.edgelist(subs,directed=FALSE)
    # add the weight as the edge count
    E(e)$weight=count.multiple(e)

    # copy the coords from the source back
    V(e)$x=V(g)$X
    V(e)$y=V(g)$Y

    # remove multiple edges
    e=simplify(e)

    # internal edges now have weight 256.
    e = e - edges(which(E(e)$weight==256))
    # internal nodes how have degree 0
    e = e - vertices(degree(e)==0)
    return(e)
}
</code></pre>
"
38241122,Ubuntu remove .libPaths() in R,2,2,2,"<p>After I installed RStudio, I ran:</p>

<pre><code>library()
</code></pre>

<blockquote>
  <p>Warning message:
      libraries '/usr/local/lib/R/site-library', 'usr/lib/R/site-library' contain no packages</p>
</blockquote>

<p>Then I input:</p>

<pre><code>.libPaths()

[1] ""/home/avalon/R/x86_64-pc-linux-gun-library/3.2""
[2] ""/usr/local/lib/R/site-library""
[3] ""/usr/lib/R/site-library""
[4] ""/usr/lib/R/library""
</code></pre>

<p>How can I remove [2] and [3] to prevent the warning message appear again?</p>

<p>Expected output:</p>

<pre><code>.libPaths()

[1] ""/home/avalon/R/x86_64-pc-linux-gun-library/3.2""
[4] ""/usr/lib/R/library""
</code></pre>
","<h2>Linux</h2>

<p>First thing to do is read the manpage on it (<code>?.libPaths</code>), and you'll see:</p>

<blockquote>
  <p>'.libPaths' is used for getting or setting the library trees that R knows about (and hence uses when looking for packages).  If called with argument ‘new’, the library search path is set to the existing directories in <strong><code>unique(c(new, .Library.site, .Library))</code></strong> and this is returned.  If given no argument, a character vector with the currently active library trees is returned.</p>
</blockquote>

<p>(emphasis added). This should clue us in to wonder what <code>.Library.site</code> holds. Oddly enough, it holds system-wide (ergo ""site"") library paths that ""should"" always be kept, so they are always maintained.</p>

<p>It further goes on to say:</p>

<blockquote>
  <p>'.Library.site' can be set via the environment variable 'R_LIBS_SITE' (as a non-empty semicolon-separated list of library trees).</p>
</blockquote>

<p>So one way to fix it is to give it an empty string <strong><em>when you start R</em></strong> (cannot be done from within R):</p>

<pre><code># in bash on the command line:
$ R_LIBS_SITE="" "" R

# in R
R&gt; .libPaths()
[1] ""/usr/lib/R/library""
</code></pre>

<p>The way to get this to work with RStudio is by creating a <code>~/.Renviron</code> file with at least the following:</p>

<pre><code>R_LIBS_SITE="" ""
</code></pre>

<p>That done, you should not have to do anything further to remove the secondary site library paths from <code>.libPaths()</code>:</p>

<pre><code>R&gt; .libPaths()
[1] ""/usr/lib/R/library""
</code></pre>

<h2>Windows</h2>

<p>Assuming you're doing the following:</p>

<pre><code>R&gt; .libPaths(c(""/home/avalon/R/x86_64-pc-linux-gun-library/3.2"", .libPaths()))
[1] ""/home/avalon/R/x86_64-pc-linux-gun-library/3.2""
[2] ""/usr/local/lib/R/site-library""
[3] ""/usr/lib/R/site-library""
[4] ""/usr/lib/R/library""
</code></pre>

<p>If you want to correct it after you've done this, then just do:</p>

<pre><code>R&gt; .libPaths( c(.libPaths()[c(1,4)]) )
[1] ""/home/avalon/R/x86_64-pc-linux-gun-library/3.2""
[2] ""/usr/lib/R/library""
</code></pre>

<p>Alternatively, you can do it this way <em>the first time</em> (i.e., while it still has three elements, two of which are not good for you):</p>

<pre><code>R&gt; .libPaths(c(""/home/avalon/R/x86_64-pc-linux-gun-library/3.2"", .libPaths()[3]))
[1] ""/home/avalon/R/x86_64-pc-linux-gun-library/3.2""
[2] ""/usr/lib/R/library""
</code></pre>

<p>There is certainly a way to filter the paths programmatically instead of blindly taking the 3rd element, but this should work for now.</p>
"
21081124,Running GUI analysis packages from RStudio server,2,2,2,"<p>RStudio server uses a headless R session and seems to pass all of the I/O operations encoded to save bandwidth. This works for everything except for packages like Rattle or Latticist, which work through their own GUI. Is there a way to use these packages through RStudio server or otherwise access the RStudio server R session to run these packages remotely?</p>

<p>Bonus if there's an efficient way to run these packages remotely without forwarding an X session over SSH.</p>
","<p>I'm not sure this is possible over the RStudio interface because of the way these graphical programs work. It's easy enough for RStudio to capture textual input and output for R. Capturing normal graphical output is pretty impressive, but that's done ""natively"" in R. Even packages like <code>ggplot2</code> and <code>lattice</code> use the builtin R plotting capabilities -- they do some rendering and data processing on their own, pass that onto <code>grid</code> and then <code>grid</code> renders the plots via R builtins when <code>plot()</code> or <code>print</code> is called (including implicitly in the REPL for interactive sessions). RCommander, RGL and the like use external libraries (Tcl/Tk, OpenGL), which render their interfaces directly over operating system services and not via R. R doesn't even see the output from these programs -- it only knows that the R wrapper function for these services hasn't returned yet. For local RStudio, this isn't a problem because the services are forwarded directly to the local display, but for RStudio server, <em>there is no display</em>!</p>

<p>Another consideration: assuming R could capture and forward X, that would imply having an X Server (in X, Server is the display/keyboard/etc, Client is the program that needs I/O) running in your browser. Modern JavaScript is pretty amazing at times, but X is a very complicated codebase and very sensitive to latency. Running X over the Internet is <strong>much</strong> slower than over the local network -- the protocol just wasn't designed for such things and most operations involve far too many roundtrips.  </p>

<p>On a more practical side, you can still do most of your work via RStudio and only do the graphical commands via X forwarding:</p>

<ol>
<li>Do everything that doesn't involve an external graphics interface.</li>
<li>Save your R Session (in the Environment tab or via the command line) as <code>.RData</code> in your project directory. (You can actually do this elsewhere, but it's generally more convenient if your workspace is saved in the working directory.)</li>
<li>Login in via SSH and X Forwarding and <code>cd</code> to the project directory.</li>
<li>Start R -- R will automatically load any existing workspaces saved as <code>.RData</code>. (You can disable this behavior with <code>--vanilla</code>. Depending on the size of your workspace, R may take a few seconds to a few minutes to load.</li>
<li>Have fun with Rattle, Latticist, RCommander, RGL, etc! Be ready for massive lag if you're doing this over the Internet and not the local network (see above).</li>
</ol>
"
45061272,"R and SSL/curl on Ubuntu linux: failed SSL connect in R, but works in curl",2,2,2,"<p>I'm a bit at a loss on how to further investigate this, so pointers would be highly appreciated. </p>

<p>I'm running Ubuntu 17.04, and I believe roughly since around my upgrade time (was running 16.10 before) I can no longer update (or use anything ""from the internet"") anything from within R -- it fails on SSL for everything. All of the ""normal"" SSL traffic outside of R works fine. </p>

<p>For instance, doing <code>install.packages(""curl"")</code>, I get this error message: </p>

<pre><code>Warning in install.packages :
URL 'https://cran.rstudio.com/src/contrib/PACKAGES.rds': status was 'SSL connect error'
Warning in install.packages :
URL 'https://cran.rstudio.com/src/contrib/PACKAGES.gz': status was 'SSL connect error'
Warning in install.packages :
URL 'https://cran.rstudio.com/src/contrib/PACKAGES': status was 'SSL connect error'
Warning in install.packages :
... [etc] ...
</code></pre>

<p>However, if I run <code>curl -v ""https://cran.rstudio.com/src/contrib/PACKAGES.rds"" -o test.curl</code> on command line, everything works. </p>

<pre><code>*   Trying 10.26.0.19...
* TCP_NODELAY set
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                             Dload  Upload   Total   Spent    Left  Speed
0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to (nil) (10.26.0.19) port 3128 (#0)
* Establish HTTP proxy tunnel to cran.rstudio.com:443
* Proxy auth using Basic with user '[redacted]'
&gt; CONNECT cran.rstudio.com:443 HTTP/1.1
&gt; Host: cran.rstudio.com:443
&gt; Proxy-Authorization: Basic [redacted]
&gt; User-Agent: curl/7.52.1
&gt; Proxy-Connection: Keep-Alive
&gt; 
&lt; HTTP/1.1 200 Connection established
&lt; 
* Proxy replied OK to CONNECT request
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /home/csafferling/programs/anaconda3/ssl/cacert.pem
  CApath: none
* TLSv1.2 (OUT), TLS header, Certificate Status (22):} [5 bytes data]
* TLSv1.2 (OUT), TLS handshake, Client hello (1):} [512 bytes data]
* TLSv1.2 (IN), TLS handshake, Server hello (2):{ [76 bytes data]
* TLSv1.2 (IN), TLS handshake, Certificate (11):{ [4787 bytes data]
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):{ [333 bytes data]
* TLSv1.2 (IN), TLS handshake, Server finished (14):{ [4 bytes data]
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):} [70 bytes data]
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):} [1 bytes data]
* TLSv1.2 (OUT), TLS handshake, Finished (20):} [16 bytes data]
* TLSv1.2 (IN), TLS change cipher, Client hello (1):{ [1 bytes data]
* TLSv1.2 (IN), TLS handshake, Finished (20):{ [16 bytes data]
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use http/1.1
* Server certificate:
*  subject: OU=Domain Control Validated; CN=cran.rstudio.com
*  start date: Jun 30 19:59:41 2015 GMT
*  expire date: Jun 30 19:59:41 2018 GMT
*  subjectAltName: host ""cran.rstudio.com"" matched cert's ""cran.rstudio.com""
*  issuer: C=US; ST=Arizona; L=Scottsdale; O=GoDaddy.com, Inc.; OU=http://certs.godaddy.com/repository/; CN=Go Daddy Secure Certificate Authority - G2
*  SSL certificate verify ok.} [5 bytes data]
&gt; GET /src/contrib/PACKAGES.rds HTTP/1.1
&gt; Host: cran.rstudio.com
&gt; User-Agent: curl/7.52.1
&gt; Accept: */*
&gt; { [5 bytes data]
&lt; HTTP/1.1 200 OK
&lt; Content-Length: 251020
&lt; Connection: keep-alive
&lt; Date: Wed, 12 Jul 2017 14:11:48 GMT
&lt; Server: Apache/2.2.22 (Ubuntu)
&lt; Last-Modified: Wed, 12 Jul 2017 13:02:43 GMT
&lt; ETag: ""d78fc54-3d48c-5541e6e7d22c0""
&lt; Accept-Ranges: bytes
&lt; Cache-Control: max-age=1800
&lt; Expires: Wed, 12 Jul 2017 14:41:48 GMT
&lt; Age: 1045
&lt; X-Cache: Hit from cloudfront
&lt; Via: 1.1 67284fcf464f6f1529cc1e521669622c.cloudfront.net (CloudFront)
&lt; X-Amz-Cf-Id: CqpfjeemEcxkxFYJueqzwUEu8Yh-qSenHJJiR2BdmqmAYLpu2_54dA==
&lt; { [15891 bytes data]
* Curl_http_done: called premature == 0 100  245k  100  245k    0     0   583k      0 --:--:-- --:--:-- --:--:--  589k
* Connection #0 to host (nil) left intact
</code></pre>

<p>One thing I notice is that command-line <code>curl</code> uses the CAs of my <code>anaconda</code> install, which is very weird indeed. Perhaps R uses the default CAs, and they don't work? Like I said, only R fails to work with SSL, everything else works. </p>

<p>Any help is highly appreciated! </p>
","<p>Dear Christoph Saffering,</p>

<p>My sense is that you have hit the CRAN <code>ssh by default</code> issue with RStudio / R. </p>

<h1>Solution</h1>

<p>Add the following to your target machines <code>.Rprofile</code></p>

<pre><code>options(download.file.method = ""wget"")
local({
         r&lt;- getOption(""repos"");
         r[""CRAN""] &lt;-""https://cran.rstudio.com/""
         options(repos=r)
})
</code></pre>

<h1>Explanation</h1>

<h2>Secure Download Methods</h2>

<p>When R transfers files over HTTP (e.g. using the install.packages or download.file function) a download method is chosen based on the download.file.method option. There are several methods available and the default behavior if no option is explicitly specified is to use R’s internal HTTP implementation. In many circumstances this internal method will not support HTTPS connections so you’ll need to override the default.</p>

<h2>R 3.2</h2>

<p>R 3.2 includes two new download methods (“libcurl” and “wininet”) that both support HTTPS connections. We recommend that you use these new methods when running under R 3.2. The requisite code to add to .Rprofile or Rprofile.site is as follows:</p>

<h3>Windows</h3>

<pre><code>options(download.file.method = ""wininet"")
</code></pre>

<p>Note that in the upcoming R 3.2.2 release this will no longer be necessary, as the default method is equivalent to “wininet”.</p>

<h3>OS X and Linux</h3>

<pre><code>options(download.file.method = ""libcurl"")
</code></pre>

<p>Note that if you built R from source the “libcurl” method may or may not have been compiled in. In the case that it wasn’t (i.e. capabilities(""libcurl"") == FALSE), you can follow the directions for earlier versions of R below to configure an alternate secure method.</p>

<h2>R 3.1 and Earlier</h2>

<p>Windows</p>

<pre><code>utils::setInternet2(TRUE)
options(download.file.method = ""internal"")
</code></pre>

<p>Note that setInternet2(TRUE) is the default value in RStudio however is not for R GUI. If you don’t want to use setInternet2(TRUE) on Windows then the only other way to configure secure downloads is to have the “wget” or “curl” utility on your PATH as described for OS X and Linux below.</p>

<p>OS X</p>

<pre><code>options(download.file.method = ""curl"")
</code></pre>

<p>Linux</p>

<pre><code>options(download.file.method = ""wget"")
</code></pre>

<p>Note that the “curl” and “wget” methods will work on any platform so long as the requisite binary is in the system PATH. The recommendations above are based on the fact that “curl” is included in OS X and “wget” is included in most Linux distributions. </p>

<p>ref: <a href=""https://support.rstudio.com/hc/en-us/articles/206827897-Secure-Package-Downloads-for-R"" rel=""nofollow noreferrer"">https://support.rstudio.com/hc/en-us/articles/206827897-Secure-Package-Downloads-for-R</a></p>
"
659725,Column Stores: Comparing Column Based Databases,2,2,2,"<p>I've really been struggling to make SQL Server into something that, quite frankly, it will never be. I need a database engine for my analytical work. The DB needs to be fast and does NOT need all the logging and other overhead found in typical databases (SQL Server, Oracle, DB2, etc.) </p>

<p>Yesterday I listened to <a href=""http://itc.conversationsnetwork.org/shows/detail4009.html"" rel=""noreferrer"">Michael Stonebraker speak at the Money:Tech conference</a> and I kept thinking, ""I'm not really crazy. There IS a better way!"" He talks about using <a href=""http://en.wikipedia.org/wiki/Column-oriented_DBMS"" rel=""noreferrer"">column stores</a> instead of row oriented databases. I went to the Wikipedia page for <a href=""http://en.wikipedia.org/wiki/Column-oriented_DBMS"" rel=""noreferrer"">column stores</a> and I see a few open source projects (which I like) and a few commercial/open source projects (which I don't fully understand). </p>

<p>My question is this: In an applied analytical environment, how do the different column based DB's differ? How should I be thinking about them? Anyone have practical experience with multiple column based systems? Can I leverage my SQL experience with these DBs or am I going to have to learn a new language?</p>

<p>I am ultimately going to be pulling data into R for analysis. </p>

<p><strong>EDIT:</strong> I was requested for some clarification in what exactly I am trying to do. So, here's an example of what I would like to do:
Create a table that has 4 million rows and 20 columns (5 dims, 15 facts). Create 5 aggregation tables that calculate max, min, and average for each of the facts. Join those 5 aggregations back to the starting table. Now calculate the percent deviation from mean, percent deviation of min, and percent deviation from max for each row and add it to the original table. This table data does not get new rows each day, it gets TOTALLY replaced and the process is repeated. Heaven forbid if the process must be stopped. And the logs... ohhhhh the logs! :)</p>
","<p>The short answer is that for analytic data, a column store will tend to be faster, with less tuning required.</p>

<p>A row store, the traditional database architecture, is good at inserting small numbers of rows, updating rows in place, and querying small numbers of rows. In a row store, these operations can be done with one or two disk block I/Os.</p>

<p>Analytic databases typically load thousands of records at a time; sometimes, as in your case, they reload everything. They tend to be denormalized, so have a lot of columns. And at query time, they often read a high proportion of the rows in the table, but only a few of these columns. So, it makes sense from an I/O standpoint to store values of the same column together.</p>

<p>Turns out that this gives the database a huge opportunity to do value compression. For instance, if a string column has an average length of 20 bytes but has only 25 distinct values, the database can compress to about 5 bits per value. Column store databases can often operate without decompressing the data.</p>

<p>Often in computer science there is an I/O versus CPU time tradeoff, but in column stores the I/O improvements often improve locality of reference, reduce cache paging activity, and allow greater compression factors, so that CPU gains also.</p>

<p>Column store databases also tend to have other analytic-oriented features like bitmap indexes (yet another case where better organization allows better compression, reduces I/O, and allows algorithms that are more CPU-efficient), partitions, and materialized views.</p>

<p>The other factor is whether to use a massively parallel (MMP) database. There are MMP row-store and column-store databases. MMP databases can scale up to hundreds or thousands of nodes, and allow you to store humungous amounts of data, but sometimes have compromises like a weaker notion of transactions or a not-quite-SQL query language.</p>

<p>I'd recommend that you give LucidDB a try. (Disclaimer: I'm a committer to LucidDB.) It is open-source column store database, optimized for analytic applications, and also has other features such as bitmap indexes. It currently only runs on one node, but utilizes several cores effectively and can handle reasonable volumes of data with not much effort.</p>
"
19838360,"How to import only one function from another package, without loading the entire namespace",2,2,2,"<p>Suppose I'm developing a package, called <code>foo</code>, which would like to use the <code>description</code> function from the <code>memisc</code> package. I don't want to import the whole <code>memisc</code> namespace because :</p>

<ol>
<li>It is bad</li>
<li><code>memisc</code> overrides the base <code>aggregate.formula</code> function, which breaks several things. For example, <code>example(aggregate)</code> would fail miserably.</li>
</ol>

<p>The package includes the following files :</p>

<p><strong>DESCRIPTION</strong> </p>

<pre><code>Package: foo
Version: 0.0
Title: Foo
Imports:
    memisc
Collate:
    'foo.R'
</code></pre>

<p><strong>NAMESPACE</strong></p>

<pre><code>export(bar)
importFrom(memisc,description)
</code></pre>

<p><strong>R/foo.R</strong></p>

<pre><code>##' bar function
##'
##' @param x something
##' @return nothing
##' @importFrom memisc description
##' @export

`bar` &lt;- function(x) {
    description(x)
}
</code></pre>

<p>I'd think that using <code>importFrom</code> would not load the entire <code>memisc</code> namespace, but only <code>namespace::description</code>, but this is not the case. Starting with a vanilla R :</p>

<pre><code>R&gt; getS3method(""aggregate"",""formula"")
## ... function code ...
## &lt;environment: namespace:stats&gt;
R&gt; library(foo)
R&gt; getS3method(""aggregate"",""formula"")
## ... function code ...
## &lt;environment: namespace:memisc&gt;
R&gt; example(aggregate)
## Fails
</code></pre>

<p>So, do you know how I can import the <code>description</code> function from <code>memisc</code> without getting <code>aggregate.formula</code> in my environment ?</p>
","<p>You can't.</p>

<p>If you declare <code>memisc</code> in the <code>Imports:</code> field, the namespace will be loaded when the package is loaded and the exported objects will be findable by your package. (If you specify it in <code>Depends:</code>, the namespace will be loaded and attached to the search path which makes the exported objects findable by any code.)</p>

<p>Part of loading a namespace is registering methods with the generic. (I looked but couldn't find a canonical documentation that says this; I will appeal to the fact that functions are declared as S3 methods in the <code>NAMESPACE</code> file as evidence.) The defined methods are kept with the generic and have the visibility of the generic function (or, perhaps, the generic function's namespace).</p>

<p>Typically, a package will define a method either for a generic it creates or for a class it defines. The S3 object system does not have a mechanism for formally defining an S3 class (or what package created the class), but the general idea is that if the package defines functions which return an object with that class attribute (and is the only package that does), that class is that package's class. If either of these two conditions hold, there will not be a problem. If the generic is defined in the package, it can only be found if the package is attached; if the class is defined in the package, objects of that class would only exist (and therefore be dispatched on) if the package is attached and used.</p>

<p>In the <code>memisc</code> example, neither holds. The <code>aggregate</code> generic is defined in the <code>stats</code> package and the <code>formula</code> object is also defined in the <code>stats</code> package (based on that package defining <code>as.formula</code>, <code>[.formula</code>, etc.) Since it is neither <code>memisc</code>'s generic nor <code>memisc</code>'s object, the effects can be seen even (and the method dispatched to) if <code>memisc</code> is simply loaded but not attached.</p>

<p>For another example of this problem, but with <code>reorder.factor</code>, see <a href=""https://stackoverflow.com/q/10939516/892313"">Reordering factor gives different results, depending on which packages are loaded</a>.</p>

<p>In general, it is not good practice to add methods to generics for which the package does not control either the object or the generic; doubly so if it overrides a method in a core package; and egregiously so if it is not a backwards compatible function to the existing function in the core packages.</p>

<p>For your example, you may be better off copying the code for <code>memisc::describe</code> into your package, although that approach has its own problems and caveats.</p>
"
41594138,do.call and order to sort each row to descending order of a matrix?,1,1,3,"<p>I want to sort this matrix row-wise to descending order</p>

<pre><code> &gt; set.seed(123); a &lt;- matrix(rbinom(100,10,0.3),ncol=10)

      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    2    6    5    6    1    1    4    4    2     1
 [2,]    4    3    4    5    3    3    1    3    4     4
 [3,]    3    4    3    4    3    4    3    4    3     2
 [4,]    5    3    7    4    2    1    2    0    4     4
 [5,]    5    1    4    0    2    3    4    3    1     2
 [6,]    1    5    4    3    1    2    3    2    3     2
 [7,]    3    2    3    4    2    1    4    2    6     4
 [8,]    5    1    3    2    3    4    4    3    5     1
 [9,]    3    2    2    2    2    5    4    2    5     3
[10,]    3    6    1    2    5    2    3    1    2     3
</code></pre>

<p>but</p>

<pre><code>&gt; do.call(order,as.list(a[1,],a[2,]))
[1] 1
</code></pre>

<p><strong>How can you sort the matrix with the do.call and order?</strong></p>

<p><em>Edit.</em> Fixed above matrix to conform with the above code.</p>
","<p>Two alternatives:</p>

<pre><code># Jaap
do.call(rbind, lapply(split(a, row(a)), sort, decreasing = TRUE))

# adaption of lmo's solution in the comments
for(i in 1:nrow(a)) a[i,] &lt;- a[i,][order(a[i,], decreasing = TRUE)]
</code></pre>

<p>gives:</p>

<pre><code>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
1     6    6    5    4    4    2    2    1    1     1
2     5    4    4    4    4    3    3    3    3     1
3     4    4    4    4    3    3    3    3    3     2
4     7    5    4    4    4    3    2    2    1     0
5     5    4    4    3    3    2    2    1    1     0
6     5    4    3    3    3    2    2    2    1     1
7     6    4    4    4    3    3    2    2    2     1
8     5    5    4    4    3    3    3    2    1     1
9     5    5    4    3    3    2    2    2    2     2
10    6    5    3    3    3    2    2    2    1     1
</code></pre>

<p>A benchmark with:</p>

<pre><code>library(microbenchmark)
microbenchmark(dc.lapply.sort = do.call(rbind, lapply(split(a, row(a)), sort, decreasing = TRUE)),
               t.apply.sort = t(apply(a, 1, sort, decreasing = TRUE)),
               for.order = for(i in 1:nrow(a)) a[i,] &lt;- a[i,][order(a[i,], decreasing = TRUE)],
               for.sort = for(i in 1:nrow(a)) a[i,] &lt;- sort(a[i,], decreasing = TRUE),
               for.sort.list = for(x in seq_len(nrow(a))) a[x,] &lt;- a[x,][sort.list(a[x,], decreasing = TRUE, method=""radix"")])
</code></pre>

<p>gives:</p>

<pre><code>Unit: microseconds
           expr     min       lq      mean   median       uq      max neval cld
 dc.lapply.sort 189.811 206.5890 222.52223 217.8070 228.0905  332.034   100   c
   t.apply.sort 185.474 200.4515 212.59608 210.4930 220.0025  286.288   100  bc
      for.order  82.631  91.1860  98.66552  97.8475 102.9680  176.666   100 a  
       for.sort 167.939 187.5025 192.90728 192.1195 198.8690  256.494   100  b 
  for.sort.list 187.617 206.4475 230.82960 215.7060 221.6115 1541.343   100   c
</code></pre>

<hr>

<p>It should be noted however that benchmarks are only meaningful on larger datasets, so:</p>

<pre><code>set.seed(123)
a &lt;- matrix(rbinom(10e5, 10, 0.3), ncol = 10)

microbenchmark(dc.lapply.sort = do.call(rbind, lapply(split(a, row(a)), sort, decreasing = TRUE)),
               t.apply.sort = t(apply(a, 1, sort, decreasing = TRUE)),
               for.order = for(i in 1:nrow(a)) a[i,] &lt;- a[i,][order(a[i,], decreasing = TRUE)],
               for.sort = for(i in 1:nrow(a)) a[i,] &lt;- sort(a[i,], decreasing = TRUE),
               for.sort.list = for(x in seq_len(nrow(a))) a[x,] &lt;- a[x,][sort.list(a[x,], decreasing = TRUE, method=""radix"")],
               times = 10)
</code></pre>

<p>gives:</p>

<pre><code>Unit: seconds
           expr      min       lq     mean   median       uq      max neval  cld
 dc.lapply.sort 6.790179 6.924036 7.036330 7.013996 7.121343 7.351729    10    d
   t.apply.sort 5.032052 5.057022 5.151560 5.081459 5.177159 5.538416    10   c 
      for.order 1.368351 1.463285 1.514652 1.471467 1.583873 1.736544    10 a   
       for.sort 5.028314 5.102993 5.317597 5.154104 5.348614 6.123278    10   c 
  for.sort.list 2.417857 2.464817 2.573294 2.519408 2.726118 2.815964    10  b  
</code></pre>

<p>Conclusion: the <code>for</code>-loop in combination with <code>order</code> is still the fastest solution.</p>

<hr>

<p>Using the <code>order2</code> and <code>sort2</code> functions of the <code>grr</code>-package can give a further improvement in speed. Comparing them with the fastest solution from above:</p>

<pre><code>set.seed(123)
a &lt;- matrix(rbinom(10e5, 10, 0.3), ncol = 10)

microbenchmark(for.order = for(i in 1:nrow(a)) a[i,] &lt;- a[i,][order(a[i,], decreasing = TRUE)],
               for.order2 = for(i in 1:nrow(a)) a[i,] &lt;- a[i,][rev(grr::order2(a[i,]))],
               for.sort2 = for(i in 1:nrow(a)) a[i,] &lt;- rev(grr::sort2(a[i,])),
               times = 10)
</code></pre>

<p>giving:</p>

<pre><code>Unit: milliseconds
       expr       min        lq      mean    median        uq      max neval cld
  for.order 1243.8140 1263.4423 1316.4662 1305.1823 1378.5836 1404.251    10   c
 for.order2  956.1536  962.8226 1110.1778 1090.9984 1233.4241 1368.416    10  b 
  for.sort2  830.1887  843.6765  920.5668  847.1601  972.8703 1144.135    10 a  
</code></pre>
"
34774256,control ylim labels with scales=free in xyplot lattice,1,3,3,"<p>I am trying to control the y axis labels in a xyplot when I have the scales argument set to free for the y axis.  I think I have been able  to set the ylim for each panel, but have not been able to figure out how to set the labels for the y axis.  Right now some panels have 2 labels and some panels have a label at the maximum y lim value.  I would like to have it consistent for all panels.  </p>

<p>Data:</p>

<pre><code> dput(datan.1)
structure(list(Cruise = c(201501L, 201501L, 201502L, 201503L, 
201501L, 201501L, 201502L, 201502L, 201503L, 201503L, 201503L, 
201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 201503L, 
201503L, 201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 
201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 201501L, 
201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 
201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 201501L, 
201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 
201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 
201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 
201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 
201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 
201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 
201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 
201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 
201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 
201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 
201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 
201501L, 201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 
201503L, 201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 
201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 201502L, 
201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 201501L, 
201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 
201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 
201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 
201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 
201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 
201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 
201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 
201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 
201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 
201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 
201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 
201501L, 201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 
201503L, 201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 
201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 201502L, 
201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 201501L, 
201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 
201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 
201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 
201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 
201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 
201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 
201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 
201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 
201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 
201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 
201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 
201501L, 201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 
201503L, 201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 
201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 201502L, 
201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 201501L, 
201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 201502L, 
201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 201503L, 
201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 201502L, 
201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 201503L, 
201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 201502L, 
201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 201503L, 
201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 201501L, 
201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 
201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 201501L, 
201501L, 201502L, 201502L, 201502L, 201502L, 201502L, 201503L, 
201503L, 201503L, 201503L, 201503L, 201501L, 201501L, 201501L, 
201501L, 201502L, 201502L, 201502L, 201502L, 201503L, 201503L, 
201503L, 201503L, 201501L, 201501L, 201502L, 201502L, 201502L, 
201503L, 201503L, 201501L, 201501L), SAMS_region = structure(c(2L, 
3L, 3L, 5L, 1L, 5L, 2L, 5L, 1L, 3L, 5L, 2L, 3L, 5L, 2L, 3L, 5L, 
3L, 5L, 1L, 2L, 3L, 5L, 1L, 2L, 3L, 5L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 
1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 
2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 
1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 
2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 
1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 
2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 
1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 
2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 
1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 
2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 2L, 3L, 4L, 5L, 2L, 3L, 4L, 5L, 2L, 3L, 4L, 5L, 2L, 5L, 
2L, 3L, 4L, 1L, 2L, 4L, 5L), .Label = c(""DMV"", ""ET"", ""HC"", ""HCsr"", 
""LI""), class = ""factor""), Length = c(7.5, 7.5, 7.5, 7.5, 12.5, 
12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 17.5, 17.5, 17.5, 17.5, 17.5, 
17.5, 17.5, 17.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 
22.5, 22.5, 22.5, 22.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 
27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 32.5, 32.5, 32.5, 32.5, 
32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 
37.5, 37.5, 37.5, 37.5, 37.5, 37.5, 37.5, 37.5, 37.5, 37.5, 37.5, 
37.5, 37.5, 37.5, 37.5, 42.5, 42.5, 42.5, 42.5, 42.5, 42.5, 42.5, 
42.5, 42.5, 42.5, 42.5, 42.5, 42.5, 42.5, 42.5, 47.5, 47.5, 47.5, 
47.5, 47.5, 47.5, 47.5, 47.5, 47.5, 47.5, 47.5, 47.5, 47.5, 47.5, 
47.5, 52.5, 52.5, 52.5, 52.5, 52.5, 52.5, 52.5, 52.5, 52.5, 52.5, 
52.5, 52.5, 52.5, 52.5, 52.5, 57.5, 57.5, 57.5, 57.5, 57.5, 57.5, 
57.5, 57.5, 57.5, 57.5, 57.5, 57.5, 57.5, 57.5, 57.5, 62.5, 62.5, 
62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 62.5, 
62.5, 62.5, 67.5, 67.5, 67.5, 67.5, 67.5, 67.5, 67.5, 67.5, 67.5, 
67.5, 67.5, 67.5, 67.5, 67.5, 67.5, 72.5, 72.5, 72.5, 72.5, 72.5, 
72.5, 72.5, 72.5, 72.5, 72.5, 72.5, 72.5, 72.5, 72.5, 72.5, 77.5, 
77.5, 77.5, 77.5, 77.5, 77.5, 77.5, 77.5, 77.5, 77.5, 77.5, 77.5, 
77.5, 77.5, 77.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 
82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 82.5, 87.5, 87.5, 87.5, 87.5, 
87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 87.5, 
92.5, 92.5, 92.5, 92.5, 92.5, 92.5, 92.5, 92.5, 92.5, 92.5, 92.5, 
92.5, 92.5, 92.5, 92.5, 97.5, 97.5, 97.5, 97.5, 97.5, 97.5, 97.5, 
97.5, 97.5, 97.5, 97.5, 97.5, 97.5, 97.5, 97.5, 102.5, 102.5, 
102.5, 102.5, 102.5, 102.5, 102.5, 102.5, 102.5, 102.5, 102.5, 
102.5, 102.5, 102.5, 102.5, 107.5, 107.5, 107.5, 107.5, 107.5, 
107.5, 107.5, 107.5, 107.5, 107.5, 107.5, 107.5, 107.5, 107.5, 
107.5, 112.5, 112.5, 112.5, 112.5, 112.5, 112.5, 112.5, 112.5, 
112.5, 112.5, 112.5, 112.5, 112.5, 112.5, 112.5, 117.5, 117.5, 
117.5, 117.5, 117.5, 117.5, 117.5, 117.5, 117.5, 117.5, 117.5, 
117.5, 117.5, 117.5, 117.5, 122.5, 122.5, 122.5, 122.5, 122.5, 
122.5, 122.5, 122.5, 122.5, 122.5, 122.5, 122.5, 122.5, 122.5, 
122.5, 127.5, 127.5, 127.5, 127.5, 127.5, 127.5, 127.5, 127.5, 
127.5, 127.5, 127.5, 127.5, 127.5, 127.5, 127.5, 132.5, 132.5, 
132.5, 132.5, 132.5, 132.5, 132.5, 132.5, 132.5, 132.5, 132.5, 
132.5, 132.5, 132.5, 132.5, 137.5, 137.5, 137.5, 137.5, 137.5, 
137.5, 137.5, 137.5, 137.5, 137.5, 137.5, 137.5, 137.5, 137.5, 
137.5, 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 
142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 147.5, 147.5, 
147.5, 147.5, 147.5, 147.5, 147.5, 147.5, 147.5, 147.5, 147.5, 
147.5, 147.5, 147.5, 147.5, 152.5, 152.5, 152.5, 152.5, 152.5, 
152.5, 152.5, 152.5, 152.5, 152.5, 152.5, 152.5, 152.5, 152.5, 
152.5, 157.5, 157.5, 157.5, 157.5, 157.5, 157.5, 157.5, 157.5, 
157.5, 157.5, 157.5, 157.5, 157.5, 157.5, 157.5, 162.5, 162.5, 
162.5, 162.5, 162.5, 162.5, 162.5, 162.5, 162.5, 162.5, 162.5, 
162.5, 167.5, 167.5, 167.5, 167.5, 167.5, 167.5, 167.5, 172.5, 
172.5), nwide = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 
0, 0, 0, 0, 0, 0, 0, 0, 1.5, 7.5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 
0, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 25.75, 1, 0, 5, 0, 72.5, 
0, 0, 0, 0, 0, 4.1, 0, 0, 5, 74.5416666666667, 1.75, 0, 5, 0, 
203, 3, 0, 0, 0, 108.75, 0, 0, 0, 14, 439.916666666667, 20, 0, 
8.25, 1, 417.105, 6.5, 3.4, 0, 0, 387, 21.65, 0, 0, 48, 994.791666666667, 
39.75, 0, 2, 1, 1002.365, 24, 0, 4, 4.95, 790.2, 35.475, 0, 0, 
68.5, 1136.625, 38.25, 1.625, 1, 6, 1023.765, 60.25, 0, 14.3333333333333, 
7.95, 855.55, 65.6, 0, 1, 40.375, 939.791666666667, 45.0416666666667, 
4.25, 7, 4, 943.375, 32, 4.4, 16.2222222222222, 13.5, 826.9, 
54.125, 0, 2, 12, 296.208333333333, 30.875, 7.5, 21, 3, 527.125, 
56.05, 11.1, 54.7777777777778, 11.25, 467.45, 71.35, 3.2, 11.2, 
11.5, 197.833333333333, 47.575, 7.45, 31.32, 7.875, 269.083333333333, 
21.4, 19.8, 74.1111111111111, 16, 307.9, 33.225, 4.2, 19.4, 18.5, 
151.916666666667, 48.35, 17, 51.73, 10.84, 163.15, 24.9, 59.4, 
126.222222222222, 41.75, 118.75, 20.1, 12.2, 17.05, 18.5, 251.833333333333, 
107.1, 27.5, 36.54, 40.16, 134.5, 27.5, 125, 111.111111111111, 
77.665, 140.9, 8.51, 17.4, 22.15, 45.1, 629.875, 114.225, 37.2, 
40.76, 55.715, 359.79, 65.9, 168.9, 87, 108.365, 294.4, 10.01, 
24, 11.7, 57.9, 1496.45833333333, 141.925, 42.325, 34.26, 24.41, 
1078.49833333333, 140.45, 215, 53, 115.95, 322.845, 23.01, 30.2, 
21.9, 29.85, 2552.33333333333, 139.65, 36.375, 36.33, 53.045, 
1728.72, 174.25, 164.6, 57.1111111111111, 166.395, 776.07, 150.245, 
27.4, 30.3, 46.3, 2561.58333333333, 224.9, 34.3, 46.06, 89.36, 
2101.99666666667, 237.55, 82.2, 56.2333333333333, 204.72, 922.745, 
286.825, 24.2, 29.6, 202.355, 2898.625, 502.266666666667, 66.825, 
75.03, 349.955, 3399.87833333333, 411.95, 116.1, 138.566666666667, 
281.815, 1048.625, 626.795, 34.4, 84.05, 598.12, 4501.70833333333, 
1488.5, 131.25, 307.65, 1020.96, 5593.03166666667, 971.5, 102.9, 
316, 557.415, 1920.21, 1267.92, 80.8, 251.5, 857.17, 6830.79166666667, 
2442.38333333333, 173.325, 467.34, 1344.51, 8411.34, 1650.5, 
96.4, 552.844444444444, 1032.13, 3298.97, 1574.205, 102.2, 408.2, 
725.27, 8797.41666666667, 2349, 150.375, 565.95, 1290.39, 9944.14833333333, 
1595.05, 99.7, 626.011111111111, 1342.755, 7052.925, 1379.015, 
158, 471.9, 653.68, 9980.20833333333, 1508.2, 110.525, 559.06, 
798.92, 10267.5533333333, 1139.75, 110.2, 659.444444444444, 1461.34, 
9324.13, 1002.965, 151.6, 524.15, 541.295, 7769.45833333333, 
853.258333333333, 119, 497.31, 537.2, 8269.565, 843.65, 154.1, 
646.055555555556, 1239.855, 7746.945, 707.575, 123.6, 537, 286.69, 
4814.125, 490.85, 137.5, 418.13, 260.465, 4799.68333333333, 513.55, 
129.7, 469.833333333333, 659.66, 5212.63, 383.635, 86.6, 439.5, 
111.595, 1866.45833333333, 336.058333333333, 106.675, 308.08, 
108.65, 2345.025, 264.9, 107.4, 293.2, 307.32, 2464.3, 271.185, 
64.8, 282.35, 36.41, 522.5, 134.2, 69.825, 135.15, 50.6, 994.215, 
112.6, 88.7, 164.333333333333, 96.165, 1254.82, 192.475, 45.2, 
150.55, 9.99, 261.708333333333, 81.975, 52.2, 51.32, 18.5, 382.933333333333, 
65, 55.4, 77.8, 43.915, 362.13, 101.585, 22, 71.5, 0, 117.625, 
24.375, 19, 13.75, 7.75, 113.875, 30.5, 14.1, 20, 6.75, 89.095, 
38.23, 4, 25.35, 1.33, 38.75, 17.75, 8.2, 2, 4.5, 74.7333333333333, 
14.5, 9, 3, 13, 10, 13, 4, 4.2, 10, 2, 3, 1, 5.95, 11, 1, 2, 
7.5, 8, 2, 4, 2, 0, 18.85, 4.25, 1, 1, 6, 1, 1.25), nfine = c(4.75, 
1, 2.25, 1, 1, 5.2, 11, 3, 2, 5.8, 3.99, 11.5, 19.7, 2.2, 154, 
17, 8, 1, 20.97, 5.25, 128.75, 141.75, 7.8, 0, 775, 95, 20, 46.8, 
17.1, 5, 25.29, 17, 610.316666666667, 298.8, 3.2, 5.1, 1, 2529.97368421053, 
347.9, 2, 42, 184.55, 60.2, 25.5, 73.32, 99.25, 2489.3, 734.85, 
9.2, 16.2, 12.125, 5208.28070175439, 889.8, 1, 132, 16, 971.21, 
202.3, 15, 268.1, 437.25, 6749.47333333333, 2213.35, 78.9736842105263, 
31, 109, 12827.8605263158, 2692.25, 7.8, 406.3, 176, 3846.86, 
749.26, 14.8, 584.86, 1374, 14372.1883333333, 4299.1, 144, 48.6, 
399.5, 20266.4087719298, 5986.6, 18.6, 603.6, 753.985294117647, 
9845.53, 2507.6, 83.4, 611.5, 2698.25, 18320.49, 6132.2, 233.373684210526, 
108.6, 1253.5, 26216.85, 9642.65, 51.8, 764.25, 1971.71176470588, 
17300.03, 6959.34, 229.7, 508.67, 2513.25, 15144.1366666667, 
6179.05, 256.615789473684, 233.8, 1913.875, 23985.0385964912, 
10083.1, 112.6, 1104.95, 3112.51470588235, 20598.95, 10323.21, 
615.5, 1079.92, 1161.25, 8409.37166666667, 4277.35, 294.857894736842, 
424.3, 1504.25, 13951.7043859649, 7022.275, 191.2, 2590.4, 2533.06764705882, 
16143, 8043.1, 1112.5, 1999.34, 349, 2995.155, 2154.55, 342.294736842105, 
783.9, 487.375, 5515.91315789474, 4247.075, 200, 3911.15, 839.594117647059, 
7403.84, 4019.14, 1346.1, 2721.35, 107.25, 857.066666666667, 
1091.55, 329.094736842105, 958.6, 151.75, 1273.05175438596, 2521.35, 
285.2, 4145.35, 295.476470588235, 2354.6, 1741.46, 1392.4, 3844.68, 
80.25, 373.6, 570.35, 372.426315789474, 1081.3, 146.375, 558.766666666667, 
1189.475, 338.4, 3527.8, 336.582352941176, 591.91, 766.88, 980.7, 
3879, 124.25, 544.313333333333, 408.95, 405.7, 724.7, 289.875, 
761.837719298246, 798.475, 454, 2338.95, 622.164705882353, 693.87, 
322.14, 572.4, 1666.23, 154.75, 1433.98333333333, 472.65, 345.457894736842, 
422.6, 311.875, 1573.97035087719, 628.575, 583, 991.75, 922.711764705882, 
1605.68, 334.49, 342.1, 708.1, 189.75, 2955.78333333333, 483.1, 
241.642105263158, 239.1, 323.75, 2861.94377192982, 604.8, 492.2, 
558.95, 1054.97058823529, 3434.17, 349.56, 159.2, 419.37, 113.5, 
3496.37, 443.35, 115.494736842105, 147.5, 193.5, 3177.50157894737, 
819.8, 277.4, 382.4, 736.211764705882, 4659.87, 477.12, 121.4, 
198.95, 63, 2392.70666666667, 235.25, 67.8684210526316, 108.7, 
151.375, 2168.86649122807, 568.9, 100.8, 257.9, 194.908823529412, 
2618.79, 659.08, 88.6, 117.52, 97.25, 1167.39166666667, 247.45, 
42.8, 83.6, 225.125, 1871.22640350877, 351.175, 78.6, 242.5, 
194.385294117647, 919.31, 606.66, 52, 108.28, 142.75, 1279.59666666667, 
362.85, 63.2, 109.3, 305.25, 2546.84675438597, 529.525, 54.2, 
228, 456.8, 697.71, 840.01, 71.2, 144.91, 151, 1724.25166666667, 
524.1, 40.5736842105263, 123.5, 439.875, 3762.77736842105, 607.3, 
52.8, 214.1, 865.055882352941, 1122.59, 780.16, 42.4, 133.13, 
134.25, 2667.27166666667, 516.6, 52, 123.1, 459.75, 4279.19236842105, 
560.4, 43.6, 188.9, 675.567647058824, 1250.52, 610.56, 61, 163.07, 
100.75, 2569.38333333333, 314.95, 28.3473684210526, 137.2, 284.375, 
2968.87473684211, 316.35, 46.8, 225.55, 480.397058823529, 1283.47, 
385.08, 41.8, 175.94, 84, 1978.4, 241.55, 27.1473684210526, 144, 
140.375, 1972.64947368421, 253.3, 43, 186.15, 223.158823529412, 
907.82, 238.26, 26.8, 141.92, 48.5, 1359.06166666667, 113.8, 
22.9736842105263, 121.8, 45.5, 802.036052631579, 114.35, 48, 
171.1, 52.9, 540.7, 122.99, 28.4, 117.87, 23, 670.626666666667, 
46.2, 26.9736842105263, 89.7, 16.75, 320.805, 42.2, 23, 103.35, 
18.45, 191, 63.36, 14, 79.66, 11.5, 192.466666666667, 29.05, 
23.9736842105263, 51.5, 11, 72.08, 22.2, 22, 57.65, 4.8, 80.3, 
47.41, 11.8, 56.36, 3, 117.99, 20.3, 4, 16.1, 1, 32.9, 8.2, 18.8, 
21.4, 1, 20.95, 11.86, 5, 11, 2, 30.53, 12, 3, 5, 2.625, 12.1, 
10.7, 11, 14.75, 0, 3.8, 8.66, 1, 3, 1, 23.555, 6, 2, 4, 0, 5.9, 
3.2, 3, 1.75, 0, 0, 4, 0, 1, 37.6666666666667, 2, 0, 0, 7.2, 
0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0)), .Names = c(""Cruise"", 
""SAMS_region"", ""Length"", ""nwide"", ""nfine""), row.names = c(83L, 
142L, 424L, 792L, 25L, 252L, 365L, 530L, 584L, 689L, 793L, 84L, 
143L, 253L, 334L, 425L, 531L, 690L, 794L, 26L, 85L, 144L, 254L, 
283L, 335L, 426L, 532L, 636L, 691L, 741L, 795L, 27L, 54L, 145L, 
197L, 255L, 308L, 336L, 427L, 478L, 533L, 637L, 692L, 742L, 796L, 
28L, 55L, 115L, 198L, 224L, 309L, 337L, 428L, 479L, 534L, 585L, 
638L, 663L, 743L, 797L, 1L, 56L, 116L, 199L, 225L, 310L, 338L, 
397L, 480L, 535L, 586L, 609L, 694L, 744L, 798L, 2L, 57L, 117L, 
200L, 226L, 284L, 339L, 398L, 454L, 536L, 587L, 610L, 664L, 745L, 
799L, 3L, 58L, 118L, 201L, 227L, 285L, 340L, 399L, 482L, 506L, 
560L, 611L, 665L, 746L, 800L, 4L, 59L, 119L, 173L, 228L, 286L, 
341L, 400L, 483L, 507L, 561L, 612L, 666L, 747L, 769L, 5L, 60L, 
120L, 174L, 229L, 287L, 342L, 401L, 455L, 508L, 562L, 613L, 667L, 
748L, 770L, 6L, 61L, 121L, 175L, 230L, 288L, 343L, 402L, 456L, 
509L, 563L, 614L, 668L, 720L, 771L, 7L, 62L, 122L, 176L, 231L, 
289L, 344L, 403L, 457L, 510L, 564L, 615L, 669L, 721L, 772L, 8L, 
63L, 123L, 177L, 232L, 290L, 345L, 404L, 458L, 511L, 565L, 616L, 
670L, 722L, 773L, 9L, 64L, 124L, 178L, 233L, 291L, 346L, 405L, 
459L, 512L, 566L, 617L, 671L, 723L, 774L, 10L, 65L, 125L, 179L, 
234L, 292L, 347L, 406L, 460L, 513L, 567L, 618L, 672L, 724L, 775L, 
11L, 66L, 126L, 180L, 235L, 293L, 348L, 407L, 461L, 514L, 568L, 
619L, 673L, 725L, 776L, 12L, 67L, 127L, 181L, 236L, 294L, 349L, 
408L, 462L, 515L, 569L, 620L, 674L, 726L, 777L, 13L, 68L, 128L, 
182L, 237L, 295L, 350L, 409L, 463L, 516L, 570L, 621L, 675L, 727L, 
778L, 14L, 69L, 129L, 183L, 238L, 296L, 351L, 410L, 464L, 517L, 
571L, 622L, 676L, 728L, 779L, 15L, 70L, 130L, 184L, 239L, 297L, 
352L, 411L, 465L, 518L, 572L, 623L, 677L, 729L, 780L, 16L, 71L, 
131L, 185L, 240L, 298L, 353L, 412L, 466L, 519L, 573L, 624L, 678L, 
730L, 781L, 17L, 72L, 132L, 186L, 241L, 299L, 354L, 413L, 467L, 
520L, 574L, 625L, 679L, 731L, 782L, 18L, 73L, 133L, 187L, 242L, 
300L, 355L, 414L, 468L, 521L, 575L, 626L, 680L, 732L, 783L, 19L, 
74L, 134L, 188L, 243L, 301L, 356L, 415L, 469L, 522L, 576L, 627L, 
681L, 733L, 784L, 20L, 75L, 135L, 189L, 244L, 302L, 357L, 416L, 
470L, 523L, 577L, 628L, 682L, 734L, 785L, 21L, 76L, 136L, 190L, 
245L, 303L, 358L, 417L, 471L, 524L, 578L, 629L, 683L, 735L, 786L, 
22L, 77L, 137L, 191L, 246L, 304L, 359L, 418L, 472L, 525L, 579L, 
630L, 684L, 736L, 787L, 23L, 78L, 138L, 192L, 247L, 305L, 360L, 
419L, 473L, 526L, 580L, 631L, 685L, 737L, 788L, 52L, 79L, 139L, 
193L, 248L, 306L, 361L, 420L, 474L, 527L, 581L, 632L, 686L, 738L, 
789L, 24L, 80L, 140L, 194L, 249L, 307L, 362L, 421L, 475L, 528L, 
582L, 633L, 687L, 739L, 790L, 81L, 141L, 195L, 250L, 363L, 422L, 
476L, 529L, 634L, 688L, 740L, 791L, 82L, 282L, 364L, 423L, 477L, 
583L, 635L, 196L, 251L), class = ""data.frame"")
</code></pre>

<p>plotting code </p>

<pre><code>xyplot(nfine~Length|as.factor(Cruise)*SAMS_region,data=datan.1,
    scales=list(y=list(relation=""free"")),
    key=list(space=""bottom"",lines=list(col=c(""red"",""blue"")),text=list(c(""Survey"",""CFTDD"")),cex=.6,columns=2,padding.text=8),    
    ylab=list(""Number"",cex=.8),xlab=list(""Length (mm)"",cex=.8),
    strip = strip.custom(bg=""white"",strip.levels = T),
    prepanel=function(x,y,subscripts,...){
    list(ylim=c(0,max(y)))
    },
    panel=function(x,y,subscripts, ...){
    panel.xyplot(x,y,type=""l"",col=""blue"")
    panel.xyplot(datan.1$Length[subscripts],datan.1$nwide[subscripts],col=""red"",type=""l"")
    },as.table=T,subscripts=T)
</code></pre>

<p>R session information </p>

<pre><code> sessionInfo ()
R version 3.2.1 (2015-06-18)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] datasets  utils     stats     graphics  grDevices methods  
[7] base     

other attached packages:
 [1] plotrix_3.5-12      reshape2_1.4.1      reshape_0.8.5      
 [4] qpcR_1.4-0          Matrix_1.2-1        robustbase_0.92-5  
 [7] rgl_0.95.1435       minpack.lm_1.2-0    lmtest_0.9-34      
[10] zoo_1.7-12          dplyr_0.4.2         plyr_1.8.3         
[13] MASS_7.3-40         RODBC_1.3-12        latticeExtra_0.6-26
[16] RColorBrewer_1.1-2  lattice_0.20-31    

loaded via a namespace (and not attached):
 [1] Rcpp_0.11.6     magrittr_1.5    R6_2.1.0       
 [4] stringr_1.0.0   tools_3.2.1     parallel_3.2.1 
 [7] grid_3.2.1      DBI_0.3.1       lazyeval_0.1.10
[10] assertthat_0.1  stringi_0.5-5   DEoptimR_1.0-4 
</code></pre>

<p>Thanks for any help.  </p>
","<p>It isn't clear to me what you want for ""consistent"".  The number of labels depends on the range of data. The number of labels printed depends on spacing.  If you resize the graphic device, you may see different labels printed.</p>

<p>If you rotate the labels to horizontal, you may get labels for all the tick marks, which is more consistent in some sense. Again, depending on the size of the graphic device. Use <code>rot=c(0,90)</code> and <code>alternating=FALSE</code>.</p>

<p><code>xyplot(nfine~Length|as.factor(Cruise)*SAMS_region,data=datan.1,
    scales=list(y=list(relation=""free""),rot=c(0,90),alternating=FALSE),    key=list(space=""bottom"",lines=list(col=c(""red"",""blue"")),text=list(c(""Survey"",""CFTDD"")),cex=.6,columns=2,padding.text=8),  ylab=list(""Number"",cex=.8),xlab=list(""Length (mm)"",cex=.8),
    strip = strip.custom(bg=""white"",strip.levels = T),
 #   prepanel=function(x,y,subscripts,...){
 #   list(ylim=c(0,max(y)))
 #   },
    panel=function(x,y,subscripts, ...){
    panel.xyplot(x,y,type=""l"",col=""blue"")
panel.xyplot(datan.1$Length[subscripts],datan.1$nwide[subscripts],col=""red"",type=""l"")
    },as.table=T,subscripts=T)</code></p>

<p>Note the prepanel function doesn't seem to be necessary.</p>

<p><strong>EDIT</strong></p>

<p>Following the example in the Lattice Book to get fixed number of y axis ticks.</p>

<pre><code>axis.CF &lt;- function(side, ...) {
 if (side == ""left"") { 
 ylim &lt;- current.panel.limits()$ylim
 top=round(ylim[2],-2)+100
 panel.axis(side = side, outside = TRUE,text.cex=.7, at = 0:4*top/4) 
} 
else axis.default(side = side, ...)
 }

xyplot(nfine~Length|as.factor(Cruise)*SAMS_region,data=datan.1,
    scales=list(y=list(relation=""free""),rot=c(0,90),alternating=FALSE), 
   key=list(space=""bottom"",lines=list(col=c(""red"",""blue"")),text=list(c(""Survey"",""CFTDD"")),cex=.6,columns=2,padding.text=8),
   ylab=list(""Number"",cex=.8),xlab=list(""Length (mm)"",cex=.8),
    strip = strip.custom(bg=""white"",strip.levels = T),axis=axis.CF,
 #    prepanel=function(x,y,subscripts,...){
 #   list(ylim=c(0,max(y)+100))
 #    },
    panel=function(x,y,subscripts, ...){
    panel.xyplot(x,y,type=""l"",col=""blue"")
    panel.xyplot(datan.1$Length[subscripts],datan.1$nwide[subscripts],col=""red"",type=""l"")
    },
as.table=T,subscripts=T)
</code></pre>
"
26785499,Select all rows up to and including first occurrence by group in a data frame,1,1,1,"<p>I ve been scratching my head a little about how to do. I'm reorganising some unbalanced panel data (stacked/long format). I need to keep all the rows up to and including the first occurrence of variable (indc=D) value by group (id) and also keep the rows for groups where this has not occurred yet. The only rows I wish to discard are rows per group where there is a second or more value of the indicator variable (indc=D). I also need to keep all the columns in the dataframe.</p>

<pre><code># Data 
id&lt;-factor(c(1,1,1,2,2,2,2,2, 3,3,3,3,3,3,4,4))
time&lt;-c(1,2,3,1,2,3,4,5, 1,2,3,4,5,6, 1,2)
indc&lt;-factor(c(""C"",""C"",""D"",""C"",""C"",""C"",""D"",""D"",""C"",""C"",""C"",""C"",""D"",""D"",""C"",""C""))
var1&lt;-sample(seq(1,8.5, by=0.5))
var2&lt;-c(rep(1,8),rep(0,8))

df&lt;-data.frame(id,time,indc,var1,var2)
</code></pre>

<p>My attempt is using by and match - problem is it returns the last variable as a match and the indices for each group. I m stuck on how to get to the final solution. </p>

<pre><code>attempt&lt;-by(df, df$id, function(x) {match(unique(x$indc==""D""), x$indc==""D"")} )

results&lt;-(do.call(""rbind"", attempt))
</code></pre>

<p>The desired result is df2 <code>df2&lt;-df[c(1:3,4:7,9:13,15:16),]</code></p>

<p>I'd be very grateful if anyone has ideas on a solution.</p>
","<p>One option is to use dplyr to group by ""id"" and then calculate a cumulative sum of the rows where ""indc == ""D"". Then check and filter all the rows where this cumsum is &lt;= 1.</p>

<pre><code>require(dplyr)
df %&gt;% group_by(id) %&gt;% filter(cumsum(indc == ""D"") &lt;= 1)
#Source: local data frame [14 x 5]
#Groups: id
#
#   id time indc var1 var2
#1   1    1    C  1.5    1
#2   1    2    C  1.0    1
#3   1    3    D  7.0    1
#4   2    1    C  2.5    1
#5   2    2    C  3.5    1
#6   2    3    C  6.5    1
#7   2    4    D  3.0    1
#8   3    1    C  2.0    0
#9   3    2    C  7.5    0
#10  3    3    C  6.0    0
#11  3    4    C  8.0    0
#12  3    5    D  8.5    0
#13  4    1    C  4.0    0
#14  4    2    C  4.5    0
</code></pre>

<hr>

<h3>Edit #1 after comments:</h3>

<p>Thanks to @akrun's comments below, here are tow more options of how to subset:</p>

<p>Option 1: using base R:</p>

<pre><code>df[with(df, ave(indc=='D', id, FUN=function(x) cumsum(x)&lt;=1)),]
</code></pre>

<p>Option 2: using data.table:</p>

<pre><code>require(data.table)
setDT(df)[,.SD[cumsum(indc=='D')&lt;=1], by=id]
</code></pre>

<p>Credit goes to @akrun</p>

<hr>

<h3>Edit #2 after comment by OP:</h3>

<p>It was not 100% clear how you want rows removed if, for example, the first ""D"" has occured and then there is another row in the same group where ""C"" occurs (or some other letter). My initial answer would keep such a row if it occured after the first ""D"" occurence. To change that behavior and remove all rows after the first ""D"" occurence, you can simply add another <code>cumsum</code> to the code, like this (for the modified data as presented below):</p>

<pre><code>df %&gt;% group_by(id2) %&gt;% filter(cumsum(cumsum(indc2 == ""D"")) &lt;= 1L)
#Source: local data frame [13 x 5]
#Groups: id2
#
#   id2 time2 indc2 var1 var2
#1    1     1     C  8.0    1
#2    1     2     C  5.0    1
#3    1     3     D  7.0    1
#4    2     1     C  1.0    1
#5    2     2     C  2.0    1
#6    2     3     D  9.0    1
#7    3     1     C  4.5    0
#8    3     2     C  3.0    0
#9    3     3     C  7.5    0
#10   3     4     C  1.5    0
#11   3     5     D  4.0    0
#12   4     1     C  6.0    0
#13   4     2     C  6.5    0
</code></pre>

<h3>data</h3>

<pre><code>df &lt;- structure(list(id2 = structure(c(1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L), .Label = c(""1"", ""2"", 
""3"", ""4""), class = ""factor""), time2 = c(1, 2, 3, 4, 1, 2, 3, 
4, 5, 1, 2, 3, 4, 5, 6, 1, 2), indc2 = structure(c(1L, 1L, 2L, 
1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L), .Label = c(""C"", 
""D""), class = ""factor""), var1 = c(8, 5, 7, 8.5, 1, 2, 9, 3.5, 
2.5, 4.5, 3, 7.5, 1.5, 4, 5.5, 6, 6.5), var2 = c(1, 1, 1, 1, 
1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)), .Names = c(""id2"", ""time2"", 
""indc2"", ""var1"", ""var2""), row.names = c(NA, -17L), class = ""data.frame"")

&gt; df
   id2 time2 indc2 var1 var2
1    1     1     C  8.0    1
2    1     2     C  5.0    1
3    1     3     D  7.0    1
4    1     4     C  8.5    1    &lt;-- this row will also be removed now
5    2     1     C  1.0    1
6    2     2     C  2.0    1
7    2     3     D  9.0    1
8    2     4     D  3.5    1
9    2     5     D  2.5    0
10   3     1     C  4.5    0
11   3     2     C  3.0    0
12   3     3     C  7.5    0
13   3     4     C  1.5    0
14   3     5     D  4.0    0
15   3     6     D  5.5    0
16   4     1     C  6.0    0
17   4     2     C  6.5    0
</code></pre>
"
17899756,Initializing MPI cluster with snowfall R,2,2,2,"<p>I've been trying to run <code>Rmpi</code> and <code>snowfall</code> on my university's clusters but for some reason no matter how many compute nodes I get allocated, my <code>snowfall</code> initialization keeps running on only one node.</p>

<p>Here's how I'm initializing it:</p>

<pre><code>sfInit(parallel=TRUE, cpus=10, type=""MPI"")
</code></pre>

<p>Any ideas? I'll provide clarification as needed.</p>
","<p>To run an Rmpi-based program on a cluster, you need to request multiple nodes using your batch queueing system, and then execute your R script from the job script via a utility such as mpirun/mpiexec.  Ideally, the mpirun utility has been built to automatically detect what nodes have been allocated by the batch queueing system, otherwise you will need to use an mpirun argument such as <code>--hostfile</code> to tell it what nodes to use.</p>

<p>In your case, it sounds like you requested multiple nodes, so the problem is probably with the way that the R script is executed.  Some people don't realize that they need to use mpirun/mpiexec, and the result is that your script runs on a single node.  If you are using mpirun, it may be that your installation of Open MPI wasn't built with support for your batch queueing system.  In that case, you would have to create an appropriate hostfile from information supplied by your batch queueing system which is usually supplied via an environment variable and/or a file.</p>

<p>Here is a typical mpirun command that I use to execute my parallel R scripts from the job script:</p>

<pre><code>mpirun -np 1 R --slave -f par.R
</code></pre>

<p>Since we build Open MPI with support for Torque, I don't use the <code>--hostfile</code> option: mpirun figures out what nodes to use from the <code>PBS_NODEFILE</code> environment variable automatically.  The use of <code>-np 1</code> may seem strange, but is needed if your program is going to spawn workers, which is typically done when using the <code>snow</code> package.  I've never used <code>snowfall</code>, but after looking over the source code, it appears to me that <code>sfInit</code> always calls <code>makeMPIcluster</code> with a ""count"" argument which will cause <code>snow</code> to spawn workers, so I think that <code>-np 1</code> is required for MPI clusters with <code>snowfall</code>.  Otherwise, mpirun will start your R script on multiple nodes, and each one will spawn 10 workers on their own node which is not what you want.  The trick is to set the <code>sfInit</code> ""cpus"" argument to a value that is consistent with the number of nodes allocated to your job by the batch queueing system.  You may find the <code>Rmpi</code> <code>mpi.universe.size</code> function useful for that.</p>

<p>If you think that all of this is done correctly, the problem may be with the way that the MPI cluster object is being created in your R script, but I suspect that it has to do with the use (or lack of use) of mpirun.</p>
"
29093057,Document term matrix in R,2,2,4,"<p>I have the following code:</p>

<pre><code>rm(list=ls(all=TRUE)) #clear data
setwd(""~/UCSB/14 Win 15/Issy/text.fwt"") #set working directory
files &lt;- list.files(); head(files) #load &amp; check working directory

fw1 &lt;- scan(what=""c"", sep=""\n"",file=""fw_chp01.fwt"")

library(tm) 
corpus2&lt;-Corpus(VectorSource(c(fw1)))
skipWords&lt;-(function(x) removeWords(x, stopwords(""english"")))

#remove punc, numbers, stopwords, etc
funcs&lt;-list(content_transformer(tolower), removePunctuation, removeNumbers, stripWhitespace, skipWords)
corpus2.proc&lt;-tm_map(corpus2, FUN = tm_reduce, tmFuns = funcs)

corpus2a.dtm &lt;- DocumentTermMatrix(corpus2.proc, control = list(wordLengths = c(1,110))) #create document term matrix
</code></pre>

<p>I'm trying use some of the operations detailed in the tm reference manual (<a href=""http://cran.r-project.org/web/packages/tm/tm.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/tm/tm.pdf</a>) with little success. For example, when I try to use the findFreqTerms, I get the following error: </p>

<pre><code>Error: inherits(x, c(""DocumentTermMatrix"", ""TermDocumentMatrix"")) is not TRUE
</code></pre>

<p>Can anyone clue me in as to why this isn't working and what I can do to fix it?</p>

<p>Edited for @lawyeR:</p>

<p>head(fw1) produces the first six lines of the text (Episode 1 of Finnegans Wake by James Joyce): </p>

<pre><code>[1] ""003.01    riverrun, past Eve and Adam's, from swerve of shore to bend""      
[2] ""003.02  of bay, brings us by a commodius vicus of recirculation back to""    
[3] ""003.03  Howth Castle and Environs.""                                         
[4] ""003.04    Sir Tristram, violer d'amores, fr'over the short sea, had passen-""
[5] ""003.05  core rearrived from North Armorica on this side the scraggy""        
[6] ""003.06  isthmus of Europe Minor to wielderfight his penisolate war: nor""  
</code></pre>

<p>inspect(corpus2) outputs each line of the text in the following format (this is the final line of the text): </p>

<pre><code>[[960]]
&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
029.36  borough. #this part differs by line of course
</code></pre>

<p>inspect(corpus2a.dtm) returns a table of all the types (there are 4163 in total( in the text in the following format: </p>

<pre><code>Docs  youths yoxen yu yurap yutah zee zephiroth zine zingzang zmorde zoom
  1        0     0  0     0     0   0         0    0        0      0    0
  2        0     0  0     0     0   0         0    0        0      0    0
</code></pre>
","<p>Here is a simplified form of what you provided and did, and <code>tm</code> does its job.  It may be that one or more of your cleaning steps caused a problem.</p>

<pre><code>&gt; library(tm) 
&gt; fw1 &lt;- c(""riverrun, past Eve and Adam's, from swerve of shore to bend      
+                                  of bay, brings us by a commodius vicus of recirculation back to
+                                  Howth Castle and Environs.      
+                                  Sir Tristram, violer d'amores, fr'over the short sea, had passen-
+                                  core rearrived from North Armorica on this side the scraggy    
+                                  isthmus of Europe Minor to wielderfight his penisolate war: nor"")
&gt; 
&gt; corpus&lt;-Corpus(VectorSource(c(fw1)))
&gt; inspect(corpus)
&lt;&lt;VCorpus (documents: 1, metadata (corpus/indexed): 0/0)&gt;&gt;

[[1]]
&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;
riverrun, past Eve and Adam's, from swerve of shore to bend      
                                 of bay, brings us by a commodius vicus of recirculation back to
                                 Howth Castle and Environs.      
                                 Sir Tristram, violer d'amores, fr'over the short sea, had passen-
                                 core rearrived from North Armorica on this side the scraggy    
                                 isthmus of Europe Minor to wielderfight his penisolate war: nor

&gt; dtm &lt;- DocumentTermMatrix(corpus)
&gt; findFreqTerms(dtm)
 [1] ""adam's,""       ""and""           ""armorica""      ""back""          ""bay,""          ""bend""         
 [7] ""brings""        ""castle""        ""commodius""     ""core""          ""d'amores,""     ""environs.""    
[13] ""europe""        ""eve""           ""fr'over""       ""from""          ""had""           ""his""          
[19] ""howth""         ""isthmus""       ""minor""         ""nor""           ""north""         ""passen-""      
[25] ""past""          ""penisolate""    ""rearrived""     ""recirculation"" ""riverrun,""     ""scraggy""      
[31] ""sea,""          ""shore""         ""short""         ""side""          ""sir""           ""swerve""       
[37] ""the""           ""this""          ""tristram,""     ""vicus""         ""violer""        ""war:""         
[43] ""wielderfight"" 
</code></pre>

<p>As another point, I find it useful at the start to load a few other complementary packages to <code>tm</code>.</p>

<pre><code>library(SnowballC); library(RWeka); library(rJava); library(RWekajars)
</code></pre>

<p>For what its worth, as compared to your somewhat complicated cleaning steps, I usually trudge along like this (replace comments$comment with your text vector):</p>

<pre><code>comments$comment &lt;- tolower(comments$comment)
comments$comment &lt;- removeNumbers(comments$comment)
comments$comment &lt;- stripWhitespace(comments$comment) 
comments$comment &lt;- str_replace_all(comments$comment, ""  "", "" "") 
# replace all double spaces internally with single space   
# better to remove punctuation with str_ because the tm function doesn't insert a space
library(stringr)
comments$comment &lt;- str_replace_all(comments$comment, pattern = ""[[:punct:]]"", "" "") 
comments$comment &lt;- removeWords(comments$comment, stopwords(kind = ""english""))
</code></pre>
"
7749749,ggplot2 and sweave - plot is in Rplots instead of main pdf?,2,2,2,"<p>I've been following the examples on similar posts, but to no avail. 
Here is an example of the problem I'm seeing.</p>

<p>Saved in tmp.Rnw:</p>

<pre><code>\documentclass[10pt]{article}
\title{Reproducible Example} 
\begin{document}
\maketitle
\begin{center}

&lt;&lt;echo=FALSE,results=hide&gt;&gt;=
library(ggplot2)
plot.to.print  = qplot( 1:10, 1:10 ) 
@ 

\section{No Figure Below This Section Title}
&lt;&lt;&lt;fig=true&gt;&gt;=
print( plot.to.print )
@ 

\end{center}
\end{document}
</code></pre>

<p>In tmp.co.r, I put the following code:</p>

<pre><code>Sweave(""tmp.Rnw"",stylepath=T)
</code></pre>

<p>And I create the tex file like this: </p>

<pre><code>/../../2.12.1/bin/R --no-save &lt; tmp.co.r
</code></pre>

<p>and then use pdflatex on the tmp.tex file that comes out. </p>

<p>The result is tmp.pdf which contains the title, section name and R code, but no figure. However, and Rplots.pdf file is also generated which contains the figure I want in tmp.pdf.</p>

<p>I'm certain that I'm making a newbie mistake, but I can't find it. Any tips?</p>
","<p>The problem is your extra <code>&lt;</code> in your fig chunk.</p>

<p>This causes it to look like <code>&lt;fig=TRUE</code>, so <code>fig</code> isn't actually set to TRUE properly.  You'll notice that your <code>.tex</code> file doesn't have the proper <code>includegraphics</code> lines in it either.</p>

<p>Why you get the <code>Rplots.pdf</code> is a little complicated, but worth knowing about.  First, every chunk that creates graphics is executed an additional time for each desired graphic type. So if you just make pdf's (the current default, I think), it's run twice; if you make pdf's and eps's it's run three times.  The first time it's run, it's run without opening a graphics device; I'm actually not sure why it runs that time, but it does.  For multiple files, it's necessary to run it separately which each file open in turn.</p>

<p>Thus the best practice is to do what you did and run all the code creating the figure in one chunk and just plot the figure in the chunk with <code>fig=TRUE</code>; this minimizes the code that is run multiple times.  However, watch out if you're using random numbers or incrementing something in <code>fig=TRUE</code> chunks; since it runs multiple times, the behavior probably will not be what you expect.</p>

<p>Second, when code that creates a graphic is run without specifying a graphics device, the default graphic type is opened anyway for the code to work on.  When you run interactively, this pops up windows with the pictures in it.  When run non-interactively the default is usually to open a pdf file, and the default name is <code>Rplots.pdf</code>.  Since this happens with all the chunks that create figures, this file ends up being a multipage pdf with all the figures you created in it.</p>

<p>Finally, methods that create the figure using R code instead of the <code>fig=TRUE</code> mechanism can sometimes be preferred so that the code is only run once; it's usually a little more bookkeeping, though that can be minimized by creating functions to help out.  Evidently the AFLP package (see Thierry's answer) has functions like this, though I've never used it.  Not too hard at all to write your own though if you'd rather, similar to what is recommended in the <a href=""http://www.stat.uni-muenchen.de/~leisch/Sweave/FAQ.html#x1-11000A.9"" rel=""nofollow"">Sweave FAQ A.9</a> for creating multiple plots at once.</p>

<p>Finally (peers into crystal ball...), I see you're using Emacs in Rnw mode, where typing <code>&lt;</code> gives you <code>&lt;&lt;&gt;&gt;=</code> with the cursor in the middle, so typing <code>&lt;&lt;</code> gives you <code>&lt;&lt;&lt;&gt;&gt;=</code>.  </p>
"
40644513,How to create & format legend in plot_ly while using the split parameter,1,1,1,"<p>I'm trying to create a scatterplot in plot_ly using the split parameter, however I'm having some difficulties with the legend &amp; formatting. Specifically, once I add labels to my chart, more items are added to the legend, the colors don't match, and I can't turn off an entire series with one click. Here's an example:</p>

<pre class=""lang-r prettyprint-override""><code>library(plotly)

plot_ly(mtcars,
        type = ""scatter"",
        x = ~hp,
        y =~qsec,
        split = ~cyl,
        text = rownames(mtcars)) %&gt;%
  add_text(textposition = ""middle right"")
</code></pre>

<p>When you run this code, you can see that the labels do appear for each of the points, however they show in random colors. Additionally, the legend has 6 items. What I want is: </p>

<ol>
<li>The labels should match the color of their series/split (the number
of cylinders)</li>
<li>The legend to have 3 items (4, 6, &amp; 8) </li>
<li>When you click
on one of the items in the legend, it should turn off/on BOTH the
markers and text labels for that series/split</li>
</ol>

<p>Thanks a lot for the help!</p>

<p><em>In case it helps:</em></p>

<ul>
<li>R: 3.3.1</li>
<li>Plotly: 4.5.6</li>
</ul>
","<p>You need to set the <code>mode</code> when using type <code>scatter</code>.  With the <code>markers+text</code> option I believe you don't have the ability to color the text individually.  If you don't mind the text being grey, the solution is:</p>

<pre><code>plot_ly(mtcars,
        type = ""scatter"",
        x = ~hp,
        y = ~qsec,
        split = ~cyl,
        mode = ""markers+text"",
        text = rownames(mtcars),
        textposition = ""middle right"") 
</code></pre>

<p>If you want to have it match your request 100%, it gets more complicated and you can't use the <code>split</code> parameter.</p>

<p>You have to create individual traces for each cylinder level, first using <code>mutate</code> to explicitly include the row names.  You then use <code>filter</code> to subset the <code>mtcars</code> for each cylinder level.  For each level create 2 traces, a marker and a text.  The <code>color</code> would then be equal to the factor level of <code>cyl</code>. Finally, you have to group the traces for each cylinder in the legend, hiding the entries you don't need.</p>

<pre><code>library(plotly)
library(dplyr)    

plot_ly(filter(mutate(mtcars, names = rownames(mtcars)), cyl == 4),
        type = ""scatter"",
        x = ~hp,
        y = ~qsec,
        mode = ""markers"",
        color = ~factor(cyl, c(4,6,8)),
        legendgroup = '4',
        name = '4',
        textposition = ""middle right"") %&gt;% 
  add_trace(data = filter(mutate(mtcars, names = rownames(mtcars)), cyl == 4),
            type = ""scatter"",
            x = ~hp+2,
            y = ~qsec,
            color = ~factor(cyl, c(4,6,8)),
            mode = ""text"",
            text = ~names, 
            legendgroup = '4',
            name = '4',
            showlegend = FALSE,
            textposition = ""middle right"") %&gt;% 
  add_trace(data = filter(mutate(mtcars, names = rownames(mtcars)), cyl == 6),
          type = ""scatter"",
          x = ~hp,
          y = ~qsec,
          mode = ""markers"",
          color = ~factor(cyl, c(4,6,8)),
          legendgroup = '6',
          name = '6',
          textposition = ""middle right"") %&gt;% 
  add_trace(data = filter(mutate(mtcars, names = rownames(mtcars)), cyl == 6),
            type = ""scatter"",
            x = ~hp+2,
            y = ~qsec,
            mode = ""text"",
            color = ~factor(cyl, c(4,6,8)),
            text = ~names, 
            legendgroup = '6',
            name = '6',
            showlegend = FALSE,
            textposition = ""middle right"") %&gt;% 
  add_trace(data = filter(mutate(mtcars, names = rownames(mtcars)), cyl == 8),
            type = ""scatter"",
            x = ~hp,
            y = ~qsec,
            mode = ""markers"",
            color = ~factor(cyl, c(4,6,8)),
            legendgroup = '8',
            name = '8',
            textposition = ""middle right"") %&gt;% 
  add_trace(data = filter(mutate(mtcars, names = rownames(mtcars)), cyl == 8),
            type = ""scatter"",
            x = ~hp+2,
            y = ~qsec,
            mode = ""text"",
            color = ~factor(cyl, c(4,6,8)),
            text = ~names, 
            legendgroup = '8', 
            name = '8',
            showlegend = FALSE,
            textposition = ""middle right"")
</code></pre>
"
22714775,How do I colour lines separately in rarecurve (vegan package),1,3,3,"<p>I'm using <code>rarecurve</code> (<code>vegan</code>) to produce rarefaction curves for nine samples, but I want them to be coloured in groups of three.</p>

<p>The parameters for <code>rarecurve</code> are:</p>

<pre><code>rarecurve(x, step = 1, sample, xlab = ""Sample Size"", ylab = ""Species"", label = TRUE, ...)
</code></pre>

<p>With the <code>...</code> passing arguments to 'plot'. However, when I replace the ellipsis with <code>col=c(rep(""blue"",3), rep(""red"",3), rep(""darkgreen"",3))</code>, all lines appear as blue. How do I colour the lines individually?</p>

<p>It takes almost three hours to compute each graph, so trial and error testing is a bit laborious!</p>
","<pre><code>## example from ?vegan::rarecurve
library(vegan)
data(BCI)
S &lt;- specnumber(BCI)
(raremax &lt;- min(rowSums(BCI)))
Srare &lt;- rarefy(BCI, raremax)
plot(S, Srare, xlab = ""Observed No. of Species"", ylab = ""Rarefied No. of Species"")
abline(0, 1)
rarecurve(BCI, step = 20, sample = raremax, col = ""blue"", cex = 0.6)
</code></pre>

<p><img src=""https://i.stack.imgur.com/xISzf.png"" alt=""enter image description here""></p>

<pre><code># using new function
plot(S, Srare, xlab = ""Observed No. of Species"", ylab = ""Rarefied No. of Species"")
abline(0, 1)
rarec(BCI, step = 20, sample = raremax, cex = 0.6)
</code></pre>

<p><img src=""https://i.stack.imgur.com/0lRv2.png"" alt=""enter image description here""></p>

<p>The problem is in these lines in <code>vegan::rarecurve</code></p>

<pre><code>for (ln in seq_len(length(out))) {
  N &lt;- attr(out[[ln]], ""Subsample"")
  lines(N, out[[ln]], ...)
</code></pre>

<p>where each line is made individually by <code>lines</code> which in turn is only taking the first color it sees in the color argument passed by <code>...</code>, which is blue in your case. After applying a simple hack to this loop:</p>

<pre><code>for (ln in seq_len(length(out))) {
  N &lt;- attr(out[[ln]], ""Subsample"")
  lines(N, out[[ln]], col = cols[ln], ...)
</code></pre>

<p>and specifying a new argument, <code>cols</code>, in the <code>rarecurve</code> function rather than passing <code>col</code> onto <code>plot</code> and <code>lines</code>:</p>

<p><code>cols = c(rep('red', nrow(x) / 2), rep('blue', nrow(x) / 2))</code></p>

<p>Here is the new function</p>

<pre><code>rarec &lt;- function (x, step = 1, sample, xlab = ""Sample Size"", ylab = ""Species"", 
          label = TRUE, cols = c(rep('red', nrow(x) / 2), rep('blue', nrow(x) / 2)), ...) {
  tot &lt;- rowSums(x)
  S &lt;- specnumber(x)
  nr &lt;- nrow(x)
  out &lt;- lapply(seq_len(nr), function(i) {
    n &lt;- seq(1, tot[i], by = step)
    if (n[length(n)] != tot[i]) 
      n &lt;- c(n, tot[i])
    drop(rarefy(x[i, ], n))
  })
  Nmax &lt;- sapply(out, function(x) max(attr(x, ""Subsample"")))
  Smax &lt;- sapply(out, max)
  plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = xlab, ylab = ylab, 
       type = ""n"", ...)
  if (!missing(sample)) {
    abline(v = sample)
    rare &lt;- sapply(out, function(z) approx(x = attr(z, ""Subsample""), 
                                           y = z, xout = sample, rule = 1)$y)
    abline(h = rare, lwd = 0.5)
  }
  for (ln in seq_len(length(out))) {
    N &lt;- attr(out[[ln]], ""Subsample"")
    lines(N, out[[ln]], col = cols[ln], ...)
  }
  if (label) {
    ordilabel(cbind(tot, S), labels = rownames(x), ...)
  }
  invisible(out)
}
</code></pre>
"
29186696,Match and summarize dataframe in R,1,1,1,"<p>I have a dataframe with timeseries data columns and start year and end year.</p>

<pre><code>df = data.frame(y2000=c(12,636),y2001=c(234, 76),y2002=c(3434, 46),y2003=c(36,35),y2004=c(6, 64),   y2005=c(56,65), y2006=c(43,65), y2007=c( 6, 56),y2008=c( 64, 66),y2009=c(63, 5656),y2010 = c(65,54),startyear= c(2006, 2001), endyear= c(2009, 2005))
</code></pre>

<p>For each row I want to calculate mean value within the start and  end years and also before and after the start and end period. The desired output looks like:     </p>

<pre><code>y2000   y2001   y2002   y2003   y2004   y2005   y2006   y2007   y2008   y2009   y2010   startyear   endyear before_mean within_mean  after_mean
12  234 3434    36  6   56  43  6   64  63  65  2006    2009    629.6666667     44  65
636 76  46  35  64  65  65  56  66  5656    54  2001    2005    636 57.2        1179.4
</code></pre>

<p>I have tried different match and index technique but can't wrap my mind around this one.  </p>
","<p><strong>1. dplyr/tidyr</strong></p>

<p>It may be better to convert the 'wide' format to 'long' format.  We could use <code>dplyr/tidyr</code> to get the <code>mean</code>.  Create a 'ind' column, reshape the data to 'long' using <code>gather</code>, split the 'variable' column into two columns ('var1', 'var2') with <code>extract</code>, group by 'ind', get the <code>mean</code> values of the 'value' column after subsetting it based on the different logical index created (i.e. <code>var2 &lt; startyear</code>, <code>var2 &gt;= startyear &amp; var2 &lt;= endyear</code>, and <code>var2 &gt;endyear</code>)</p>

<pre><code>library(dplyr)
library(tidyr)

dS &lt;-  df %&gt;%
          mutate(ind=row_number()) %&gt;%
          gather(variable, value, starts_with('y')) %&gt;%
          extract(variable, c('var1', 'var2'), '([^0-9]+)([0-9]+)',
                        convert=TRUE) %&gt;%
          group_by(ind) %&gt;%
          summarise(before_mean= mean(value[var2 &lt; startyear]), 
                   within_mean = mean(value[var2 &gt;= startyear &amp; 
                                            var2 &lt;= endyear]),
                   after_mean=mean(value[var2 &gt;endyear])) %&gt;% 
         as.data.frame()

nm1 &lt;-  paste(c('before', 'within', 'after'), 'mean', sep=""_"")
dS
#   ind before_mean within_mean after_mean
#1   1    629.6667        44.0       65.0
#2   2    636.0000        57.2     1179.4
</code></pre>

<p>We can create additional columns in 'df' from the above output</p>

<pre><code>df[nm1] &lt;- dS
</code></pre>

<p><strong>2. base R</strong></p>

<p>We can use <code>base R</code> methods and without changing the format of the dataset.  From the original dataset ('df'), make an index ('indx') of  numeric column names, remove the non-numeric part and convert to numeric ('v1'). </p>

<pre><code> indx &lt;- grep('\\d+', names(df))
 v1 &lt;- as.numeric(sub('[^0-9]+', '', names(df)[indx]))
</code></pre>

<p>Loop the rows of 'df' (<code>lapply</code>), <code>match</code> the 'startyear' with 'v1', use that index ('i1') to get the columns, <code>unlist</code>, and calculate the <code>mean</code>.  The same can be done by matching the 'endyear' with 'v1' to get the index ('i2').  Based on 'i1', and 'i2', calculate the 'within_mean' and 'after_mean'.  <code>rbind</code> the list elements and assign the output to new columns ('nm1') in 'df'.</p>

<pre><code>df[nm1] &lt;- do.call(rbind,lapply(1:nrow(df), function(i) {
       i1 &lt;- match(df$startyear[i], v1)
       before_mean&lt;-  mean(unlist(df[i,1:(i1-1),drop=FALSE]))
       i2 &lt;- match(df$endyear[i], v1)
       within_mean &lt;- mean(unlist(df[i,i2:i1]))
      after_mean &lt;- mean(unlist(df[i,match(v1[(i2+1):length(v1)],v1)]))
       data.frame(before_mean,within_mean, after_mean) }))
 df[nm1]
 #    before_mean within_mean after_mean
 #1    629.6667        44.0       65.0
 #2    636.0000        57.2     1179.4
</code></pre>
"
6432646,Wrap horizontal legend across multiple rows,1,3,3,"<p>Suppose I have data like the following:</p>

<pre><code>    lab &lt;- ""A really really long string!""
    dat &lt;- data.frame(grp = paste(1:6,lab),x=1:6,y=runif(6))
</code></pre>

<p>When plotting a legend with strings this long, sometimes it can be a challenge to get the legend to fit nicely. If I have to I can always abbreviate the strings to shorten them, but I was wondering if it's possible (most likely using some <code>grid</code> magic) to 'wrap' a legend across multiple rows or columns. For instance, say I position the legend on the bottom, horizontally:</p>

<pre><code>    ggplot(dat,aes(x=x,y=y,colour=grp)) + geom_point() + 
        opts(legend.position=""bottom"",legend.direction=""horizontal"")
</code></pre>

<p>Is it possible to get this legend to display as two rows of three, rather than one row of six?</p>
","<p>To wrap long strings, use <code>strwrap</code>.</p>

<pre><code>lipsum &lt;- ""Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ullamcorper tellus vitae libero placerat aliquet egestas tortor semper. Maecenas pellentesque euismod tristique. Donec semper interdum magna, commodo vehicula ante hendrerit vitae. Maecenas at diam sollicitudin magna mollis lobortis. In nibh elit, tincidunt eu lobortis ac, molestie a felis. Proin turpis leo, iaculis non commodo quis, venenatis at justo. Duis in magna vel erat fringilla gravida quis non nisl. Nunc lacus magna, varius eu luctus vel, luctus tristique sapien. Suspendisse mi dolor, vestibulum at facilisis elementum, lacinia vitae metus. Etiam ut nisl urna, vel tempus mi. In hac habitasse platea dictumst. Quisque pretium volutpat felis, nec tempor diam faucibus at. Praesent volutpat posuere sapien, eu vulputate risus molestie vitae. Proin iaculis quam non leo porttitor hendrerit.""

strwrap(lipsum)
cat(strwrap(lipsum), sep = ""\n"")
# Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ullamcorper tellus
# vitae libero placerat aliquet egestas tortor semper. Maecenas pellentesque euismod
# tristique. Donec semper interdum magna, commodo vehicula ante hendrerit vitae. Maecenas
# at diam sollicitudin magna mollis lobortis. In nibh elit, tincidunt eu lobortis ac,
# molestie a felis. Proin turpis leo, iaculis non commodo quis, venenatis at justo. Duis
# in magna vel erat fringilla gravida quis non nisl. Nunc lacus magna, varius eu luctus
# vel, luctus tristique sapien. Suspendisse mi dolor, vestibulum at facilisis elementum,
# lacinia vitae metus. Etiam ut nisl urna, vel tempus mi. In hac habitasse platea
# dictumst. Quisque pretium volutpat felis, nec tempor diam faucibus at. Praesent
# volutpat posuere sapien, eu vulputate risus molestie vitae. Proin iaculis quam non leo
# porttitor hendrerit.
</code></pre>
"
19949435,3D plot of bivariate distribution using R or Matlab,1,3,3,"<p>i would like to know if someone could tell me how you plot something similar to this
<img src=""https://i.stack.imgur.com/B4cuc.png"" alt=""enter image description here""> with histograms of the sample generates from the code below under the two curves. Using R or Matlab but preferably R.</p>

<pre><code># bivariate normal with a gibbs sampler...

gibbs&lt;-function (n, rho) 
{
  mat &lt;- matrix(ncol = 2, nrow = n)
  x &lt;- 0
  y &lt;- 0
  mat[1, ] &lt;- c(x, y)
  for (i in 2:n) {
    x &lt;- rnorm(1, rho * y, (1 - rho^2))
    y &lt;- rnorm(1, rho * x,(1 - rho^2))
    mat[i, ] &lt;- c(x, y)
  }
  mat
}



bvn&lt;-gibbs(10000,0.98)
par(mfrow=c(3,2))
plot(bvn,col=1:10000,main=""bivariate normal distribution"",xlab=""X"",ylab=""Y"")
plot(bvn,type=""l"",main=""bivariate normal distribution"",xlab=""X"",ylab=""Y"")

hist(bvn[,1],40,main=""bivariate normal distribution"",xlab=""X"",ylab="""")
hist(bvn[,2],40,main=""bivariate normal distribution"",xlab=""Y"",ylab="""")
par(mfrow=c(1,1))`
</code></pre>

<p>Thanks in advance</p>

<p>Best regards,</p>

<p>JC T.</p>
","<p>You could do it in Matlab programmatically.</p>

<p>This is the result:</p>

<p><img src=""https://i.stack.imgur.com/WbUim.png"" alt=""Matlab plot""></p>

<p>Code:</p>

<pre><code>% Generate some data.
data = randn(10000, 2);

% Scale and rotate the data (for demonstration purposes).
data(:,1) = data(:,1) * 2;
theta = deg2rad(130);
data = ([cos(theta) -sin(theta); sin(theta) cos(theta)] * data')';

% Get some info.
m = mean(data);
s = std(data);
axisMin = m - 4 * s;
axisMax = m + 4 * s;

% Plot data points on (X=data(x), Y=data(y), Z=0)
plot3(data(:,1), data(:,2), zeros(size(data,1),1), 'k.', 'MarkerSize', 1);

% Turn on hold to allow subsequent plots.
hold on

% Plot the ellipse using Eigenvectors and Eigenvalues.
data_zeroMean = bsxfun(@minus, data, m);
[V,D] = eig(data_zeroMean' * data_zeroMean / (size(data_zeroMean, 1)));
[D, order] = sort(diag(D), 'descend');
D = diag(D);
V = V(:, order);
V = V * sqrt(D);
t = linspace(0, 2 * pi);
e = bsxfun(@plus, 2*V * [cos(t); sin(t)], m');
plot3(...
    e(1,:), e(2,:), ...
    zeros(1, nPointsEllipse), 'g-', 'LineWidth', 2);

maxP = 0;
for side = 1:2
    % Calculate the histogram.
    p = [0 hist(data(:,side), 20) 0];
    p = p / sum(p);
    maxP = max([maxP p]);
    dx = (axisMax(side) - axisMin(side)) / numel(p) / 2.3;
    p2 = [zeros(1,numel(p)); p; p; zeros(1,numel(p))]; p2 = p2(:);
    x = linspace(axisMin(side), axisMax(side), numel(p));
    x2 = [x-dx; x-dx; x+dx; x+dx]; x2 = max(min(x2(:), axisMax(side)), axisMin(side));

    % Calculate the curve.
    nPtsCurve = numel(p) * 10;
    xx = linspace(axisMin(side), axisMax(side), nPtsCurve);

    % Plot the curve and the histogram.
    if side == 1
        plot3(xx, ones(1, nPtsCurve) * axisMax(3 - side), spline(x,p,xx), 'r-', 'LineWidth', 2);
        plot3(x2, ones(numel(p2), 1) * axisMax(3 - side), p2, 'k-', 'LineWidth', 1);
    else
        plot3(ones(1, nPtsCurve) * axisMax(3 - side), xx, spline(x,p,xx), 'b-', 'LineWidth', 2);
        plot3(ones(numel(p2), 1) * axisMax(3 - side), x2, p2, 'k-', 'LineWidth', 1);
    end

end

% Turn off hold.
hold off

% Axis labels.
xlabel('x');
ylabel('y');
zlabel('p(.)');

axis([axisMin(1) axisMax(1) axisMin(2) axisMax(2) 0 maxP * 1.05]);
grid on;
</code></pre>
"
31507081,How can optimization be used as a solver?,2,2,2,"<p>In a question on <a href=""https://stats.stackexchange.com/"">Cross Validated</a> (<a href=""https://stats.stackexchange.com/questions/159080/how-to-simulate-censored-data"">How to simulate censored data</a>), I saw that the <code>optim</code> function was used as a kind of solver instead of as an optimizer. Here is an example:</p>

<blockquote>
  <pre class=""lang-r prettyprint-override""><code>optim(1, fn=function(scl){(pweibull(.88, shape=.5, scale=scl, lower.tail=F)-.15)^2})
# $par
# [1] 0.2445312
# ...
pweibull(.88, shape=.5, scale=0.2445312, lower.tail=F)
# [1] 0.1500135
</code></pre>
</blockquote>

<p>I have found a tutorial on <code>optim</code> <a href=""http://www.magesblog.com/2013/03/how-to-use-optim-in-r.html"" rel=""nofollow noreferrer"">here</a>, but I am still not able to figure out how to use <code>optim</code> to work as a solver.  I have several questions:  </p>

<ol>
<li><p>What is first parameter (i.e., the value <code>1</code> being passed in)?</p></li>
<li><p>What is the function that is passed in?</p></li>
<li><p>I can understand that it is taking the Weibull probability distribution and subtracting 0.15, but why are we squaring the result?</p></li>
</ol>
","<p>I believe you are referring to my answer.  Let's walk through a few points:  </p>

<ul>
<li><p>The OP (of that question) wanted to generate (pseudo-)random data from a Weibull distribution with specified shape and scale parameters, and where the censoring would be applied for all data past a certain censoring time, and end up with a prespecified censoring rate.  The problem is that once you have specified any three of those, the fourth is necessarily fixed.  You cannot specify all four simultaneously unless you are very lucky and the values you specify happen to fit together perfectly.  As it happened, the OP was not so lucky with the four preferred values&mdash;it was impossible to have all four as they were inconsistent.  At that point, you can decide to specify any three and solve for the last.  The code I presented were examples of how to do that.  </p></li>
<li><p>As noted in the documentation for <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html"" rel=""nofollow"">?optim</a>, the first argument is <code>par</code> ""[i]nitial values for the parameters to be optimized over"".  </p>

<p>Very loosely, the way the optimization routine works is that it calculates an output value given a function and an input value.  Then it 'looks around' to see if moving to a different input value would lead to a better output value.  If that appears to be the case, it moves in that direction and starts the process again.  (It stops when it does not appear that moving in either direction will yield a better output value.)  </p>

<p>The point is that is has to start somewhere, and the user is obliged to specify that value.  In each case, I started with the OP's preferred value (although really I could have started most anywhere).  </p></li>
<li><p>The function that I passed in is <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Weibull.html"" rel=""nofollow"">?pweibull</a>.  It is the <a href=""https://en.wikipedia.org/wiki/Cumulative_distribution_function"" rel=""nofollow"">cumulative distribution function</a> (CDF) of the <a href=""https://en.wikipedia.org/wiki/Weibull_distribution"" rel=""nofollow"">Weibull distribution</a>.  It takes a quantile (<strong>X</strong> value) as its input and returns the proportion of the distribution that has been passed through up to that point.  Because the OP wanted to censor the most extreme 15% of that distribution, I specified that <code>pweibull</code> return the proportion that <em>had not yet been passed through</em> instead (that is the <code>lower.tail=F</code> part).  I then subtracted<code>.15</code> from the result.  </p></li>
<li><p>Thus, the ideal output (from my point of view) would be <strong>0</strong>.  However, it is possible to get values below zero by finding a scale parameter that makes the output of <code>pweibull</code> &lt; .15.  Since <code>optim</code> (or really most any optimizer) finds the input value that minimizes the output value, that is what it would have done.  To keep that from happening, I squared the difference.  That means that when the optimizer went 'too far' and found a scale parameter that yielded an output of <code>.05</code> from <code>pweibull</code>, and the difference was <code>-.10</code> (i.e., <strong>&lt; 0</strong>), the squaring makes the ultimate output <code>+.01</code> (i.e., <strong>> 0</strong>, or worse).  This would push the optimizer back towards the scale parameter that makes <code>pweibull</code> output <strong>(<code>.15</code>-.15)^2 = 0</strong>.  </p></li>
<li><p>In general, the distinction you are making between an ""optimizer"" and a ""solver"" is opaque to me.  They seem like <a href=""https://en.wikipedia.org/wiki/Blind_men_and_an_elephant"" rel=""nofollow"">two different views of the same elephant</a>.  </p></li>
<li><p>Another possible confusion here involves optimization vs. regression.  <a href=""https://en.wikipedia.org/wiki/Mathematical_optimization"" rel=""nofollow"">Optimization</a> is simply about finding an input value[s] that minimizes (maximizes) the output of a function.  In <a href=""https://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">regression</a>, we conceptualize data as draws from a data generating process that is a <a href=""https://en.wikipedia.org/wiki/Stochastic"" rel=""nofollow"">stochastic</a> function.  Given a set of realized values and a functional form, we use optimization techniques to estimate the parameters of the function, thus extracting the data generating process from noisy instances.  Part of regression analyses partakes of optimization then, but other aspects of regression are less concerned with optimization and optimization itself is much larger than regression.  For example, the functions optimized in my answer to the other question are deterministic, and there were no ""data"" being analyzed.  </p></li>
</ul>
"
33844602,R SQLite converts char to numeric when using COUNT,2,2,4,"<p>I'm summarising a SQLite database table through R, however when I use COUNT() it is converting my character field to numeric, i.e. if I have observations <code>0, 1, 2, n</code> I'm getting <code>0, 1, 2, 0</code> back. I've tried the query in SQLite Manager for Firefox, which works as expected; so I expect the issue is to do with RSQLite.</p>

<p>He's a working example of the problem:</p>

<pre><code>install.packages(""RSQLite"")
library(RSQLite)

# Connect
db = dbConnect(SQLite(), ""~/Desktop/test.sqlite"")

# Make a table
dbSendQuery(db, ""
   CREATE TABLE data
      (Year INT,
      Station TXT,
      Obs TXT)
"")

# Generate dummy data
x = data.frame(Year=rep(1995:2004, 100),
    Station=rep(c(""Bob"", ""Ringo"", ""Jarrett"", ""Heron""), 250),
    Obs=rep(c(0:3, ""n""), 200))

# Write dummy data to db
dbWriteTable(db, ""data"", x, append=T, row.names=F)

# Get summary of data back
y = dbGetQuery(db, ""
   SELECT Year, Station, Obs, COUNT(Obs) AS num FROM data
   GROUP BY Year, Station, Obs
"")
</code></pre>

<p>I'd like to know:</p>

<ol>
<li><strong>Why is it doing this?</strong></li>
<li><strong>How can I fix it?</strong></li>
</ol>

<p>I'm using: R 3.2.2, RStudio 0.99.489, Ubuntu 14.04, RSQLite 1.0.0 &amp; DBI 0.3.1</p>
","<p>The reason that happened is that the <code>Obs</code> column was declared to be of type <code>TXT</code> which is not a builtin type.  If it had been declared as type <code>TEXT</code> it would have worked as expected.  </p>

<p>In SQLite the type of a column is only a hint and each row of that column can still have a different type.  Since <code>TXT</code> is not a known type that is in fact what happens.  In the example below the <code>txt</code> and <code>txt2</code> columns are declared to have type <code>TXT</code> in the <code>CREATE</code> statement and <code>TXT</code> is not a builtin type in SQLite (<code>TEXT</code> is).  As we see in the <code>y</code> output below the <code>typeof(txt)</code> column showing the type of each row of the <code>txt</code> column does have different types for different rows.  Ditto for the <code>txt2</code> column as shown in the <code>typeof(txt2)</code> column.   When such data is read back into R the first entry is used by R as the type thus <code>txt</code> having a first entry of <code>0</code> with ijnteger type is read into R as integer and <code>txt2</code> having a first entry of <code>n</code> with text type is read into R as character.  This can be seen by looking at the <code>str(y)</code> output below.  We also note that the <code>text</code> column is read into R as character since <code>text</code> is a known datatype in SQLite.</p>

<pre><code>library(RSQLite)
db &lt;- dbConnect(SQLite(), "":memory:"")

dbSendQuery(db, ""CREATE TABLE d (txt TXT, txt2 TXT, text TEXT)"")
x &lt;- data.frame(txt = c(0, ""n""), txt2 = c(""n"", 0), text2 = c(0, ""n"")) 
dbWriteTable(db, ""d"", x, append = TRUE, row.names = FALSE)

y &lt;- dbGetQuery(db, 
         ""select txt, typeof(txt), txt2, typeof(txt2), text, typeof(text) from d"")

y
##  txt typeof(txt) txt2 typeof(txt2) text typeof(text)
## 1   0     integer    n         text    0         text
## 2   0        text    0      integer    n         text

str(y)
## 'data.frame':   2 obs. of  6 variables:
## $ txt         : int  0 0   &lt;------ becomes int since 1st entry of txt is int
## $ typeof(txt) : chr  ""integer"" ""text""
## $ txt2        : chr  ""n"" ""0""  &lt;---- chr since 1st entry is text
## $ typeof(txt2): chr  ""text"" ""integer""
## $ text        : chr  ""0"" ""n""  &lt;--- chr since column declared as text in create
## $ typeof(text): chr  ""text"" ""text""
</code></pre>

<p>For more info see the <a href=""https://www.sqlite.org/datatype3.html"" rel=""nofollow"">sqlite documentation on datatypes</a>.</p>
"
38296236,R crash using xml2 package in a loop,2,2,2,"<p>I have a directory with approximately 46000 links (I called links3) and I want to scrape each of them with the following code:  </p>

<pre><code>library(reshape)
library(plyr)
library(rvest)
library(xml2)

base &lt;- matrix(, nrow = (nrow(links3)), ncol = 19)
basedes &lt;- matrix(, nrow = (nrow(links3)), ncol = 19)
coor &lt;- matrix(, nrow = (nrow(links3)), ncol = 3)


for (i in 1:(nrow(links3))){
x &lt;- links3[i,1]
doc &lt;- read_html(paste0(x))
val &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[1]/h3/b""))
if(val==""&lt;b&gt;Tipo Inmueble&lt;/b&gt;""){
basedes[i,1] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[1]/h3/b""))
basedes[i,2] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[3]/h3/b""))
basedes[i,3] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[5]/h3/b""))
basedes[i,4] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[7]/h3/b""))
basedes[i,5] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[9]/h3/b""))
basedes[i,6] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[11]/h3/b""))
basedes[i,7] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[13]/h3/b""))
basedes[i,8] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[15]/h3/b""))
basedes[i,9] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[17]/h3/b""))
basedes[i,10] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[19]/h3/b""))
basedes[i,11] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[21]/h3/b""))
basedes[i,12] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[23]/h3/b""))
basedes[i,13] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[1]/h3/b""))
basedes[i,14] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[3]/h3/b""))
basedes[i,15] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[5]/h3/b""))
basedes[i,16] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[7]/h3/b""))
basedes[i,17] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[9]/h3/b""))
basedes[i,18] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[11]/h3/b""))

base[i,1] &lt;- toString(xml_find_all(doc, xpath=""*//*[@id='info_nombre']/ul[1]/li[2]/h4""))
base[i,2] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[4]/h4""))
base[i,3] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[6]/h4""))
base[i,4] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[8]/h4""))
base[i,5] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[10]/h4""))
base[i,6] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[12]/h4""))
base[i,7] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[14]/h4""))
base[i,8] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[16]/h4""))
base[i,9] &lt;- toString(xml_find_all(doc, xpath=""//***[@id='info_nombre']/ul[1]/li[18]/h4""))
base[i,10] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[20]/h4""))
base[i,11] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[22]/h4""))
base[i,12] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[1]/li[24]/h4""))
base[i,13] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[2]/h4""))
base[i,14] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[4]/h4""))
base[i,15] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[6]/h4""))
base[i,16] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[8]/h4""))
base[i,17] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[10]/h4""))
base[i,18] &lt;- toString(xml_find_all(doc, xpath=""//*[@id='info_nombre']/ul[2]/li[12]/h4""))


lon &lt;- toString(xml_find_all(doc, xpath=""*//input[@type='hidden']""))
lon &lt;- gsub("".*longitud"", """", lon)
lon &lt;- gsub(""/&gt;.*"", """", lon)
lon &lt;- gsub(""value="", """", lon)
lon &lt;- gsub(""\"""", """", lon)
lon &lt;- gsub("" "", """", lon)
coor[i,1] &lt;- lon

lat &lt;- toString(xml_find_all(doc, xpath=""*//input[@type='hidden']""))
lat &lt;- gsub("".*latitud"", """", lat)
lat &lt;- gsub(""/&gt;.*"", """", lat)
lat &lt;- gsub(""value="", """", lat)
lat &lt;- gsub(""\"""", """", lat)
lat &lt;- gsub("" "", """", lat)
coor[i,2] &lt;- lat

cod&lt;-  toString(xml_find_all(doc, xpath=""*//div[@class='code']""))
cod &lt;- gsub("".*web: "", """", cod)
cod &lt;- gsub(""&lt;.*"", """", cod)
coor[i,3] &lt;- cod
remove(val)
remove(lat)
remove(lon)
}else{}
}
</code></pre>

<p>But after approximately 5000 iterations I receive the following message: </p>

<pre><code>r for windows gui frontend stopped working 
</code></pre>

<p>I have to close R session and start again. </p>

<p>I have looked my system configuration and It shows the following: </p>

<pre><code>Processes: 93;
CPU Usage 6%;
Physical Memory: 51%;
RAM:8.0 GB;
Windows 7;
</code></pre>

<p>Why I'm having this message? How can I solve it?</p>
","<p>It is not 100% clear from the data and code you have given here what the problem is. However, when you are creating a lot of variables. Each variable lives in active memory, in your RAM. </p>

<p>And despite the configuration at 51%, depending on your system, whether or not there are any java packages in play and your use of garbage collection you could be running out of usable RAM available to R and its subprocesses.</p>

<p>If it were me hitting this problem and I was sure the source was not crashing me (by going to the actual iterations it crashes on directly and check for errors or bad data one at a time) or that the script had a bug in it, I would look at what my data needs are, which of the variables created need to persist for the next step and which do not. </p>

<p>Then I would aggressively use <code>rm()</code> and <code>gc()</code> to remove and garbage collect to clear out the cruft and see if it goes further the next time. You may need to do this at a few steps <em>INSIDE</em> your loop to remove byproducts that are not clearing themselves.</p>

<p>If you cannot purge any of the data acquired, then I would still use <code>rm()</code> and <code>gc()</code> to remove some of the needed data just to see if it continues through the entire loop. If it does, then you know that it is in fact a memory management issue. </p>

<p>Now it becomes about writing a script that gathers data in bunches, converts it to the most compact format and stores it in persistent memory until all the processing is done, then loads it into RAM for you analysis. You can use csv's or json files.</p>

<p>R is a fabulously powerful language that works very well in many circumstances, but demands that you manage interim data and variables well when the data gets large. A simple table may persist in as many as 4 instances depending on how it is manipulated and managed. So, if you have a properly functioning script, then you will need to work on how you manage what you get out of that script and bring it back into the analytical process.</p>

<p>For me, I tend to work at big files like I would eat an entire cow, one bite at a time. Poke into static memory for values as needed and update static files as you go.</p>

<p>I hope this helps.</p>
"
21425662,Is there a way to prevent the download page from opening in R Shiny?,2,2,2,"<p>Once the download button has been clicked in a shiny app, a new page opens initialising the download. However, my download handler takes some time to produce the downloadable file which is shown in a progress bar on the main shiny page. Is there a way to keep the user on the main page or to prevent the download page from opening or postponing the download page till the file has been produced?</p>

<p>Many Thanks</p>

<p>Marcus</p>
","<p>Vincent's solution using two buttons, an action button for the calculation and a download button for the download is the one I went with. An additional bonus to this solution is the progress bar that also comes in the shinyIncubator package.</p>

<p>An explanation of my code incase someone else wants to do the same thing:</p>

<p>The ui.R has an action button and a dynamic download button:</p>

<pre><code>actionButton(""makePlots"", ""Calculate Results""),
uiOutput(""download_button"")
</code></pre>

<p>and a progress initialisation for the progress bar:</p>

<pre><code>  mainPanel(
    progressInit(),
      uiOutput(""mytabs"")) # dynamic rendering of the tabs
</code></pre>

<p>The server.R is a little more complex. So that the download button is only shown when there is something to download I used the dynamic uiOutput with the following code:</p>

<pre><code>    output$download_button &lt;- renderUI({
          if(download){
              downloadButton(""downloadPlots"", ""Download Results"")
          }
     })
</code></pre>

<p>The download button is only shown when <code>download==TRUE</code>. At the start of server.R the variable is initialised: <code>download&lt;-FALSE</code></p>

<p>As the action button increases by 1 everytime it is clicked I included a counter (initial value 0) that increases after each ""use"" of the action button. Reason for this is the first if statement.</p>

<pre><code>makePlots&lt;-reactive({

    if(input$makePlots&gt;counter){ # tests whether action button has been clicked

       dir.create(""new_directory_for_output"")

       withProgress(session, min=1, max=15, expr={ # setup progress bar

       for(i in 1:15){

         png(paste0(""new_directory_for_output/plot"",i,"".png""))
         plot(i)
         dev.off()

         setProgress(message = 'Calculation in progress',
              detail = 'This may take a while...',
              value=i)

       } # end for

       }) # end progress bar

       counter&lt;&lt;-counter+1 # needs the &lt;&lt;- otherwise the value of counter
                           # is only changed within the function

       download&lt;&lt;-TRUE     # something to download     

     } # end if

}) # end function
</code></pre>

<p>At this stage the function makePlots() doesn't have an output and isn't called anywhere so it does nothing. I therefore placed makePlots() at the beginning of each tab so that no matter which tab the user is on, once the action button has been clicked the plots are made and saved.</p>

<p>The final piece of teh puzzle is the download handler:</p>

<pre><code>output$downloadPlots &lt;- downloadHandler(

    filename = function() { my_filename.zip },
    content = function(file){

      fname &lt;- paste(file,""zip"",sep=""."") 
      zip(fname,new_directory_for_output) # zip all files in the directory
      file.rename(fname,file)

      unlink(new_directory_for_output,recursive = TRUE) # delete temp directory
      download&lt;&lt;-FALSE # hide download button 
    }

    ) # end download handler
</code></pre>
"
41383804,regex - define boundary using characters & delimiters,2,2,4,"<p>I realize this is a rather simple question and I have searched throughout this site, but just can't seem to get my syntax right for the following regex challenges.  I'm looking to do two things.  First have the regex to pick up the first three characters and stop at a semicolon.  For example, my string might look as follows:</p>

<pre><code>Apt;House;Condo;Apts;
</code></pre>

<p>I'd like to go here</p>

<pre><code>Apartment;House;Condo;Apartment
</code></pre>

<p>I'd also like to create a regex to substitute a word in between delimiters, while keep others unchanged.  For example, I'd like to go from this:</p>

<pre><code>feline;labrador;bird;labrador retriever;labrador dog; lab dog;
</code></pre>

<p>To this:</p>

<pre><code>feline;dog;bird;dog;dog;dog;
</code></pre>

<p>Below is the regex I'm working with.  I know ^ denotes the beginning of the string and $ the end.  I've tried many variations, and am making substitutions, but am not achieving my desired out put.  I'm also guessing one regex could work for both?  Thanks for your help everyone. </p>

<pre><code>df$variable &lt;- gsub(""^apt$;"", ""Apartment;"", df$variable, ignore.case = TRUE)
</code></pre>
","<p>Here is an approach that uses look behind (so you need <code>perl=TRUE</code>):</p>

<pre><code>&gt; tmp &lt;- c(""feline;labrador;bird;labrador retriever;labrador dog; lab dog;"",
+          ""lab;feline;labrador;bird;labrador retriever;labrador dog; lab dog"")
&gt; gsub( ""(?&lt;=;|^) *lab[^;]*"", ""dog"", tmp, perl=TRUE)
[1] ""feline;dog;bird;dog;dog;dog;""   
[2] ""dog;feline;dog;bird;dog;dog;dog""
</code></pre>

<p>The <code>(?&lt;=;|^)</code> is the look behind, it says that any match must be preceded by either a semi-colon or the beginning of the string, but what is matched is not included in the part to be replaced.  The <code>*</code> will match 0 or more spaces (since your example string had one case where there was space between the semi-colon and the <code>lab</code>.  It then matches a literal <code>lab</code> followed by 0 or more characters other than a semi-colon.  Since <code>*</code> is by default greedy, this will match everything up to, but not including' the next semi-colon or the end of the string.  You could also include a positive look ahead <code>(?=;|$)</code> to make sure it goes all the way to the next semi-colon or end of string, but in this case the greediness of <code>*</code> will take care of that.</p>

<p>You could also use the non-greedy modifier, then force to match to end of string or semi-colon:</p>

<pre><code>&gt; gsub( ""(?&lt;=;|^) *lab.*?(?=;|$)"", ""dog"", tmp, perl=TRUE)
[1] ""feline;dog;bird;dog;dog;dog;""   
[2] ""dog;feline;dog;bird;dog;dog;dog""
</code></pre>

<p>The <code>.*?</code> will match 0 or more characters, but as few as it can get away with, stretching just until the next semi-colon or end of line.</p>

<p>You can skip the look behind (and <code>perl=TRUE</code>) if you match the delimiter, then include it in the replacement:</p>

<pre><code>&gt; gsub(""(;|^) *lab[^;]*"", ""\\1dog"", tmp)
[1] ""feline;dog;bird;dog;dog;dog;""   
[2] ""dog;feline;dog;bird;dog;dog;dog""
</code></pre>

<p>With this method you need to be careful that you only match the delimiter on one side (the first in my example) since the match consumes the delimiter (not with the look-ahead or look-behind), if you consume both delimiters, then the next will be skipped and only every other field will be considered for replacement.</p>
"
17241981,Counting and permuting elements in a list,1,1,3,"<p>I have a list that looks like this:</p>

<pre><code>list.1 &lt;- list(a=c(0.1,0.2,0.2,0.3,0.12), 
             b=c(0.1,0.2), 
             c=c(0.3,0.1,0.2), 
             d=c(0.1,0.3,0.4,0.5))
</code></pre>

<p>and I would like to generate a new list with random (permuted) values from <code>list.1</code>, my approach is the following:</p>

<pre><code>rand.list &lt;- lapply(list.1, FUN=function(x) replicate(10, sample(x,1)))
</code></pre>

<p>However, if the length of an element in the list is less than a number, e.g 4, then I would like to use also other elements (previous and next) in the list for calculating the permutations, taking in account that the length of all these elements is > 4. For example, in my list, <code>length(list.1$b) == 2</code> and <code>length(list.1$c)==3</code>, I would like to use for randomising <code>list.1$b</code> values from <code>list.1$a</code> and <code>list.1$c</code>, and for randomising <code>list.1$c</code> values from <code>list.1$b</code> and <code>list.1$d</code>. Any ideas how to achieve this??</p>

<p>Many thanks in advance </p>

<p>UPDATE</p>

<p>A solution provide for @dardisco:</p>

<pre><code>l2 &lt;- list()
ll1 &lt;- length(list.1)
length(l2) &lt;- ll1
set.seed(4)
for (i in 1:ll1){
   vec1 &lt;- list.1[[i]]
    jl &lt;- 1;jr&lt;-1; #i've added two counters one for left one for right
    while (length(vec1) &lt; 4){
       if(i==1) {
          vec1 &lt;- c(vec1, list.1[[i+jr]])
          jr &lt;- jr+1
        } else if (i==ll1 || jr+i==ll1 ){ #to avoid out of boundaries, so many elements with less than 4 elements close to the end
           vec1 &lt;- c(vec1, list.1[[i-jl]])
           jl &lt;- jl+1
    }else {
           vec1 &lt;- c(vec1, list.1[[i-jl]], list.1[[i+jr]])
               jl &lt;- jl+1
        jr &lt;- jr+1
        } 
 } 
   l2[[i]] &lt;- sample2(vec1, 10, replace=TRUE)   
}
</code></pre>
","<p>Following on from @DWins advice - is this what you're looking for? </p>

<pre><code>l2 &lt;- list()
length(l2) &lt;- length(list.1)
set.seed(1)
for (i in 1:length(list.1)){
    if ( length(list.1[[i]]) &gt;=4 ){
        l2[[i]] &lt;- sample(list.1[[i]], 10, replace=TRUE)
        } else {
            l2[[i]] &lt;- sample(c(list.1[[i]],list.1[[i-1]],list.1[[i+1]]),
                                10, replace=TRUE)
            }
    }
</code></pre>

<p>Note that this assumes the first and last elements in the list have >=4 elements.</p>

<p><strong>Update</strong></p>

<p>In light of your comment - start with an example which better illustrates the problem:</p>

<pre><code>list.1 &lt;- list(a=letters[1:2],
               b=letters[3],
               c=letters[10:14],
               d=letters[25:26])
</code></pre>

<p>then</p>

<pre><code>l2 &lt;- list()
ll1 &lt;- length(list.1)
### ll1 = length of list.1
length(l2) &lt;- ll1
set.seed(4)
for (i in 1:ll1){
### vec1 = default vector from which to sample
    vec1 &lt;- list.1[[i]]
### j = counter for position relative to current
    j &lt;- 1
### if sample size &lt;4 (the threshold) then include additional elements in list until &gt;=4
### change this to 50 if required, as in:
### while (length(vec1) &lt;50){
    while (length(vec1) &lt;4){
### check if at first element
        if(i==1) {
### keep adding successive elements from list.1 to end of vec1
            vec1 &lt;- c(vec1, list.1[[i+j]])
            j &lt;- j+1
### if at last element, add preceding elements
        } else if (i==ll1 ){
            vec1 &lt;- c(vec1, list.1[[i-j]])
            j &lt;- j+1
        } else {
### you could add both in one step, like so:
### vec1 &lt;- c(vec1, list.1[[i-j]], list.1[[i+j]])
### j &lt;- j+1
### }
### or do it in two steps as below:
###
### k = counter to indicate moving forward or back 
            k &lt;- 1
### if odd, add next element
            if (!k %% 2==0){
                vec1 &lt;- c(vec1, list.1[[i+j]])
            } else {
### if even, add preceding element and increment counter for relative position
                vec1 &lt;- c(vec1, list.1[[i-j]])
                j &lt;- j+1
            }
            k &lt;- k+1
        }
    }
    l2[[i]] &lt;- sample(vec1, 10, replace=TRUE)
}
</code></pre>

<p>This should do what you want, although there may be a prettier way. Gains from vectorisation are likely to be modest at best.</p>
"
29952517,R: Pivoting using 'spread' function,1,1,1,"<p>Continuing from my previous <a href=""https://stackoverflow.com/questions/29773714/r-pivot-the-rows-into-columns-and-use-n-as-for-missing-values"">post</a>, I am now having 1 more column of ID values that I need to use to pivot rows into columns. </p>

<pre><code>    NUM &lt;- c(1,2,3,1,2,3,1,2,3,1)
    ID &lt;- c(""DJ45"",""DJ45"",""DJ45"",""DJ46"",""DJ46"",""DJ46"",""DJ47"",""DJ47"",""DJ47"",""DJ48"")
    Type &lt;- c(""A"", ""F"", ""C"", ""B"", ""D"", ""A"", ""E"", ""C"", ""F"", ""D"")
    Points &lt;- c(9.2,60.8,22.9,1012.7,18.7,11.1,67.2,63.1,16.7,58.4)

    df1 &lt;- data.frame(ID,NUM,Type,Points)

df1:
    +------+-----+------+--------+
    | ID   | Num | Type | Points |
    +------+-----+------+--------+
    | DJ45 |   1 | A    | 9.2    |
    | DJ45 |   2 | F    | 60.8   |
    | DJ45 |   3 | C    | 22.9   |
    | DJ46 |   1 | B    | 1012.7 |
    | DJ46 |   2 | D    | 18.7   |
    | DJ46 |   3 | A    | 11.1   |
    | DJ47 |   1 | E    | 67.2   |
    | DJ47 |   2 | C    | 63.1   |
    | DJ47 |   3 | F    | 16.7   |
    | DJ48 |   1 | D    | 58.4   |
    +------+-----+------+--------+
</code></pre>

<p>My desired output is </p>

<pre><code>+------+-----+------+--------+------+------+------+------+
| ID   | Num |  A   |   B    |  C   |  D   |  E   |  F   |
+------+-----+------+--------+------+------+------+------+
| DJ45 |   1 | 9.2  | N/A    | N/A  | N/A  | N/A  | N/A  |
| DJ45 |   2 | N/A  | N/A    | N/A  | N/A  | N/A  | 60.8 |
| DJ45 |   3 | N/A  | N/A    | 22.9 | N/A  | N/A  | N/A  |
| DJ46 |   1 | N/A  | 1012.7 | N/A  | N/A  | N/A  | N/A  |
| DJ46 |   2 | N/A  | N/A    | N/A  | 18.7 | N/A  | N/A  |
| DJ46 |   3 | 11.1 | N/A    | N/A  | N/A  | N/A  | N/A  |
| DJ47 |   1 | N/A  | N/A    | N/A  | N/A  | 67.2 | N/A  |
| DJ47 |   2 | N/A  | N/A    | 63.1 | N/A  | N/A  | N/A  |
| DJ47 |   3 | N/A  | N/A    | N/A  | N/A  | N/A  | 16.7 |
| DJ48 |   1 | N/A  | N/A    | N/A  | 58.4 | N/A  | N/A  |
+------+-----+------+--------+------+------+------+------+
</code></pre>

<p>I am using <code>spread</code> function in R but getting errors saying duplicate identifiers. This is because I have 2 columns now (ID &amp; NUM) instead of one (NUM) that I had previously. Please let me know how I could do this. </p>
","<p>Not knowing what you've tried, I would suggest:</p>

<pre><code>spread(df1, Type, Points)
#      ID NUM    A      B    C    D    E    F
# 1  DJ45   1  9.2     NA   NA   NA   NA   NA
# 2  DJ45   2   NA     NA   NA   NA   NA 60.8
# 3  DJ45   3   NA     NA 22.9   NA   NA   NA
# 4  DJ46   1   NA 1012.7   NA   NA   NA   NA
# 5  DJ46   2   NA     NA   NA 18.7   NA   NA
# 6  DJ46   3 11.1     NA   NA   NA   NA   NA
# 7  DJ47   1   NA     NA   NA   NA 67.2   NA
# 8  DJ47   2   NA     NA 63.1   NA   NA   NA
# 9  DJ47   3   NA     NA   NA   NA   NA 16.7
# 10 DJ48   1   NA     NA   NA 58.4   NA   NA
</code></pre>

<p>If you are getting an error about duplicate identifiers, it is because the combination of ""ID"" and ""Num"" in your actual data have one or more duplicate entries (in your sample data, they don't).</p>

<p>If that is the case, you need to add another column to make them unique.</p>

<p>Adding <code>dplyr</code> into the chain, it might be something like:</p>

<pre><code>df1 %&gt;%
  group_by(ID, NUM) %&gt;%
  mutate(id2 = sequence(n())) %&gt;%
  spread(Type, Points)
</code></pre>

<hr>

<p>Demo of <em>assumed</em> error:</p>

<pre><code>df2 &lt;- rbind(df1, df1[1:3, ]) ## Duplicate the first three rows
spread(df2, Type, Points)
# Error: Duplicate identifiers for rows (1, 11), (3, 13), (2, 12)    

library(dplyr)

df2 %&gt;%
  group_by(ID, NUM) %&gt;%
  mutate(id2 = sequence(n())) %&gt;%
  spread(Type, Points)
# Source: local data frame [13 x 9]
# 
#      ID NUM id2    A      B    C    D    E    F
# 1  DJ45   1   1  9.2     NA   NA   NA   NA   NA
# 2  DJ45   1   2  9.2     NA   NA   NA   NA   NA
# 3  DJ45   2   1   NA     NA   NA   NA   NA 60.8
# 4  DJ45   2   2   NA     NA   NA   NA   NA 60.8
# 5  DJ45   3   1   NA     NA 22.9   NA   NA   NA
# 6  DJ45   3   2   NA     NA 22.9   NA   NA   NA
# 7  DJ46   1   1   NA 1012.7   NA   NA   NA   NA
# 8  DJ46   2   1   NA     NA   NA 18.7   NA   NA
# 9  DJ46   3   1 11.1     NA   NA   NA   NA   NA
# 10 DJ47   1   1   NA     NA   NA   NA 67.2   NA
# 11 DJ47   2   1   NA     NA 63.1   NA   NA   NA
# 12 DJ47   3   1   NA     NA   NA   NA   NA 16.7
# 13 DJ48   1   1   NA     NA   NA 58.4   NA   NA
</code></pre>
"
21667581,Changing attribute of nodes during breadth first search in R,1,3,3,"<p>I have created a random (Erdos-Renyi) graph that has 100 nodes. I have set an attribute value for all 100 nodes as 0. I find the node with the maximum degree (the most neighbors), and change its attribute value from 0 to 1. Then, using the node as the root node, and another node as a second root node, I do a breadth first search (BFS) on the network.</p>

<p>This is related to this <a href=""https://stackoverflow.com/questions/20671888/changing-attribute-values-of-nodes-in-a-network-during-breadth-first-search-in-r"">question</a>.</p>

<p>I do the breadth first search like this:</p>

<pre><code># BFS on the network
bfs &lt;- graph.bfs(graph, root = c(root_node, root_node2), unreachable = FALSE,
    order = TRUE, dist = TRUE)
</code></pre>

<p>I want to look at the neighbors of the first root node, then the neighbors of the second root node, then the neighbors of the first root node's neighbors, then the neighbors of the second root node's neighbors, and so on.</p>

<p>So something like this:</p>

<pre><code>                O                        # Note: O* is the first root node
                |                        # and O! is the second root node
                |
O----O----O!----O----O*----O----O----O
          |          |
          |          |
          O          O
</code></pre>

<p>So, to start with, the neighbors of the first root node are looked at:</p>

<pre><code>                O                        # Note: double connections are
                |                        # the paths taken to the neighbors
                |
O----O----O!----O====O*====O----O----O
          |          ||
          |          ||
          O          O
</code></pre>

<p>Then the neighbors of the second root node are looked at:</p>

<pre><code>                O
                |
                |
O----O====O!====O----O*----O----O----O
          ||         |
          ||         |
          O          O
</code></pre>

<p>Then, the neighbors of the first root node's neighbors:</p>

<pre><code>                O
                ||
                ||
O----O----O!----O----O*----O====O----O
          |          |
          |          |
          O          O
</code></pre>

<p>Then the neighbors of the second root node's neighbors:</p>

<pre><code>                O
                |
                |
O====O----O!----O----O*----O----O----O
          |          |
          |          |
          O          O
</code></pre>

<p>And so on until all of the nodes have been looked at:</p>

<pre><code>                O
                |
                |
O----O----O!----O----O*----O----O====O
          |          |
          |          |
          O          O
</code></pre>

<p>As each node is looked at, I want to change its attribute value from 0 to 1, so that if another path comes to it, it that knows this node has already been looked at.</p>

<p>Also, is there a way to count how many iterations if takes to look through all of the nodes? For example, here it is 6 (including the original).</p>

<p>Note: the two root nodes are connected in some way (i.e. there is a path between them).</p>

<p>Sorry about the images, but that's the basic idea. Hope this makes sense.</p>

<p>Any help would be much appreciated. Thanks!</p>
","<p>Here is how to do it. First, here is a randomly generated graph.</p>

<pre><code>numnodes &lt;- 50
the.graph &lt;- grg.game(numnodes, 0.3)

V(the.graph)$visited &lt;- 0
graph.degree &lt;- degree(the.graph)
</code></pre>

<p>Now, we take the maximum vertex and a random vertex. (You didn't specify how you chose the second one). We randomly repick the vertex until it is connected to and is not the maximum degree vertex.</p>

<pre><code>maxvertex &lt;- sample(which(graph.degree == max(graph.degree)),1)
randvertex &lt;- as.integer(sample(V(the.graph),1))
while((randvertex == maxvertex) ||
      (shortest.paths(the.graph,maxvertex,randvertex) == Inf)) {
  randvertex &lt;- sample(V(the.graph),1)
}
</code></pre>

<p>When traversing graphs like this, I like to keep track of where I am. Here is the starting position and a line to mark these initial nodes as visited.</p>

<pre><code>curpos &lt;- c(maxvertex, randvertex)
for(num in curpos) V(the.graph)[num]$visited &lt;- 1
</code></pre>

<p>Now we actually do the search and mark nodes as visited. The loop will terminate if all of the nodes are marked as visited or if there are no more connected nodes to explore. If the code is bugged, we know there shouldn't be more iterations than steps for the search, so we know if it goes over the graph is not connected and we needn't continue. For each iteration, we go through the vector containing our currently occupied nodes. If any of its neighbors haven't been visited, we mark them as visited and add them to the vector for next time. Once we have visited all of the nodes for this iteration, we start the next cycle.</p>

<pre><code>maxloops = length(V(the.graph))
curloop = 0
while((curloop &lt; maxloops) &amp;&amp; (length(curpos)&gt;0) &amp;&amp;
      (sum(V(the.graph)$visited) &lt; numnodes)) {
  nextpos &lt;- c()
  while(length(curpos)&gt;0) {
    curnode &lt;- curpos[1]
    curpos &lt;- curpos[-1]

    adjnodes &lt;- which(the.graph[curnode] == 1)
    for(adjnode in adjnodes) {
      if(!V(the.graph)[adjnode]$visited) {
        nextpos &lt;- c(nextpos,adjnode)
        V(the.graph)[adjnode]$visited &lt;- 1
      }
    }
  }
  curpos &lt;- nextpos
  curloop &lt;- curloop + 1
}
</code></pre>

<p>Now we have visited all nodes connected to the maximal degree node. We now print the number of iterations it took to traverse the graph. If any nodes are not visited, this will additionally print a message stating that the graph is not connected.</p>

<pre><code>print(curloop)
if(sum(V(the.graph)$visited) &lt; numnodes) print(""Not a connected graph."")
</code></pre>
"
29667104,Using regular expressions in R to extract information from string,2,2,4,"<p>I searched the stack overflow a little and all I found was, that regex in R are a bit tricky and not convenient compared to Perl or Python.</p>

<p>My problem is the following. I have long file names with informations inside. The look like the following:</p>

<pre><code>20150416_QEP1_EXT_GR_1234_hs_IP_NON_060.raw
20150416_QEP1_EXT_GR_1234-1235_hs_IP_NON_060.raw
20150416_QEP1_EXT_GR_1236_hs_IP_NON_060_some_other_info.raw
20150416_QEP1_EXT_GR_1237_hs_IP_NON_060
</code></pre>

<p>I want to extract the parts from the filename and convert them conveniently into values, for example the first part is a date, the second is machine abbreviation, the next an institute abbreviation, group abbreviation, sample number(s) etc...</p>

<p>What I do at the moment is constructing a regex, to make (almost) sure, I grep the correct part of the string:</p>

<pre><code>regex &lt;- '([:digit:]{8})_([:alnum:]{1,4})_([:upper:]+)_ etc'
</code></pre>

<p>Then I use <code>sub</code> to save each snipped into a variable:</p>

<pre><code>date &lt;- sub(regex, '\\1', filename)
machine &lt;- sub(regex, '\\2', filename)
etc
</code></pre>

<p>This works, if the filename has the correct convention. It is overall very hard to read and I am search for a more convenient way of doing the work. I thought about splitting the filename by _ and accessing the string by index might be a good solution. But sometimes, since the filenames often get created by hand, there are terms missing or additional information in the names and I am looking for a better solution to this.</p>

<p>Can anyone suggest a better way of doing so?</p>

<p><strong>EDIT</strong></p>

<p>What I want to create is an object, which has all the information of the filenames extracted and accessible... such as <code>my_object$machine</code> or so....</p>
","<p>The help page for <code>?regex</code> actually gives an example that is exactly equivalent to Python's <code>re.match(r""(?P&lt;first_name&gt;\w+) (?P&lt;last_name&gt;\w+)"", ""Malcolm Reynolds"")</code> (as per your comment):</p>

<blockquote>
<pre><code>## named capture
notables &lt;- c(""  Ben Franklin and Jefferson Davis"",
              ""\tMillard Fillmore"")
#name groups 'first' and 'last'
name.rex &lt;- ""(?&lt;first&gt;[[:upper:]][[:lower:]]+) (?&lt;last&gt;[[:upper:]][[:lower:]]+)""
(parsed &lt;- regexpr(name.rex, notables, perl = TRUE))
gregexpr(name.rex, notables, perl = TRUE)[[2]]
parse.one &lt;- function(res, result) {
  m &lt;- do.call(rbind, lapply(seq_along(res), function(i) {
    if(result[i] == -1) return("""")
    st &lt;- attr(result, ""capture.start"")[i, ]
    substring(res[i], st, st + attr(result, ""capture.length"")[i, ] - 1)
  }))
  colnames(m) &lt;- attr(result, ""capture.names"")
  m
}
parse.one(notables, parsed)
</code></pre>
</blockquote>

<p>The normal way (i.e. the R way) to extract from a string is the following:</p>

<pre><code>text &lt;- ""Malcolm Reynolds""
x &lt;- gregexpr(""\\w+"", text) #Don't forget to escape the backslash
regmatches(text, x)
[[1]]
[1] ""Malcolm""  ""Reynolds""
</code></pre>

<p>You can use however Perl-style group naming by using argument <code>perl=TRUE</code>:</p>

<pre><code>regexpr(""(?P&lt;first_name&gt;\\w+) (?P&lt;last_name&gt;\\w+)"", text, perl=TRUE)
</code></pre>

<p>However <code>regmatches</code> does not support it, hence the need to create your own function to handle that, which is given in the help page:</p>

<pre><code>parse.one &lt;- function(res, result) {
       m &lt;- do.call(rbind, lapply(seq_along(res), function(i) {
         if(result[i] == -1) return("""")
         st &lt;- attr(result, ""capture.start"")[i, ]
         substring(res[i], st, st + attr(result, ""capture.length"")[i, ] - 1)
       }))
       colnames(m) &lt;- attr(result, ""capture.names"")
       m
     }
</code></pre>

<p>Applied to your example:</p>

<pre><code> text &lt;- ""Malcolm Reynolds""
 x &lt;- regexpr(""(?P&lt;first_name&gt;\\w+) (?P&lt;last_name&gt;\\w+)"", text, perl=TRUE)
 parse.one(text, x)
     first_name last_name 
[1,] ""Malcolm""  ""Reynolds""
</code></pre>

<p>To go back to your initial problem:</p>

<pre><code>filenames &lt;- c(""20150416_QEP1_EXT_GR_1234_hs_IP_NON_060.raw"", ""20150416_QEP1_EXT_GR_1234-1235_hs_IP_NON_060.raw"", ""20150416_QEP1_EXT_GR_1236_hs_IP_NON_060_some_other_info.raw"", ""20150416_QEP1_EXT_GR_1237_hs_IP_NON_060"")
regex &lt;- '(?P&lt;date&gt;[[:digit:]]{8})_(?P&lt;machine&gt;[[:alnum:]]{1,4})_(?P&lt;whatev&gt;[[:upper:]]+)'
x &lt;- regexpr(regex,filenames,perl=TRUE)
parse.one(filenames,x)
     date       machine whatev
[1,] ""20150416"" ""QEP1""  ""EXT"" 
[2,] ""20150416"" ""QEP1""  ""EXT"" 
[3,] ""20150416"" ""QEP1""  ""EXT"" 
[4,] ""20150416"" ""QEP1""  ""EXT"" 
</code></pre>
"
11505625,How to find the distance to nearest non-overlapping element?,2,3,3,"<p>I have a table like the one below, where each cluster (column 1) contains annotations of different elements (column 4) in small regions with a start (column 2) and an end (column 3) coordinate. For each entry, I would like to add a column corresponding to the distance to the nearest other element in that cluster. But I want to exclude cases where a pair of elements in the cluster have identical start/end coordinates or overlapping regions. How can I produce such extra <code>nearest_distance</code> column for such data frame?</p>

<pre><code>cluster-47593-walk-0125 252     306     AR    
cluster-47593-walk-0125 6       23      ZNF148
cluster-47593-walk-0125 357     381     CEBPA 
cluster-47593-walk-0125 263     276     CEBPB 
cluster-47593-walk-0125 246     324     NR3C1 
cluster-47593-walk-0125 139     170     HMGA1 
cluster-47593-walk-0125 139     170     HMGA2 
cluster-47593-walk-0125 207     227     IRF8  
cluster-47593-walk-0125 207     227     IRF1  
cluster-47593-walk-0125 207     245     IRF2  
cluster-47593-walk-0125 207     227     IRF3  
cluster-47593-walk-0125 207     227     IRF4  
cluster-47593-walk-0125 207     227     IRF5  
cluster-47593-walk-0125 207     227     IRF6  
cluster-47593-walk-0125 204     245     IRF7  
cluster-47593-walk-0125 13      36      PATZ1 
cluster-47593-walk-0125 14      143     PAX4  
cluster-47593-walk-0125 4       25      RREB1 
cluster-47593-walk-0125 73      87      SMAD1 
cluster-47593-walk-0125 73      87      SMAD2 
cluster-47593-walk-0125 73      87      SMAD3 
cluster-47593-walk-0125 71      89      SMAD4 
cluster-47593-walk-0125 11      40      SP1   
cluster-47593-walk-0125 11      38      SP2   
cluster-47593-walk-0125 7       38      SP3   
cluster-47593-walk-0125 11      38      SP4   
cluster-47593-walk-0125 13      33      GTF2I 
cluster-47593-walk-0125 281     352     YY1   
cluster-47586-walk-0222 252     306     AR    
cluster-47586-walk-0222 6       23      ZNF148
[...]
</code></pre>
","<p>First, some column names</p>

<pre><code>names(data) &lt;- c(""cluster"", ""start"", ""end"", ""element"")
data
                   cluster start end element
1  cluster-47593-walk-0125   252 306      AR
2  cluster-47593-walk-0125     6  23  ZNF148
3  cluster-47593-walk-0125   357 381   CEBPA
4  cluster-47593-walk-0125   263 276   CEBPB
</code></pre>

<p>Now creating new column</p>

<pre><code>data$nearest_distance &lt;- apply(data, 1, function(x) 
 {
     cluster &lt;- x[1]
     start &lt;- as.numeric(x[2])
     end &lt;- as.numeric(x[3])
     elem &lt;- x[4]
     posb &lt;- data[data$cluster == cluster &amp; data$element != elem &amp; 
                  ((data$start &gt; end) | (data$end &lt; start)), ]
     startDist &lt;- as.matrix(dist(c(end, posb$start)))[, 1]
     endDist &lt;- as.matrix(dist(c(start, posb$end)))[, 1]
     best.dist &lt;- min(startDist[startDist &gt; 0], endDist[endDist &gt; 0])
     return(best.dist)
  }
)
</code></pre>

<p>I don't really like at least the beginning of the function, but I couldn't come up with a better solutions.. So we have</p>

<pre><code>                   cluster start end element nearest_distance
1  cluster-47593-walk-0125   252 306      AR                7
2  cluster-47593-walk-0125     6  23  ZNF148               48
3  cluster-47593-walk-0125   357 381   CEBPA                5
4  cluster-47593-walk-0125   263 276   CEBPB                5
5  cluster-47593-walk-0125   246 324   NR3C1                1 
.....
</code></pre>

<p><strong>Edit:</strong> after fixing <code>system.time()</code> test it appeared that this is a very inefficient way. Obviously, it is redundant to compute whole <code>dist()</code> matrix , so we can change these two lines to</p>

<pre><code>startDist &lt;- abs(end-posb$start)
endDist &lt;- abs(start-posb$end)
</code></pre>

<p>Another minor change is that we can delete constraint <code>data$element != elem</code> because later there is <code>&gt; 0</code>. Testing this function on 1 000 clusters with 30 rows each took more than three minutes.. There remains subsetting problem, so I tried to split data into a list and this allows us to use matrices instead of data frames (since constraint for cluster disappears) , which improves efficiency too. This time we have 10 000 clusters with 30 rows each</p>

<pre><code>data &lt;- data[rep(1:30, each = 10000), ]
data$cluster &lt;- factor(rep(1:10000, 30))

spl &lt;- split(data[, c(2:3)], data$cluster)
spl &lt;- lapply(spl, data.matrix)

system.time({
x = lapply(spl, function(z) {
     apply(z, 1, function(x) {
       start &lt;- x[1]
       end &lt;- x[2]
       posb &lt;- z[z[,1] &gt; end | z[,2] &lt; start, , drop = FALSE]
       startDist &lt;- abs(end-posb[, 1])
       endDist &lt;- abs(start-posb[, 2])
       best.dist &lt;- min(startDist[startDist &gt; 0], endDist[endDist &gt; 0])
       return(best.dist)
     })
  })
})
data$nearest_distance = unsplit(x, data$cluster)


user  system elapsed 
18.16    0.00   18.35 
</code></pre>
"
15012188,R plot - normal curves with color gradient,1,3,3,"<p>How can I make curves with a color gradient in R. Take a look at this <img src=""https://i.stack.imgur.com/WMOwS.jpg"" alt=""enter image description here"">flame.</p>

<p>It should look like that. I tried to make a normal curve and then another normal curve, but technically speaking, you can't make such a figure with a bunch of normal curves because they won't come down and intersect at the right spot on either side. How can I make such a figure in R? Any ideas?</p>
","<p>The best I've been able to do so far is:</p>

<pre><code> par(bg=""black"")
 plot(seq(0.15,0.85,by=0.01), 
      5*dbeta(seq(0.15,0.85,by=0.01),10,10 ), 
      type=""l"" , ylim=c(0,700) )  # this just sets up the plotting framework.

 for( i in 1:200 ) { lines(x= seq(0.15,0.85,by=0.01), 
                           y= i*dbeta(seq(0.15,0.85,by=0.01),10,10 ), 
                     col= colorRampPalette(c(""yellow"", ""orange"", ""red"", ""hotpink"", 
                             ""violet"", ""blue"", ""lightblue"", ""lightgreen"", ""darkgreen"",
                             ""black""))(200)[i], 
                      lwd=13) }
 par(bg=""white"")
</code></pre>

<p>I did discover that putting a ""black"" color at the beginning of that series add an extra ""glow"" to the overall result, but I'm not posting that result.</p>

<p><img src=""https://i.stack.imgur.com/vYyoG.png"" alt=""enter image description here""></p>

<h1>-----------------</h1>

<p>This is what I started with and then there are successive approximation and tweaks appearing below:</p>

<pre><code>plot(seq(0.15,0.85,by=0.01), 5*dbeta(seq(0.15,0.85,by=0.01),10,10 ), 
            type=""l"" , ylim=c(0,100))
for( i in seq(0.2, 5) ) { lines(seq(0.15,0.85,by=0.01), 
                     i*5*dbeta(seq(0.15,0.85,by=0.01),10,10 ) ) }
</code></pre>

<p><img src=""https://i.stack.imgur.com/oU6P1.png"" alt=""enter image description here""></p>

<p>For colors:</p>

<pre><code>plot(seq(0.15,0.85,by=0.01), 5*dbeta(seq(0.15,0.85,by=0.01),10,10 ), type=""l"" ,
          ylim=c(0,130))
for( i in 1:35 ) {lines(seq(0.15,0.85,by=0.01), i*dbeta(seq(0.15,0.85,by=0.01), 10,10 ), 
                           col=colorRampPalette(c(""yellow"", ""orange"", ""red"", ""violet"", 
                                        ""blue"", ""lightblue"", ""lightgreen""))(35)[i],
                   lwd=3) }
</code></pre>

<p><img src=""https://i.stack.imgur.com/MoQ06.png"" alt=""enter image description here""></p>

<p>For black background and denser colors and a fade to black at top:</p>

<pre><code> par(bg = 'black')
 plot(seq(0.15,0.85,by=0.01), 5*dbeta(seq(0.15,0.85,by=0.01),10,10 ), type=""l"", 
          ylim=c(0,130) )
 for( i in 1:35 ) { lines(seq(0.15,0.85,by=0.01), i*dbeta(seq(0.15,0.85,by=0.01),10,10), 
                          col=colorRampPalette(c(""yellow"", ""orange"", ""red"", ""violet"", 
                                ""blue"", ""lightblue"", ""lightgreen"", ""darkgreen"", 
                                 ""black"")) (35)[i], 
                           lwd=13) }
</code></pre>

<p><img src=""https://i.stack.imgur.com/UEFog.png"" alt=""enter image description here""></p>

<p>I noticed that the fading to black aspect also controlled the line width at the sides. I hadn't been expecting that but it seems to be a desirable feature. The other aspect not addressed here is the possibility of adding transparency. There is an alpha argument in the R RGB functions.</p>

<p>One useful trick for finding colors byname:</p>

<pre><code>grep(""purple"", colors(), value=TRUE)
 [1] ""mediumpurple""  ""mediumpurple1"" ""mediumpurple2"" ""mediumpurple3"" ""mediumpurple4""
 [6] ""purple""        ""purple1""       ""purple2""       ""purple3""       ""purple4""      
</code></pre>

<p>If you are playing with the iteration to make the gradient smoother then you will need also adjust the ylim argument: choose 0.5^9*0.5^9/beta(10,10)*[iterations] , since that will be the maximum at x=0.5.</p>
"
36574141,R with roxygen2: How to use a single function from another package?,2,2,2,"<p>I'm creating an R package that will use <strong>a single function</strong> from <code>plyr</code>. According to <a href=""https://cran.r-project.org/web/packages/roxygen2/vignettes/namespace.html"" rel=""nofollow noreferrer"">this roxygen2 vignette</a>:</p>

<blockquote>
  <p>If you are using just a few functions from another package, the
  recommended option is to note the package name in the Imports: field
  of the DESCRIPTION file and call the function(s) explicitly using ::,
  e.g., pkg::fun().</p>
</blockquote>

<p>That sounds good. I'm using <code>plyr::ldply()</code> - the full call with <code>::</code> - so I list <code>plyr</code> in <code>Imports:</code> in my <code>DESCRIPTION</code> file. However, when I use <code>devtools::check()</code> I get this:</p>

<pre><code>* checking dependencies in R code ... NOTE
All declared Imports should be used:
  ‘plyr’
  All declared Imports should be used.
</code></pre>

<p><em>Why do I get this note?</em></p>

<p>I am able to avoid the note by adding <code>@importFrom dplyr ldply</code> in the file that is using <code>plyr</code>, but then I end but having <code>ldply</code> in my package namespace. Which I do not want, and should not need as I am using <code>plyr::ldply()</code> the single time I use the function.</p>

<p>Any pointers would be appreciated!</p>

<p>(<a href=""https://stackoverflow.com/questions/28335991/package-imports-error-on-devtoolscheck"">This question</a> might be relevant.)</p>
","<p><em>If <code>ldply()</code> is important for your package's functionality</em>, then you do want it in your package namespace. That is the point of namespace imports. Functions that you need, should be in the package namespace because this is where R will look first for the definition of functions, before then traversing the base namespace and the attached packages. It means that no matter what other packages are loaded or unloaded, attached or unattached, your package will always have access to that function. In such cases, use:</p>

<pre><code>@importFrom plyr ldply
</code></pre>

<p>And you can just refer to <code>ldply()</code> without the <code>plyr::</code> prefix just as if it were another function in your package.</p>

<p><em>If <code>ldply()</code> is not so important</em> - perhaps it is called only once in a not commonly used function - then, <a href=""https://cran.r-project.org/doc/manuals/r-devel/R-exts.html#Specifying-imports-and-exports"">Writing R Extensions 1.5.1</a> gives the following advice:</p>

<blockquote>
  <p>If a package only needs a few objects from another package it can use a fully qualified variable reference in the code instead of a formal import. A fully qualified reference to the function <code>f</code> in package <code>foo</code> is of the form <code>foo::f</code>. This is slightly less efficient than a formal import and also loses the advantage of recording all dependencies in the <code>NAMESPACE</code> file (but they still need to be recorded in the <code>DESCRIPTION</code> file). Evaluating <code>foo::f</code> will cause package <code>foo</code> to be loaded, but not attached, if it was not loaded already—this can be an advantage in delaying the loading of a rarely used package.</p>
</blockquote>

<p>(I think this advice is actually a little outdated because it is implying more separation between <code>DESCRIPTION</code> and <code>NAMESPACE</code> than currently exists.) It implies you should use <code>@import plyr</code> and refer to the function as <code>plyr::ldply()</code>. But in reality, it's actually suggesting something like putting <code>plyr</code> in the <code>Suggests</code> field of <code>DESCRIPTION</code>, which isn't exactly accommodated by roxygen2 markup nor exactly compliant with <code>R CMD check</code>.</p>

<p><em>In sum</em>, the official line is that Hadley's advice (which you are quoting) is only preferred for rarely used functions from rarely used packages (and/or packages that take a considerable amount of time to load). Otherwise, just do <code>@importFrom</code> like <em>WRE</em> advises:</p>

<blockquote>
  <p>Using <code>importFrom</code> selectively rather than <code>import</code> is good practice and recommended notably when importing from packages with more than a dozen exports.</p>
</blockquote>
"
32428278,How to do double loop and create table?,1,1,3,"<p>I am new to R and I come from a Stata and SAS background. I normally used loops to create variables and to speed up processes.</p>

<p>I am doing the following: 1) I need to run a loop ""I"" 4 times for values 10,20,100 and 1000, and store 4 values in a vector or in a table. Loop I runs a process 100 times. Therefore, we would have 400 repetitions combined.</p>

<p>First. My loop needs to make 100 samples of size 10, see how many of those samples fulfill a condition and store the number.
Then make 100 samples of size 20, and see how many of those samples meet the condition...etc.</p>

<pre><code>This is the variable for the sample size

    v=c(10,20,100,1000)  
This variable will store the number of observations that meet the condition
    sum.x=c(0,0,0,0)  
This is the variable for the 100 repetitions.

    sample = matrix(0,ncol=1,nrow=100)  
This is the loop.

    for (x in seq(along=v)){
    for (i in 1:100) {
      data=rnorm(`v`,0.25,1)
      test=t.test(data)
      sample[i,1]=test$p.value
    }
    sum.x[v]=sum(sample&gt;0.05)  
      }
</code></pre>

<p>The code runs fine, except that it just does not do what I need to: </p>

<p>1) sum.x has 1000 observations instead of 4. I know that !""[v]"" is indicating the position of the row or something. I just want to create a vector/variable named as below and create a small table: </p>

<pre><code>table=data.fram(sum.x10, sum.x20, sum.x100, sum.x1000).
</code></pre>

<p>2) The data has only 4 observations instead of 100.The main problem is that I do not know how to make the connection between the four sample sizes with the 100.</p>

<p>Thanks. </p>
","<p>Most R people depreciate loops. Perhaps this works:</p>

<pre><code>v=c(10,20,100,1000)  

f &lt;- function(n)
{
  sum( sapply( rep(n,100),
       function(n){t.test(rnorm(n,0.25,1))[[""p.value""]] &gt; 0.05} ) )
}

table &lt;- sapply(v,f)
</code></pre>

<p>.</p>

<pre><code>&gt; table
[1] 83 81 40  0
</code></pre>

<ul>
<li><p>The function</p>

<pre><code>function(n){t.test(rnorm(n,0.25,1))[[""p.value""]] &gt; 0.05}
</code></pre>

<p>inside the function <code>f</code> takes <strong><em>one</em></strong> random sample of size <code>n</code> from
a normal distribution with mean 0.25 and standard deviation 1,
performs a t-test using this random sample as data values, and
checks if its p-value is larger than 0,05.
The resulting logical value is TRUE if and only if the random sample
meets the condition that the p-value is larger than 0.05.</p></li>
<li><p>Then</p>

<pre><code>sapply(rep(n,100),function(n){...})
</code></pre>

<p>applies this function to each component of the vector rep(n,100).
This means that the test is repeated 100 times.
The result is a logical vector of length 100.</p></li>
<li><p>Now</p>

<pre><code>sum(sapply(...))
</code></pre>

<p>sums the entries of this logical vector,
where TRUE and FALSE are taken as 1 and 0, respectively.<br>
Hence the result is the number of t-tests
which meet the condition that the p-value is larger than 0.05.</p></li>
</ul>

<p>Finally</p>

<pre><code>sapply(c(10,20,100,1000),f)
</code></pre>

<p>creates the vector</p>

<pre><code>c(f(10,f(20),f(100),f(1000)).
</code></pre>

<p>To get more information than just this final table, one can split up the calculation as follows:</p>

<pre><code>v=c(10,20,100,1000)  

g &lt;- function(n,m)
{
  sapply( rep(n,m),
          function(n){t.test(rnorm(n,0.25,1))} )
}

f &lt;- function(n)
{
  sum(g(n,100)[""p.value"",]&gt;0.05)
}

table &lt;- sapply(v,f)
</code></pre>

<p>Now <code>g(n,m)</code> is a matrix with 9 rows and <code>m</code> columns. The i-th column contains the result of the i-th t-test, e.g.</p>

<pre><code>&gt; g(10,5)
            [,1]                [,2]                [,3]                [,4]                [,5]               
statistic   1.117835            0.3290155           1.610792            1.399736            0.8213012          
parameter   9                   9                   9                   9                   9                  
p.value     0.2925871           0.749671            0.1416849           0.195105            0.4326913          
conf.int    Numeric,2           Numeric,2           Numeric,2           Numeric,2           Numeric,2          
estimate    0.2408269           0.06949928          0.5203193           0.4262958           0.2347281          
null.value  0                   0                   0                   0                   0                  
alternative ""two.sided""         ""two.sided""         ""two.sided""         ""two.sided""         ""two.sided""        
method      ""One Sample t-test"" ""One Sample t-test"" ""One Sample t-test"" ""One Sample t-test"" ""One Sample t-test""
data.name   ""rnorm(n, 0.25, 1)"" ""rnorm(n, 0.25, 1)"" ""rnorm(n, 0.25, 1)"" ""rnorm(n, 0.25, 1)"" ""rnorm(n, 0.25, 1)""
&gt; 
</code></pre>
"
45634155,Parse Nested XML (with namespaces) in R,2,2,4,"<p>I am trying to parse an xml response from a web API.</p>

<p>For a simple xml as below, I am able to work with xpathSApply and get the relevant data out very easily.</p>

<p>Following is example.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;CATALOG&gt;
    &lt;PLANT&gt;
        &lt;COMMON&gt;Bloodroot&lt;/COMMON&gt;
        &lt;BOTANICAL&gt;Sanguinaria canadensis&lt;/BOTANICAL&gt;
        &lt;ZONE&gt;4&lt;/ZONE&gt;
        &lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
        &lt;PRICE&gt;$2.44&lt;/PRICE&gt;
        &lt;AVAILABILITY&gt;031599&lt;/AVAILABILITY&gt;
    &lt;/PLANT&gt;
    &lt;PLANT&gt;
        &lt;COMMON&gt;Columbine&lt;/COMMON&gt;
        &lt;BOTANICAL&gt;Aquilegia canadensis&lt;/BOTANICAL&gt;
        &lt;ZONE&gt;3&lt;/ZONE&gt;
        &lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
        &lt;PRICE&gt;$9.37&lt;/PRICE&gt;
        &lt;AVAILABILITY&gt;030699&lt;/AVAILABILITY&gt;
    &lt;/PLANT&gt;
&lt;/CATALOG&gt;

&gt;library(XML)
&gt;doc&lt;-xmlTreeParse(""example.xml"",useInternal=TRUE) 
&gt;rootNode&lt;-xmlRoot(doc)
&gt;xpathSApply(rootNode,""//COMMON"",xmlValue)
[1] ""Bloodroot"" ""Columbine""

&gt; getNodeSet(doc,""//PLANT"")
[[1]]
&lt;PLANT&gt;
  &lt;COMMON&gt;Bloodroot&lt;/COMMON&gt;
  &lt;BOTANICAL&gt;Sanguinaria canadensis&lt;/BOTANICAL&gt;
  &lt;ZONE&gt;4&lt;/ZONE&gt;
  &lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
  &lt;PRICE&gt;$2.44&lt;/PRICE&gt;
  &lt;AVAILABILITY&gt;031599&lt;/AVAILABILITY&gt;
&lt;/PLANT&gt; 

[[2]]
&lt;PLANT&gt;
  &lt;COMMON&gt;Columbine&lt;/COMMON&gt;
  &lt;BOTANICAL&gt;Aquilegia canadensis&lt;/BOTANICAL&gt;
  &lt;ZONE&gt;3&lt;/ZONE&gt;
  &lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
  &lt;PRICE&gt;$9.37&lt;/PRICE&gt;
  &lt;AVAILABILITY&gt;030699&lt;/AVAILABILITY&gt;
&lt;/PLANT&gt; 

attr(,""class"")
[1] ""XMLNodeSet""

&gt; xmlSApply(getNodeSet(rootNode,""//PRICE""),xmlValue) #provides a list of all PRICE values in the xml
[1] ""$2.44"" ""$9.37""
</code></pre>

<p>However, the same commands do not work for the below xml which have namespace details. Is there anyway that I can fetch the data in the nodes / tags.</p>

<p>Following is example1.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;s:Envelope xmlns:s=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:u=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd""&gt;&lt;s:Body&gt;&lt;GetByFilterTradeResponse xmlns=""http://entrader.contigoenergy.com/Contigo.Entrader.Service""&gt;&lt;GetByFilterTradeResult xmlns:i=""http://www.w3.org/2001/XMLSchema-instance""&gt;
&lt;CATALOG&gt;
    &lt;CATEGORY&gt;
        &lt;FAMILY&gt;
            &lt;PLANT&gt;
                &lt;COMMON&gt;Bloodroot&lt;/COMMON&gt;
                &lt;BOTANICAL&gt;Sanguinaria canadensis&lt;/BOTANICAL&gt;
                &lt;ZONE&gt;4&lt;/ZONE&gt;
                &lt;DETAILS&gt;
                    &lt;PRICEINBULK&gt;2.3&lt;/PRICEINBULK&gt;
                    &lt;MINVOLUME&gt;100&lt;/MINVOLUME&gt;
                &lt;/DETAILS&gt;
                &lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
                &lt;PRICE&gt;$2.44&lt;/PRICE&gt;
                &lt;AVAILABILITY&gt;031599&lt;/AVAILABILITY&gt;
            &lt;/PLANT&gt;
            &lt;PLANT&gt;
                &lt;COMMON&gt;Columbine&lt;/COMMON&gt;
                &lt;BOTANICAL&gt;Aquilegia canadensis&lt;/BOTANICAL&gt;
                &lt;ZONE&gt;3&lt;/ZONE&gt;
                &lt;DETAILS&gt;
                    &lt;PRICEINBULK&gt;9.00&lt;/PRICEINBULK&gt;
                    &lt;MINVOLUME&gt;100&lt;/MINVOLUME&gt;
                &lt;/DETAILS&gt;
                &lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
                &lt;PRICE&gt;$9.37&lt;/PRICE&gt;
                &lt;AVAILABILITY&gt;030699&lt;/AVAILABILITY&gt;
            &lt;/PLANT&gt;
        &lt;/FAMILY&gt;
    &lt;/CATEGORY&gt; 
&lt;/CATALOG&gt;
&lt;/GetByFilterTradeResult&gt;&lt;/GetByFilterTradeResponse&gt;&lt;/s:Body&gt;&lt;/s:Envelope&gt;
</code></pre>

<p>Following commands does not extract the node values from the above xml</p>

<pre><code>&gt;doc&lt;-xmlTreeParse(""example1.xml"",useInternal=TRUE) 
&gt;rootNode&lt;-xmlRoot(doc) 
&gt; xpathSApply(rootNode,""//COMMON"",xmlValue) 
list()

&gt; getNodeSet(doc,""//PLANT"")
list()
attr(,""class"")
[1] ""XMLNodeSet""

&gt; xmlSApply(getNodeSet(rootNode,""//PRICE""),xmlValue) 
list()
</code></pre>
","<p>USe <code>name()</code> or <code>local-name()</code> in your XPATH:</p>

<pre><code>library(XML)

appText &lt;- '&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;s:Envelope xmlns:s=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:u=""http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd""&gt;
&lt;s:Body&gt;&lt;GetByFilterTradeResponse xmlns=""http://entrader.contigoenergy.com/Contigo.Entrader.Service""&gt;
&lt;GetByFilterTradeResult xmlns:i=""http://www.w3.org/2001/XMLSchema-instance""&gt;
&lt;CATALOG&gt;
&lt;CATEGORY&gt;
&lt;FAMILY&gt;
&lt;PLANT&gt;
&lt;COMMON&gt;Bloodroot&lt;/COMMON&gt;
&lt;BOTANICAL&gt;Sanguinaria canadensis&lt;/BOTANICAL&gt;
&lt;ZONE&gt;4&lt;/ZONE&gt;
&lt;DETAILS&gt;
&lt;PRICEINBULK&gt;2.3&lt;/PRICEINBULK&gt;
&lt;MINVOLUME&gt;100&lt;/MINVOLUME&gt;
&lt;/DETAILS&gt;
&lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
&lt;PRICE&gt;$2.44&lt;/PRICE&gt;
&lt;AVAILABILITY&gt;031599&lt;/AVAILABILITY&gt;
&lt;/PLANT&gt;
&lt;PLANT&gt;
&lt;COMMON&gt;Columbine&lt;/COMMON&gt;
&lt;BOTANICAL&gt;Aquilegia canadensis&lt;/BOTANICAL&gt;
&lt;ZONE&gt;3&lt;/ZONE&gt;
&lt;DETAILS&gt;
&lt;PRICEINBULK&gt;9.00&lt;/PRICEINBULK&gt;
&lt;MINVOLUME&gt;100&lt;/MINVOLUME&gt;
&lt;/DETAILS&gt;
&lt;LIGHT&gt;Mostly Shady&lt;/LIGHT&gt;
&lt;PRICE&gt;$9.37&lt;/PRICE&gt;
&lt;AVAILABILITY&gt;030699&lt;/AVAILABILITY&gt;
&lt;/PLANT&gt;
&lt;/FAMILY&gt;
&lt;/CATEGORY&gt; 
&lt;/CATALOG&gt;
&lt;/GetByFilterTradeResult&gt;&lt;/GetByFilterTradeResponse&gt;&lt;/s:Body&gt;&lt;/s:Envelope&gt;'
doc &lt;- xmlParse(appText)
&gt; xpathSApply(doc,""//*[name()='COMMON']"", xmlValue)
[1] ""Bloodroot"" ""Columbine""
</code></pre>

<p>alternatively  explicitly define the namespace:</p>

<pre><code>&gt; xpathSApply(doc,""//n:COMMON"",xmlValue, namespaces = 
+                 c(s = ""http://schemas.xmlsoap.org/soap/envelope/"", 
+                   n = ""http://entrader.contigoenergy.com/Contigo.Entrader.Service"", 
+                   i = ""http://www.w3.org/2001/XMLSchema-instance"")) 
[1] ""Bloodroot"" ""Columbine""
</code></pre>

<p>or using <code>xml2</code> package:</p>

<pre><code>library(xml2)
doc &lt;- read_xml(appText)
# check namespaces
&gt; xml_ns(doc)
d1 &lt;-&gt; http://entrader.contigoenergy.com/Contigo.Entrader.Service
i  &lt;-&gt; http://www.w3.org/2001/XMLSchema-instance
s  &lt;-&gt; http://schemas.xmlsoap.org/soap/envelope/
u  &lt;-&gt; http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd

&gt; xml_text(xml_find_all(doc, ""//d1:COMMON""))
[1] ""Bloodroot"" ""Columbine""
</code></pre>
"
5221870,convolution in R,2,3,3,"<p>I tried to do convolution in R directly and using FFTs then taking inverse. But it seems from simple observation it is not correct. Look at this example:</p>

<pre><code># DIRECTLY
&gt; x2$xt
[1] 24.610 24.605 24.610 24.605 24.610
&gt; h2$xt
[1] 0.003891051 0.003875910 0.003860829 0.003845806 0.003830842
&gt; convolve(h2$xt,x2$xt)
[1] 0.4750436 0.4750438 0.4750435 0.4750437 0.4750435

# USING INVERSE FOURIER TRANSFORM
&gt; f=fft(fft(h2$xt)*fft(x2$xt), inv=TRUE)
&gt; Re(f)/length(f) 
[1] 0.4750438 0.4750435 0.4750437 0.4750435 0.4750436
&gt;
</code></pre>

<p>Lets take the index 0. At 0, the convolution should simply be the last value of x2$xt (24.610) multiplied by first value of h2$xt (0.003891051) which should give convolution at index 0 = 24.610*0.003891051 = 0.09575877 which is way off from 0.4750436.</p>

<p>Am I doing something wrong? Why is the values so different from expected?</p>
","<p>Both <code>convolve</code> and <code>fft</code> are <em>circular</em>. The first element of convolution must be the dot product of these two series. The results you obtain are correct in this sense.</p>

<p>To perform a linear convolution use:</p>

<pre><code>convolve(h2$xt,x2$xt,type=""open"")
</code></pre>

<p>Circular convolution is also applied in this case but a required amount of zeros are padded to inputs to achieve linear convolution.</p>

<p>I believe there is not a direct way to achieve linear convolution with <code>fft</code> in R. However this doesn't really matter beacuse <code>convolve</code> itself uses the FFT approach you posted.</p>

<hr>

<h2>Circular Convolution</h2>

<p>A discrete signal <strong>x</strong> is periodic if there is a period <strong>N</strong> such that <strong>x[n] = x[n+N]</strong> for all <strong>n</strong>. Such signals can be represented by <strong>N</strong> samples from <strong>x[0]</strong> to <strong>x[N-1]</strong>.</p>

<pre><code>... x[-2] x[-1] x[0] x[1] x[2] ... x[N-2] x[N-1] x[N] x[N+1] ...
                ^    this part is sufficient   ^
</code></pre>

<p>A regular definition of <em>convolution</em> between aperiodic <strong>x</strong> and <strong>y</strong> is defined as:</p>

<pre><code>(x * y)[n] = sum{k in [-inf, inf]}(x[k]y[n-k])
</code></pre>

<p>However, for periodic signals, this formula does not produce finite results. To overcome this problem we define the <em>circular convolution</em> between periodic <strong>x</strong> and <strong>y</strong>.</p>

<pre><code>(x * y)[n] = sum{k in [0, N-1]}(x[i]y[n-k])
</code></pre>

<p>When these two signals are represented with <strong>N</strong> values only, we can use <strong>y[n-k+N]</strong> in place of <strong>y[n-k]</strong> for negative values of <strong>n-k</strong>.</p>

<p>The cool thing with circular convolution is that it can calculate the <em>linear convolution</em> between box signals, which are discrete signals that have a finite number of non-zero elements.</p>

<p>Box signals of length <strong>N</strong> can be fed to circular convolution with <strong>2N</strong> periodicity, <strong>N</strong> for original samples and <strong>N</strong> zeros padded at the end. The result will be a circular convolution with <strong>2N</strong> samples with <strong>2N-1</strong> for linear convolution and an extra zero.</p>

<p>Circular convolution is generally faster than a direct linear convolution implementation, because it can utilize the <em>Fast Fourier Transform</em>, a fast algorithm to calculate the <em>Discrete Fourier Transform</em>, which is only defined for periodic discrete signals.</p>

<hr>

<p>Please see:</p>

<ul>
<li><a href=""http://www.centerspace.net/blog/nmath/convolution-correlation-and-the-fft/"">http://www.centerspace.net/blog/nmath/convolution-correlation-and-the-fft/</a></li>
</ul>

<p>Also see:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Circular_convolution"">http://en.wikipedia.org/wiki/Circular_convolution</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Discrete_Fourier_transform"">http://en.wikipedia.org/wiki/Discrete_Fourier_transform</a></li>
</ul>
"
15268147,Regular Expression to anonymize emails,2,2,4,"<p>i use in R the regular expression</p>

<pre><code>regexp &lt;- ""(^|[^([:alnum:]|.|_)])abc@abc.de($|[^[:alnum:]])""
</code></pre>

<p>to find the email-adress <code>abc@abc.de</code> in an spefic text and replace it by an <code>anonym-mail-adress</code>.</p>

<pre><code>tmp &lt;- c(""aaaaabc@abc.debbbb"",        ## &lt;- should not be matched
      ""aaaa abc@abc.de bbbb"",      ## &lt;- should be matched
      ""abc@abc.de"",                ## &lt;- should be matched
      ""aaa.abc@abc.de"",            ## &lt;- should not be matched
      ""aaaa_abc@abc.de"",           ## &lt;- should not be matched
      ""(abc@abc.de)"",              ## &lt;- should be matched
      ""aaaa (abc@abc.de) bbbb"")    ## &lt;- should be matched


replacement &lt;- paste(""\\1"", ""anonym@anonym.de"", ""\\2"", sep="""")
gsub(regexp, replacement, tmp, ignore.case=TRUE)
</code></pre>

<p>as result I get</p>

<pre><code>&gt; gsub(regexp, replacement, tmp, ignore.case=TRUE)
[1] ""aaaaabc@abc.debbbb""         ""aaaa anonym@anonym.de bbbb""
[3] ""anonym@anonym.de""           ""aaa.abc@abc.de""            
[5] ""aaaa_abc@abc.de""            ""(abc@abc.de)""              
[7] ""aaaa (abc.abc.de) bbbb""  
</code></pre>

<p>I don't know why the last two elements of the array are not matched?</p>

<p>Thank you and best regards.</p>
","<p>How about this?</p>

<pre><code>gsub(""^(abc@abc)|(?&lt;=[ (])(abc@abc)"", ""anonym@anonym"", tmp, perl=T)
</code></pre>

<p>The pattern before <code>|</code>: <code>^(abc@abc)</code> checks for beginning with <code>abc@abc</code>, of course.</p>

<p>The pattern after <code>|</code> uses <code>positive lookbehind</code> and searches for <code>abc@abc</code> preceded by <code>space</code> or <code>(</code> (left paranthesis) and if found, replaces with <code>anonym@anonym</code>.</p>

<p>This is what I get: (Note: I replaced <code>abc.abc</code> in the last string with <code>abc@abc</code>)</p>

<pre><code>[1] ""aaaaabc@abc.debbbb""           ""aaaa anonym@anonym.de bbbb""  
[3] ""anonym@anonym.de""             ""aaa.abc@abc.de""              
[5] ""aaaa_abc@abc.de""              ""(anonym@anonym.de)""          
[7] ""aaaa (anonym@anonym.de) bbbb""
</code></pre>

<p><strong>Edit:</strong> To explain the problem with your regexp, it seems like a problem with the part:</p>

<pre><code>[^([:alnum:]|.|_)]
</code></pre>

<p>I think the negation has to be present in every <code>|</code> statement. Also, you should use <code>[.]</code> instead of <code>.</code> as the latter implies any character. Alternatively, instead of using negation in for every character you're checking, we can condense this part by removing all unncessary <code>|</code> as:</p>

<pre><code>[^.[:alpha:]_] # not a . or _ or any alphanumeric
# using gsub on it:
gsub(""(^|[^.[:alpha:]_])abc@abc"", "" anonym@anonym"", tmp)

# [1] ""aaaaabc@abc.debbbb""           ""aaaa anonym@anonym.de bbbb""  
# [3] "" anonym@anonym.de""            ""aaa.abc@abc.de""              
# [5] ""aaaa_abc@abc.de""              "" anonym@anonym.de)""          
# [7] ""aaaa  anonym@anonym.de) bbbb""
</code></pre>

<p>You get every <code>abc@abc</code> replaced. But, you'll lose the character before <code>abc@abc</code> everytime because you're checking for it in the pattern as well. So, you'll have to use the capture group. That is, if you wrap a regular expression with <code>()</code> then you can refer to that ""capture"" using special variables such as <code>\\1, \\2 etc..</code>. Here, we have captured <code>(^|[^.[:alpha:]_])</code>, i.e., the part before <code>abc@abc</code>. Since it is the first capture, we'll refer to it as <code>\\1</code> to use it to recover the missing character in the previous result:</p>

<pre><code>gsub(""(^|[^.[:alpha:]_])abc@abc"", ""\\1anonym@anonym"", tmp)

# [1] ""aaaaabc@abc.debbbb""           ""aaaa anonym@anonym.de bbbb""  
# [3] ""anonym@anonym.de""             ""aaa.abc@abc.de""              
# [5] ""aaaa_abc@abc.de""              ""(anonym@anonym.de)""          
# [7] ""aaaa (anonym@anonym.de) bbbb""
</code></pre>

<p>This is the result you needed. And this is the same as my initial answer using positive look-behind. In that case, since it just checks if it is preceded by something, you don't have to capture anything special. Only the <code>abc@abc</code> part got replaced. Hope this helps.</p>
"
18405687,Rotate grid of plot with R,1,3,3,"<p>for example :</p>

<pre><code>plot(1:10, 1:10)
grid(col=""red"")
</code></pre>

<p>Is it possible to rotate the red grid around the intersection of x axis and y axis (the origin {0,0}) , by an arbitrary angle ? It doesn't mean anything with this example but I want to do that.</p>
","<p>The grid function probably cannot do that in base R anyway. It's not an object-oriented plotting paradigm. I think you would come close with <code>abline</code>:</p>

<pre><code>plot(1:10, 1:10,xlim=c(0,10), ylim=c(0,10))
sapply(seq(0,20,by=2), function(a) abline(a=a, b=-1,lty=3,col=""red""))
sapply(seq(-10,10,by=2), function(a) abline(a,b=1,lty=3,col=""red""))
</code></pre>

<p><img src=""https://i.stack.imgur.com/XCqD7.png"" alt=""enter image description here""></p>

<p>It's a minor application of coordinate geometry to rotate by an arbitrary angle.</p>

<pre><code>angle=pi/8; rot=tan(angle); backrot=tan(angle+pi/2)
sapply(seq(-10,10,by=2), function(inter) abline(a=inter, 
                                                b=rot,lty=3,col=""red""))
sapply(seq(0,40,by=2), function(inter) abline(a=inter, 
                                           b=backrot, lty=3,col=""red""))
</code></pre>

<p>This does break down when angle = pi/2 so you might want to check this if building a function and in that case just use <code>grid</code>. The one problem I found was to get the spacing pleasing. If one iterates on the y- and x-axes with the same intervals you get ""compression"" of one of the set of gridlines. I think that is why the method breaks down at high angles.</p>

<p>I'm thinking a more general solution might be to construct a set of grid end-points that spans and extends beyond the plot area by a factor of at least sqrt(2) and then apply a rotation matrix. And then use <code>segments</code> or <code>lines</code>. Here is that implementation:</p>

<pre><code>plot(1:10, 1:10,xlim=c(0,10), ylim=c(0,10)); angle=pi/8; rot=tan(angle);backrot=tan(angle+pi/2)
x0y0 &lt;- matrix( c(rep(-20,41), -20:20), 41)
x1y1 &lt;- matrix( c(rep(20,41), -20:20), 41)
# The rot function will construct a rotation matrix
rot &lt;- function(theta) matrix(c( cos( theta ) , sin( theta ) ,
 -sin( theta ), cos( theta ) ), 2)
# Leave origianal set of point untouched but create rotated version
 rotx0y0 &lt;- x0y0%*%rot(pi/8)
 rotx1y1 &lt;- x1y1%*%rot(pi/8)
 segments(rotx0y0[,1] ,rotx0y0[,2], rotx1y1[,1], rotx1y1[,2], col=""blue"")
# Use originals again, ... or could write to rotate the new points
 rotx0y0 &lt;- x0y0%*%rot(pi/8+pi/2)
 rotx1y1 &lt;- x1y1%*%rot(pi/8+pi/2)
 segments(rotx0y0[,1] ,rotx0y0[,2], rotx1y1[,1], rotx1y1[,2], col=""blue"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/BXwCI.png"" alt=""enter image description here""></p>
"
38943648,R Shiny: Output functions don't work within eventReactive(),2,2,2,"<p>I want to modify UI using <code>uiOutput()</code> and <code>plotOutput()</code> when the button is clicked. Everything works fine if it doesn't wait for the event and changes the content immediately. What should i change to make <code>output$body.on.button</code> having the same output as <code>output$body.immediately</code> when the button is clicked?</p>

<p><strong>server.R:</strong></p>

<pre><code>shinyServer(function(input, output, session) {

  output$plot &lt;- renderPlot({plot(1:5)})
  output$body.ui &lt;- renderUI({tags$p('This works only within renderUI')})

  output$body.on.button &lt;- eventReactive(input$goButton, {
    switch(
      input$select,
      ""text"" = 'This works within both functions',
      ""ui"" = uiOutput(""body.ui""), # This doesn't work
      ""plot"" = plotOutput(""plot"") # This doesn't work
    )
  })

  output$body.immediately &lt;- renderUI({
    switch(
      input$select,
      ""text"" = 'This works within both functions',
      ""ui"" = uiOutput(""body.ui""), # This works
      ""plot"" = plotOutput(""plot"") # This works
    )
  })
})
</code></pre>

<p><strong>ui.R:</strong></p>

<pre><code>library(markdown)

shinyUI(
  navbarPage(
    ""Title"",

    tabPanel(
      ""First element"",
      fluidRow(

        column(
          4,
          wellPanel(
            selectInput(""select"", ""Select"", c('text', 'ui', 'plot')),
            actionButton(""goButton"", ""Go!"")
          ) # end of wellPanel
        ), # end of column

        column(
          4,
          uiOutput(""body.immediately"")
        ), # end of column

        column(
          4,
          uiOutput(""body.on.button"")
        ) # end of column


      ) # end of fluidROw
    ) # end of tabPanel
  ) # end of navbarPage
) # end of shinyUI
</code></pre>

<p>I have also tried function <code>observeEvent()</code> but it didn't help.</p>
","<p>It doesn't work because you can't have two outputs with the same ID. In your example ""Text"" works because it is just a text - not an output with an ID.</p>

<p>In the example below I've added three new outputs with unique IDs. uiOutput <code>body.on.button</code> also dynamically receives a specific output after clicking on a button. </p>

<hr>

<p>You can try to remove this three lines</p>

<pre><code>output$text2 &lt;- renderText({ 'This works within both functions' })
output$body.ui2 &lt;- renderUI({tags$p('This works only within renderUI')})
output$plot2 &lt;- renderPlot({plot(1:10)})
</code></pre>

<p>and then change <code>IDs</code> within eventReactive from ""text2"" to ""text"" (similarly with the other two) to see that the outputs after clicking on a button are not going to be shown. </p>

<p>If you want to have, for example, two same plots, then you should create two <code>plotOutputs</code> with unique IDs and then use twice <code>renderPlot</code> functions which produce the same plots.</p>

<p>I hope this helps.</p>

<hr>

<p><strong>Full example:</strong> </p>

<pre><code>server &lt;- shinyServer(function(input, output, session) {

  # Create outputs with unique ID

  output$text &lt;- renderText({ 'This works within both functions' })
  output$body.ui &lt;- renderUI({tags$p('This works only within renderUI')})
  output$plot &lt;- renderPlot({plot(1:5)})

  output$text2 &lt;- renderText({ 'This works within both functions' })
  output$body.ui2 &lt;- renderUI({tags$p('This works only within renderUI')})
  output$plot2 &lt;- renderPlot({plot(1:10)})


  # Save the selected type of the output and then pass it to renderUI via react()
  react &lt;- eventReactive(input$goButton, {
    switch(
      input$select,
      ""text"" = textOutput(""text2""),
      ""ui"" = uiOutput(""body.ui2""), 
      ""plot"" = plotOutput(""plot2"") 
    )
  })

  output$body.on.button &lt;- renderUI({
    react()
  })

  output$body.immediately &lt;- renderUI({
    switch(
      input$select,
      ""text"" = textOutput(""text""),
      ""ui"" = uiOutput(""body.ui""), # This works
      ""plot"" = plotOutput(""plot"") # This works
    )
  })
})


  library(markdown)


ui &lt;-shinyUI(
  navbarPage(
    ""Title"",

    tabPanel(
      ""First element"",
      fluidRow(

        column(
          4,
          wellPanel(
            selectInput(""select"", ""Select"", c('text', 'ui', 'plot')),
            actionButton(""goButton"", ""Go!"")
          ) # end of wellPanel
        ), # end of column

        column(
          4,
          uiOutput(""body.immediately"")
        ), # end of column

        column(
          4,
          uiOutput(""body.on.button"")
        ) # end of column


      ) # end of fluidROw
    ) # end of tabPanel
  ) # end of navbarPage
) # end of shinyUI
shinyApp(ui, server)
</code></pre>
"
4859482,How to generate random shapes given a specified area.(R language).?,1,3,3,"<p>My question is this.. I am working on some clustering algorithms.. For this first i am experimenting with 2d shapes..</p>

<p>Given a particular area say 500sq units .. I need to generate random shapes for a particular area</p>

<p>say a Rect, Square, Triangle of 500 sq units.. etc .. Any suggestions on how i should go about this problem.. I am using R language..</p>
","<p>It's fairly straightforward to do this for regular polygon.</p>

<p>The area of an n-sided regular polygon, with a circumscribed circle of radius R is</p>

<p><code>A = 1/2 nR^2 * sin((2pi)/n)</code></p>

<p>Therefore, knowing n and A you can easily find R</p>

<p><code>R = sqrt((2*A)/(n*sin((2pi)/n))</code></p>

<p>So, you can pick the center, go at distance R and generate n points at <code>2pi/n</code> angle increments.</p>

<p>In R:</p>

<pre><code>regular.poly &lt;- function(nSides, area)
    {
    # Find the radius of the circumscribed circle
    radius &lt;- sqrt((2*area)/(nSides*sin((2*pi)/nSides)))

    # I assume the center is at (0;0) and the first point lies at (0; radius)
    points &lt;- list(x=NULL, y=NULL)
    angles &lt;- (2*pi)/nSides * 1:nSides

    points$x &lt;- cos(angles) * radius
    points$y &lt;- sin(angles) * radius

    return (points);
    }


# Some examples
par(mfrow=c(3,3))

for (i in 3:11)
    {
    p &lt;- regular.poly(i, 100)
    plot(0, 0, ""n"", xlim=c(-10, 10), ylim=c(-10, 10), xlab="""", ylab="""", main=paste(""n="", i))
    polygon(p)
    }
</code></pre>

<p>We can extrapolate to a generic convex polygon.</p>

<p>The area of a convex polygon can be found as: 
<code>A = 1/2 * [(x1*y2 + x2*y3 + ... + xn*y1) - (y1*x2 + y2*x3 + ... + yn*x1)]</code></p>

<p>We generate the polygon as above, but deviate angles and radii from those of the regular polygon.</p>

<p>We then scale the points to get the desired area. </p>

<pre><code>convex.poly &lt;- function(nSides, area)
    {
    # Find the radius of the circumscribed circle, and the angle of each point if this was a regular polygon
    radius &lt;- sqrt((2*area)/(nSides*sin((2*pi)/nSides)))
    angle &lt;- (2*pi)/nSides

    # Randomize the radii/angles
    radii &lt;- rnorm(nSides, radius, radius/10)
    angles &lt;- rnorm(nSides, angle, angle/10) * 1:nSides
    angles &lt;- sort(angles)

    points &lt;- list(x=NULL, y=NULL)
    points$x &lt;- cos(angles) * radii
    points$y &lt;- sin(angles) * radii

    # Find the area of the polygon
    m &lt;- matrix(unlist(points), ncol=2)
    m &lt;- rbind(m, m[1,])
    current.area &lt;- 0.5 * (sum(m[1:nSides,1]*m[2:(nSides+1),2]) - sum(m[1:nSides,2]*m[2:(nSides+1),1]))

    points$x &lt;- points$x * sqrt(area/current.area)
    points$y &lt;- points$y * sqrt(area/current.area)

    return (points)
    }
</code></pre>
"
27926505,What type of object is an R package?,2,2,2,"<p>Probably a pretty basic question but a friend and I tried to run <code>str(packge_name)</code> and R threw us an error.  Now that I'm looking at it, I'm wondering if an R package is like a .zip file in that it is a collection of objects, say pictures and songs, but not a picture or song itself. </p>

<p>If I tried to open a zip of pictures with an image viewer, it wouldn't know what to do until I unzipped it - just like I can't call <code>str(forecast)</code> but I can call <code>str(ts)</code> once I've loaded the forecast package into my library...</p>

<p>Can anyone set me straight?</p>
","<p>R packages are generally distributed as compressed bundles of files. They can either be in ""binary"" form which are preprocessed at a repository to compile any C or Fortran source and create the proper headers, or they can be in source form where the various required files are available to be used in the installation process, but this requires that the users have the necessary compilers and tools installed at locations where the R build process using OS system resources can get at them.</p>

<p>If you read the documentation for a package at CRAN you see they are distributed in set of compressed formats that vary depending on the OS-targets:</p>

<pre><code>Package source:     Rcpp_0.11.3.tar.gz  # the Linus/UNIX targets
Windows binaries:   r-devel: Rcpp_0.11.3.zip, r-release: Rcpp_0.11.3.zip, r-oldrel: Rcpp_0.11.3.zip
OS X Snow Leopard binaries:     r-release: Rcpp_0.11.3.tgz, r-oldrel: Rcpp_0.11.3.tgz
OS X Mavericks binaries:    r-release: Rcpp_0.11.3.tgz
Old sources:    Rcpp archive   # not really a file but a web link
</code></pre>

<p>Once installed an R package will have a specified directory structure. The DESCRIPTION file is a text file with specific entries for components that determine whether the local installation meets the dependencies of the package. There are NAMESPACE, LICENSE, and INDEX files. There are directories named '/help', '/html', '/Meta', '/R', and possibly '/libs', '/demo', '/data', '/unitTests', and others.</p>

<p>This is the tree at the top of the <code>../library/Rcpp</code> package directory:</p>

<pre><code>$ ls
CITATION    NAMESPACE   THANKS      examples    libs
DESCRIPTION NEWS.Rd     announce    help        prompt
INDEX       R       discovery   html        skeleton
Meta        README      doc     include     unitTests
</code></pre>

<p>So in the ""life-cycle"" of a package, there will be initially a series of required and optional files, which then get processed by the BUILD and CHECK mechanisms into an installed package, which than then get compressed for distribution, and later unpacked into a specified directory tree on the users machine. See these help pages:</p>

<pre><code>?.libPaths  # also describes .Library()
?package.skeleton
?install.packages
?INSTALL
</code></pre>

<p>And of course read Writing R Extensions, a document that ships with every installation of R.</p>
"
5070121,Basic R - Outputting basic R correlation table -> LaTex or text,2,2,4,"<p>I am generating a correlation table with <a href=""http://myowelt.blogspot.com/2008/04/beautiful-correlation-tables-in-r.html"" rel=""nofollow"">http://myowelt.blogspot.com/2008/04/beautiful-correlation-tables-in-r.html</a></p>

<p>I am not successful, however, in outputting the file to a usable LaTex file or text file.
I have been unsuccessful using <code>sink()</code> to save the data to a text file.</p>

<p>Suppose I am  using the following command:</p>

<pre><code>corstarsl(lpp_axis1)
</code></pre>

<p>How would I pipe the output to a text file? I've read the documentation on sink and I'm missing a step somewhere. (I open the connection, execute the command, unlink the file and then I find nothing.)</p>

<p>I've also tried using the output from <code>xtable(cortstarsl(lpp_axis1))</code> in a tex file yet I receive a ""element table not found error. I do not know enough about Tex to track the source of the problem.</p>

<p>Suggestions for outputting this data?
Other suggestions for creating a correlation table?</p>
","<p>Using the code from the web page you link to, I get (with the built in <code>airquality</code> data):</p>

<pre><code>&gt; require(Hmisc)
&gt; require(xtable)
&gt; xtable(corstarsl(airquality))
% latex table generated in R 2.12.1 by xtable 1.5-6 package
% Mon Feb 21 20:00:34 2011
\begin{table}[ht]
\begin{center}
\begin{tabular}{rlllll}
  \hline
 &amp; ozone &amp; solar.r &amp; wind &amp; temp &amp; month \\ 
  \hline
ozone &amp;  &amp;  &amp;  &amp;  &amp;  \\ 
  solar.r &amp;  0.35*** &amp;  &amp;  &amp;  &amp;  \\ 
  wind &amp; -0.60*** &amp; -0.06  &amp;  &amp;  &amp;  \\ 
  temp &amp;  0.70*** &amp;  0.28*** &amp; -0.46*** &amp;  &amp;  \\ 
  month &amp;  0.16  &amp; -0.08  &amp; -0.18*  &amp;  0.42*** &amp;  \\ 
  day &amp; -0.01  &amp; -0.15  &amp;  0.03  &amp; -0.13  &amp; -0.01  \\ 
   \hline
\end{tabular}
\end{center}
\end{table}
</code></pre>

<p>So the question then is how to get this TeX output to a file. Here <code>capture.output()</code> is one friend:</p>

<pre><code>&gt; capture.output(xtable(corstarsl(airquality)), file = ""mytable.tex"")
</code></pre>

<p>Which outputs the code to file named <code>mytable.tex</code>:</p>

<pre><code>$ cat mytable.tex 
% latex table generated in R 2.12.1 by xtable 1.5-6 package
% Mon Feb 21 20:01:03 2011
\begin{table}[ht]
\begin{center}
\begin{tabular}{rlllll}
  \hline
 &amp; ozone &amp; solar.r &amp; wind &amp; temp &amp; month \\ 
  \hline
ozone &amp;  &amp;  &amp;  &amp;  &amp;  \\ 
  solar.r &amp;  0.35*** &amp;  &amp;  &amp;  &amp;  \\ 
  wind &amp; -0.60*** &amp; -0.06  &amp;  &amp;  &amp;  \\ 
  temp &amp;  0.70*** &amp;  0.28*** &amp; -0.46*** &amp;  &amp;  \\ 
  month &amp;  0.16  &amp; -0.08  &amp; -0.18*  &amp;  0.42*** &amp;  \\ 
  day &amp; -0.01  &amp; -0.15  &amp;  0.03  &amp; -0.13  &amp; -0.01  \\ 
   \hline
\end{tabular}
\    end{center}
\end{table}
</code></pre>

<p>For ""plain"" delimited text output, perhaps to dump into a word processor or Spreadsheet, try <code>write.table()</code>, eg:</p>

<pre><code>&gt; write.table(corstarsl(airquality), file = ""mytable2.txt"")
</code></pre>

<p>Which results in a file like this:</p>

<pre><code>$ cat mytable2.txt 
""ozone"" ""solar.r"" ""wind"" ""temp"" ""month""
""ozone"" """" """" """" """" """"
""solar.r"" "" 0.35***"" """" """" """" """"
""wind"" ""-0.60***"" ""-0.06 "" """" """" """"
""temp"" "" 0.70***"" "" 0.28***"" ""-0.46***"" """" """"
""month"" "" 0.16 "" ""-0.08 "" ""-0.18* "" "" 0.42***"" """"
""day"" ""-0.01 "" ""-0.15 "" "" 0.03 "" ""-0.13 "" ""-0.01 ""
</code></pre>

<p>You can alter the quoting and delimiter to your heart's content - see <code>?write.table</code>.</p>
"
32770004,Most Efficient way to create a symmetric matrix,2,3,3,"<p>I have the following matrix/ dataframe:</p>

<pre><code>&gt; e
  V1 V2 V3 V4 V5
1  0  2  3  4  5
2  0  0  6  8 10
3  0  0  0 12 15
4  0  0  0  0 20
5  0  0  0  0  0
</code></pre>

<p>In this case N=5 (number of rows = number of columns). I would like to fill in the missing values in this symmetric matrix (e[1,2]=e[2,1] etc.). Is there a most efficient way to fill in the missing values (N the matrix size in my case quite big)? Is there a better way than nested loops?</p>
","<p>Just for completion I would like to also show this technique. The addition of the transpose will not work if the lower part of the matrix (under the diagonal) has values filled in, as it will add them to the upper part of the matrix.</p>

<p>Using the Matrix package we can create a sparse Matrix, which in case of creating the symmetric of a big matrix would require much less memory and even speed it up.</p>

<p>In order to create a symmetric sparse matrix from matrix <code>e</code> we would do:</p>

<pre><code>library(Matrix)
rowscols &lt;- which(upper.tri(e), arr.ind=TRUE)
sparseMatrix(i=rowscols[,1],    #rows to fill in
             j=rowscols[,2],    #cols to fill in
             x=e[upper.tri(e)], #values to use (i.e. the upper values of e)
             symmetric=TRUE,    #make it symmetric
             dims=c(nrow(e),nrow(e))) #dimensions
</code></pre>

<p>Output:</p>

<pre><code>5 x 5 sparse Matrix of class ""dsCMatrix""

[1,] .  2  3  4  5
[2,] 2  .  6  8 10
[3,] 3  6  . 12 15
[4,] 4  8 12  . 20
[5,] 5 10 15 20  .
</code></pre>

<p>Microbenchmark:</p>

<p>Let's make a function to make a symmetric Matrix out of a matrix (copies the upper part of matrix to the lower by default):</p>

<pre><code>symmetrise &lt;- function(mat){
  rowscols &lt;- which(upper.tri(mat), arr.ind=TRUE)
  sparseMatrix(i=rowscols[,1], 
               j=rowscols[,2], 
               x=mat[upper.tri(mat)], 
               symmetric=TRUE, 
               dims=c(nrow(mat),ncol(mat)) )  
}
</code></pre>

<p>And test:</p>

<pre><code>&gt; microbenchmark(
e + t(e),
symmetrise(e),
e[lower.tri(e)] &lt;- t(e)[lower.tri(e)],
times=1000
)
Unit: microseconds
                                  expr      min       lq      mean   median        uq       max neval cld
                              e + t(e)   75.946   99.038  117.1984  110.841  134.9590   246.825  1000 a  
                         symmetrise(e) 5530.212 6246.569 6950.7681 6921.873 7034.2525 48662.989  1000   c
 e[lower.tri(e)] &lt;- t(e)[lower.tri(e)]  261.193  322.771  430.4479  349.968  395.3815 36873.894  1000  b 
</code></pre>

<p>As you can see <code>symmetrise</code> is actually much slower than <code>e + t(e)</code> or <code>df[lower.tri(df)] &lt;- t(df)[lower.tri(df)]</code> but at least you have a function that automatically symmetrises a matrix (takes the upper part and creates the lower by default) and in case you have a big matrix where memory is an issue, this might come in handy.</p>

<p>P.S. Wherever you see a <code>.</code> in the Matrix this represents a zero. By using a different system a sparse Matrix is a kind of 'compressed' object making it more memory efficient.</p>
"
37066808,What is the use of RTVS if you have Rstudio already?,2,2,2,"<p>What is the advantage go R tools for Visual Studio when you already have Rstudio installed on your machine. Even if I am using Visual Studio and lets say in that I am working on some C# project, that will be a completely different environment than that of RTVS IDE. So in what scenario RTVS will be useul?</p>
","<p>As mentioned in the RTVS release page, R-Studio is a mature product with an awesome set of features, and RTVS has a fair way to go still in order to catch up. RTVS is in preview, it is not considered to be a completed and shipping product yet. </p>

<p>I expect I will need to edit this answer a lot in the coming months, however currently:</p>

<ul>
<li>One place where RTVS is notably better is the variable explorer, which allows you to browse and drill-down into your variables interactively, as opposed to R-Studio's version that has no drill-down ability at this point. </li>
<li>Display of variable values in tooltips while debugging is also very nice, a feature R-Studio lacks. </li>
<li>RTVS has some very nice Excel exporting capabilities in the new version.</li>
<li>Visual Studio allows the windows to be detached, which makes using multiple monitors during development more useful. I love this.</li>
<li>RTVS has some pretty impressive intellisense and code-snippets.</li>
<li>The new package explorer is pretty impressive.
Some newer features:</li>
<li>The way RTVS handles multiple plots is impressive, it has a plot window history and you can even drag and drop plots between devices.</li>
<li>It has a workspaces concept makes it very easy to change between multiple R installations, both locally and remotely. This is an important Enterprise feature as you will undoubtably develop and deploy on different environments.</li>
</ul>

<p>Both R-Studio and RTVS are open source, so the community can add features to them both if so desired.</p>

<p>Over time I expect RTVS to excel at integration with Microsoft's every expanding set of R offerings (the Revolution R engine is being integrated into SQL and other places for example) that are particularly interesting for production deployment.</p>

<p>RTVS should also be able to leverage Visual Studio's advanced debugging and code factoring features, however R-Studio can be expected to respond to those advances as well.</p>

<p>In general I think this will bring some welcome competition and variation into the R development world. Note that R has far fewer development GUI options currently than something like Python does for example.</p>
"
23377050,Purging data frame of unwanted rows based on two vector sets,1,1,1,"<p>Looking for a way of purging rows from a data frame whose values do not match two sets of vectors, one set applies to column1, and the second to column2. I want this to be an either or both (i.e. if row x has a col1 match but not col2, row x is not purged). This is what I was going for but the code doesn't work:</p>

<pre><code>purged_frame &lt;- original_frame[(original_frame$Column1 %in% vectorsetforcol1 | original_frame$Column2 %in% vectorsetforcol2),]
</code></pre>

<p>Thanks!</p>

<p>Edit: need an either or both scenario - wasn't very clear in my original question - anybody?</p>
","<p>It is best to share a reproducible example, (<a href=""https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example"">How to make a great R reproducible example?</a>)</p>

<p>Using the dataset available in base R <code>mtcars</code> and <code>data(mtcars)</code>, create two conditional vectors</p>

<pre><code>vectorsetforcol1&lt;- mtcars$mpg[mtcars$mpg&lt;15]
vectorsetforcol2&lt;-unique(mtcars$carb[mtcars$carb==2])
</code></pre>

<p><strong>Output condition 1: (mpg &lt; 15)</strong></p>

<pre><code>&gt; mtcars[mtcars$mpg %in% vectorsetforcol1,]
                     mpg cyl disp  hp drat    wt  qsec vs am gear carb
Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4
Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4
Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4
</code></pre>

<p><strong>Output condition 2: (carb == 2)</strong></p>

<pre><code>&gt; mtcars[mtcars$carb %in% vectorsetforcol2,]
                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Honda Civic       30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Dodge Challenger  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
AMC Javelin       15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Pontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
Porsche 914-2     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Lotus Europa      30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
Volvo 142E        21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
&gt; 
</code></pre>

<p><strong>Combine conditions 1 and 2</strong></p>

<pre><code>&gt; cond.df&lt;-mtcars[(mtcars$mpg %in% vectorsetforcol1 | mtcars$carb %in% vectorsetforcol2  ),]
&gt; cond.df
                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
&gt; 
</code></pre>

<p><strong>Test condition 1: (NOT(mpg &lt; 15))</strong></p>

<p>The cases where condition 1 is violated are present because they follow condition 2 (carb ==2)</p>

<pre><code>&gt; cond.test.col1&lt;-cond.df[!cond.df$mpg %in% vectorsetforcol1, ]
&gt; cond.test.col1
                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb
Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
Merc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
Merc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
Honda Civic       30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
Dodge Challenger  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
AMC Javelin       15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
Pontiac Firebird  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
Porsche 914-2     26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
Lotus Europa      30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
Volvo 142E        21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
&gt; 
</code></pre>

<p><strong>Test condition 2: (NOT(carb == 2))</strong></p>

<p>The cases where condition 2 is violated are present because they follow condition 1 (mpg &lt;15)</p>

<pre><code>&gt; cond.test.col2&lt;-cond.df[!cond.df$carb %in% vectorsetforcol2, ]
&gt; cond.test.col2
                     mpg cyl disp  hp drat    wt  qsec vs am gear carb
Duster 360          14.3   8  360 245 3.21 3.570 15.84  0  0    3    4
Cadillac Fleetwood  10.4   8  472 205 2.93 5.250 17.98  0  0    3    4
Lincoln Continental 10.4   8  460 215 3.00 5.424 17.82  0  0    3    4
Chrysler Imperial   14.7   8  440 230 3.23 5.345 17.42  0  0    3    4
Camaro Z28          13.3   8  350 245 3.73 3.840 15.41  0  0    3    4
</code></pre>

<p>This is the same approach as yours, if you had provided a working example someone would have pointed out the issue...</p>
"
16129876,"ggplot2 multiple scales/legends per aesthetic, revisited",1,3,1,"<p>I have an example where I'd like to highlight several properties of sequence alignments using ggplot. I'm using geom_tile and want to have two sets of differently coloured tiles for two score properties. I am only able to visualize one.</p>

<p>I am aware of the limitation of one scale per aesthetic (<a href=""https://stackoverflow.com/questions/11752709/using-two-scale-colour-gradients-on-one-ggplot"">and the logic behind it</a>), but maybe someone has an idea how to hack it for cases like this where it would make sense to have different colour scales in one 'plot'. </p>

<p>Perhaps with adding manually the Grobs, but I wouldn't know where to start...</p>

<p>an additional question: for some reason the <code>override.aes=list(shape = ""A"")</code> does not work, any ideas why? </p>

<p>one more: any method to scale text proportionally to the size of the tile (or the other way around)?</p>

<pre><code>library(ggplot2)
library(grid)

pd = data.frame(
  letters = strsplit(""AGTGACCGACTATCATAGTGACCCAGAATCATAGTGACCGAGTATGAT"", """")[[1]],
  species = rep(c(""Human"", ""Armadillo"", ""Porcupine""), each=16),
  x       = rep(1:16, 3),
  change  = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
              0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,
              0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0),
  score1  = c(0,0,0,0,0,0,1,1,2,2,2,3,3,3,4,3,
              0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,
              0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),
  score2  = c(0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,
              0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,
              0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0)
)


ggplot(pd[pd$score1 != 0,], aes(x=x, y=species)) +
  coord_fixed(ratio = 1.5, xlim=c(0.5,16.5), ylim=c(0.5, 3.5)) +
  geom_tile(aes(fill=score1)) +
  scale_fill_gradient2(""Score 1"", limits=c(0,4),low=""#762A83"", mid=""white"", high=""#1B7837"", guide=guide_colorbar(title.position=""top"")) +
  geom_text(data=pd, aes(label=letters, color=factor(change)), size=rel(5), family=""mono"") +
  scale_color_manual(""Change"", values=c(""black"", ""#F2A11F""), labels=c(""None"", ""Some""), guide=guide_legend(direction=""vertical"", title.position=""top"", override.aes=list(shape = ""A""))) +
  theme(panel.background=element_rect(fill=""white"", colour=""white""),
        axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(family=""mono"", size=rel(2)),
        axis.text.x = element_text(size=rel(0.7)),
        legend.text = element_text(size=rel(0.7)),
        legend.key.size = unit(0.7, ""lines""),
        legend.position = ""bottom"", legend.box = ""horizontal"") +
  ggtitle(""What about Score2?"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/MDtIt.png"" alt=""How to add another layer of tiles with different color scale?""></p>
","<p>I managed to get a satisfactory result by combining grobs from two separately generated plots. I'm sure the solution can be generalized better to accommodate different grob indices ...</p>

<pre><code>library(ggplot2)
library(grid)

pd = data.frame(
  letters = strsplit(""AGTGACCGACTATCATAGTGACCCAGAATCATAGTGACCGAGTATGAT"", """")[[1]],
  species = rep(c(""Human"", ""Armadillo"", ""Porcupine""), each=16),
  x       = rep(1:16, 3),
  change  = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
              0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,
              0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0),
  score1  = c(0,0,0,0,0,0,1,1,2,2,2,3,3,3,4,3,
              0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,
              0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),
  score2  = c(0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,
              0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,
              0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0)
)


p1=ggplot(pd[pd$score1 != 0,], aes(x=x, y=species)) +
  coord_fixed(ratio = 1.5, xlim=c(0.5,16.5), ylim=c(0.5, 3.5)) +
  geom_tile(aes(fill=score1)) +
  scale_fill_gradient2(""Score 1"", limits=c(0,4),low=""#762A83"", mid=""white"", high=""#1B7837"", guide=guide_colorbar(title.position=""top"")) +
  geom_text(data=pd, aes(label=letters, color=factor(change)), size=rel(5), family=""mono"") +
  scale_color_manual(""Change"", values=c(""black"", ""#F2A11F""), labels=c(""None"", ""Some""), guide=guide_legend(direction=""vertical"", title.position=""top"", override.aes=list(shape = ""A""))) +
  theme(panel.background=element_rect(fill=""white"", colour=""white""),
        axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(family=""mono"", size=rel(2)),
        axis.text.x = element_text(size=rel(0.7)),
        legend.text = element_text(size=rel(0.7)),
        legend.key.size = unit(0.7, ""lines""),
        legend.position = ""bottom"", legend.box = ""horizontal"") +
  ggtitle(""Voila, the Score2!"")

p2=ggplot(pd[pd$score2 != 0,], aes(x=x, y=species)) +
  coord_fixed(ratio = 1.5, xlim=c(0.5,16.5), ylim=c(0.5, 3.5)) +
  geom_tile(aes(fill=score2)) +
  scale_fill_gradient2(""Score 2"", limits=c(0,3),low=""#1B7837"", mid=""white"", high=""#762A83"", guide=guide_colorbar(title.position=""top"")) +
  geom_text(data=pd, aes(label=letters, color=factor(change)), size=rel(5), family=""mono"") +
  scale_color_manual(""Change"", values=c(""black"", ""#F2A11F""), labels=c(""None"", ""Some""), guide=guide_legend(direction=""vertical"", title.position=""top"", override.aes=list(shape = ""A""))) +
  theme(panel.background=element_rect(fill=""white"", colour=""white""),
        axis.title = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_text(family=""mono"", size=rel(2)),
        axis.text.x = element_text(size=rel(0.7)),
        legend.text = element_text(size=rel(0.7)),
        legend.key.size = unit(0.7, ""lines""),
        legend.position = ""bottom"", legend.box = ""horizontal"") +
  ggtitle(""What about Score2?"")


p1g=ggplotGrob(p1)
p2g=ggplotGrob(p2)

combo.grob = p1g

combo.grob$grobs[[8]] = cbind(p1g$grobs[[8]][,1:4], 
                              p2g$grobs[[8]][,3:5], 
                              size=""first"")

combo.grob$grobs[[4]] = reorderGrob(
                          addGrob(p1g$grobs[[4]], 
                                  getGrob(p2g$grobs[[4]], 
                                          ""geom_rect.rect"", 
                                          grep=TRUE)), 
                          c(1,2,5,3,4))
grid.newpage()
grid.draw(combo.grob) 
</code></pre>

<p><img src=""https://i.stack.imgur.com/mNzSl.png"" alt=""Two scales in one plot""></p>
"
24664382,"chartSeries with XTS, can't plot points and technical indicators simultaneously",2,3,3,"<p>Edit: I rigged a smaller example so you can reproduce it if you want.</p>

<p>I have a OHLC XTS table I'm using (of Euro/$)</p>

<pre><code>&gt; theBars
                     Open  High   Low Close
2014-06-17 01:42:26 13835 13836 13835 13836
2014-06-17 01:42:59 13836 13838 13835 13837
2014-06-17 01:43:21 13837 13838 13837 13837
2014-06-17 01:43:51 13837 13837 13837 13837
2014-06-17 01:44:23 13837 13837 13837 13837
2014-06-17 01:44:51 13837 13838 13837 13838
2014-06-17 01:45:28 13837 13840 13837 13840
2014-06-17 01:45:59 13840 13842 13840 13842
2014-06-17 01:46:22 13842 13843 13842 13843
2014-06-17 01:46:58 13843 13844 13843 13844
2014-06-17 01:47:29 13843 13844 13843 13843
2014-06-17 01:47:58 13843 13843 13841 13843
2014-06-17 01:48:22 13843 13843 13842 13843
2014-06-17 01:48:59 13843 13843 13842 13842
2014-06-17 01:49:05 13842 13842 13841 13841
2014-06-17 01:49:54 13841 13841 13840 13841
2014-06-17 01:50:18 13841 13841 13841 13841
2014-06-17 01:50:44 13840 13840 13839 13840
2014-06-17 01:52:55 13839 13839 13838 13839
2014-06-17 01:53:42 13838 13839 13838 13838
2014-06-17 01:54:22 13837 13838 13837 13838
2014-06-17 01:54:58 13837 13838 13836 13837
2014-06-17 01:55:29 13836 13836 13834 13835
2014-06-17 01:55:59 13835 13837 13835 13837
2014-06-17 01:56:28 13837 13839 13837 13838
2014-06-17 01:56:59 13838 13838 13837 13837
2014-06-17 01:57:29 13837 13838 13837 13838
2014-06-17 01:57:59 13838 13838 13838 13838
2014-06-17 01:58:29 13838 13838 13836 13837
2014-06-17 01:58:58 13837 13837 13836 13836
2014-06-17 01:59:29 13836 13841 13836 13840
2014-06-17 01:59:59 13840 13840 13835 13837
2014-06-17 02:00:29 13837 13837 13836 13836
2014-06-17 02:00:58 13836 13836 13835 13836
2014-06-17 02:01:29 13835 13837 13835 13837
2014-06-17 02:01:58 13837 13837 13836 13836
</code></pre>

<p>and I'm plotting points on it with</p>

<pre><code>points.default(x=timeIndex*tMult+1, #aligns with tMult = 3 when candles are candles, 1 when they are matchsticks
                     y=as.numeric(dataCol[i]), #the price, its around 13818
                     cex=dotSize,
                     pch=dotType,
                     col=thecolor)
</code></pre>

<p>Which works as intended.
In this example, here are the values</p>

<pre><code>      x       y     cex     pch     col 
   ""19"" ""13841""     ""2""     ""2"" ""green""
        x         y       cex       pch       col 
     ""19""   ""13841""       ""3""       ""2"" ""#7070FF"" 
        x         y       cex       pch       col 
     ""19""   ""13841""       ""4""       ""2"" ""#7070FF""
</code></pre>

<p>(The center of each point is the same because it was multiple trades at one price at the same time)
I'm also graphing some technical indicators with </p>

<pre><code>plot(addMACD(fast,slow,signal,maType,histogram))
</code></pre>

<p>and</p>

<pre><code> plot(addSMA(n,overlay = overlay))
</code></pre>

<p>etc. which also work as intended just using the default values.</p>

<p>If I do a technical which overlays the technical onto the graph like bbands and SMA, the points can be graphed simultaneously. However, if I have a plot like MACD which is in a separate box that segregates the graph into two graphs in the same plot window, I can no longer plot the points. Why?!</p>
","<p>The y-axis is reindexed according to the indicator added.
I found that it's better to reindex it myself, then plot the graph by adjusting where the points should be. I'll include all the constants I found through that search here.</p>

<p>assuming your data is in OHLC in a variable called ""theBars"", do this <code>par(usr=c(0,1,min(theBars),max(theBars)),xpd=TRUE)</code></p>

<p>I did all of this on a 1920x1200 resolution monitor, so all of the pixel measurements are in terms of that. This is compatible with any resolution by using <code>par(""fin"")</code>
Also, this executes in a fraction of a second. This was written for clarity, not speed.</p>

<p>Horizontal alignment across [0,1] with varying numbers of candles:</p>

<pre><code>if(numBars &lt; 10) stop(""cannot plot less than 10 candles"")
    horizNumerator &lt;- if(numBars == 10) 1547 else
                      if(numBars == 11) 1559 else
                      if(numBars == 12) 1568 else
                      if(numBars == 13) 1574 else
                      if(numBars == 14) 1581 else
                      if(numBars == 15) 1588 else
                      if(numBars == 16) 1592 else
                      if(numBars == 17) 1597 else
                      if(numBars == 18) 1601 else
                      if(numBars == 19) 1603 else
                      if(numBars == 20) 1606 else
                      if(numBars &lt;= 30) 1626 + (numBars-30)/(10/20) else
                      if(numBars &lt;= 40) 1636 + (numBars-40)/(10/10) else
                      if(numBars &lt;= 50) 1641 + (numBars-50)/(10/7) else
                      if(numBars &lt;= 60) 1646 + (numBars-60)/(10/5) else
                      if(numBars &lt;= 100) 1651 + (numBars-100)/(40/5) else
                      if(numBars &lt;= 140) 1655 + (numBars-140)/(40/4) else
                      if(numBars &lt;= 312) 1658 + (numBars-312)/(172/3) else
                      if(numBars &gt; 312) 1662



   horizUnit &lt;- (horizNumerator/1802)/(length(theBars[,1])-1)*ifelse(plotNum&lt;2,1,1693/1655)*par(""fin"")[1]/20
    startHoriz &lt;- ifelse(plotNum&lt;2,55/1802,35/1802)*par(""fin"")[1]/20
</code></pre>

<p>Vertical alignment, with <code>plotNum</code> being the number of indicators already plotted and <code>deltaP &lt;- max(theBars)-min(theBars)</code>:</p>

<p>This deals with the indicators squishing the plot area</p>

<pre><code>pixelStretch &lt;- (switch(as.character(plotNum),""0""=975,""1""=665,""2""=551,""3""=465, ""4""=400,""5""=352,""6""= 313))/(983) 
deltaP &lt;- max(theBars)-min(theBars)
</code></pre>

<p>This deals with the max moving slightly higher after an indicator is plotted</p>

<pre><code>maxRaise &lt;- if(plotNum == 0) 0 else
                if(plotNum == 1) 15 else
                if(plotNum == 2) 31 else
                if(plotNum &gt;  2) 36
    maxRaise &lt;- maxRaise*par(""fin"")[2]/12.0625/983
</code></pre>

<p>then plot points in a data column <code>dataCol</code> like this</p>

<pre><code>  points.default(x=startHoriz + horizUnit*timeIndex,
                 y= maxRaise * deltaP + max(theBars)-((max(theBars)-as.numeric(dataCol[i]))*(pixelStretch)),
                 cex=dotSize,
                 pch=dotType,
                 col=thecolor)
</code></pre>
"
37197052,What is happening here in lubridate::unique.Interval?,2,3,4,"<p>Sample data adapted from here: <a href=""https://www.reddit.com/r/rstats/comments/4j2efe/help_counting_unique_days_in_r_with_overlap_and/"" rel=""nofollow"">https://www.reddit.com/r/rstats/comments/4j2efe/help_counting_unique_days_in_r_with_overlap_and/</a></p>

<pre><code>df = read.table(text = ""Start            End
           1/8/2015         1/9/2015
           1/8/2015         1/9/2015
           1/13/2015        1/15/2015
           1/7/2015         1/17/2015
           1/12/2015        1/22/2015
           1/8/2015         1/16/2015"" , header = T)
</code></pre>

<p>Create interval </p>

<pre><code>df %&gt;% transmute(Start = mdy(Start), End = mdy(End), Interval = interval(Start, End)) 

       Start        End                       Interval
1 2015-01-08 2015-01-09 2015-01-08 UTC--2015-01-09 UTC
2 2015-01-08 2015-01-09 2015-01-08 UTC--2015-01-09 UTC
3 2015-01-13 2015-01-15 2015-01-13 UTC--2015-01-15 UTC
4 2015-01-07 2015-01-17 2015-01-07 UTC--2015-01-17 UTC
5 2015-01-12 2015-01-22 2015-01-12 UTC--2015-01-22 UTC
6 2015-01-08 2015-01-16 2015-01-08 UTC--2015-01-16 UTC
</code></pre>

<p>Find unique interval. What happened to this interval? 2015-01-12 UTC--2015-01-22 UTC is gone. Is this intended behavior?</p>

<pre><code>.Last.value %&gt;% select(Interval) %&gt;% unique

                        Interval
1 2015-01-08 UTC--2015-01-09 UTC
3 2015-01-13 UTC--2015-01-15 UTC
4 2015-01-07 UTC--2015-01-17 UTC
6 2015-01-08 UTC--2015-01-16 UTC
</code></pre>
","<p>2015-01-12 UTC--2015-01-22 UTC is removed because it is a duplicated case for 2015-01-07 UTC--2015-01-17 UTC, even though they are not identical objects but they are equal to each other under <code>==</code> operator.</p>

<pre><code>&gt; intervalDf
       Start        End                       Interval
1 2015-01-08 2015-01-09 2015-01-08 UTC--2015-01-09 UTC
2 2015-01-08 2015-01-09 2015-01-08 UTC--2015-01-09 UTC
3 2015-01-13 2015-01-15 2015-01-13 UTC--2015-01-15 UTC
4 2015-01-07 2015-01-17 2015-01-07 UTC--2015-01-17 UTC
5 2015-01-12 2015-01-22 2015-01-12 UTC--2015-01-22 UTC
6 2015-01-08 2015-01-16 2015-01-08 UTC--2015-01-16 UTC
&gt; intervalDf[4,3]
[1] 2015-01-07 UTC--2015-01-17 UTC

&gt; intervalDf[5,3]
[1] 2015-01-12 UTC--2015-01-22 UTC
&gt; intervalDf[4,3] == intervalDf[5,3]
[1] TRUE
</code></pre>

<p>However</p>

<pre><code>&gt; identical(intervalDf[4,3], intervalDf[5,3])
[1] FALSE
</code></pre>

<p>This possibly also implies that <code>unique</code> uses the <code>==</code> as the comparison function. If you want to keep them, you may convert the <code>Interval</code> column to character then apply the unique function.</p>

<p><strong>Update</strong>:
Inconsistency of <code>unique</code> function on single and multiple columns data frame.</p>

<pre><code>&gt; dfTest
  x                       Interval
1 1 2015-01-08 UTC--2015-01-09 UTC
2 1 2015-01-08 UTC--2015-01-09 UTC
3 1 2015-01-13 UTC--2015-01-15 UTC
4 1 2015-01-07 UTC--2015-01-17 UTC
5 1 2015-01-12 UTC--2015-01-22 UTC
6 1 2015-01-08 UTC--2015-01-16 UTC
&gt; unique(dfTest)
  x                       Interval
1 1 2015-01-08 UTC--2015-01-09 UTC
3 1 2015-01-13 UTC--2015-01-15 UTC
4 1 2015-01-07 UTC--2015-01-17 UTC
5 1 2015-01-12 UTC--2015-01-22 UTC
6 1 2015-01-08 UTC--2015-01-16 UTC
&gt; dfTest1
                        Interval
1 2015-01-08 UTC--2015-01-09 UTC
2 2015-01-08 UTC--2015-01-09 UTC
3 2015-01-13 UTC--2015-01-15 UTC
4 2015-01-07 UTC--2015-01-17 UTC
5 2015-01-12 UTC--2015-01-22 UTC
6 2015-01-08 UTC--2015-01-16 UTC
&gt; unique(dfTest1)
                        Interval
1 2015-01-08 UTC--2015-01-09 UTC
3 2015-01-13 UTC--2015-01-15 UTC
4 2015-01-07 UTC--2015-01-17 UTC
6 2015-01-08 UTC--2015-01-16 UTC
</code></pre>

<p>Two methods definition that explains the difference.</p>

<pre><code>&gt; getAnywhere(""unique.data.frame"") A single object matching ‘unique.data.frame’ was found It was found in the following places   package:base   registered S3 method for unique from namespace base   namespace:base with value

function (x, incomparables = FALSE, fromLast = FALSE, ...)  {
    if (!identical(incomparables, FALSE)) 
        .NotYetUsed(""incomparables != FALSE"")
    x[!duplicated(x, fromLast = fromLast, ...), , drop = FALSE] } &lt;bytecode: 0x10c2ab0a0&gt; &lt;environment: namespace:base&gt;
&gt; getAnywhere(""duplicated.data.frame"") A single object matching ‘duplicated.data.frame’ was found It was found in the following places package:base   registered S3 method for duplicated from namespace base namespace:base with value

function (x, incomparables = FALSE, fromLast = FALSE, ...)  {
    if (!identical(incomparables, FALSE)) 
        .NotYetUsed(""incomparables != FALSE"")
    if (length(x) != 1L) 
        duplicated(do.call(""paste"", c(x, sep = ""\r"")), fromLast = fromLast)
    else duplicated(x[[1L]], fromLast = fromLast, ...) } &lt;bytecode: 0x10c33a4b0&gt; &lt;environment: namespace:base&gt;
</code></pre>
"
33836375,Connect to a complex SOAP Web Service,2,2,4,"<p>I'm tryng to use to this web service in order to get financial information but since I know very little about this language I find it very hard to even connect:</p>

<p><a href=""http://www2.sentinelperu.com/ws/asentinelws02.aspx?wsdl"" rel=""nofollow"">http://www2.sentinelperu.com/ws/asentinelws02.aspx?wsdl</a></p>

<p>I would really appreciate any help. So far I tried this:</p>

<pre><code>    library(RCurl)

    headerFields =
     c(Accept = ""text/xml"",
     Accept = ""multipart/*"",
     'Content-Type' = ""text/xml; charset=utf-8"",
     SOAPAction = ""http://www2.sentinelperu.com/ws/asentinelws02.aspx?wsdl"")

    body = '&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
            &lt;soap:Envelope
            xmlns:soap=""http://www.w3.org/2001/12/soap-envelope""
            soap:encodingStyle=""http://www.w3.org/2001/12/soap-encoding""&gt;

            &lt;soap:Envelope xmlns:xsi=""http://www.w3.org/2001/XMLSchema-                         instance"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema""  xmlns:soap=""http://www.w3.org/2001/12/soap-envelope""&gt;
            &lt;soap:Body&gt;

            &lt;Usuario&gt;XXXX&lt;/Usuario&gt;
            &lt;Password&gt;XXXX&lt;/Password&gt;
            &lt;Servicio&gt;005&lt;/Servicio&gt;
            &lt;Tipodoc&gt;42945948&lt;/Tipodoc&gt;
            &lt;Nrodoc&gt;06594002&lt;/Nrodoc&gt;

            &lt;/soap:Body&gt;
            &lt;/soap:Envelope&gt;'


            curlPerform(url = ""http://www2.sentinelperu.com/ws/asentinelws02.aspx?wsdl"",
            httpheader = headerFields,
            postfields = body
            )
</code></pre>

<p>The output is:</p>

<pre><code>     OK 
     0 
     &lt;?xml version = ""1.0"" encoding = ""utf-8""?&gt;
     &lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:SOAP-ENC=""http://schemas.xmlsoap.org/soap/encoding/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&gt;
     &lt;/SOAP-ENV:Envelope&gt;
</code></pre>
","<p>Nothing wrong with the R part, but you have to respect the namespaces in your request - change the body to:</p>

<pre><code>body &lt;- 
  '&lt;soapenv:Envelope 
    xmlns:soapenv=""http://schemas.xmlsoap.org/soap/envelope/"" 
    xmlns:afp=""AFPrivado""&gt;
    &lt;soapenv:Header/&gt;
    &lt;soapenv:Body&gt;
      &lt;afp:SentinelWS02.Execute&gt;
        &lt;afp:Usuario&gt;XXXX&lt;/afp:Usuario&gt;
        &lt;afp:Password&gt;XXXX&lt;/afp:Password&gt;
        &lt;afp:Servicio&gt;005&lt;/afp:Servicio&gt;
        &lt;afp:Tipodoc&gt;42945948&lt;/afp:Tipodoc&gt;
        &lt;afp:Nrodoc&gt;06594002&lt;/afp:Nrodoc&gt;
      &lt;/afp:SentinelWS02.Execute&gt;
    &lt;/soapenv:Body&gt;
  &lt;/soapenv:Envelope&gt;'
</code></pre>

<p>Response:</p>

<pre><code>&lt;?xml version = ""1.0"" encoding = ""utf-8""?&gt;
&lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:SOAP-ENC=""http://schemas.xmlsoap.org/soap/encoding/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&gt;
  &lt;SOAP-ENV:Body&gt;
    &lt;SentinelWS02.ExecuteResponse xmlns=""AFPrivado""&gt;
      &lt;Cnsdtconrapsms xmlns=""AFPrivado""&gt;
        &lt;Documento&gt;06594002&lt;/Documento&gt;
        &lt;RazonSocial /&gt;
        &lt;FechaProceso&gt;1900-01-01&lt;/FechaProceso&gt;
        &lt;Semaforos /&gt;
        &lt;Score&gt;0.0000&lt;/Score&gt;
        &lt;NroBancos&gt;0&lt;/NroBancos&gt;
        &lt;DeudaTotal&gt;0.00&lt;/DeudaTotal&gt;
        &lt;VencidoBanco&gt;0.00&lt;/VencidoBanco&gt;
        &lt;Calificativo /&gt;
        &lt;Veces24m /&gt;
        &lt;ScorePromedio&gt;0.0000&lt;/ScorePromedio&gt;
        &lt;SemaActual /&gt;
        &lt;SemaPrevio /&gt;
        &lt;SemaPeorMejor /&gt;
        &lt;Documento2 /&gt;
        &lt;EstDomic /&gt;
        &lt;CondDomic /&gt;
        &lt;DeudaTributaria&gt;0.00&lt;/DeudaTributaria&gt;
        &lt;DeudaLaboral&gt;0.00&lt;/DeudaLaboral&gt;
        &lt;DeudaImpaga&gt;0.00&lt;/DeudaImpaga&gt;
        &lt;DeudaProtestos&gt;0.00&lt;/DeudaProtestos&gt;
        &lt;DeudaSBS&gt;0.00&lt;/DeudaSBS&gt;
        &lt;TarCtas&gt;0&lt;/TarCtas&gt;
        &lt;RepNeg&gt;0&lt;/RepNeg&gt;
        &lt;TipoActv /&gt;
        &lt;FechIniActv /&gt;
        &lt;CodigoWS&gt;1&lt;/CodigoWS&gt;
      &lt;/Cnsdtconrapsms&gt;
    &lt;/SentinelWS02.ExecuteResponse&gt;
  &lt;/SOAP-ENV:Body&gt;
&lt;/SOAP-ENV:Envelope&gt;
</code></pre>

<p>To read the result you will need this:</p>

<pre><code>h = basicTextGatherer()
response &lt;- 
  curlPerform(url = ""http://www2.sentinelperu.com/ws/asentinelws02.aspx?wsdl"",
    httpheader = headerFields,
    postfields = body,
    writefunction = h$update
  )


# parse the result    
library(XML)
xml_ret &lt;- xmlParse( h$value(), asText = TRUE)

# get the score value for example:
score_element &lt;- getNodeSet(xml_ret, ""//x:Score"", namespaces = c(x = ""AFPrivado""))[[1]]
score_value  &lt;- XML::xmlValue(score_element)
</code></pre>
"
14179373,"Reshaping\Stacking Multiple Variables in R, Triads to Dyads",1,1,1,"<p>I have data describing an interaction of an individual (player 1) with two others (player 2 and player 3). Each row describes a unique combination of players, but I would like to analyze the player 1 to player 2 and player 1 to player 3 dyads separately. To accomplish this I am envisioning some sort of stacking where I can melt descriptive variables for players two and three while maintaining the data for player 1 in each row. Making matters more complicated I have multiple descriptive variables for each individual.</p>

<p>Here is a small bit of data to work with (I actually have far more descriptive variables for players 2 and 3 which I would like to stack/melt):</p>

<pre><code>    p1_id &lt;- c(1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1032, 1032, 1032, 1032, 1032, 1032)
    p1_age &lt;- c(53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 45, 45, 45, 45, 45)
    p2_id &lt;- c(14372, 15022,  9072, 15052, 2161, 18381, 15032, 14451, 16322, 11142, 8182,  1131, 7092, 4071, 16191, 18142, 4222, 11052, 2202, 16151)
    p2_money &lt;- c(4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 0, 0, 10, 0, 6, 6, 4, 6, 6)
    p2_age &lt;- c(50, 33, 56, 23, 29, 26, 28, 34, 20, 41, 34, 45, 23, 35, 25, 30, 40, 41, 45, 28)
    p3_id &lt;- c(5151, 16181, 5182, 18462, 7231, 14372, 3052, 14532, 4152, 15012, 19212, 9062, 9032, 18351, 14461, 16291, 17102, 10102, 7051, 16282)
    p3_money &lt;- c(4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 10, 10, 0, 10, 6, 6, 4, 6, 4)
    p3_age &lt;- c(30, 29, 22, 22, 43, 50, 23, 32, 31, 46, 36, 36, 21, 27, 49, 38, 40, 48, 26, 32)
    df &lt;- data.frame(p1_id, p1_age, p2_id, p2_money, p2_age, p3_id, p3_money, p3_age)
</code></pre>

<p>The dataframe:</p>

<pre><code>     p1_id p1_age p2_id p2_money p2_age p3_id p3_money p3_age
     1   1021     53 14372        4     50  5151        4     30
     2   1021     53 15022        2     33 16181        2     29
     3   1021     53  9072        2     56  5182        2     22
     4   1021     53 15052        2     23 18462        2     22
     5   1021     53  2161        2     29  7231        2     43
     6   1021     53 18381        2     26 14372        2     50
     7   1021     53 15032        2     28  3052        2     23
     8   1021     53 14451        2     34 14532        2     32
     9   1021     53 16322        2     20  4152        2     31
     10  1021     53 11142        2     41 15012        2     46
     11  1021     53  8182       10     34 19212        0     36
     12  1021     53  1131        0     45  9062       10     36
     13  1021     53  7092        0     23  9032       10     21
     14  1021     53  4071       10     35 18351        0     27
     15  1032     53 16191        0     25 14461       10     49
     16  1032     45 18142        6     30 16291        6     38
     17  1032     45  4222        6     40 17102        6     40
     18  1032     45 11052        4     41 10102        4     48
     19  1032     45  2202        6     45  7051        6     26
     20  1032     45 16151        6     28 16282        4     32
</code></pre>

<p>In case my description above was too confusing, here is how I would like the reshaped data to look:</p>

<pre><code>     row p1_id p1_age p23_id p23_money p23_age 
     1   1021     53 14372        4     50  
     2   1021     53 15022        2     33 
     3   1021     53  9072        2     56  
     4   1021     53 15052        2     23 
     5   1021     53  2161        2     29  
     6   1021     53 18381        2     26 
     7   1021     53 15032        2     28 
     8   1021     53 14451        2     34
     9   1021     53 16322        2     20
     10  1021     53 11142        2     41 
     11  1021     53  8182       10     34 
     12  1021     53  1131        0     45  
     13  1021     53  7092        0     23  
     14  1021     53  4071       10     35 
     15  1032     53 16191        0     25 
     16  1032     45 18142        6     30 
     17  1032     45  4222        6     40 
     18  1032     45 11052        4     41 
     19  1032     45  2202        6     45  
     20  1032     45 16151        6     28 
     21  1021     53  5151        4     30  
     22  1021     53 16181        2     29
     23  1021     53  5182        2     22
     24  1021     53 18462        2     22
     25  1021     53  7231        2     43 
     26  1021     53 14372        2     50
     27  1021     53  3052        2     23 
     28  1021     53 14532        2     32 
     28  1021     53  4152        2     31
     30  1021     53 19212        0     36
     31  1021     53  9062       10     36 
     32  1021     53  9032       10     21 
     33  1021     53 18351        0     27
     34  1032     53 16191        0     25
     35  1032     53 14461       10     49
     36  1032     53 16291        6     38
     37  1032     53 17102        6     40 
     38  1032     53 10102        4     48 
     39  1032     53  7051        6     26 
     40  1032     53 16282        4     32
</code></pre>

<p>Thanks for any help!</p>
","<p>This is pretty easily done if you modify your column names as follows:</p>

<pre><code>names(df) &lt;- gsub(""(.*)_(.*)"", ""\\2\\.\\1"", names(df))
names(df)
# [1] ""id.p1""    ""age.p1""   ""id.p2""    ""money.p2"" 
# [5] ""age.p2""   ""id.p3""    ""money.p3"" ""age.p3""
</code></pre>

<p>Next, use the ""row.names"" of your <code>data.frame</code> as your ""idvar"" in base R <code>reshape()</code>.</p>

<pre><code>reshape(df, direction = ""long"", idvar = ""row.names"",
        timevar = ""person"", varying = 3:8)
#       id.p1 age.p1 person    id money age row.names
# 1.p2   1021     53     p2 14372     4  50         1
# 2.p2   1021     53     p2 15022     2  33         2
# 3.p2   1021     53     p2  9072     2  56         3
# 4.p2   1021     53     p2 15052     2  23         4
# 5.p2   1021     53     p2  2161     2  29         5
# 6.p2   1021     53     p2 18381     2  26         6
# 7.p2   1021     53     p2 15032     2  28         7
# 8.p2   1021     53     p2 14451     2  34         8
# 9.p2   1021     53     p2 16322     2  20         9
# 10.p2  1021     53     p2 11142     2  41        10
# 11.p2  1021     53     p2  8182    10  34        11
# 12.p2  1021     53     p2  1131     0  45        12
# 13.p2  1021     53     p2  7092     0  23        13
# 14.p2  1021     53     p2  4071    10  35        14
# 15.p2  1032     53     p2 16191     0  25        15
# 16.p2  1032     45     p2 18142     6  30        16
# 17.p2  1032     45     p2  4222     6  40        17
# 18.p2  1032     45     p2 11052     4  41        18
# 19.p2  1032     45     p2  2202     6  45        19
# 20.p2  1032     45     p2 16151     6  28        20
# 1.p3   1021     53     p3  5151     4  30         1
# 2.p3   1021     53     p3 16181     2  29         2
# 3.p3   1021     53     p3  5182     2  22         3
# 4.p3   1021     53     p3 18462     2  22         4
# 5.p3   1021     53     p3  7231     2  43         5
# 6.p3   1021     53     p3 14372     2  50         6
# 7.p3   1021     53     p3  3052     2  23         7
# 8.p3   1021     53     p3 14532     2  32         8
# 9.p3   1021     53     p3  4152     2  31         9
# 10.p3  1021     53     p3 15012     2  46        10
# 11.p3  1021     53     p3 19212     0  36        11
# 12.p3  1021     53     p3  9062    10  36        12
# 13.p3  1021     53     p3  9032    10  21        13
# 14.p3  1021     53     p3 18351     0  27        14
# 15.p3  1032     53     p3 14461    10  49        15
# 16.p3  1032     45     p3 16291     6  38        16
# 17.p3  1032     45     p3 17102     6  40        17
# 18.p3  1032     45     p3 10102     4  48        18
# 19.p3  1032     45     p3  7051     6  26        19
# 20.p3  1032     45     p3 16282     4  32        20
</code></pre>

<h3>Update: Using <code>dcast()</code> from ""reshape2""</h3>

<p>Hopefully someone more well-versed in the ""reshape2"" package (or perhaps with ""plyr"") would  be able to come up with a more concise solution than the one below. This solution involves:</p>

<ol>
<li>A dummy ""id"" column.</li>
<li>""Melting"" the dataset.</li>
<li>Using <code>colsplit()</code> (from ""reshape2"") to generate a couple of new columns.</li>
<li>Using <code>dcast()</code> to get to the desired form.</li>
</ol>

<p>Here's what it looks like:</p>

<pre><code>df$id &lt;- 1:nrow(df)
df2 &lt;- melt(df, id.vars=c(""id"", ""p1_id"", ""p1_age""))
df2 &lt;- cbind(df2[-4], 
             colsplit(df2$variable, ""_"", c(""person"", ""var"")))
head(df2)
out &lt;- dcast(df2, id + p1_id + p1_age + person ~ var)
list(head(out), tail(out))
# [[1]]
#   id p1_id p1_age person age    id money
# 1  1  1021     53     p2  50 14372     4
# 2  1  1021     53     p3  30  5151     4
# 3  2  1021     53     p2  33 15022     2
# 4  2  1021     53     p3  29 16181     2
# 5  3  1021     53     p2  56  9072     2
# 6  3  1021     53     p3  22  5182     2
# 
# [[2]]
#    id p1_id p1_age person age    id money
# 35 18  1032     45     p2  41 11052     4
# 36 18  1032     45     p3  48 10102     4
# 37 19  1032     45     p2  45  2202     6
# 38 19  1032     45     p3  26  7051     6
# 39 20  1032     45     p2  28 16151     6
# 40 20  1032     45     p3  32 16282     4
</code></pre>

<p>So, basically, whatever approach you take, it looks like you'll have to do some pre-processing of your <code>data.frame</code> to get it in a format more friendly for such transformations.</p>
"
43856555,"counting words in ""lines"" tokens",2,2,4,"<p>I'm completely new in R, so this question may seem obvious. However, I didn't manage and didn't find solution</p>

<p>How can I count number of words within my tokens while they are lines (reviews, actually)?
So, there is a dataset with reviews(reviewText) connected with ID of products(asin)</p>

<p><code>amazonr_tidy_sent = amazonr_tidy_sent%&gt;%unnest_tokens(word, reviewText, token = ""lines"")
amazonr_tidy_sent = amazonr_tidy_sent %&gt;% anti_join(stop_words)%&gt;%ungroup()</code></p>

<p>I tried to do in the following way</p>

<p><code>wordcounts &lt;- amazonr_tidy_sent %&gt;%
  group_by(word, asin)%&gt;%
  summarize(word = n())</code></p>

<p>but it was not appropriate. I assume, that there is no way to count because line as a token cannot be ""separated""</p>

<p>Thanks a lot</p>
","<p>You can use <code>unnest_tokens()</code> more than once, if it is appropriate to your analysis.</p>

<p>First, you can use <code>unnest_tokens()</code> to get the lines that you want. Notice that I am adding a column to keep track of the id of each line; you could call that whatever you want, but the important thing is to have a column that will note which line you are on.

<br/></p>

<pre class=""lang-r prettyprint-override""><code>library(tidytext)
library(dplyr)
library(janeaustenr)


d &lt;- data_frame(txt = prideprejudice)

d_lines &lt;- d %&gt;%
    unnest_tokens(line, txt, token = ""lines"") %&gt;%
    mutate(id = row_number())

d_lines

#&gt; # A tibble: 10,721 × 2
#&gt;                                                                        line
#&gt;                                                                       &lt;chr&gt;
#&gt;  1                                                      pride and prejudice
#&gt;  2                                                           by jane austen
#&gt;  3                                                                chapter 1
#&gt;  4  it is a truth universally acknowledged, that a single man in possession
#&gt;  5                            of a good fortune, must be in want of a wife.
#&gt;  6   however little known the feelings or views of such a man may be on his
#&gt;  7 first entering a neighbourhood, this truth is so well fixed in the minds
#&gt;  8 of the surrounding families, that he is considered the rightful property
#&gt;  9                                 of some one or other of their daughters.
#&gt; 10 ""my dear mr. bennet,"" said his lady to him one day, ""have you heard that
#&gt; # ... with 10,711 more rows, and 1 more variables: id &lt;int&gt;
</code></pre>

<p>Now you can use <code>unnest_tokens()</code> <em>again</em>, but this time with <code>words</code> so that you will get a row for each word. Notice that you still know which line each word came from.

<br/></p>

<pre class=""lang-r prettyprint-override""><code>d_words &lt;- d_lines %&gt;%
    unnest_tokens(word, line, token = ""words"")

d_words
#&gt; # A tibble: 122,204 × 2
#&gt;       id      word
#&gt;    &lt;int&gt;     &lt;chr&gt;
#&gt;  1     1     pride
#&gt;  2     1       and
#&gt;  3     1 prejudice
#&gt;  4     2        by
#&gt;  5     2      jane
#&gt;  6     2    austen
#&gt;  7     3   chapter
#&gt;  8     3         1
#&gt;  9     4        it
#&gt; 10     4        is
#&gt; # ... with 122,194 more rows
</code></pre>

<p>Now you can do any kind of counting you want, for example, maybe you want to know how many words each line had in it?

<br/></p>

<pre class=""lang-r prettyprint-override""><code>d_words %&gt;%
    count(id)

#&gt; # A tibble: 10,715 × 2
#&gt;       id     n
#&gt;    &lt;int&gt; &lt;int&gt;
#&gt;  1     1     3
#&gt;  2     2     3
#&gt;  3     3     2
#&gt;  4     4    12
#&gt;  5     5    11
#&gt;  6     6    15
#&gt;  7     7    13
#&gt;  8     8    11
#&gt;  9     9     8
#&gt; 10    10    15
#&gt; # ... with 10,705 more rows
</code></pre>
"
33308551,matching dataframes with data.table,1,1,1,"<p>I need to fill a matrix (MA) with information from a long data frame (DF) using another matrix as identifier (ID.MA).</p>

<p>An idea of my three matrices:
MA.ID creates an identifier to look in the big DF the needed variables:</p>

<pre><code>     a      b      c
a    ID.aa  ID.ab  ID.ac
b    ID.ba  ID.bb  ID.bc
c    ID.ca  ID.cb  ID.cc
</code></pre>

<p>The original big data frame has useless information but has also the rows that are useful for me to fill the target MA matrix:</p>

<pre><code>ID     1990 1991 1992
ID.aa  10   11   12
ID.ab  13   14   15
ID.ac  16   17   18
ID.ba  19   20   21
ID.bb  22   23   24
ID.bc  25   26   27
ID.ca  28   29   30
ID.cb  31   32   33
ID.cc  34   35   36
ID.xx  40   40   55
ID.xy  50   51   45
....
</code></pre>

<p>MA should be filled with cross-information. In my example it should look like that for a chosen column of DF (let's say, 1990):</p>

<pre><code>     a    b    c
a    10   13   16
b    19   22   25
c    28   31   34
</code></pre>

<p>I've tried to use match but honestly it didn't work out:</p>

<pre><code>MA$a = DF[match(MA.ID$a, DF$ID),2]
</code></pre>

<p>I was recommended to use the <code>data.table</code> package but I couldn't see how that would help me. </p>

<p>Anyone has any good way to approach this problem?</p>
","<p>Supposing that your input are <em>dataframes</em>, then you could do the following:</p>

<pre><code>library(data.table)
setDT(ma)[, lapply(.SD, function(x) x = unlist(df[match(x,df$ID),""1990""])),
          .SDcols=colnames(ma)]
</code></pre>

<p>which returns:</p>

<pre><code>    a  b  c
1: 10 13 16
2: 19 22 25
3: 28 31 34
</code></pre>

<p><em>Explanation</em>:</p>

<ul>
<li>With <code>setDT(ma)</code> you transform the <em>dataframe</em> into a <em>datatable</em> (which is an enhanced <em>dataframe</em>).</li>
<li>With <code>.SDcols=colnames(ma)</code> you specify on which columns the transformation has to be applied.</li>
<li><code>lapply(.SD, function(x) x = unlist(df[match(x,df$ID),""1990""]))</code> performs the matching operation on each column specified with <code>.SDcols</code>.</li>
</ul>

<hr>

<p>An alternative approach with <code>data.table</code> is first transforming <code>ma</code> to a long <em>data.table</em>:</p>

<pre><code>ma2 &lt;- melt(setDT(ma), measure.vars = c(""a"",""b"",""c""))
setkey(ma2, value)    # set key by which 'ma' has to be indexed
setDT(df, key=""ID"")   # transform to a datatable &amp; set key by which 'df' has to be indexed

# joining the values of the 1990 column of df into
# the right place in the value column of 'ma'
ma2[df, value :=  `1990`]
</code></pre>

<p>which gives:</p>

<pre><code>&gt; ma2
   variable value
1:        a    10
2:        b    13
3:        c    16
4:        a    19
5:        b    22
6:        c    25
7:        a    28
8:        b    31
9:        c    34
</code></pre>

<p>The only drawback of this method is that the numeric values in the 'value' column get stored as character values. You can correct this by extending it as follows:</p>

<pre><code>ma2[df, value :=  `1990`][, value := as.numeric(value)]
</code></pre>

<p>If you want to change it back to wide format you could use the new <code>rowid</code> function from the <a href=""https://github.com/Rdatatable/data.table/wiki/Installation"" rel=""nofollow"">development version of <code>data.table</code> (<em>v1.9.7+</em>)</a>:</p>

<pre><code>ma3 &lt;- dcast(ma2, rowid(variable)~variable, value.var = ""value"")[, variable:=NULL]
</code></pre>

<p>which gives:</p>

<pre><code>&gt; ma3
    a  b  c
1: 10 13 16
2: 19 22 25
3: 28 31 34
</code></pre>

<hr>

<p>Used data:</p>

<pre><code>ma &lt;- structure(list(a = structure(1:3, .Label = c(""ID.aa"", ""ID.ba"", ""ID.ca""), class = ""factor""), 
                     b = structure(1:3, .Label = c(""ID.ab"", ""ID.bb"", ""ID.cb""), class = ""factor""), 
                     c = structure(1:3, .Label = c(""ID.ac"", ""ID.bc"", ""ID.cc""), class = ""factor"")), 
                .Names = c(""a"", ""b"", ""c""), class = ""data.frame"", row.names = c(NA, -3L))

df &lt;- structure(list(ID = structure(1:9, .Label = c(""ID.aa"", ""ID.ab"", ""ID.ac"", ""ID.ba"", ""ID.bb"", ""ID.bc"", ""ID.ca"", ""ID.cb"", ""ID.cc""), class = ""factor""), 
                     `1990` = c(10L, 13L, 16L, 19L, 22L, 25L, 28L, 31L, 34L), 
                     `1991` = c(11L, 14L, 17L, 20L, 23L, 26L, 29L, 32L, 35L), 
                     `1992` = c(12L, 15L, 18L, 21L, 24L, 27L, 30L, 33L, 36L)), 
                .Names = c(""ID"", ""1990"", ""1991"", ""1992""), class = ""data.frame"", row.names = c(NA, -9L))
</code></pre>
"
14561801,Logical Merging of Dataframes,1,1,1,"<p>I have two data.frames, of these, one contains the particular order of a number of experiments done in triplicate (DF1 the design table); the other contains the results of these experiments (in triplicate, DF2 the results table).  The first dataframe has a randomised order of experiments, the results table has a different order.</p>

<p>The first six columns of DF1 contain the factors of the experiment, eg temperature, equivalents of reagents, etc...  The results table, DF2, also has the same six columns as well as further columns containing the results of the experiments, eg yields, conversions of various reagents etc ...</p>

<p>The tables differ by the number of rows.  The results table has three less rows than the design table.</p>

<p>How can I merege these two tables so that I have the results attached to the design such that the experiment parameters in the design table match the appropriate results in the experiment table.</p>

<p>DF1</p>

<pre><code>T1  A1  B1
T2  A1  B1
T1  A2  B1
T2  A2  B1
T1  A1  B2
T2  A1  B2
T1  A2  B2
T2  A2  B2
</code></pre>

<p>but in triplicate.</p>

<p>DF2</p>

<pre><code>T1  A2  B2  1
T1  A2  B1  3
T2  A2  B1  3
T1  A1  B1  1
T2  A1  B2  2
T2  A2  B2  2
T2  A1  B1  2
</code></pre>

<p>again in triplicate, noting that there is one less row.  Note that there are more results columns than the one displayed.</p>

<p>As to the point of all of this work: I'm looking at whether or not I can apply the package RcmdrPlugin.DoE to some real data.</p>

<p>As to what I've tried ... well,   I thought about using sapply, cbind and ifelse with the logic function</p>

<pre><code>sapply(
DF3 &lt;- ifelse( DF1[,1] == DF2[,1] | DF1[,2] == DF2[,2] | DF2[,3] == DF2[,3],
cbind(DF1, DF2[,3]), NA)
)
</code></pre>

<p>I've got a propblem with the NA in this code.  But before I got to the NA I found that I had a argument 'FUN' is missing error.</p>

<p>I think I'm either way off the mark or very close to the answer, but which of the two.  Can anyone point me in the right direction, please?</p>

<p>Edit ... a sample of seven rows of the data that I have where I've changed the headings to A, B, C, and D which are the ones common to both data.frames.</p>

<pre><code>      run.no run.no.std.rp Block.ccd   A     B C     D
C0.17      1         C0.17         0 400 147.5 5 2.675
C0.7       2          C0.7         0 450 120.0 2 4.000
C0.6       3          C0.6         0 350 175.0 2 4.000
C0.3       4          C0.3         0 450 120.0 8 4.000
C0.4       5          C0.4         0 350 120.0 8 4.000
C0.16      6         C0.16         0 350 120.0 2 1.350
C0.15      7         C0.15         0 450 120.0 2 1.350
</code></pre>

<p>The other data.frame has headings A, B, C, and D as well as columns with yield, conversion and other results.  I need the first data.frame to be exactly as shown with the yield etc tagged on to the end.</p>
","<p>Your title mentions ""merge"" but you seem to have not tried the <code>merge</code> function. (Or am I missing something?)</p>

<p>Here are your first two example <code>data.frame</code>s:</p>

<pre><code>DF1 &lt;- structure(list(T1 = c(""T2"", ""T1"", ""T2"", ""T1"", ""T2"", ""T1"", ""T2""
  ), A1 = c(""A1"", ""A2"", ""A2"", ""A1"", ""A1"", ""A2"", ""A2""), B1 = c(""B1"", 
  ""B1"", ""B1"", ""B2"", ""B2"", ""B2"", ""B2"")), .Names = c(""T1"", ""A1"", 
  ""B1""), class = ""data.frame"", row.names = c(NA, -7L))

DF2 &lt;- structure(list(T1 = c(""T1"", ""T2"", ""T1"", ""T2"", ""T2"", ""T2""), A2 = c(""A2"", 
  ""A2"", ""A1"", ""A1"", ""A2"", ""A1""), B2 = c(""B1"", ""B1"", ""B1"", ""B2"", 
  ""B2"", ""B1""), X1 = c(3L, 3L, 1L, 2L, 2L, 2L)), .Names = c(""T1"", 
  ""A2"", ""B2"", ""X1""), class = ""data.frame"", row.names = c(NA, -6L))
</code></pre>

<p>Here's how you use <code>merge</code> from base R. The <code>by.x</code> and <code>by.y</code> arguments should include the names of the columns that you should have in common in both <code>data.frame</code>s. The <code>all</code> argument says to not drop any ""blanks"" but fill them with <code>NA</code> instead.</p>

<pre><code>merge(DF1, DF2, 
      by.x = c(""T1"", ""A1"", ""B1""), 
      by.y = c(""T1"", ""A2"", ""B2""), 
      all = TRUE)
#   T1 A1 B1 X1
# 1 T1 A1 B1  1
# 2 T1 A1 B2 NA
# 3 T1 A2 B1  3
# 4 T1 A2 B2 NA
# 5 T2 A1 B1  2
# 6 T2 A1 B2  2
# 7 T2 A2 B1  3
# 8 T2 A2 B2  2
</code></pre>

<p>Here's the result of <code>merge</code> on the two <code>data.frame</code>s that Arun created. Notice that we don't need to specify which columns to merge on since they have common column names.</p>

<pre><code>merge(df1, df2, all = TRUE)
#   V1 V2 V3 run.no run.no.std.rp Block.ccd   A     B  C     D
# 1 T1 A1 B1      4          C0.3         0 450 120.0  8 4.000
# 2 T1 A1 B2     NA          &lt;NA&gt;        NA  NA    NA NA    NA
# 3 T1 A2 B1      2          C0.7         0 450 120.0  2 4.000
# 4 T1 A2 B2      1         C0.17         0 400 147.5  5 2.675
# 5 T2 A1 B1      7         C0.15         0 450 120.0  2 1.350
# 6 T2 A1 B2      5          C0.4         0 350 120.0  8 4.000
# 7 T2 A2 B1      3          C0.6         0 350 175.0  2 4.000
# 8 T2 A2 B2      6         C0.16         0 350 120.0  2 1.350
</code></pre>
"
19472503,opencpu.js requests to my own install of opencpu?,2,2,2,"<p>Is it possible to use opencpu.js with my own install of OpenCPU on Ubuntu? I've done a quick scan of the code and can't see where i would set the host name.</p>
","<p>Edit: This feature has been improved in opencpu.js 0.3. You can now set the path of the opencpu server using `opencpu.seturl(). The argument must point to a package, for example:</p>

<pre><code>opencpu.seturl(""/ocpu/library/mypackage/R"")
</code></pre>

<p>or if your browser supports CORS you can do:</p>

<pre><code>opencpu.seturl(""//public.opencpu.org/ocpu/library/mypackage/R"")
</code></pre>

<p>or</p>

<pre><code>opencpu.seturl(""//public.opencpu.org/ocpu/github/yourname/yourpackage/R"")
</code></pre>

<p>After setting this, the client will check if this server is online and print some debugging stuff to the javascript console. If all is OK, you can use the library just as if it were part of the app. Have a look at <a href=""http://www.stat.ucla.edu/~jeroen/appdemo/"" rel=""nofollow"">http://www.stat.ucla.edu/~jeroen/appdemo/</a> to see this in action. </p>

<p>That said, the <code>opencpu.js</code> library is designed to be included in apps. An OpenCPU app is an R package which includes some web page(s) that call the R functions in the package using the OpenCPU API. </p>

<p>Because apps are R packages which are installed on the OpenCPU server, the <code>opencpu.js</code> library assumes the server is running on the <em>current</em> host, and uses a relative path to call the server. Moreover, because <code>opencpu.js</code> is included with a particular R package, the functions in <code>opencpu.js</code> have no argument as to specify a package either: it assumes you are calling the R function the <em>current</em> package.</p>

<p>This might sound a little confusing at first, but relative paths are important to keep the application portable. For example, the same app should work regardless of whether it is hosted as:</p>

<pre><code>https://public.opencpu.org/ocpu/gitstats/www
https://public.opencpu.org/ocpu/github/SChamberlain/gitstats/www/
https://public.opencpu.org/ocpu/user/jeroen/library/gitstats/www/
</code></pre>

<p>The big advantage of this design is that your R package will be a self-contained, standalone application, which can be deployed anywhere simply by installing the package. This prevents a lot of trouble with cross-domain requests and software versioning. Because your web pages and R functions are part of one and the same package, you can make sure the js and R code contain compatible versions. If you would be hosting the js on a separate server, than things might break when a new version of the R package is installed.</p>

<p>Perhaps the best way to get started is by having a look at one of the <a href=""https://public.opencpu.org/apps.html"" rel=""nofollow"">public apps</a>. The source code for these apps is available on the opencpu <a href=""https://github.com/opencpu/nabel"" rel=""nofollow"">github repo</a>, and each app is built on the <code>opencpu.js</code> library.</p>
"
8898469,Is it possible to use R package data in testthat tests or run_examples()?,2,2,2,"<p>I'm working on developing an R package, using devtools, testthat, and roxygen2.  I have a couple of data sets in the data folder (foo.txt and bar.csv).</p>

<p>My file structure looks like this:</p>

<pre><code>/ mypackage
    / data
        * foo.txt, bar.csv
    / inst
        / tests
            * run-all.R, test_1.R
    / man
    / R
</code></pre>

<p>I'm pretty sure 'foo' and 'bar' are documented correctly:</p>

<pre><code>    #' Foo data
    #'
    #' Sample foo data
    #'
    #' @name foo
    #' @docType data
    NULL
    #' Bar data
    #'
    #' Sample bar data
    #'
    #' @name bar
    #' @docType data
    NULL
</code></pre>

<p>I would like to use the data in 'foo' and 'bar' in my documentation examples and unit tests.  </p>

<p>For example, I would like to use these data sets in my testthat tests by calling:</p>

<pre><code>    data(foo)
    data(bar)
    expect_that(foo$col[1], equals(bar$col[1]))
</code></pre>

<p>And, I would like the examples in the documentation to look like this:</p>

<pre><code>    #' @examples
    #' data(foo)
    #' functionThatUsesFoo(foo)
</code></pre>

<p>If I try to call data(foo) while developing the package, I get the error ""data set 'foo' not found"".  However, if I build the package, install it, and load it - then I can make the tests and examples work.  </p>

<p>My current work-arounds are to not run the example:</p>

<pre><code>    #' @examples
    #' \dontrun{data(foo)}
    #' \dontrun{functionThatUsesFoo(foo)}
</code></pre>

<p>And in the tests, pre-load the data using a path specific to my local computer:</p>

<pre><code>    foo &lt;- read.delim(pathToFoo, sep=""\t"", fill = TRUE, comment.char=""#"")
    bar &lt;- read.delim(pathToBar, sep="";"", fill = TRUE, comment.char=""#""
    expect_that(foo$col[1], equals(bar$col[1]))
</code></pre>

<p>This does not seem ideal - especially since I'm collaborating with others - requiring all the collaborators to have the same full paths to 'foo' and 'bar'.  Plus, the examples in the documentation look like they can't be run, even though once the package is installed, they can.</p>

<p>Any suggestions?  Thanks much.</p>
","<h1>Importing non-RData files within examples/tests</h1>

<p>I found a solution to this problem by peering at <a href=""http://cran.r-project.org/web/packages/RJSONIO/index.html"" rel=""noreferrer"">the JSONIO package</a>, which obviously needed to provide some examples of reading files other than those of the .RData variety.</p>

<p>I got this to work in function-level examples, and satisfy both <code>R CMD check mypackage</code> as well as <code>testthat::test_package()</code>.</p>

<p>(1) Re-organize your package structure so that example data directory is within <code>inst</code>. At some point <code>R CMD check mypackage</code> told me to move non-RData data files to <code>inst/extdata</code>, so in this new structure, that is also renamed.</p>

<pre><code>/ mypackage
    / inst
        / tests
            * run-all.R, test_1.R
        / extdata
            * foo.txt, bar.csv
    / man
    / R
    / tests
        * run-testthat-mypackage.R
</code></pre>

<p>(2) (Optional) Add a top-level <code>tests</code> directory so that your new testthat tests are now also run during <code>R CMD check mypackage</code>.</p>

<p>The <code>run-testthat-mypackage.R</code> script should have at minimum the following two lines:</p>

<pre><code>library(""testthat"")
test_package(""mypackage"")
</code></pre>

<p>Note that this is the part that allows testthat to be called during <code>R CMD check mypackage</code>, and not necessary otherwise. You should add <code>testthat</code> as a ""Suggests:"" dependency in your DESCRIPTION file as well.</p>

<p>(3) Finally, the secret-sauce for specifying your within-package path:</p>

<pre><code>barfile &lt;- system.file(""extdata"", ""bar.csv"", package=""mypackage"")
bar &lt;- read.csv(barfile)
# remainder of example/test code here...
</code></pre>

<p>If you look at the output of the <code>system.file()</code> command, it is returning the full system path to your package within the R framework. On Mac OS X this looks something like:</p>

<pre><code>""/Library/Frameworks/R.framework/Versions/2.15/Resources/library/mypackage/extdata/bar.csv""
</code></pre>

<p>The reason this seems okay to me is that you don't hard code any path features other than those within your package, so this approach should be robust relative to other R installations on other systems.</p>

<h1><code>data()</code> approach</h1>

<p>As for the <code>data()</code> semantics, as far as I can tell this is specific to R binary (<code>.RData</code>) files in the top-level <code>data</code> directory. So you can circumvent my example above by pre-importing the data files and saving them with the <code>save()</code> command into your data-directory. However, this assumes you only need to show an example in which the data is already loaded into R, as opposed to also reproducibly demonstrating the upstream process of importing the files.</p>
"
39222302,find locations within certain lat/lon distance in r,1,3,3,"<p>I have a gridded dataset, with data available at the following locations:</p>

<pre><code>lon &lt;- seq(-179.75,179.75, by = 0.5)
lat &lt;- seq(-89.75,89.75, by = 0.5)
</code></pre>

<p>I would like to find all of the data points that are within 500 km of the location:</p>

<pre><code>mylat &lt;- 47.9625
mylon &lt;- -87.0431
</code></pre>

<p>I aim to use the geosphere package in R, but the method I've currently written does not seem very efficient:</p>

<pre><code>require(geosphere)
dd2 &lt;- array(dim = c(length(lon),length(lat)))
for(i in 1:length(lon)){
  for(ii in 1:length(lat)){
    clon &lt;- lon[i]
    clat &lt;- lat[ii]
    dd &lt;- as.numeric(distm(c(mylon, mylat), c(clon, clat), fun = distHaversine))
    dd2[i,ii] &lt;- dd &lt;= 500000
  }
}
</code></pre>

<p>Here, I loop through each grid in the data and find if the distance is less than 500 km. I then store a variable with either TRUE or FALSE, which I can then use to average the data (other variable). From this method, I want a matrix with TRUE or FALSE for the locations within 500 km from the lat and lon shown. Is there a more efficient method for doing this? </p>
","<p><strong>Timings:</strong></p>

<p>Comparing @nicola's and my version gives:</p>

<pre><code>Unit: milliseconds

               min         lq      mean     median         uq       max neval
nicola1 184.217002 219.924647 297.60867 299.181854 322.635960 898.52393   100
floo01   61.341560  72.063197  97.20617  80.247810  93.292233 286.99343   100
nicola2   3.992343   4.485847   5.44909   4.870101   5.371644  27.25858   100
</code></pre>

<p>My original solution: (IMHO nicola's second version is much cleaner and faster.)</p>

<p>You can do the following (explanation below)</p>

<pre><code>require(geosphere)
my_coord &lt;- c(mylon, mylat)
dd2 &lt;- matrix(FALSE, nrow=length(lon), ncol=length(lat))
outer_loop_state &lt;- 0
for(i in 1:length(lon)){
    coods &lt;- cbind(lon[i], lat)
    dd &lt;- as.numeric(distHaversine(my_coord, coods))
    dd2[i, ] &lt;- dd &lt;= 500000
    if(any(dd2[i, ])){
      outer_loop_state &lt;- 1
    } else {
      if(outer_loop_state == 1){
        break
      }
    }
  }
</code></pre>

<p>Explanation:</p>

<p>For the loop i apply the following logic:
<a href=""https://i.stack.imgur.com/moFjx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/moFjx.jpg"" alt=""enter image description here""></a></p>

<p><code>outer_loop_state</code> is initialized with 0. If a row with at least one raster-point inside the circle is found <code>outer_loop_state</code> is set to 1. Once there are no more points within the circle for a given row <code>i</code> break.</p>

<p>The <code>distm</code> call in @nicola version basically does the same without this trick. So it calculates all rows.</p>

<p>Code for timings:</p>

<pre><code>microbenchmark::microbenchmark(
  {allCoords&lt;-cbind(lon,rep(lat,each=length(lon)))
  res&lt;-matrix(distm(cbind(mylon,mylat),allCoords,fun=distHaversine)&lt;=500000,nrow=length(lon))},
  {my_coord &lt;- c(mylon, mylat)
  dd2 &lt;- matrix(FALSE, nrow=length(lon), ncol=length(lat))
  outer_loop_state &lt;- 0
  for(i in 1:length(lon)){
    coods &lt;- cbind(lon[i], lat)
    dd &lt;- as.numeric(distHaversine(my_coord, coods))
    dd2[i, ] &lt;- dd &lt;= 500000
    if(any(dd2[i, ])){
      outer_loop_state &lt;- 1
    } else {
      if(outer_loop_state == 1){
        break
      }
    }
  }},
  {#intitialize the return
    res&lt;-matrix(FALSE,nrow=length(lon),ncol=length(lat))
    #we find the possible value of longitude that can be closer than 500000
    #How? We calculate the distance between us and points with our same lat 
    longood&lt;-which(distm(c(mylon,mylat),cbind(lon,mylat))&lt;500000)
    #Same for latitude
    latgood&lt;-which(distm(c(mylon,mylat),cbind(mylon,lat))&lt;500000)
    #we build the matrix with only those values to exploit the vectorized
    #nature of distm
    allCoords&lt;-cbind(lon[longood],rep(lat[latgood],each=length(longood)))
    res[longood,latgood]&lt;-distm(c(mylon,mylat),allCoords)&lt;=500000}
)
</code></pre>
"
37775548,"Insertion sort in r with error ""missing value where TRUE/FALSE needed""",2,1,3,"<p>I want to build insertion sort algorithm in r:</p>

<ol>
<li>Given a vector x, let the initial unsorted vector u be equal to x,<br>
and the initial sorted vector s be a vector of length 0.</li>
<li>Remove first element of u and insert it into s so that s is still sorted.</li>
<li>If u is not empty then go back to step 2.</li>
</ol>

<p>Here is my code:</p>

<pre><code>x &lt;- round(runif(1000,1,100)
u &lt;- x
s &lt;- vector(mode=""numeric"", length=0


    for(i in 1:length(u)){
        number &lt;-u[i]
        u &lt;- u[-i]
        for(j in 1:length(s)){
            if(length(s) == 0){
                s &lt;- c(s,number)
                break
            }else{
                if(s[j]&gt;=number){
                    s &lt;- append(s,number,j-1)
                    break
                }
            }
            if(s[length(s)]&lt;number){
                s &lt;- c(s,number)
            }
        }
    }
</code></pre>

<p>First of all, when length of u = 500 it throws me an error:</p>

<pre><code>Error in if (s[j] &gt;= number) { : missing value where TRUE/FALSE needed
</code></pre>

<p>next, it sorted incorrectly(for example it could be more ones than in original vector u, or for example less twos then in original vector u)</p>

<p>So I have two questions:</p>

<p>1)How can we fix that in THIS code?</p>

<p>2)Can you suggest another code, which is more efficient than my?</p>

<p>P.S Of course the code should be without sort and order command.
Many thanks</p>
","<p>Your first question is an easy fix: when you extract a number out of your vector u, what you are really doing, is just drawing a random number from the sample without replacement. So just always take the first value.</p>

<pre><code># Change the current to this:
number &lt;-u[1]
u &lt;- u[-1]
</code></pre>

<hr>

<p>For your second question: what a fun exercise! My go at it was to just implement the <a href=""https://en.wikipedia.org/wiki/Selection_sort#Analysis"" rel=""nofollow"">selection sort pseudo-code from Wikipedia</a>, (which you could just do) but with blindfolds: I cannot look into ""the bag"" of <code>x</code> but only see my just-drawn item - this I found to be more like what you are asking for.</p>

<p>How do I go about solving this? Simple: I draw a value from <code>x</code>. Call it my <code>control</code> variable. Then for every draw I put the new value into a pile of <code>small</code> if less than (or equal) to control. Otherwise I put it into the <code>large</code> pile. When I have distributed all values, I do this algorithm again for each pile. I continue until my piles are all of size 1. The implementation of this below.</p>

<pre><code>mysort &lt;- function(x){
    if(length(x) &lt;= 1){
        return(x)       ## If nothing to sort, return itself
    }

    control &lt;- x[1]
    small &lt;- c()
    big &lt;- c()

    for(test in x[-1]){
        if(test &lt;= control){
            small &lt;- c(small,test)  ## Less than control in small
        }
        if(test &gt; control){
            big &lt;- c(big,test)      ## Bigger than control in big
        }
    }

    ## Sort the new piles with the same procedure recursively.
    small &lt;- mysort(small)          
    big &lt;- mysort(big)

    ## Return the improved order
    c(small,control,big)
}

mysort(c(2,1,1,2,2,2,3,3,-3,-Inf,Inf,0,pi))
# [1]      -Inf -3.000000  0.000000  1.000000  1.000000  2.000000  2.000000  2.000000
# [9]  2.000000  3.000000  3.000000  3.141593       Inf
</code></pre>

<hr>

<p>We can compare speeds with the <code>microbenchmark</code> package, if we wrap your implementation (without the superfluous <code>while</code>-loop) in a function <code>yoursort</code>. </p>

<pre><code>library(microbenchmark)
a &lt;- rnorm(1e3)
microbenchmark(b &lt;- mysort(a),times = 10)
# Unit: milliseconds
#           expr      min      lq     mean   median       uq      max neval
# b &lt;- mysort(a) 37.76747 39.2302 41.96171 40.99288 43.07412 47.85377    10

microbenchmark(c &lt;- yoursort(a),times = 10)
# Unit: milliseconds
#             expr      min       lq     mean   median       uq      max neval
# c &lt;- yoursort(a) 786.4544 808.2312 861.8072 840.7868 879.4946 1059.913    10
microbenchmark(sort(a),times = 10)
# Unit: microseconds
#    expr     min      lq     mean   median      uq     max neval
# sort(a) 192.763 194.384 242.7633 201.1335 263.497 390.386    10
</code></pre>

<p>Neither are any match to the already implemented <code>sort</code> function.</p>

<p>And of course, do they actually do the correct sorting?</p>

<pre><code>any(b != sort(a)) ## Are there any elements that do not match?
# [1] FALSE
any(c != sort(a))
# [1] FALSE
</code></pre>
"
16666183,Find values in a given interval without a vector scan,1,1,3,"<p>With a the R package <code>data.table</code> is it possible to find the values that are in a given interval without a full vector scan of the data. For example</p>

<pre><code>&gt;DT&lt;-data.table(x=c(1,1,2,3,5,8,13,21,34,55,89))
&gt;my.data.table.function(DT,min=3,max=10)
   x
1: 3
2: 5
3: 8
</code></pre>

<p>Where <code>DT</code> can be a very big table.</p>

<p>Bonus question: 
is it possible to do the same thing for a set of non-overlapping intervals such as</p>

<pre><code>&gt;I&lt;-data.table(i=c(1,2),min=c(3,20),max=c(10,40))
&gt;I
   i min max
1: 1   3  10
2: 2  20  40
&gt; my.data.table.function2(DT,I)
   i  x
1: 1  3
2: 1  5
3: 1  8
4: 2 21
5: 2 34
</code></pre>

<p>Where both <code>I</code> and <code>DT</code> can be very big. 
Thanks a lot</p>
","<p>First of all, <code>vecseq</code> isn't exported as a visible function from <code>data.table</code>, so its syntax and/or behavior here could change without warning in future updates to the package.  Also, this is <em>untested</em> besides the simple <code>identical</code> check at the end.</p>

<p>That out of the way, we need a bigger example to exhibit difference from vector scan approach:</p>

<pre><code>require(data.table)

n &lt;- 1e5L
f &lt;- 10L
ni &lt;- n / f

set.seed(54321)
DT &lt;- data.table(x = 1:n + sample(-f:f, n, replace = TRUE))
IT &lt;- data.table(i = 1:ni, 
                 min = seq(from = 1L, to = n, by = f) + sample(0:4, ni, replace = TRUE),
                 max = seq(from = 1L, to = n, by = f) + sample(5:9, ni, replace = TRUE))
</code></pre>

<p><code>DT</code>, the Data Table is a not-<em>too</em>-random subset of <code>1:n</code>.  <code>IT</code>, the Interval Table is <code>ni = n / 10</code> non-overlapping intervals in <code>1:n</code>.  Doing the repeated vector scan on all <code>ni</code> intervals takes a while:</p>

<pre><code>system.time({
  ans.vecscan &lt;- IT[, DT[x &gt;= min &amp; x &lt;= max], by = i]
})
 ##  user  system elapsed 
 ## 84.15    4.48   88.78
</code></pre>

<p>One can do two rolling joins on the interval endpoints (see the <code>roll</code> argument in <code>?data.table</code>) to get everything in one swoop:</p>

<pre><code>system.time({
  # Save time if DT is already keyed correctly
  if(!identical(key(DT), ""x"")) setkey(DT, x)

  DT[, row := .I]

  setkey(IT, min)

  target.low &lt;- IT[DT, roll = Inf, nomatch = 0][, list(min = row[1]), keyby = i]

  # Non-overlapping intervals =&gt; (sorted by min =&gt; sorted by max)
  setattr(IT, ""sorted"", ""max"")

  target.high &lt;- IT[DT, roll = -Inf, nomatch = 0][, list(max = last(row)), keyby = i]

  target &lt;- target.low[target.high, nomatch = 0]
  target[, len := max - min + 1L]


  rm(target.low, target.high)

  ans.roll &lt;- DT[data.table:::vecseq(target$min, target$len, NULL)][, i := unlist(mapply(rep, x = target$i, times = target$len, SIMPLIFY=FALSE))]
  ans.roll[, row := NULL]
  setcolorder(ans.roll, c(""i"", ""x""))
})
 ## user  system elapsed 
 ## 0.12    0.00    0.12
</code></pre>

<p>Ensuring the same row order verifies the result:</p>

<pre><code>setkey(ans.vecscan, i, x)
setkey(ans.roll, i, x)
identical(ans.vecscan, ans.roll)
## [1] TRUE
</code></pre>
"
11926410,Converting Twitter Stream JSON to R dataframe,2,2,4,"<p>I have a text file in JSON format that I downloaded using code from this <a href=""https://stackoverflow.com/questions/8243681/connect-to-the-twitter-streaming-api-using-r"">link</a>.  It looks something like this:</p>

<pre><code>""{\""contributors\"":null,\""coordinates\"":null,\""in_reply_to_user_id\"":null,\""truncated\"":false,\""text\"":\""'ello miss...?\"",\""entities\"":{\""hashtags\"":[],\""urls\"":[],\""user_mentions\"":[]},\""place\"":null,\""id_str\"":\""234702954853199872\"",\""favorited\"":false,\""geo\"":null,\""source\"":\""\u003Ca href=\\""http:\/\/www.tweetdeck.com\\"" rel=\\""nofollow\\""\u003ETweetDeck\u003C\/a\u003E\"",\""retweet_count\"":0,\""in_reply_to_status_id_str\"":null,\""in_reply_to_screen_name\"":null,\""created_at\"":\""Sun Aug 12 17:28:39 +0000 2012\"",\""in_reply_to_user_id_str\"":null,\""user\"":{\""show_all_inline_media\"":false,\""lang\"":\""en\"",\""friends_count\"":145,\""profile_sidebar_border_color\"":\""181A1E\"",\""location\"":\""Chile\"",\""profile_background_image_url_https\"":\""https:\/\/si0.twimg.com\/images\/themes\/theme9\/bg.gif\"",\""id_str\"":\""76862348\"",\""listed_count\"":5,\""profile_use_background_image\"":true,\""profile_image_url_https\"":\""https:\/\/si0.twimg.com\/profile_images\/1547896482\/egotuiter_normal.png\"",\""description\"":\""Geek. Amante de la m\u00fasica. Ateo. Nortino. Estudio Derecho en la U. de Chile. Nadie lee mi blog. Cobreloino. Afortunadamente ya no uso lentes :D\"",\""follow_request_sent\"":null,\""following\"":null,\""profile_text_color\"":\""666666\"",\""default_profile\"":false,\""profile_background_image_url\"":\""http:\/\/a0.twimg.com\/images\/themes\/theme9\/bg.gif\"",\""followers_count\"":181,\""is_translator\"":false,\""time_zone\"":\""Buenos Aires\"",\""profile_link_color\"":\""2FC2EF\"",\""protected\"":false,\""created_at\"":\""Thu Sep 24 05:11:20 +0000 2009\"",\""profile_background_color\"":\""1A1B1F\"",\""name\"":\""Camilo Rojas\"",\""default_profile_image\"":false,\""contributors_enabled\"":false,\""statuses_count\"":36450,\""geo_enabled\"":false,\""notifications\"":null,\""profile_background_tile\"":false,\""url\"":\""http:\/\/unnombreingenioso.blogspot.com\"",\""profile_image_url\"":\""http:\/\/a0.twimg.com\/profile_images\/1547896482\/egotuiter_normal.png\"",\""screen_name\"":\""rojas_altazor\"",\""id\"":76862348,\""verified\"":false,\""utc_offset\"":-10800,\""favourites_count\"":42,\""profile_sidebar_fill_color\"":\""252429\""},\""retweeted\"":false,\""in_reply_to_status_id\"":null,\""id\"":234702954853199872}
""
""{\""contributors\"":null,\""coordinates\"":null,\""in_reply_to_user_id\"":null,\""truncated\"":false,\""text\"":\""VALENTINA CORTANTE!\"",\""entities\"":{\""hashtags\"":[],\""urls\"":[],\""user_mentions\"":[]},\""place\"":null,\""id_str\"":\""234702954861580288\"",\""favorited\"":false,\""geo\"":null,\""source\"":\""web\"",\""retweet_count\"":0,\""in_reply_to_status_id_str\"":null,\""in_reply_to_screen_name\"":null,\""created_at\"":\""Sun Aug 12 17:28:39 +0000 2012\"",\""in_reply_to_user_id_str\"":null,\""user\"":{\""show_all_inline_media\"":false,\""lang\"":\""es\"",\""friends_count\"":251,\""profile_sidebar_border_color\"":\""CC3366\"",\""location\"":\""\"",\""profile_background_image_url_https\"":\""https:\/\/si0.twimg.com\/images\/themes\/theme11\/bg.gif\"",\""id_str\"":\""292675295\"",\""listed_count\"":0,\""profile_use_background_image\"":true,\""profile_image_url_https\"":\""https:\/\/si0.twimg.com\/profile_images\/2494400748\/7zd2z67iclw88rc0q8sr_normal.jpeg\"",\""description\"":\""Soy hermosaaaa y valen me ama\u2665\"",\""follow_request_sent\"":null,\""following\"":null,\""profile_text_color\"":\""362720\"",\""default_profile\"":false,\""profile_background_image_url\"":\""http:\/\/a0.twimg.com\/images\/themes\/theme11\/bg.gif\"",\""followers_count\"":399,\""is_translator\"":false,\""time_zone\"":\""Santiago\"",\""profile_link_color\"":\""B40B43\"",\""protected\"":false,\""created_at\"":\""Wed May 04 01:35:06 +0000 2011\"",\""profile_background_color\"":\""FF6699\"",\""name\"":\""Valen me amodora\u2020\"",\""default_profile_image\"":false,\""contributors_enabled\"":false,\""statuses_count\"":2643,\""geo_enabled\"":false,\""notifications\"":null,\""profile_background_tile\"":true,\""url\"":null,\""profile_image_url\"":\""http:\/\/a0.twimg.com\/profile_images\/2494400748\/7zd2z67iclw88rc0q8sr_normal.jpeg\"",\""screen_name\"":\""CamiCelie\"",\""id\"":292675295,\""verified\"":false,\""utc_offset\"":-14400,\""favourites_count\"":2,\""profile_sidebar_fill_color\"":\""E5507E\""},\""retweeted\"":false,\""in_reply_to_status_id\"":null,\""id\"":234702954861580288}
""
</code></pre>

<p>I'm curious how to extract individual pieces to an R data frame.  For instance this would ideally go to a data frame like this:</p>

<pre><code>screen_name        text
rojas_altazor       'ello miss...?
CamiCelie           VALENTINA CORTANTE!
...                  ...
</code></pre>

<p>This is my first foray into using JSON files so any suggestions would be appreciated.</p>
","<pre><code>twit&lt;-c(""{\""contributors\"":null,\""coordinates\"":null,\""in_reply_to_user_id\"":null,\""truncated\"":false,\""text\"":\""'ello miss...?\"",\""entities\"":{\""hashtags\"":[],\""urls\"":[],\""user_mentions\"":[]},\""place\"":null,\""id_str\"":\""234702954853199872\"",\""favorited\"":false,\""geo\"":null,\""source\"":\""\\u003Ca href=\\\""http:\\/\\/www.tweetdeck.com\\\"" rel=\\\""nofollow\\\""\\u003ETweetDeck\\u003C\\/a\\u003E\"",\""retweet_count\"":0,\""in_reply_to_status_id_str\"":null,\""in_reply_to_screen_name\"":null,\""created_at\"":\""Sun Aug 12 17:28:39 +0000 2012\"",\""in_reply_to_user_id_str\"":null,\""user\"":{\""show_all_inline_media\"":false,\""lang\"":\""en\"",\""friends_count\"":145,\""profile_sidebar_border_color\"":\""181A1E\"",\""location\"":\""Chile\"",\""profile_background_image_url_https\"":\""https:\\/\\/si0.twimg.com\\/images\\/themes\\/theme9\\/bg.gif\"",\""id_str\"":\""76862348\"",\""listed_count\"":5,\""profile_use_background_image\"":true,\""profile_image_url_https\"":\""https:\\/\\/si0.twimg.com\\/profile_images\\/1547896482\\/egotuiter_normal.png\"",\""description\"":\""Geek. Amante de la m\\u00fasica. Ateo. Nortino. Estudio Derecho en la U. de Chile. Nadie lee mi blog. Cobreloino. Afortunadamente ya no uso lentes :D\"",\""follow_request_sent\"":null,\""following\"":null,\""profile_text_color\"":\""666666\"",\""default_profile\"":false,\""profile_background_image_url\"":\""http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme9\\/bg.gif\"",\""followers_count\"":181,\""is_translator\"":false,\""time_zone\"":\""Buenos Aires\"",\""profile_link_color\"":\""2FC2EF\"",\""protected\"":false,\""created_at\"":\""Thu Sep 24 05:11:20 +0000 2009\"",\""profile_background_color\"":\""1A1B1F\"",\""name\"":\""Camilo Rojas\"",\""default_profile_image\"":false,\""contributors_enabled\"":false,\""statuses_count\"":36450,\""geo_enabled\"":false,\""notifications\"":null,\""profile_background_tile\"":false,\""url\"":\""http:\\/\\/unnombreingenioso.blogspot.com\"",\""profile_image_url\"":\""http:\\/\\/a0.twimg.com\\/profile_images\\/1547896482\\/egotuiter_normal.png\"",\""screen_name\"":\""rojas_altazor\"",\""id\"":76862348,\""verified\"":false,\""utc_offset\"":-10800,\""favourites_count\"":42,\""profile_sidebar_fill_color\"":\""252429\""},\""retweeted\"":false,\""in_reply_to_status_id\"":null,\""id\"":234702954853199872}"", 
"""", ""{\""contributors\"":null,\""coordinates\"":null,\""in_reply_to_user_id\"":null,\""truncated\"":false,\""text\"":\""VALENTINA CORTANTE!\"",\""entities\"":{\""hashtags\"":[],\""urls\"":[],\""user_mentions\"":[]},\""place\"":null,\""id_str\"":\""234702954861580288\"",\""favorited\"":false,\""geo\"":null,\""source\"":\""web\"",\""retweet_count\"":0,\""in_reply_to_status_id_str\"":null,\""in_reply_to_screen_name\"":null,\""created_at\"":\""Sun Aug 12 17:28:39 +0000 2012\"",\""in_reply_to_user_id_str\"":null,\""user\"":{\""show_all_inline_media\"":false,\""lang\"":\""es\"",\""friends_count\"":251,\""profile_sidebar_border_color\"":\""CC3366\"",\""location\"":\""\"",\""profile_background_image_url_https\"":\""https:\\/\\/si0.twimg.com\\/images\\/themes\\/theme11\\/bg.gif\"",\""id_str\"":\""292675295\"",\""listed_count\"":0,\""profile_use_background_image\"":true,\""profile_image_url_https\"":\""https:\\/\\/si0.twimg.com\\/profile_images\\/2494400748\\/7zd2z67iclw88rc0q8sr_normal.jpeg\"",\""description\"":\""Soy hermosaaaa y valen me ama\\u2665\"",\""follow_request_sent\"":null,\""following\"":null,\""profile_text_color\"":\""362720\"",\""default_profile\"":false,\""profile_background_image_url\"":\""http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme11\\/bg.gif\"",\""followers_count\"":399,\""is_translator\"":false,\""time_zone\"":\""Santiago\"",\""profile_link_color\"":\""B40B43\"",\""protected\"":false,\""created_at\"":\""Wed May 04 01:35:06 +0000 2011\"",\""profile_background_color\"":\""FF6699\"",\""name\"":\""Valen me amodora\\u2020\"",\""default_profile_image\"":false,\""contributors_enabled\"":false,\""statuses_count\"":2643,\""geo_enabled\"":false,\""notifications\"":null,\""profile_background_tile\"":true,\""url\"":null,\""profile_image_url\"":\""http:\\/\\/a0.twimg.com\\/profile_images\\/2494400748\\/7zd2z67iclw88rc0q8sr_normal.jpeg\"",\""screen_name\"":\""CamiCelie\"",\""id\"":292675295,\""verified\"":false,\""utc_offset\"":-14400,\""favourites_count\"":2,\""profile_sidebar_fill_color\"":\""E5507E\""},\""retweeted\"":false,\""in_reply_to_status_id\"":null,\""id\"":234702954861580288}"", 
"""")


library(RJSONIO)

twit1&lt;-fromJSON(twit[1])

&gt; twit1$user$screen_name
[1] ""rojas_altazor""

&gt; twit1$text
[1] ""'ello miss...?""

&gt; twit2&lt;-fromJSON(twit[3])
&gt; twit2$user$screen_name
[1] ""CamiCelie""
&gt; twit2$text
[1] ""VALENTINA CORTANTE!""
</code></pre>
"
35650785,Using an R Markdown Document as a source for functions,2,2,2,"<p>I'm looking into R Markdown for documenting functions I regularly use. I will put them into an R Markdown file to document them and then be able to read my thinking behind the function if i come back to it months later</p>

<p>My question is, if i start a new R project, Is it possible to source the r markdown file and use the library of functions i have created just by calling them similarly to if i was sourcing a regular R file. I dont really wish to maintain two sets of function files</p>

<p>I appreciate this may be a beginners question but any help pointing to tutorials and the like would be greatly appreciated</p>

<p>Thanks</p>
","<p>As was mentioned in the comments, you should probably create a package for this purpose. But if you insist on putting function definitions in scripts and document them using RMarkdown files, using <code>read_chunk()</code> from the <code>knitr</code> package might be the way to go.</p>

<p>Note that this approach differs slightly from what you requested. You wanted to have the function definition in the markdown file together with the documentation. And then you wanted to somehow source that file into your R script in order to use the function. I did not find a way to do this (even though it might be possible).</p>

<p>The alternative that I propose puts the function definition in its own R script, say <code>fun.R</code>. The Rmarkdown file then reads the function definition from <code>fun.R</code> and adds documentation. If you want to use the function in some other script, you can simply source <code>fun.R</code> (and not the markdown file). This still means that you have to maintain the code for the function definition only once.</p>

<p>So let me show this with an example. This is <code>fun.R</code>:</p>

<pre><code>## ---- fun
fun &lt;- function(x) x^2
</code></pre>

<p>The first line is an identifier that will be used later. The markdown file is as follows:</p>

<pre><code>---
title: ""Documentation of fun()""
output: html_document
---

This documents the function `fun()` defined in `fun.R`.
```{r,cache = FALSE}
knitr::read_chunk(""fun.R"")
```

This is the function definition
```{r fun}
```

This is an example of how  to use `fun()`:
```{r use_fun}
fun(3)
```
</code></pre>

<p>The first chunk reads in <code>fun.R</code> using <code>knitr::read_chunk</code>. Later on, you can define an empty chunk that has the identifier that was used in <code>fun.R</code> as its name. This will act as if the contents of <code>fun.R</code> were written directly in this file. As you see, you can also use <code>fun()</code> in later chunks. This  is a screenshot of the resulting html file:</p>

<p><a href=""https://i.stack.imgur.com/0fIAK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0fIAK.png"" alt=""enter image description here""></a></p>

<p>In a script where you want to use <code>fun()</code> you simply add <code>source(""fun.R"")</code> to source the function definition.</p>

<p>You could also have several functions in a single R file and still document them separately. Simply put an identifier starting with <code>## ----</code> before each function definition and then create empty chunks referring to each one of the identifiers.</p>

<p>This is admittedly somewhat more complicated than what you asked for, because it involves two files instead of just one. But at least there is no redundancy</p>
"
38352786,How to check that a user-defined function works in r?,2,2,2,"<p>THis is probably a very silly question, but how can I check if a function written by myself will work or not?</p>

<p>I'm writing a not very simple function involving many other functions and loops and was wondering if there are any ways to check for errors/bugs, or simply just check if the function will work. Do I just create a simple fake data frame and test on it?</p>

<p>As suggested by other users in the comment, I have added the part of the function that I have written. So basically I have a data frame with good and bad data, and bad data are marked with flags. I want to write a function that allows me to produce plots as usual (with the flag points) when user sets flag.option to 1, and remove the flag points from the plot when user sets flag.option to 0.</p>

<pre><code>AIR.plot &lt;- function(mydata, flag.option) {
if (flag.option == 1) {
par(mfrow(2,1))
conc &lt;- tapply(mydata$CO2, format(mydata$date, ""%Y-%m-%d %T""), mean)
dates &lt;- seq(mydata$date[1], mydata$date[nrow(mydata(mydata))], length =     nrow(conc))
plot(dates, conc,
     type = ""p"",
     col = ""blue"",
     xlab = ""day"",
     ylab = ""CO2""), error = function(e) plot.new(type = ""n"")
barplot(mydata$lines, horiz = TRUE, col = c(""red"", ""blue"")) # this is just a   small bar plot on the bottom that specifies which sample-taking line (red or  blue) is providing the samples
} else if (flag.option == 0) {
# I haven't figured out how to write this part yet but essentially I want to  remove all
# of the rows with flags on
}
}
</code></pre>

<p>Thanks in advance, I'm not an experienced R user yet so please help me.</p>
","<p>Before we (meaning, at my workplace) release any code to our production environment we run through a series of testing procedures to make sure our code behaves the way we want it to.  It usually involves several people with different perspectives on the code.  </p>

<p>Ideally, such verification should start before you write any code.  Some questions you should be able to answer are:</p>

<ol>
<li>What should the code do?</li>
<li>What inputs should it accept? (including type, ranges, etc)</li>
<li>What should the output look like?</li>
<li>How will it handle missing values?</li>
<li>How will it handle NULL values?</li>
<li>How will it handle zero-length values?</li>
</ol>

<p>If you prepare a list of requirements <em>and</em> write your documentation <em>before</em> you begin writing any code, the probability of success goes up pretty quickly.  Naturally, as you begin writing your code, you may find that your requirements need to be adjusted, or the function arguments need to be modified.  That's okay, but <em>document those changes when they happen</em>.</p>

<p>While you are writing your function, use a package like <code>assertthat</code> or <code>checkmate</code> to write as many argument checks as you need in your code.  Some of the best, most reliable code where I work consists of about 100 lines of argument checks and 3-4 lines of what the code actually is intended to do.  It may seem like overkill, but you prevent a lot of problems from bad inputs that you never intended for users to provide.</p>

<p>When you've finished writing your function, you should at this point have a list of requirements <em>and</em> clearly documented expectations of your arguments.  This is where you make use of the <code>testthat</code> package.  </p>

<ul>
<li>Write tests that verify all of the requirements you wrote are met.  </li>
<li>Write tests that verify you can no put in unintended inputs and get the results you want.  </li>
<li>Write tests that verify you get the output  you intended on your test data.</li>
<li>Write tests that test any edge cases you can think of.</li>
</ul>

<p>It can take a long time to write all of these tests, but once it is done, any further development is easier to check since anything that violates your existing requirements should fail the test.</p>

<p>That being said, I'm really bad at following this process in my own work.  I have the tendency to write code, then document what I did.  But the best code I've written has been where I've planned it out conceptually, wrote my documentation, coded, and then tested against my documentation.</p>
"
29445716,Problems creating a new list,1,3,3,"<p>Here I have a function which I want to equalize the number of stops from a object in motion. To carry out this, there is a position list (in which this function is named Trip), and duration which is the length of trip but will be used in further development of the code.</p>

<p>Now to know the number of the stop throughout the trip what I have to do is for each Trip, which has different positions as:</p>

<p>x,y,z</p>

<p>10,11,13</p>

<p>12,11,14</p>

<p>13,11,15,
....</p>

<p>**20,11,35</p>

<p>20,11,35</p>

<p>20,11,35**
Compare themselves to know which are equal.</p>

<p>On this last positions as the object remain on the same location we can conclude  was stopped. So, in order to know the stop we need to compare each position with the next ones.</p>

<p>I write this code:</p>

<pre><code>StopsNumber &lt;- function(Trip,Duration)
{
  i=1
  aux = Trip

  while(i&lt;length(Trip))
  {
    if(aux[i] == aux[i+1]  &amp;&amp; aux[i] == aux[i+2]){
      Stop = aux[i]
      NStops = Nstops+1
    }
    aux = [aux+1]
    i=i+1
  } # end 

  return (Stop,Nstops)
}
</code></pre>

<p>MThe problem I think is that i do not know how to create list of things. For instance: on Stop = aux[i] I don't know if it is working out properly. Because i want to do Stop be a list (or a vector, with aux, (those positions where the object has been quiet).And doing this if there are more than one stops, the last one will replace the rest.
May somebody help me?
Thank you</p>
","<p>Your definitions of movement, intervals and stops are unclear. Therefore the code is fairly long to avoid misunderstandings. Otherwise it could be boiled down one or two lines. First some clear cut definitions</p>

<ul>
<li>An interval is some time between between to xyz-points</li>
<li>Movement has occured in an interval, if start and end point differ in space</li>
<li>A stop is an interval of no movement after an interval of movement</li>
<li>You can choose to assume the object was(or was not) in movement before the first interval. Thus a stop can happen already in first interval.</li>
</ul>

<p>a tip: try out the loop-functions apply(), sapply(), lapply() and foreach() instead of the low-level for() and while().</p>

<h1>the code</h1>

<pre><code> #your data added some more positions
 mixed.vector = c(
 10,11,13,
 12,11,14,
 13,11,15,
 20,11,35,
 20,11,35, #this is a stop
 20,11,35, 
 13,11,25,
 10,20,30,
 10,20,30) #this is a stop

 #convert data to appropiate data structure.
 #I suggest a matrix. List of vector would also do
 #some tricks to convert (multiple ways to do this)
 #mixed vector to matrix
 xyz.matrix = matrix(mixed.vector,ncol=3,byrow=TRUE)
 print(xyz.matrix) #each row is a position, columns are x, y and z respectively.

 #matrix to list of vectors (if this structure is preferred)
 list_of_vectors = split(xyz.matrix,1:dim(xyz.matrix)[1])
 print(list_of_vectors)

 #list of vectors to matrix (if this is how your data initially is ordered)
 xyz.matrix = do.call(rbind,list_of_vectors)
 print(xyz.matrix) #and we're back with a matrix

 #function checking if intervals have no movement
 #(total number of intervals is number of positions minus 1)
 find_interval_with_movement = function(XYZ.m) {
   nrows = dim(XYZ.m)[1] #scalar of n.position rows
   interval_with_movement = !apply(XYZ.m[-nrows,]==XYZ.m[-1,],1,all)  #check pairs of row if x y z match.
   return(interval_with_movement)
 }

 #function finding stops, optional assuming the object was moving before first interval
 find_stops = function(interval_movements,object.moving.at.t0=TRUE) {
   intervals_to_search= c(object.moving.at.t0,interval_movements)
   len = length(intervals_to_search)
   #search for intervals with no movement where previous interval has movement
   did.stop = sapply(2:len,function(i) all(intervals_to_search[(i-1):i] == c(T,F)))
   return(did.stop)
 }


 #these intervals has no movement
 print(!find_interval_with_movement(xyz.matrix))

 #these intervals had no movement, where previous had
 print(find_stops(find_interval_with_movement(xyz.matrix)))

 #and the full number of stops
 print(sum(find_stops(find_interval_with_movement(xyz.matrix))))
</code></pre>
"
16069406,Text-mining with the tm-package - word stemming,2,2,4,"<p>I am doing some text mining in R with the <code>tm</code>-package. Everything works very smooth. However, one problem occurs after stemming (<a href=""http://en.wikipedia.org/wiki/Stemming"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Stemming</a>). Obviously, there are some words, which have the same stem, but it is important that they are not ""thrown together"" (as those words mean different things). </p>

<p>For an example see the 4 texts below. Here you cannnot use ""lecturer"" or ""lecture"" (""association"" and ""associate"") interchangeable. However, this is what is done in step 4. </p>

<p>Is there any elegant solution how to implement this for some cases/words manually (e.g. that ""lecturer"" and ""lecture"" are kept as two different things)?</p>

<pre><code>texts &lt;- c(""i am member of the XYZ association"",
""apply for our open associate position"", 
""xyz memorial lecture takes place on wednesday"", 
""vote for the most popular lecturer"")

# Step 1: Create corpus
corpus &lt;- Corpus(DataframeSource(data.frame(texts)))

# Step 2: Keep a copy of corpus to use later as a dictionary for stem completion
corpus.copy &lt;- corpus

# Step 3: Stem words in the corpus
corpus.temp &lt;- tm_map(corpus, stemDocument, language = ""english"")  

inspect(corpus.temp)

# Step 4: Complete the stems to their original form
corpus.final &lt;- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)  

inspect(corpus.final)
</code></pre>
","<p>I'm not 100% what you're after and don't totally get how <code>tm_map</code> works.  If I understand then the following works.  As I understand you want to supply a list of words that should not be stemmed.  I'm using the qdap package mostly because I'm lazy and it has a function <code>mgsub</code> I like.</p>

<p>Note that I got frustrated with using <code>mgsub</code> and <code>tm_map</code> as it kept throwing an error so I just used <code>lapply</code> instead.</p>

<pre><code>texts &lt;- c(""i am member of the XYZ association"",
    ""apply for our open associate position"", 
    ""xyz memorial lecture takes place on wednesday"", 
    ""vote for the most popular lecturer"")

library(tm)
# Step 1: Create corpus
corpus.copy &lt;- corpus &lt;- Corpus(DataframeSource(data.frame(texts)))

library(qdap)
# Step 2: list to retain and indentifier keys
retain &lt;- c(""lecturer"", ""lecture"")
replace &lt;- paste(seq_len(length(retain)), ""SPECIAL_WORD"", sep=""_"")

# Step 3: sub the words you want to retain with identifier keys
corpus[seq_len(length(corpus))] &lt;- lapply(corpus, mgsub, pattern=retain, replacement=replace)

# Step 4: Stem it
corpus.temp &lt;- tm_map(corpus, stemDocument, language = ""english"")  

# Step 5: reverse -&gt; sub the identifier keys with the words you want to retain
corpus.temp[seq_len(length(corpus.temp))] &lt;- lapply(corpus.temp, mgsub, pattern=replace, replacement=retain)

inspect(corpus)       #inspect the pieces for the folks playing along at home
inspect(corpus.copy)
inspect(corpus.temp)

# Step 6: complete the stem
corpus.final &lt;- tm_map(corpus.temp, stemCompletion, dictionary = corpus.copy)  
inspect(corpus.final)
</code></pre>

<p><strong>Basically it works by:</strong></p>

<ol>
<li>subbing out a unique identifier key for the supplied ""NO STEM"" words (the <code>mgsub</code>)</li>
<li>then you stem (using <code>stemDocument</code>) </li>
<li>next you reverse it and sub the identifier keys with the ""NO STEM"" words (the <code>mgsub</code>)</li>
<li>last complete the Stem (<code>stemCompletion</code>)</li>
</ol>

<p><strong>Here's the output:</strong></p>

<pre><code>## &gt;     inspect(corpus.final)
## A corpus with 4 text documents
## 
## The metadata consists of 2 tag-value pairs and a data frame
## Available tags are:
##   create_date creator 
## Available variables in the data frame are:
##   MetaID 
## 
## $`1`
## i am member of the XYZ associate
## 
## $`2`
##  for our open associate position
## 
## $`3`
## xyz memorial lecture takes place on wednesday
## 
## $`4`
## vote for the most popular lecturer
</code></pre>
"
13589884,merge date vectors inside a data frame,1,1,1,"<p>I have a data structure <code>df</code> with some date vectors, one thou five, like this,</p>

<pre><code>df &lt;- structure(list(one.date = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, ""2012-09-23"", ""2012-09-23"", NA, NA, NA, NA, NA, NA, NA, NA, NA), two.date = c(NA, ""2012-11-13"", NA, ""2012-11-19"", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ""2012-09-24"", NA, NA, NA), three.date = c(NA, NA, ""2012-11-19"", NA, NA, NA, NA, NA, ""2012-09-22"", NA, NA, NA, NA, NA, NA, NA, NA, ""2012-09-24"", NA, NA), four.date = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ""2012-09-02"", ""2012-09-10"",""2012-09-23"", NA, NA, NA, NA, NA, NA), five.date = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ""2012-09-24"", ""2014-09-09"", NA, NA, NA, NA)), .Names = c(""one.date"", ""two.date"", ""three.date"", ""four.date"", ""five.date""), row.names = c(NA, 20L), class = ""data.frame"")
# &gt; df
     one.date   two.date three.date  four.date  five.date
1        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
2        &lt;NA&gt; 2012-11-13       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
3        &lt;NA&gt;       &lt;NA&gt; 2012-11-19       &lt;NA&gt;       &lt;NA&gt;
4        &lt;NA&gt; 2012-11-19       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
5        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
6        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
7        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
8        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
9        &lt;NA&gt;       &lt;NA&gt; 2012-09-22       &lt;NA&gt;       &lt;NA&gt;
10 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
11 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
12       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-02       &lt;NA&gt;
13       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-10       &lt;NA&gt;
14       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-23       &lt;NA&gt;
15       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-24
16       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2014-09-09
17       &lt;NA&gt; 2012-09-24       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
18       &lt;NA&gt;       &lt;NA&gt; 2012-09-24       &lt;NA&gt;       &lt;NA&gt;
19       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
20       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
</code></pre>

<p>Now, I would like to combine, or merge, them into one vector. Like this,</p>

<pre><code>            date  one.date   two.date three.date  four.date  five.date
  1        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  2  2012-11-13       &lt;NA&gt; 2012-11-13       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  3  2012-11-19       &lt;NA&gt;       &lt;NA&gt; 2012-11-19       &lt;NA&gt;       &lt;NA&gt;
  4  2012-11-19       &lt;NA&gt; 2012-11-19       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  5        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  6        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  7        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  8        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  9  2012-09-22       &lt;NA&gt;       &lt;NA&gt; 2012-09-22       &lt;NA&gt;       &lt;NA&gt;
  10 2012-09-23 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  11 2012-09-23 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  12 2012-09-02       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-02       &lt;NA&gt;
  13 2012-09-10       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-10       &lt;NA&gt;
  14 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-23       &lt;NA&gt;
  15 2012-09-24       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-24
  16 2014-09-09       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2014-09-09
  17 2012-09-24       &lt;NA&gt; 2012-09-24       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  18 2012-09-24       &lt;NA&gt;       &lt;NA&gt; 2012-09-24       &lt;NA&gt;       &lt;NA&gt;
  19       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
  20       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
</code></pre>

<p>Any help would be appreciated.</p>
","<p>I'm assuming that there is at most 1 non-<code>NA</code> entry per row:</p>

<pre><code>as.vector(apply(df,1,function(x) ifelse(all(is.na(x)),NA,x[!is.na(x)])))
 [1] NA           ""2012-11-13"" ""2012-11-19"" ""2012-11-19"" NA          
 [6] NA           NA           NA           ""2012-09-22"" ""2012-09-23""
[11] ""2012-09-23"" ""2012-09-02"" ""2012-09-10"" ""2012-09-23"" ""2012-09-24""
[16] ""2014-09-09"" ""2012-09-24"" ""2012-09-24"" NA           NA
</code></pre>

<p>Or to have it as a factor as you have in your original data:</p>

<pre><code>cbind(list(date=as.factor(apply(df,1,function(x) ifelse(all(is.na(x)),NA,x[!is.na(x)])))),df)
         date   one.date   two.date three.date  four.date  five.date
1        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
2  2012-11-13       &lt;NA&gt; 2012-11-13       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
3  2012-11-19       &lt;NA&gt;       &lt;NA&gt; 2012-11-19       &lt;NA&gt;       &lt;NA&gt;
4  2012-11-19       &lt;NA&gt; 2012-11-19       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
5        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
6        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
7        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
8        &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
9  2012-09-22       &lt;NA&gt;       &lt;NA&gt; 2012-09-22       &lt;NA&gt;       &lt;NA&gt;
10 2012-09-23 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
11 2012-09-23 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
12 2012-09-02       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-02       &lt;NA&gt;
13 2012-09-10       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-10       &lt;NA&gt;
14 2012-09-23       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-23       &lt;NA&gt;
15 2012-09-24       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2012-09-24
16 2014-09-09       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt; 2014-09-09
17 2012-09-24       &lt;NA&gt; 2012-09-24       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
18 2012-09-24       &lt;NA&gt;       &lt;NA&gt; 2012-09-24       &lt;NA&gt;       &lt;NA&gt;
19       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
20       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;
</code></pre>
"
9968578,Speeding up Julia's poorly written R examples,2,3,3,"<p>The Julia examples to compare performance against R <a href=""https://stackoverflow.com/questions/9965747/linking-r-and-julia#comment12732477_9965747"">seem particularly convoluted</a>.  <a href=""https://github.com/JuliaLang/julia/blob/master/test/perf/perf.R"" rel=""nofollow noreferrer"">https://github.com/JuliaLang/julia/blob/master/test/perf/perf.R</a></p>

<p>What is the fastest performance you can eke out of the two algorithms below (preferably with an explanation of what you changed to make it more R-like)?</p>

<pre><code>## mandel

mandel = function(z) {
    c = z
    maxiter = 80
    for (n in 1:maxiter) {
        if (Mod(z) &gt; 2) return(n-1)
        z = z^2+c
    }
    return(maxiter)
}

mandelperf = function() {
    re = seq(-2,0.5,.1)
    im = seq(-1,1,.1)
    M = matrix(0.0,nrow=length(re),ncol=length(im))
    count = 1
    for (r in re) {
        for (i in im) {
            M[count] = mandel(complex(real=r,imag=i))
            count = count + 1
        }
    }
    return(M)
}

assert(sum(mandelperf()) == 14791)

## quicksort ##

qsort_kernel = function(a, lo, hi) {
    i = lo
    j = hi
    while (i &lt; hi) {
        pivot = a[floor((lo+hi)/2)]
        while (i &lt;= j) {
            while (a[i] &lt; pivot) i = i + 1
            while (a[j] &gt; pivot) j = j - 1
            if (i &lt;= j) {
                t = a[i]
                a[i] = a[j]
                a[j] = t
            }
            i = i + 1;
            j = j - 1;
        }
        if (lo &lt; j) qsort_kernel(a, lo, j)
        lo = i
        j = hi
    }
    return(a)
}

qsort = function(a) {
  return(qsort_kernel(a, 1, length(a)))
}

sortperf = function(n) {
    v = runif(n)
    return(qsort(v))
}

sortperf(5000)
</code></pre>
","<p>Hmm, in the Mandelbrot example the matrix M has its dimensions transposed</p>

<pre><code>M = matrix(0.0,nrow=length(im), ncol=length(re))
</code></pre>

<p>because it's filled by incrementing <code>count</code> in the inner loop (successive values of <code>im</code>). My implementation creates a vector of complex numbers in <code>mandelperf.1</code> and operates on all elements, using an index and subsetting to keep track of which elements of the vector have not yet satisfied the condition <code>Mod(z) &lt;= 2</code></p>

<pre><code>mandel.1 = function(z, maxiter=80L) {
    c &lt;- z
    result &lt;- integer(length(z))
    i &lt;- seq_along(z)
    n &lt;- 0L
    while (n &lt; maxiter &amp;&amp; length(z)) {
        j &lt;- Mod(z) &lt;= 2
        if (!all(j)) {
            result[i[!j]] &lt;- n
            i &lt;- i[j]
            z &lt;- z[j]
            c &lt;- c[j]
        }
        z &lt;- z^2 + c
        n &lt;- n + 1L
    }
    result[i] &lt;- maxiter
    result
}

mandelperf.1 = function() {
    re = seq(-2,0.5,.1)
    im = seq(-1,1,.1)
    mandel.1(complex(real=rep(re, each=length(im)),
                     imaginary=im))
}
</code></pre>

<p>for a 13-fold speed-up (the results are equal but not identical because the original returns numeric rather than integer values).</p>

<pre><code>&gt; library(rbenchmark)
&gt; benchmark(mandelperf(), mandelperf.1(),
+           columns=c(""test"", ""elapsed"", ""relative""),
+           order=""relative"")
            test elapsed relative
2 mandelperf.1()   0.412  1.00000
1   mandelperf()   5.705 13.84709

&gt; all.equal(sum(mandelperf()), sum(mandelperf.1()))
[1] TRUE
</code></pre>

<p>The quicksort example doesn't actually sort</p>

<pre><code>&gt; set.seed(123L); qsort(sample(5))
[1] 2 4 1 3 5
</code></pre>

<p>but my main speed-up was to vectorize the partition around the pivot</p>

<pre><code>qsort_kernel.1 = function(a) {
    if (length(a) &lt; 2L)
        return(a)
    pivot &lt;- a[floor(length(a) / 2)]
    c(qsort_kernel.1(a[a &lt; pivot]), a[a == pivot], qsort_kernel.1(a[a &gt; pivot]))
}

qsort.1 = function(a) {
    qsort_kernel.1(a)
}

sortperf.1 = function(n) {
    v = runif(n)
    return(qsort.1(v))
}
</code></pre>

<p>for a 7-fold speedup (in comparison to the uncorrected original)</p>

<pre><code>&gt; benchmark(sortperf(5000), sortperf.1(5000),
+           columns=c(""test"", ""elapsed"", ""relative""),
+           order=""relative"")
              test elapsed relative
2 sortperf.1(5000)    6.60 1.000000
1   sortperf(5000)   47.73 7.231818
</code></pre>

<p>Since in the original comparison Julia is about 30 times faster than R for mandel, and 500 times faster for quicksort, the implementations above are still not really competitive.</p>
"
27625532,how to divide all values in a data.frame by one row,1,1,1,"<p>I have a    data.frame like this:</p>

<pre><code>df=head(df)       
                                                                                                        t20coreB t20crustB t30crustB t0core t0crust
CellularProcesses 0 1 0 0 0
CellularProcesses2 0 1 0 0 0
CellularProcesses3 0 0 4 4 0
CellularProcesses4 0 0 0 0 0
CellularProcesses5 0 1 0 0 0
CellularProcesses6 12 6 1 12 13

                                                                                                        t10coreA t10crustA t10coreB t10crustB
CellularProcesses 0 0 0 0
CellularProcesses2 0 0 0 0
CellularProcesses3 0 1 15 0
CellularProcesses4 1 0 0 0
CellularProcesses5 1 0 0 0
CellularProcesses6 0 11 5 1

                                                                                                        t30coreA t30crustA t30coreB
CellularProcesses 0 0 0
CellularProcesses2 0 0 0
CellularProcesses3 0 3 1
CellularProcesses4 2 1 0
CellularProcesses5 0 0 1
CellularProcesses6 1 28 5
</code></pre>

<p>I'd like to divide the values in each column for the value in the last row (in the corresponding column). </p>

<p>I tried:</p>

<pre><code>df.normalized=df[1:3,]/df[4,]
</code></pre>

<p>But it didn't work and I got an error.</p>

<p>Any suggestions?</p>
","<p>You can also divide by keeping it as a <code>data.frame</code></p>

<pre><code>df[] &lt;- lapply(df, function(x) as.numeric(gsub('[.]', '', x)))
res &lt;- df[1:3,]/unlist(df[4,])[col(df)[-4,]]
</code></pre>

<p>Or</p>

<pre><code>res &lt;- t(t(df[1:3,])/unlist(df[4,]))

res[,1]
#[1] 1.638736e-06 3.277471e-07 7.374310e-06

df[1:3,1]/df[4,1]
#[1] 1.638736e-06 3.277471e-07 7.374310e-06
</code></pre>

<h3>Update</h3>

<p>The updated data have <code>numeric</code> last row <code>6</code></p>

<pre><code> res &lt;- df[1:5,]/unlist(df[6,])[col(df)[-6,]]
 res[,6]
 #[1] NaN NaN NaN Inf Inf
</code></pre>

<p>You get <code>NaN</code>, <code>Inf</code> because</p>

<pre><code> df[,6]
 #[1] 0 0 0 1 1 0

 0/0
 #[1] NaN
 1/0
#[1] Inf
</code></pre>

<p>If you want the <code>NaN</code> in any of the columns to be transformed to <code>0</code> </p>

<pre><code> res[] &lt;- lapply(res, function(x) replace(x, is.nan(x),0))
</code></pre>

<p>For both <code>NaN</code> and <code>Inf</code> to <code>0</code></p>

<pre><code> res[] &lt;- lapply(res, function(x) replace(x, !is.finite(x),0))
</code></pre>

<p>Another option for <code>NaN</code> is</p>

<pre><code> res[res=='NaN'] &lt;- 0
</code></pre>

<h3>data</h3>

<pre><code>df &lt;- structure(list(S1 = c(""10"", ""2"", ""45"", ""6.102.266""), S2 = c(""20"", 
""5"", ""8"", ""8.392.734""), S3 = c(""5"", ""88"", ""74"", ""6.329.533""), 
S4 = c(""6"", ""32"", ""5"", ""6.393.165""), S5 = c(""18"", ""85"", ""61"", 
""7.127.333""), S6 = c(12, 45, 23, 810.613), S7 = c(""7"", ""6"", 
""2"", ""5.832.144""), S8 = c(""8"", ""1"", ""20"", ""8.560.084""), S9 = c(""91"", 
""20"", ""0"", ""9.133.783""), S10 = c(""12"", ""33"", ""8"", ""3.537.480""
), S11 = c(""11"", ""22"", ""9"", ""24.708.786""), S12 = c(""75"", 
""85"", ""55"", ""5.928.850"")), .Names = c(""S1"", ""S2"", ""S3"", ""S4"", 
""S5"", ""S6"", ""S7"", ""S8"", ""S9"", ""S10"", ""S11"", ""S12""), class = ""data.frame"", 
row.names = c(""A"", ""B"", ""C"", ""Reads""))
</code></pre>

<h3>New data</h3>

<pre><code>df &lt;- structure(list(t20coreB = c(0L, 0L, 0L, 0L, 0L, 12L), t20crustB = c(1L, 
1L, 0L, 0L, 1L, 6L), t30crustB = c(0L, 0L, 4L, 0L, 0L, 1L), t0core = c(0L, 
0L, 4L, 0L, 0L, 12L), t0crust = c(0L, 0L, 0L, 0L, 0L, 13L), t10coreA = c(0L, 
0L, 0L, 1L, 1L, 0L), t10crustA = c(0L, 0L, 1L, 0L, 0L, 11L), 
t10coreB = c(0L, 0L, 15L, 0L, 0L, 5L), t10crustB = c(0L, 
0L, 0L, 0L, 0L, 1L), t30coreA = c(0L, 0L, 0L, 2L, 0L, 1L), 
t30crustA = c(0L, 0L, 3L, 1L, 0L, 28L), t30coreB = c(0L, 
0L, 1L, 0L, 1L, 5L)), .Names = c(""t20coreB"", ""t20crustB"", 
""t30crustB"", ""t0core"", ""t0crust"", ""t10coreA"", ""t10crustA"", ""t10coreB"", 
""t10crustB"", ""t30coreA"", ""t30crustA"", ""t30coreB""), row.names =   c(""CellularProcesses;Transportandcatabolism;Lysosome[PATH:ko04142];E32125,MANBA,manB;beta-mannosidase[EC:32125]"", 
 ""CellularProcesses;Transportandcatabolism;Lysosome[PATH:ko04142];GBA,srfJ;glucosylceramidase[EC:32145]"", 
     ""CellularProcesses;Transportandcatabolism;Lysosome[PATH:ko04142];HEXA_B;hexosaminidase[EC:32152]"", 
 ""CellularProcesses;Transportandcatabolism;Lysosome[PATH:ko04142];NEU1;sialidase-1[EC:32118]"", 
 ""CellularProcesses;Transportandcatabolism;Lysosome[PATH:ko04142];uidA,GUSB;beta-glucuronidase[EC:32131]"", 
  ""CellularProcesses;Transportandcatabolism;Peroxisome[PATH:ko04146];ACSL,fadD;long-chainacyl-CoAsynthetase[EC:6213]""
   ), class = ""data.frame"")
</code></pre>
"
44155653,R conditionally copy row dates,1,1,1,"<p>I am trying to create a data frame in R but finding it hard. I would appreciate any help. I have the following type data frame. </p>

<pre><code>       case_id    event  eventDate  
1           1       A    2000-07-25  
2           1       A    2014-02-25
3           1       B    2014-07-07
4           2       A    2000-03-12
5           2       A    2000-06-06
6           2       A    2000-09-05
7           2       B    2015-12-16
8           2       A    2016-07-28
9           2       A    2017-03-03
10          3       A    2002-05-13
11          3       A    2002-06-12
12          3       B    2004-06-27
13          3       A    2004-07-11
14          4       B    2011-08-31
15          4       A    2012-04-21
16          4       B    2013-01-10
</code></pre>

<p>I would like to get an output which looks like this:
Firstly, countof_eventA_before_B variable is the count of event A before event B by group (case_id). Secondly, finish date for the event B is the date of the following event A or today's date if no event A, also by group. For example, row 3 is today's date if no A event after event B, row 7 is copy of the next A event date after B in a group case_id</p>

<pre><code>        case_id    event  eventDate  countof_eventA_before_B   finishDate
1           1       A    2000-07-25   NA                       NA 
2           1       A    2014-02-25   NA                       NA
3           1       B    2014-07-07   2                        2017-05-24  
4           2       A    2000-03-12   NA                       NA 
5           2       A    2000-06-06   NA                       NA
6           2       A    2000-09-05   NA                       NA
7           2       B    2015-12-16   3                        2016-07-28  
8           2       A    2016-07-28   NA                       NA
9           2       A    2017-03-03   NA                       NA
10          3       A    2002-05-13   NA                       NA
11          3       A    2002-06-12   NA                       NA
12          3       B    2004-06-27   2                        2004-06-27 
13          3       A    2004-07-11   NA                       NA
14          4       B    2011-08-31   0                        2012-04-21 
15          4       A    2012-04-21   NA                       NA
16          4       B    2013-01-10   1                        2017-05-24 
</code></pre>

<p>Can anyone please help? Thank you in advance.</p>
","<p>Here is a little bit clumsy implementation but it works:</p>

<pre><code>#Read the data:
data&lt;-fread(""case_id    event  eventDate  countof_eventA_before_B   finishDate
1           1       A    2000-07-25   NA                       NA 
2           1       A    2014-02-25   NA                       NA
3           1       B    2014-07-07   2                        2017-05-24  
4           2       A    2000-03-12   NA                       NA 
5           2       A    2000-06-06   NA                       NA
6           2       A    2000-09-05   NA                       NA
7           2       B    2015-12-16   3                        2016-07-28  
8           2       A    2016-07-28   NA                       NA
9           2       A    2017-03-03   NA                       NA
10          3       A    2002-05-13   NA                       NA
11          3       A    2002-06-12   NA                       NA
12          3       B    2004-06-27   2                        2004-06-27 
13          3       A    2004-07-11   NA                       NA
14          4       B    2011-08-31   0                        2012-04-21 
15          4       A    2012-04-21   NA                       NA
16          4       B    2013-01-10   1                        2017-05-24 "")
data[,V1:=NULL]
setnames(data,c(""case_id"",""event"",""eventDate"",""countof_eventA_before_B"",""finishDate""))

#Order data by case_id and eventDate:
data[,eventDatenum:=as.numeric(gsub(""-"","""",eventDate))]
data&lt;-data[order(case_id,eventDate)]

#Obtain all events by case_id:
data[,all_cases:=list(list(event)),by=""case_id""]
data[,all_dates:=list(list(eventDate)),by=""case_id""]
#Obtain order number of an event within case_id
data[,seq_nr:=seq_len(.N),by=c(""case_id"")]

#Define function calculating number of events:
function_seq&lt;-function(x,y){
  as.numeric(sum(x[[1]][1:(y-1)]==""A""))
}

#Obtain function calculating needed date:
function_date&lt;-function(x,y,z){
  x_aux&lt;-x[[1]][(z+1):length(x[[1]])]
  y_aux&lt;-y[[1]][(z+1):length(x[[1]])]
  if (sum(x_aux==""A"",na.rm=TRUE)&gt;0){
    as.character(y_aux[x_aux==""A""][1])
  } else{
    as.character(Sys.Date())
  }
}
data[,neeed_nr_events:=ifelse(event==""A"",as.numeric(NA),function_seq(all_cases,seq_nr)),by=1:nrow(data)]
data[,neeed_dates:=ifelse(event==""A"",as.character(NA),function_date(all_cases,all_dates,seq_nr)),by=1:nrow(data)]

data

    case_id event  eventDate countof_eventA_before_B finishDate eventDatenum   all_cases                                                         all_dates seq_nr neeed_nr_events neeed_dates
 1:       1     A 2000-07-25                      NA         NA     20000725       A,A,B                                  2000-07-25,2014-02-25,2014-07-07      1              NA          NA
 2:       1     A 2014-02-25                      NA         NA     20140225       A,A,B                                  2000-07-25,2014-02-25,2014-07-07      2              NA          NA
 3:       1     B 2014-07-07                       2 2017-05-24     20140707       A,A,B                                  2000-07-25,2014-02-25,2014-07-07      3               2  2017-05-24
 4:       2     A 2000-03-12                      NA         NA     20000312 A,A,A,B,A,A 2000-03-12,2000-06-06,2000-09-05,2015-12-16,2016-07-28,2017-03-03      1              NA          NA
 5:       2     A 2000-06-06                      NA         NA     20000606 A,A,A,B,A,A 2000-03-12,2000-06-06,2000-09-05,2015-12-16,2016-07-28,2017-03-03      2              NA          NA
 6:       2     A 2000-09-05                      NA         NA     20000905 A,A,A,B,A,A 2000-03-12,2000-06-06,2000-09-05,2015-12-16,2016-07-28,2017-03-03      3              NA          NA
 7:       2     B 2015-12-16                       3 2016-07-28     20151216 A,A,A,B,A,A 2000-03-12,2000-06-06,2000-09-05,2015-12-16,2016-07-28,2017-03-03      4               3  2016-07-28
 8:       2     A 2016-07-28                      NA         NA     20160728 A,A,A,B,A,A 2000-03-12,2000-06-06,2000-09-05,2015-12-16,2016-07-28,2017-03-03      5              NA          NA
 9:       2     A 2017-03-03                      NA         NA     20170303 A,A,A,B,A,A 2000-03-12,2000-06-06,2000-09-05,2015-12-16,2016-07-28,2017-03-03      6              NA          NA
10:       3     A 2002-05-13                      NA         NA     20020513     A,A,B,A                       2002-05-13,2002-06-12,2004-06-27,2004-07-11      1              NA          NA
11:       3     A 2002-06-12                      NA         NA     20020612     A,A,B,A                       2002-05-13,2002-06-12,2004-06-27,2004-07-11      2              NA          NA
12:       3     B 2004-06-27                       2 2004-06-27     20040627     A,A,B,A                       2002-05-13,2002-06-12,2004-06-27,2004-07-11      3               2  2004-07-11
13:       3     A 2004-07-11                      NA         NA     20040711     A,A,B,A                       2002-05-13,2002-06-12,2004-06-27,2004-07-11      4              NA          NA
14:       4     B 2011-08-31                       0 2012-04-21     20110831       B,A,B                                  2011-08-31,2012-04-21,2013-01-10      1               0  2012-04-21
15:       4     A 2012-04-21                      NA         NA     20120421       B,A,B                                  2011-08-31,2012-04-21,2013-01-10      2              NA          NA
16:       4     B 2013-01-10                       1 2017-05-24     20130110       B,A,B                                  2011-08-31,2012-04-21,2013-01-10      3               1  2017-05-24
</code></pre>

<p>And here is a one row solution:</p>

<pre><code>data[,c(""all_cases"",""all_dates"",""seq_nr""):=list(list(event),list(eventDate),seq_len(.N)),by=c(""case_id"")][,c(""needed_nr_events"",""neeed_dates""):=list(ifelse(event==""A"",as.numeric(NA),function_seq(all_cases,seq_nr)),
                                                                                                                                                     ifelse(event==""A"",as.character(NA),function_date(all_cases,all_dates,seq_nr))),
                                                                                                          by=1:nrow(data)]
</code></pre>
"
31749544,Plotting marginal histograms (as factors) and scatterplot (as numeric) from the same variable in R,1,3,3,"<p>I'm trying to create a scatterplot with marginal histograms as in <a href=""https://stackoverflow.com/questions/8545035/scatterplot-with-marginal-histograms-in-ggplot2"">this question</a>.
My data are two (numeric) variables which share seven discrete (somewhat) logarithmically-spaced levels.</p>

<p>I've successfully done this with the help of <code>ggMarginal</code> in the <code>ggExtra</code> package, however I'm not happy with the outcome as when plotting the marginal histograms using the same data as for the scatterplots, things don't line up.
As can be seen below, the histogram bars are biased a little to the right or left of the datapoints themselves.</p>

<pre><code>library(ggMarginal)
library(ggplot2)
x &lt;- rep(log10(c(1,2,3,4,5,6,7)), times=c(3,7,12,18,12,7,3))
y &lt;- rep(log10(c(1,2,3,4,5,6,7)), times=c(3,1,13,28,13,1,3))
d &lt;- data.frame(""x"" = x,""y"" = y)
p1 &lt;- ggMarginal(ggplot(d, aes(x,y)) + geom_point() + theme_bw(), type = ""histogram"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/kQBB4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kQBB4.png"" alt=""p1""></a></p>

<p>A possible solution for this may be change the variables used in the histograms into factors, so they are nicely aligned with the scatterplot axes.
This works well when creating histograms using <code>ggplot</code>:</p>

<pre><code>p2 &lt;- ggplot(data.frame(lapply(d, as.factor)), aes(x = x)) + geom_histogram()
</code></pre>

<p><a href=""https://i.stack.imgur.com/rCt2f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rCt2f.png"" alt=""p2""></a></p>

<p>However, when I try to do this using <code>ggMarginal</code>, I do not get the desired result -- it appears that the <code>ggMarginal</code> histogram is still treating my variables as numeric.</p>

<pre><code>p3 &lt;- ggMarginal(ggplot(d, aes(x,y)) + geom_point() + theme_bw(), x = as.factor(x), 
y = as.factor(y), type = ""histogram"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/kKZQP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kKZQP.png"" alt=""p3""></a></p>

<p><strong>How can I ensure my histogram bars are centred over the data points?</strong> I'm absolutely willing to accept an answer which does not involve use of <code>ggMarginal</code>.</p>
","<p>If you are willing to give baseplotting a try, here is a function:</p>

<pre><code>plots$scatterWithHists &lt;- function(x, y, histCols=c(""lightblue"",""lightblue""), lhist=20, xlim=range(x), ylim=range(y), ...){
  ## set up layout and graphical parameters
  layMat &lt;- matrix(c(1,4,3,2), ncol=2)
  layout(layMat, widths=c(5/7, 2/7), heights=c(2/7, 5/7))
  ospc &lt;- 0.5                                                  # outer space
  pext &lt;- 4                                                    # par extension down and to the left
  bspc &lt;- 1                                                    # space between scatter plot and bar plots
  par. &lt;- par(mar=c(pext, pext, bspc, bspc), oma=rep(ospc, 4)) # plot parameters

  ## barplot and line for x (top)
  xhist &lt;- hist(x, breaks=seq(xlim[1], xlim[2], length.out=lhist), plot=FALSE)
  par(mar=c(0, pext, 0, 0))
  barplot(xhist$density, axes=FALSE, ylim=c(0, max(xhist$density)), space=0, col=histCols[1])

  ## barplot and line for y (right)
  yhist &lt;- hist(y, breaks=seq(ylim[1], ylim[2], length.out=lhist), plot=FALSE)
  par(mar=c(pext, 0, 0, 0))
  barplot(yhist$density, axes=FALSE, xlim=c(0, max(yhist$density)), space=0, col=histCols[2], horiz=TRUE)

  ## overlap
  dx &lt;- density(x)
  dy &lt;- density(y)
  par(mar=c(0, 0, 0, 0))
  plot(dx, col=histCols[1], xlim=range(c(dx$x, dy$x)), ylim=range(c(dx$y, dy$y)),
       lwd=4, type=""l"", main="""", xlab="""", ylab="""", yaxt=""n"", xaxt=""n"", bty=""n""
       )
  points(dy, col=histCols[2], type=""l"", lwd=3)

  ## scatter plot
  par(mar=c(pext, pext, 0, 0))
  plot(x, y, xlim=xlim, ylim=ylim, ...)
}
</code></pre>

<p>Just do:</p>

<pre><code>scatterWithHists(x,y, histCols=c(""lightblue"",""orange""))
</code></pre>

<p>And you get:</p>

<p><a href=""https://i.stack.imgur.com/zsiTy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zsiTy.png"" alt=""marginalHists""></a></p>

<p>If you absolutely want to use <code>ggMargins</code> then look up <code>xparams</code> and <code>yparams</code>. It says you can send additional arguments to x-margin and y-margin using those. I was only successful in sending trivial things like color. But maybe sending something like <code>xlim</code> would help.</p>
"
29928712,How to create mirrored histograms,1,3,3,"<p>I would like present propensity score matching stats on 2 groups (unmatched in BW, matched in color) and would like to use mirrored histograms like the following</p>

<p><img src=""https://i.stack.imgur.com/cKJ2C.jpg"" alt=""""></p>

<p>Is it possible to overlay 4 different histograms in base R? Is there any package providing this functionality?</p>

<p>The x-axis is [0,1] bounded (it is a probability) and the BW columns are always bigger or equal to the colored columns (that is there cannot be BW columns ""behind"" the colored columns).</p>

<p>Image from <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22244556"" rel=""noreferrer"">http://www.ncbi.nlm.nih.gov/pubmed/22244556</a></p>
","<p>You can use something like the following. You would want to pre-calculate the <code>hist</code> objects to get the correct ylim values, then use <code>axis</code> and <code>mtext</code> or <code>title</code> to properly label your graph.</p>

<pre><code>set.seed(1234)
x &lt;- rnorm(100, 0, 1)

plot.new()
plot.window(ylim = c(-40, 40), xlim = range(x))
p &lt;- list(axes = FALSE, xlab = """", ylab = """", main = """")
par(new = TRUE)  
do.call(hist, c(list(x = x, ylim = c(-40, 40)), p))
par(new = TRUE)
do.call(hist, c(list(x = x, ylim = c(40, -40)), p))
axis(side = 2, 
     at = pretty(par()$usr[3:4]), 
     labels = abs(pretty(par()$usr[3:4])))
axis(side = 1)
</code></pre>

<p><img src=""https://i.stack.imgur.com/cyHgN.png"" alt=""enter image description here""></p>

<p><strong>EDIT</strong></p>

<pre><code>## Create some fake data
set.seed(1234)
d &lt;- rnorm(250, 0, 1)
e &lt;- rnorm(250, 1, 1)
f &lt;- rlnorm(100, 0, .2)
g &lt;- rlnorm(100, 1, .2)

## Function for plotting
multhist &lt;- function(..., bin.width, col, dir, xlab = NULL, ylab = NULL,
                     main = NULL) {

  vals &lt;- list(...)
  vrng &lt;- range(vals)

  brks &lt;- seq(vrng[1] - abs(vrng[1]*0.1), 
              vrng[2] + abs(vrng[2]*0.1), 
              by = bin.width)

  yrng &lt;- max(sapply(lapply(vals, hist, breaks = brks), ""[["", ""counts""))
  yrng &lt;- 1.2*c(-1*yrng, yrng)

  plot.new()
  plot.window(ylim = yrng, xlim = vrng)

  addhist &lt;- function(x, col, dir) {
    par(new = TRUE)  
    hist(x = x, ylim = dir*yrng, col = col, xlab = """", ylab = """", 
         main = """", axes = FALSE, breaks = brks)
  }

  mapply(addhist, x = vals, col = col, dir = dir)

  py &lt;- pretty(yrng)
  py &lt;- py[py &gt;= yrng[1] &amp; py &lt;= yrng[2]]
  axis(side = 2, at = py, labels = abs(py))
  axis(side = 1)
  title(main = main, xlab = xlab, ylab = ylab)

}
</code></pre>

<p>You can give the function numeric vectors, as well as vectors for the corresponding colors and directions (1 or -1). I did not do the formal checking on the lengths of vals, col, and dir, but it is pretty straight forward.</p>

<pre><code>## Use the function
multhist(d, e, f, g, bin.width = 0.5, 
         col = c(""white"", ""white"", ""lightgreen"", ""darkgreen""), 
         dir = c(1, -1, 1, -1), xlab = ""xlabel"", ylab = ""ylabel"", 
         main = ""title"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/Ifm1K.png"" alt=""enter image description here""></p>
"
19143383,increase efficiency and speed of R function,1,1,3,"<p>When using R I always have in mind:  ""Avoid using loops if possible"".  However, I am stuck right now, I haven't been able to figure out a CRANTASTIC way to code what I need.  </p>

<p>For the record, after several comments, my statement above is not the right statement, there's no need to avoid loops here to improve the efficiency.</p>

<p>I have two string vectors as input, let us call them, <code>a</code> and <code>b</code> - they can only contain the letters <code>""M""</code>, <code>""I""</code> and <code>""D""</code>.</p>

<pre><code>a = c(""M"",""I"",""D"",""D"",""M"",""M"",""M"",""M"",""M"",""M"")
b = c(""M"",""M"",""M"",""M"",""M"",""M"",""D"",""M"",""M"")
</code></pre>

<p>My desired output is:</p>

<pre><code>d = c(""M"",""I"",""D"",""D"",""M"",""M"",""M"",""M"",""I"",""M"",""M"")
</code></pre>

<p>The following function gives me such output:</p>

<pre><code>my.function &lt;- function(a, b)
{
  nrow.df = length(a) + length(which(b==""D""))
  my.df = data.frame(a = rep(NA, nrow.df),  
                      b = rep(NA, nrow.df), 
                      d = rep(NA, nrow.df))
  my.df$a[1:length(a)] = a
  my.df$b[1:length(b)] = b
  for (i in 1:nrow.df)
  {
    if(my.df$a[i] == ""D"") {
      my.df$d[i] = ""D""
      my.df$b[(i+1):nrow.df] = my.df$b[i:(nrow.df-1)]
    } else if (my.df$b[i] == ""D"") {
      my.df$d[i] = ""I""
      my.df$a[(i+1):nrow.df] = my.df$a[i:(nrow.df-1)]
    } else if (my.df$a[i] == ""I"") {
      my.df$d[i] = ""I""
    } else if (my.df$b[i] == ""I"") {
      my.df$d[i] = ""D""
    } else {
      my.df$d[i] = my.df$a[i]
    }
  }
  return(my.df$d)
}

&gt; d = my.function(a,b)
&gt; d
 [1] ""M"" ""I"" ""D"" ""D"" ""M"" ""M"" ""M"" ""M"" ""I"" ""M"" ""M""
</code></pre>

<p>The function logic is as follows, whenever there is a <code>""D""</code> in <code>a</code>, it puts a <code>""D""</code> in <code>d</code> and shift the vector <code>b</code> by 1, and vice versa, whenever there is a <code>""D""</code> in <code>b</code>, it puts an <code>""I""</code> in <code>d</code> and shifts <code>a</code> by 1.  </p>

<p>Next, when there is an <code>""I""</code> in <code>a</code>, but not a <code>""D""</code> in <code>b</code>, put an <code>""I""</code> in <code>a</code>, and vice versa, whenever there is an <code>""I""</code> in <code>b</code>, and not a <code>""D""</code> in <code>a</code>, put a <code>""D""</code> in <code>d</code>.  Otherwise, <code>d = a</code>.</p>

<p>It is not a complex function but I am struggling on how to make it R efficient.  I am applying this function millions of times with mclapply so having a fast implementation of such function would save me lots of time.</p>

<p>Do you recommend using Rcpp?  Would it be much faster?  Is there any slow down on communicating R with Cpp millions of time, or it's just auto with Rcpp?</p>
","<p>OK, here's the Rcpp solution, and as expected, it beats the R solution by a lot:</p>

<pre><code>rcppFun&lt;-""
CharacterVector fcpp(CharacterVector a,CharacterVector b,int size){
int aSkipped = 0;
int bSkipped = 0;
int asize = a.size();
Rcpp::CharacterVector d(size);
for(int i=0; i&lt;size; i++){
    if(i-aSkipped&lt;asize &amp;&amp; a[i-aSkipped][0] == 'D') {
      d[i] = \""D\"";
      bSkipped++;
    } else if (b[i-bSkipped][0] == 'D') {
      d[i] = \""I\"";
      aSkipped++;
    } else if (a[i-aSkipped][0] == 'I') {
      d[i] = \""I\"";
    } else if (b[i-bSkipped][0] == 'I') {
      d[i] = \""D\"";
    } else {
      d[i] = a[i-aSkipped];
    }
}
 return d;
}""
require(""Rcpp"")
fcpp&lt;-cppFunction(rcppFun)

f3&lt;-function(a,b){
  fcpp(a,b,as.integer(length(a)+sum(b==""D"")))
}
</code></pre>

<p>Warning: that function does no parameter checking at all, so if you feed it bad data you can easily get a seg fault.  </p>

<p>If you are going to be calling this a lot, Rcpp is definitely the way to go:</p>

<pre><code>&gt; with(ab(10),microbenchmark(f(a,b),f3(a,b),f2(a,b),my.function.v(a,b)))
Unit: microseconds
                expr     min       lq   median       uq     max neval
             f(a, b) 103.993 107.5155 108.6815 109.7455 178.801   100
            f3(a, b)   7.354   8.1305   8.5575   9.1220  18.014   100
            f2(a, b)  87.081  90.4150  92.2730  94.2585 146.502   100
 my.function.v(a, b)  84.389  86.5140  87.6090  88.8340 109.106   100
&gt; with(ab(100),microbenchmark(f(a,b),f3(a,b),f2(a,b),my.function.v(a,b)))
Unit: microseconds
                expr     min        lq    median        uq      max neval
             f(a, b) 992.082 1018.9850 1032.0180 1071.0690 2784.710   100
            f3(a, b)  12.873   14.3605   14.7370   15.5095   35.582   100
            f2(a, b) 119.396  125.4405  129.3015  134.9915 1909.930   100
 my.function.v(a, b) 769.618  786.7865  802.2920  824.0820  905.737   100
&gt; with(ab(1000),microbenchmark(f(a,b),f3(a,b),f2(a,b),my.function.v(a,b)))
Unit: microseconds
                expr      min        lq     median        uq       max neval
             f(a, b) 9816.295 10065.065 10233.1350 10392.696 12383.373   100
            f3(a, b)   66.057    67.869    83.9075    87.231  1167.086   100
            f2(a, b) 1637.972  1760.258  2667.6985  3138.229 47610.317   100
 my.function.v(a, b) 9692.885 10272.425 10997.2595 11402.602 54315.922   100
&gt; with(ab(10000),microbenchmark(f(a,b),f3(a,b),f2(a,b)))
Unit: microseconds
     expr        min         lq      median          uq        max neval
  f(a, b) 101644.922 103311.678 105185.5955 108342.4960 144620.777   100
 f3(a, b)    607.702    610.039    669.8515    678.1845    785.415   100
 f2(a, b) 221305.641 247952.345 254478.1580 341195.5510 656408.378   100
&gt; 
</code></pre>
"
20778948,How to add an additional single column heatmap at the side of main heatmap in R,1,3,3,"<p>I have the following scripts:</p>

<pre><code>library(""gplots"")
mydata &lt;- mtcars
mydata.nr &lt;- nrow(mydata)
mydata.newval &lt;-  data.frame(row.names=rownames(mydata),new.val=-log(runif(mydata.nr)))

# Functions
hclustfunc &lt;- function(x) hclust(x, method=""complete"")
distfunc &lt;- function(x) dist(x,method=""euclidean"")

# Set colors
hmcols &lt;- rev(redgreen(256));

# Plot the scaled data
heatmap.2(as.matrix(mydata),dendrogram=""row"",scale=""row"",col=hmcols,trace=""none"", margin=c(8,9), hclust=hclustfunc,distfun=distfunc);
</code></pre>

<p>Which generate the following heatmap:
<img src=""https://i.stack.imgur.com/v2d3i.jpg"" alt=""enter image description here""></p>

<p>Now given a new data.frame which contain new values for each cars:</p>

<pre><code>mydata.nr &lt;- nrow(mydata)
mydata.newval &lt;-  data.frame(row.names=rownames(mydata),new.val=-log(runif(mydata.nr)))
</code></pre>

<p>I want to create a single column heatmap with gradient gray positioned next to row names.
How can I achieve that in R heatmap.2?</p>
","<p>Does this do what you want? You can use the <code>RowSideColors</code> option to add a column to the side of the heatmap.</p>

<pre><code>new.vals = mydata.newval[,1]
mydata.newval$scaled = ( new.vals - min(new.vals) ) / 
                       ( max(new.vals) - min(new.vals) )
mydata.newval$gray = gray( mydata.newval$scaled )

heatmap.2( as.matrix(mydata), 
           dendrogram = ""row"", scale = ""row"",
           col = hmcols, trace = ""none"", 
           margin = c(8,9), 
           hclust = hclustfunc, distfun = distfunc,
           RowSideColors=mydata.newval$gray )
</code></pre>

<p><img src=""https://i.stack.imgur.com/mWRvv.png"" alt=""enter image description here""></p>

<p>If you want the gray column in between the heatmap and the labels, there isn't a simple
way to do that with <code>heatmap.2</code>; I don't think it was designed for
such purposes. One way to hack it together would be to make the gray values
go from 10 to 11 (or something out of the range of the rest of the data). Then
you would change the colors mapped to the breaks (see <a href=""https://stackoverflow.com/questions/20535635/how-to-assign-your-color-scale-on-raw-data-in-heatmap-2"">here</a>). However, this
would make your key look pretty funky.</p>

<pre><code># heatmap.2 does the clustering BEFORE the scaling. 
# Clustering after scaling might give different results
# heatmap.2 also reorders the dendrogram according to rowMeans.
# (Code copied directly from the heatmap.2 function)
x = as.matrix(mydata)
Rowv = rowMeans(x, na.rm = TRUE)
hcr = hclustfunc(distfunc(x))
ddr = as.dendrogram(hcr)
ddr = reorder(ddr, Rowv) # the row dendrogram

# Scale the data as heatmap.2 does
rm = rowMeans(x, na.rm = TRUE)
x = sweep(x, 1, rm)
sx =  apply(x, 1, sd, na.rm = TRUE)
x = sweep(x, 1, sx, ""/"")

# add the new data as a column
new.vals = mydata.newval[,1]
new.vals.scaled = ( new.vals - min(new.vals) ) / 
                  ( max(new.vals) - min(new.vals) ) # scaled from 0 to 1
x = cbind( x, gray = max(x) + new.vals.scaled + 0.1 )

# make the custom breaks and colors
edge = max(abs(x-1.1))
breaks = seq(-edge,edge+1.1,length.out=1000)
gradient1 = greenred( sum( breaks[-length(breaks)] &lt;= edge ) )
gradient2 = colorpanel( sum( breaks[-length(breaks)] &gt; edge ), ""white"", ""black"" )
hm.colors = c(gradient1,gradient2)

hm = heatmap.2( x, col=hm.colors, breaks=breaks,
           scale=""none"", 
           dendrogram=""row"", Rowv=ddr,
           trace=""none"", margins=c(8,9) )
</code></pre>

<p><img src=""https://i.stack.imgur.com/zYbNK.png"" alt=""enter image description here""></p>

<p>Although this hack works, I would look for a more robust solution using more flexible packages that play with different viewports using the <code>grid</code> package.</p>
"
17077183,Can I vectorize code when data is in a list?,2,2,3,"<p>I am in the process of optimizing my code, and I am running into some problems. I know that the greatest speed ups in R come from vectorizing code instead of using loops. However, I have my data in lists, and I am not sure if I can vectorize my code or not. I have tried using the <code>apply</code> functions (like <code>lapply</code>, <code>vapply</code>), but I read that these functions are just for writing cleaner code and are actually using loops under the hood!</p>

<p>Here are my three biggest bottlenecks in my code, though I do not think anything can be done for the first part.</p>

<p>1) Reading data</p>

<p>I work with batches of 1000 matrices of dimensions 277x349. This is the biggest bottleneck in my script, but I alleviated the problem a little bit by using the <code>doMC</code> package to take advantage of multiple cores with the <code>foreach</code> function. This results in a list containing 1000 277x349 matrices.</p>

<p>For the purposes of the question, say we have a list of 1000 matrices of dimensions 277 x 349</p>

<pre><code># Fake data
l &lt;- list()
for(i in 1:1000) {
  l[[i]] &lt;- matrix(rnorm(277*349), nrow=277, ncol=349)
}
</code></pre>

<p>2) Bottleneck #1</p>

<p>I need to make comparisons to some reference matrix (of same dimensions). This leads to comparing the 1000 matrices in my list to my reference matrix to get a vector of 1000 distances. If I know that the matrices are of the same dimensions, can I vectorize this step?</p>

<p>Here is some code:</p>

<pre><code># The reference matrix
r &lt;- matrix(rnorm(277*349), nrow=277, ncol=349)
# The number of non NA values in matrix. Do not need to worry about this...
K &lt;- 277*349

# Make a function to calculate distances
distance &lt;- function(xi, xj, K, na.rm=TRUE) {
  sqrt(sum((xi - xj)^2, na.rm=na.rm)/K)
}

# Get a vector containing all the distances
d &lt;- vapply(l, distance, c(0), xj=r, K=K)
</code></pre>

<p>This step is bearably fast using <code>vapply</code>, but it is the third slowest part of the code.</p>

<p>3) Bottleneck #2</p>

<p>I now want to make a weighted average matrix of the J ""closest"" matrices to my reference matrix. (There is a sorting step, but assume that <code>d[1] &lt; d[2] &lt; ... &lt; d[1000]</code> for simplicity). I want to get the weighted average matrix for when J=1,2,...,1000</p>

<pre><code># Get the weighted matrix
weightedMatrix &lt;- function(listOfData, distances, J) {
  # Calculate weights:
  w &lt;- d[1:J]^{-2} / sum(d[1:J]^{-2})

  # Get the weighted average matrix
  # *** I use a loop here ***
  x_bar &lt;- matrix(0, nrow=nrow(listOfData[[1]]), ncol=ncol(listOfData[[1]]))
  for(i in 1:J) {
    x_bar &lt;- x_bar + {listOfData[[i]] * w[i]}
  }

  return(x_bar)
}

# Oh no! Another loop...
res &lt;- list()
for(i in 1:length(l) ) {
  res[[i]] &lt;- weightedMatrix(l, d, J=i)
}
</code></pre>

<p>I am a little stumped. I do not see an intuitive way to vectorize operations on a list of matrices.</p>

<p>The script that I am writing will be called fairly often, so even a little improvement can add up!</p>

<hr>

<p><strong>EDIT:</strong></p>

<p><strong>RE: 1) Reading data</strong></p>

<p>I forgot to mention that my data is in a special format, so I have to use a special data reading function to read the data in R. The files are in <a href=""http://en.wikipedia.org/wiki/NetCDF"" rel=""nofollow"">netcdf4</a> format, and I am using the <code>nc_open</code> function from the package <code>ncdf4</code> to access the files, and then I have to use the <code>ncvar_get</code> function to read the variable of interest. The nice thing is that the data in the files can be read from disk, and then I can read the data into memory with <code>ncvar_get</code> to do operations on them with R.</p>

<p>That being said, although I know the size of my matrices and how many of them I will have, I asked my question with a list of data because the <code>foreach</code> function that enables me to do parallel computing outputs the results from the parallel-ized loop in a list. I found that with the <code>foreach</code> function, the data reading step was about 3x faster.</p>

<p>I imagine that I can arrange the data as a 3d array afterwards, but maybe the time it takes to allocate the 3d array may take more time than it saves? I will have to try it tomorrow.</p>

<hr>

<p><strong>EDIT 2:</strong></p>

<p>Here are some of the timings I took of my script.</p>

<p>Original Script:</p>

<pre><code>[1] ""Reading data to memory""
user  system elapsed 
176.063  44.070  26.611 

[1] ""Calculating Distances""
user  system elapsed 
2.312   0.000   2.308 

[1] ""Calculating the best 333 weighted matrices""
user  system elapsed 
63.697  28.495   9.092 
</code></pre>

<p>I made the following improvements thus far: (1) Pre-allocate the list before reading data, (2) Improved the weighted matrix calculations, as per Martin Morgan's suggestion.</p>

<pre><code>[1] ""Reading data to memory""
user  system elapsed 
192.448  38.578  27.872 

[1] ""Calculating Distances""
user  system elapsed 
2.324   0.000   2.326 

[1] ""Calculating all 1000 weighted matrices""
user  system elapsed 
1.376   0.000   1.374 
</code></pre>

<p>Some notes:</p>

<p>I use 12 cores in my <code>foreach</code> loop to read in the data (<code>registerDoMC(12)</code>). The whole script takes approximately 40s / 36s to run before / after the improvements.</p>

<p>The timing for my Bottleneck #2 has improved by quite a bit. Previously, I had been computing only the top third (i.e. 333) of the weighted matrices, but now the script can just calculate all the weighted matrices in a fraction of the original time.</p>

<p>Thanks for the help, I will try tweaking my code later to see if I can change my script to work with 3D arrays instead of lists. I am going to take some time now to verify the calculations just to be sure they work!</p>
","<p>My 'low hanging fruit' (<code>scan</code>; pre-allocate and fill) seem to be not relevant, so...</p>

<p>The operations in the distance calculation sort of look vectorized enough to me. Probably you can squeeze some extra speed out of doing a single distance calculation over all your matrices, but this probably makes the code less understandable.</p>

<p>The weightedMatrix calculation looks like there is room for improvement. Let's calculate</p>

<pre><code>w &lt;- d^(-2) / cumsum(d^(-2))
</code></pre>

<p>For a weighted matrix <code>m</code> I think the relationship between successive matrices is just <code>m' = m * (1 - w[i]) + l[[i]] * w[i]</code>, so</p>

<pre><code>res &lt;- vector(""list"", length(l))
for (i in seq_along(l))
    if (i == 1L) {
        res[[i]] = l[[i]] * w[[i]]
    } else  {
        res[[i]] = res[[i - 1]] * (1 - w[[i]])  + l[[i]] * w[[i]]
    }
</code></pre>

<p>This changes the calculation of <code>res</code> from quadratic to linear. My thoughts about better than linear performance were just a (probably also misguided) hunch; I haven't pursued that.</p>

<p>Returning to pre-allocate and fill and @flodel's comment, we have</p>

<pre><code>f0 &lt;- function(n) {
    ## good: pre-allocate and fill
    l = vector(""list"", n)
    for (i in seq_along(l))
        l[[i]] = 1
    l
}

f1 &lt;- function(n) {
    ## bad: copy and append
    l = list()
    for (i in seq_len(n))
        l[[i]] = 1
    l
}
</code></pre>

<p>which produce the same result</p>

<pre><code>&gt; identical(f0(100), f1(100))
[1] TRUE
</code></pre>

<p>but with different performance</p>

<pre><code>&gt; sapply(10^(1:5), function(i) system.time(f0(i))[3])
elapsed elapsed elapsed elapsed elapsed 
  0.000   0.000   0.002   0.014   0.134 
&gt; sapply(10^(1:5), function(i) system.time(f1(i))[3])
elapsed elapsed elapsed elapsed elapsed 
  0.000   0.001   0.005   0.253  24.520 
</code></pre>

<p>Even though this does not matter for the scale of the current problem does not matter, it seems like one should adopt the better pre-allocate and fill strategy to avoid having to guess whether it's relevant or not. Better, use the <code>*apply</code> or in this case <code>replicate</code> family to avoid having to think about it</p>

<pre><code>l &lt;- replicate(1000, matrix(rnorm(277*349), nrow=277, ncol=349), simplify=FALSE)
</code></pre>
"
42622876,I'm unable to parse this news content,2,2,4,"<p>I don't know why this error comes?? I am trying to parse news content in the format like title, link, description,date &amp; save it in data frame using xmlparse function but it throws error like... </p>

<pre><code>site = ""http://www.federalreserve.gov/feeds/prates.xml""
doc &lt;- tryCatch(xmlParse(site),  error=function(e) e);      
Unknown IO errorfailed to load external entity    
""http://www.federalreserve.gov/feeds/prates.xml""
src &lt;- xpathApply(xmlRoot(doc), ""//item"") 
Error in UseMethod(""xmlRoot"") :no applicable method for 'xmlRoot'applied to an object of class ""c('XMLParserErrorList', 'simpleError', 'error',     
'condition')""
for (i in 1:length(src)) {
if (i==1) {
       foo&lt;-xmlSApply(src[[i]], xmlValue)
       temp&lt;-data.frame(t(foo), stringsAsFactors=FALSE)
       DATA=data.frame(title=temp$title,link=temp$link,description=temp$description,pubDate=temp$pubDate)
     }
   else {
       foo&lt;-xmlSApply(src[[i]], xmlValue)
       temp&lt;-data.frame(t(foo), stringsAsFactors=FALSE)
       temp1=data.frame(title=temp$title,link=temp$link,description=temp$description,pubDate=temp$pubDate)
       DATA&lt;-rbind(DATA, temp1)
     }
 }
 Error: object 'src' not found
</code></pre>
","<p>That error means the URL redirects to HTTPS as mentioned in my comment...</p>

<pre><code>site         &lt;- ""http://www.federalreserve.gov/feeds/prates.xml""
correct_site &lt;- ""https://www.federalreserve.gov/feeds/prates.xml""

curlGetHeaders(site)
 [1] ""HTTP/1.1 301 Moved Permanently\r\n""                                                                                                        
 [2] ""Location: https://www.federalreserve.gov/feeds/prates.xml\r\n""                                                                             
 ...    

xmlParse(site)
Unknown IO errorfailed to load external entity ""http://www.federalreserve.gov/feeds/prates.xml""
</code></pre>

<p><code>xmlParse</code> cannot read from https, so use readLines (ignore the warning) or the <code>xml2</code> package or many other options to read from secure HTTP.</p>

<pre><code>xmlParse( correct_site)
Error: XML content does not seem to be XML: 'https://www.federalreserve.gov/feeds/prates.xml'

x &lt;- readLines(correct_site)
Warning message:
In readLines(correct_site) :
  incomplete final line found on 'https://www.federalreserve.gov/feeds/prates.xml'


xmlParse(x)
&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;rdf:RDF xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"" xmlns=""http://purl.org/rss/1.0/"" xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:dcterms=""http://purl.org/dc/terms/"" xmlns:cb=""http://www.cbwiki.net/wiki/index.php/Specification_1.1"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://www.w3.org/1999/02/22-rdf-syntax-ns# rdf.xsd""&gt;
  &lt;channel rdf:about=""http://www.federalreserve.gov/feeds/""&gt;
    &lt;title&gt;FRB: DDP: Policy Rates&lt;/title&gt;
...

library(xml2)
read_xml( correct_site)

{xml_document}
&lt;RDF schemaLocation=""http://www.w3.org/1999/02/22-rdf-syntax-ns# rdf.xsd"" xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"" xmlns=""http://purl.org/rss/1.0/"" xmlns:dc=""http://purl.org/dc/elements/1.1/"" xmlns:dcterms=""http://purl.org/dc/terms/"" xmlns:cb=""http://www.cbwiki.net/wiki/index.php/Specification_1.1"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&gt;
[1] &lt;channel rdf:about=""http://www.federalreserve.gov/feeds/""&gt;\n  &lt;title&gt;FRB: DDP: Policy Rates&lt;/title&gt;\n   ...
[2] &lt;item rdf:about=""http://www.federalreserve.gov/feeds/PRATES.html#1765""&gt;\n  &lt;title&gt;Change to the Publica ...
[3] &lt;item rdf:about=""http://www.federalreserve.gov/feeds/PRATES.html#953""&gt;\n  &lt;title&gt;Change to the Payment  .
</code></pre>
"
